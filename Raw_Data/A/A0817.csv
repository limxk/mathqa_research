parentid,id,body
"2376219","2376242","<p>Did you listen when we said in your previous questions that $K/F$ is Galois iff $F = K^G$ where $G= Gal(K/F)$ is a finite group of automorphisms of $K$ ? </p>

<p>For $\alpha \in K$, it means that the polynomial with distinct roots $f(x) = \prod_{\beta \in G (\alpha)} (x-\beta) \in K[x]$ has coefficients in the fixed field, ie. $f \in F[x]$, therefore it is the minimal polynomial of $\alpha$. </p>

<p>$G( \alpha)= \{ \beta \in K, \exists \sigma \in G, \sigma(\alpha) = \beta\}$.</p>

<p>$K^G = \{ \alpha \in K, \forall \sigma \in G, \sigma(\alpha) = \alpha\}$.</p>

<p>$f(x) =\prod_{\beta \in G (\alpha)} (x-\beta)= \sum_{n=0}^d c_n x^n, \quad \sum_{n=0}^d \sigma(c_n) x^n = \prod_{\beta \in G (\alpha)} (x-\sigma(\beta))= f(x)$.</p>

<p>$f \in F[x] \land f(\alpha) = 0 \land \sigma \in Gal(K/F) \implies f(\sigma(\alpha)) =\sigma(f(\alpha)) = 0$.</p>

<hr>

<ul>
<li><p>Show the main statement when $K = F(\alpha)$ and use induction. It is equivalent to $|Gal(K/F)| = [K:F]$.</p></li>
<li><p>Because $x^p-1 \equiv (x-1)^p \bmod p$ it means $\zeta_p$ doesn't exist in $\mathbb{F}_p$ and $x^p-t^p$ is the non-separable minimal polynomial of $t$ over $\mathbb{F}_p(t^p)$ so that $\mathbb{F}_p(t)/\mathbb{F}_p(t^p)$ is a non-separable finite extension. </p></li>
<li><p>If $E=F(\alpha)$ where the minimal polynomial $f$ of $\alpha$ is separable then its normal closure (the splitting field of $f$) is Galois. </p></li>
</ul>
"
"2376224","2376239","<p>Basically $S$ is a set of commuting operators and $\vert s\rangle$ is a common eigenvector of elements of this set.  </p>

<p>To generate the two vectors you have, one might as well assume that elements of the set are diagonal, so pick them to be
$$
s_1=\hat 1\, , \qquad s_2=\sigma_z
$$
with $\sigma_z$ the Pauli matrix.  The common eigenvectors are $[1,0]^T$ and $[0,1]^T$.</p>

<p>To get $\psi=(\alpha,\beta)^T$ note first that $\vert\alpha\vert^2 +\vert \beta\vert^2=1$, construct the matrix
$$
U=\left(\begin{array}{cc}
\alpha&amp;-\beta^* \\ \beta&amp;\alpha^*\end{array}\right)\, .
$$
and note that $U[1,0]^T=(\alpha,\beta)$ and $U$ is unitary.  Thus to get the set that will have $\psi$ as eigenvectors you need to conjugate $\hat 1$ and $\sigma_z$ by $U$, i.e. your new set is $\{\hat 1,U\sigma_zU^{-1}\}$.  </p>

<p>If you need elements in $S$ to be unitary you can just use $e^{-i\alpha \sigma_z}$ instead of $\sigma_z$.  The key point is that the eigenvalues of your second operator should be distinct.</p>
"
"2376243","2376254","<p>If you have a collection of infinitely-many elements with only finitely-many distinct ones, there must be, for cardinality reasons, at least one, say $s_k$ that repeats infinitely often. Then the subsequence given by $s_k,s_k,....,s_k,..$ is constant and therefore convergent.</p>
"
"2376244","2376247","<p>You can do it by using the Pythagorean identity: $\sin^2 x+\cos^2 x =1$. This can be rewritten two different ways:</p>

<p>$$\sin^2 x = 1- \cos^2 x$$</p>

<p>and </p>

<p>$$\cos^2 x = 1 - \sin^2 x$$</p>

<p>Use either of these formulas to replace the $\sin^2 x$, or the $\cos^2 x$, on the right side of your identity. That will give you the other two forms.</p>
"
"2376257","2376273","<p>This is a partial answer.  We'll reduce the question to one of representation theory (of Lie groups), and give an answer in two ""extreme"" cases.</p>

<p>Suppose $\nabla$ is the Levi-Civita connection of some Riemannian metric $g_1$.  Since $\nabla$ is torsion-free, we know that $\nabla$ will be the Levi-Civita connection of a metric $g_2$ if and only if $\nabla g_2 = 0$.  This means that we have to understand which (positive-definite) symmetric $2$-tensor fields are $\nabla$-parallel.  Understanding which tensor fields are $\nabla$-parallel can be accomplished via:</p>

<blockquote>
  <p><strong>The Holonomy Principle:</strong> Let $\nabla$ be a connection on a connected smooth manifold $M$.  Let $\text{Hol}_x \leq \text{GL}(T_xM)$ denote the holonomy group (really, holonomy representation) of $\nabla$ at $x \in M$.</p>
  
  <p>(a) If $T \in \Gamma(TM^{\otimes r} \otimes T^*M^{\otimes s})$ is a parallel tensor field on $M$, then $T|_x$ is fixed by the $\text{Hol}_x$-action on $T_xM^{\otimes r} \otimes T_x^*M^{\otimes s}$.</p>
  
  <p>(b) Conversely: If $T_0$ is a tensor at $x$ fixed by the $\text{Hol}_x$-action on $T_xM^{\otimes r} \otimes T_x^*M^{\otimes s}$, then there exists a unique parallel tensor field $T$ on $M$ with $T|_x = T_0$.</p>
</blockquote>

<p>Since our connection $\nabla$ is the Levi-Civita connection of some (let's say Riemannian) metric $g_1$, we have $\text{Hol}_x \leq \text{SO}(T_xM, g_1) \cong \text{SO}(n)$.  The question is now: What is the space of (positive-definite) symmetric $2$-tensors at $x$ which are fixed by the $\text{Hol}_x$-action on $\text{Sym}^2(T^*_xM) \subset T_x^*M^{\otimes 2}$.  This is a question of representation theory.</p>

<blockquote>
  <p><strong>Example:</strong> (The trivial example) Suppose $\nabla$ is the Levi-Civita connection of a flat metric $g_1$, and suppose $M$ is connected and simply-connected.  Then $\text{Hol}_x = 0$ is the identity group, so every element of $\text{Sym}^2(T_x^*M)$ is fixed.</p>
  
  <p>Concretely: If $g_0$ is any (positive-definite) symmetric $2$-tensor at $x$, then (by the Holonomy Principle) we can extend $g_0$ uniquely to a $\nabla$-parallel tensor field $g$ on all of $M$.  The upshot is that, for the sort of connections $\nabla$ in this example, we essentially have an $\binom{n+1}{2}$-dimensional space of compatible metrics.</p>
</blockquote>

<p>Note that the dimension $\binom{n+1}{2}$ is the largest possible.  For the Levi-Civita connection of a ""generic"" Riemannian metric $g_1$, the holonomy group will be all of $\text{SO}(n)$, and the space of compatible metrics will be $1$-dimensional.</p>

<p>I do not know about the intermediate cases -- i.e., when $\nabla$ is not generic and not flat.  (My representation theory needs some work!)</p>
"
"2376260","2376329","<p>We can see that $\sum_{n=1}^{\infty} \frac{x^n}{n^2}$ is not convergent for $x &gt; 1$, which is clear considering that $\lim_{n\to \infty} \frac{x^n}{n^2} = \infty$ for $x &gt; 1$ (if this is not obvious, use l'HÃ´spital's rule twice). Therefore, we consider only $x\in (0, 1]$. Notice that $$\sum_{n=1}^{\infty} \frac{x^{n-1}}{n} = \frac{1}{x}\sum_{n=1}^{\infty} \frac{x^n}{n} = -\frac{\log(1-x)}{x}$$ for $x\in (0, 1)$. Convergence is readily apparent by the ratio test, and then we have uniform convergence on compact sets by Dini's theorem (or by the Weierstrass M-test, but Dini's theorem is easy here). For $x = 1$, we get the harmonic series, which diverges. Note that $g(x) = -\frac{\log(1-x)}{x}$ is $C^{\infty}$-smooth on $(0, 1)$.</p>
"
"2376263","2376268","<p>The key step is as follows: \begin{align*} \lvert x_m-x_n\rvert &amp;= \left\lvert \sum_{k=0}^{m-n-1} x_{m-k}-x_{m-k-1}\right\rvert \\ &amp;\leq \sum_{k=0}^{m-n-1} \lvert x_{m-k}-x_{m-k-1}\rvert \end{align*}</p>

<p>The inequality here follows from the triangle inequality (i.e. $\left\lvert \sum_{k=1}^n a_k\right\rvert\leq \sum_{k=1}^n \lvert a_k\rvert$).</p>
"
"2376276","2376283","<p>$1 &gt; p-1 &gt; 0$ is the same as $2 &gt; p &gt; 1$ for which this integral $\int t^{-p}dt$ converges. In fact it converges for all $p &gt; 1$ as the solution states.</p>

<p>In response to your comment: $t^n \to \infty$ as $t\to\infty$ if $n&gt;0$, which includes things such as $n=\frac 1 2 \text{ or } 0.1$ etc.</p>
"
"2376279","2376320","<p>Correct! Those 3 axioms plus modus ponens form a <em>complete</em> proof system: every valid argument involving $\neg$ and $\rightarrow$ can be proven to be valid using just those 3 axioms plus modus ponens.  And since $\{ \neg, \rightarrow \}$ is an expressively complete set of operators, any propositional logic operator can be defined in terms of them, and hence any argument as well and again: if it is valid, it can be proven to be valid within this very simple system (the proofs themselves, of course, are often far from simple!  Though there <em>are</em> algorithms to automatically create a proof for any valid argument)</p>

<p>You also asked about transforming axioms into inference rules. I am not exactly sure what you mean, but obviously each axiom can be treated as an inference rule: from nothing, you can infer a statement of the form as indicated. </p>

<p>Oh, and please note: the letters $p$, $q$, and $r$ are sentence <em>variables</em>, meaning that you can fill in <em>any</em> sentence for them. Thus, for example:</p>

<p>$$(A \rightarrow \neg B) \rightarrow (\neg \neg C \rightarrow (A \rightarrow \neg B))$$</p>

<p>would be an instance of axiom 1, where we use $A \rightarrow \neg B$ for $p$ and $\neg \neg C$ for $q$</p>
"
"2376285","2376290","<p>You can have a look at the <a href=""https://www.oeis.org"" rel=""nofollow noreferrer""> On-Line Encyclopedia of Integer Sequences </a> which stores a lot of integer sequences with interesting comments, formulas and references.</p>

<p>For this sequence you will find <a href=""http://oeis.org/search?q=1%2C1%2C2%2C3%2C3%2C4%2C5%2C6%2C6%2C7%2C8%2C9%2C10%2C10%2C11%2C12%2C13%2C14&amp;sort=&amp;language=english&amp;go=Search"" rel=""nofollow noreferrer""> 3 answers </a>. I won't reproduce the formulas they give here but the simplest one is maybe to see this sequence as ""the sequence of nonnegative integers with the triangular numbers repeated"".</p>
"
"2376294","2376302","<p>Hint: &nbsp;let $a_n=2^{n}x_n\,$, then the recurrence becomes:</p>

<p>$$
2^{n}x_{n} = 11 \cdot 2^{n-1}x_{n-1} - 40 \cdot 2^{n-2}x_{n-2} + 48 \cdot 2^{n-3}x_{n-3} + n\cdot 2^n \\ \iff\quad 2 x_n = 11 x_{n-1}-20 x_{n-2}+12 x_{n-3}+2n 
$$</p>

<p>Let $y_n=x_n-x_{n-1}\,$, then subtracting two consecutive relations in $x_n$ gives:</p>

<p>$$
2y_n = 11 y_{n-1} -20y_{n-2}+12y_{n-3}+2 
$$</p>

<p>The latter is a standard linear recurrence with constant coefficients and ""nice"" roots for the characteristic polynomial. Solve for $y_n$, then calculate $x_n$, then $a_n$.</p>
"
"2376301","2376362","<p>You get a power series $$f(z)=\sum_k (-1)^k c_kz^k $$ (here $z=x^4$, $u(x)=f(x^4)$) with $c_0=1$, $c_k=q_1q_2Â·â¦Â·q_k$, $1&gt;q_{j+1}&gt;q_j\to0$ for $j\to\infty$.</p>

<p>This means that for any $N$ and $R_{N+1}=q_N^{-1}$ the partial sequence of $c_kz^k$ with $k&gt;N$ is monotonically decreasing for $0\le z&lt;R_{N+1}$. By the <a href=""https://en.wikipedia.org/wiki/Alternating_series_test"" rel=""nofollow noreferrer"">Leibniz test</a> and its error bounds, the power series converges for $0\le z&lt;R_{N+1}$ and the value of the power series is contained between the partial sums $s_{N-1}(z)$ and $s_{N}(z)$ as
\begin{align}
(-1)^N\left(f(z)-s_{N-1}(z)\right)
&amp;=\sum_{k=0}^\infty (-1)^kc_{N+k}z^{N+k} \\
&amp;=\sum_{k=0}^\infty c_{N+2j}z^{N+2j}\left(1-q_{N+2j+1}z\right) &amp;&amp;(\ &gt;0\ \ )\\
&amp;=c_{N}z^N-\sum_{k=0}^\infty c_{N+2j+1}z^{N+2j+1}\left(1-q_{N+2j+2}z\right)&amp;&amp;(\ &lt;c_{N}z^N\ ).
\end{align}</p>

<hr>

<p>In my answer <a href=""https://math.stackexchange.com/a/2348258/115115"">https://math.stackexchange.com/a/2348258/115115</a> to your previous question there are more details on this problem. For $q_k=\frac1{4k(4k-1)}$, $R=\sqrt7$ (or $N=1$, $|x|&lt;\sqrt[4\,]{R_2}=\sqrt[4\,]{56}$) you get 
$$
1-\frac{x^4}{3Â·4}=s_1(x^4)&lt; u(x)&lt;s_2(x^4)= 1-\frac{x^4}{3Â·4}\left(1-\frac{x^4}{7Â·8}\right)&lt;s_0(x^4)=1
$$
and the root is contained between the roots of the polynomials on both sides,
$$
2Â·\sqrt[4\,]{1-\frac14}&lt;x_0&lt;2Â·\sqrt[4\,]{1+\frac1{2(3+\sqrt7)}}.
$$</p>
"
"2376306","2376346","<p>As also noted by @bof, this is known as <a href=""https://en.wikipedia.org/wiki/Dobi%C5%84ski%27s_formula"" rel=""noreferrer"">DobiÅski's formula</a>, i.e. for $x\in \mathbb{N}$, $$B_n = \frac{1}{e}\sum_{k=0}^{\infty} \frac{k^x}{k!}$$ where $B_n$ is the $n$th <a href=""https://en.wikipedia.org/wiki/Bell_number"" rel=""noreferrer"">Bell number</a>. These numbers are catalogued as <a href=""http://oeis.org/A000110"" rel=""noreferrer"">A000110</a> in the OEIS. However, for $x\notin \mathbb{N}$, the series $\sum_{k=0}^{\infty} \frac{k^x}{k!}$ is unlikely to have a closed form, although we can clearly see that the series will converge for all $x\in \mathbb{R}$ using the ratio test: $$\lim_{k\to \infty} \left\lvert \frac{k!(k+1)^x}{(k+1)!k^x}\right\rvert = \lim_{k\to \infty} \frac{(k+1)^{x-1}}{k^x} = \lim_{k\to \infty} \frac{1}{k+1}\left(\frac{k+1}{k}\right)^x = 0$$</p>
"
"2376309","2376700","<p>The scaled Bring quintic
$$x^5-\frac5nx+\frac4n=0\tag1$$
is solved by,
$$x =\frac45\,_4F_3\left(\frac{1}{5},\frac{2}{5},\frac{3}{5},\frac{4}{5};\frac{1}{2},\frac{3}{4},\frac{5}{4};n\right)$$
while the decic,
$$y^4(n y^2-5) (n y^2+5)^2 + \frac{16^2}n = 16 y^3 (2 n y^2+5)\tag2$$
is solved by,
$$y=\left(\frac45\right)^2\,\left|\,_4F_3\left(\frac{1}{5},\frac{2}{5},\frac{3}{5},\frac{4}{5};\frac{1}{2},\frac{3}{4},\frac{5}{4};n\right)\right|^2$$
with <em>absolute value</em> $|u|$. Eq $(2)$ then explains the $80$-deg since the argument $n=\sqrt{\phi}$ with <em>golden ratio</em>  is a quartic root so $10\times4\times2 = 80$. Eliminating $n$ between $(1)$ and $(2)$, easily done using <em>Mathematica</em>, we get a solvable polynomial relationship for $x,y$, implying if one is a radical (or not), then so is the other.</p>

<p>However, the <strong><em>complete</em></strong> radical parameterization to $(1)$ is known and given by the <em><a href=""http://mathworld.wolfram.com/QuinticEquation.html"" rel=""nofollow noreferrer"">Blair-Spearman quintic</a></em>,</p>

<p>$$x^5+\frac{5u^4(4v+3)}{v^2+1}x+\frac{4u^5(2v+1)(4v+3)}{v^2+1}=0\tag3$$</p>

<p>Equating coefficients between $(1)$ and $(3)$, then eliminating $v$, we get the sextic in $u$,</p>

<p>$$n=\frac{5u^2+2u+1}{4(2-u)u^5}\tag4$$</p>

<p>Thus, for any radical $n$ such that $u$ is radical, then $x$ is also a radical.</p>

<blockquote>
  <p>Example. If $n=81$, then $u=1/3$ so we know,
  $$x =\frac45\,_4F_3\left(\frac{1}{5},\frac{2}{5},\frac{3}{5},\frac{4}{5};\frac{1}{2},\frac{3}{4},\frac{5}{4};81\right)\approx 0.42429 - 0.239932 i$$
  is a radical quintic root of $x^5-\frac5{81}x+\frac4{81}=0$.</p>
</blockquote>

<p>The problem then for Reshetnikov's $80$-deg is to establish if $u$ is a radical in,</p>

<p>$$\sqrt{\phi}=\frac{5u^2+2u+1}{4(2-u)u^5}\tag5$$</p>

<p>But eq $(5)$ is a $24$-deg with integer coefficients and <em>Magma</em> says this has permutation group $G$ of order $207360000 = 2^{12} \cdot 3^4 \cdot 5^4$. Therefore it is not solvable, hence the $80$-deg root is not a radical.</p>
"
"2376310","2376314","<p>For any function $f$ from the set of primes to the set $(\mathbb N-\{0\})\cup\{\infty\}$, there is a subgroup $S_f$ of the desired sort consisting of those rational numbers expressible as reduced fractions $\frac mn$ such that, in the prime factorization of the denominator $n$, each prime $p$ occurs fewer than $f(p)$ times.  If $f$ is the constant function $1$, then $S_f=\mathbb Z$. If $f$ is the constant function $\infty$, then $S_f=\mathbb Q$.  In-between choices of $f$ give you in-between groups.  (For eample, if $f$ is the constant function $2$, then $S_f$ is the group of rational numbers with square-free denominators.)  I believe that these subgroups $S_f$ are all of the subgroups between $\mathbb Z$ and $\mathbb Q$, but I don't have a proof at the moment.</p>
"
"2376311","2376331","<p>Here's a common technique. First find any unbiased estimator for $p$. In this case, it's easy to see that, </p>

<p>$$ T(\textbf{X}) = \begin{cases}1 &amp; X_1 = 1\\
0 &amp; otherwise \end{cases} $$</p>

<p>is unbiased for $p$. By conditioning on the complete sufficient statistic, $\mathbb{E}[T(\textbf{X}) | S]$ is unbiased and must be the UMVUE by Lehmann-Scheffe. Try to find a closed form for $\mathbb{E}[T(\textbf{X}) | S]$. To get you started,</p>

<p>$$ \mathbb{E}[T(\textbf{X}) | S = s] = \mathbb{P}(X_1 = 1 | \sum\limits_{i=1}^nX_i = s) $$</p>
"
"2376323","2376338","<p>Clearly, $\sum_{r=1}^{\infty} (-1)^{r-1}r^3$ does not converge, as $\lim_{r\to \infty} (-1)^{r-1}r^3$ does not exist (and therefore does not equal $0$). Also, note that \begin{align*} \sum_{r=1}^{2n+2} (-1)^{r-1}r^3 &amp;= (-1)^{2n}(2n+1)^3+(-1)^{2n+1}(2n+2)^3+\sum_{r=1}^{2n} (-1)^{r-1}r^3 \\ &amp;= (-12n^2-18n-7)+\sum_{r=1}^{2n} (-1)^{r-1}r^3 \end{align*} Therefore, if we let $s_n := \sum_{r=1}^{2n} (-1)^{r-1}r^3$, we have $s_{n+1}-s_n = -12n^2-18n-7$ with initial condition $s_0 = 0$. The <a href=""https://en.wikipedia.org/wiki/Recurrence_relation#Solving_non-homogeneous_linear_recurrence_relations_with_constant_coefficients"" rel=""nofollow noreferrer"">method of symbolic differentiation</a> tells us that the solution to this will look like $s_n = c_3n^3+c_2n^2+c_1n+c_0$. Plugging in and solving the resulting linear system will give us $c_3 = -4$, $c_2 = -3$, and $c_1 = 0$, and the initial condition will give us $c_0 = 0$. Thus, $$\sum_{r=1}^{2n} (-1)^{r-1}r^3 = -4n^3-3n^2$$</p>
"
"2376335","2377100","<p>I'll assume this is approximating $y'(t) = f(t,y)$.  The method is 
$$
2y_{n+3} =-3 y_{n+2} + 6 y_{n+1} - y_n + 6 h f (t_{n+2}, y_{n+2})
$$
To find the (local) error, taylor expand each term:
\begin{align*}
2 y_{n+3} &amp;= 2\left( y_n + 3h y_n ' + \frac{(3h)^2}{2} y_n '' + \frac{(3h)^3}{6} y_n ''' + \frac{(3h)^4}{24} +O(h^5)\right) \\
-3 y_{n+2} &amp;= -3 \left( y_n + 2h y_n ' + \frac{(2h)^2}{2} y_n '' + \frac{(2h)^3}{6} y_n ''' + \frac{(2h)^4}{24} + O(h^5) \right) \\
6 y_{n+1} &amp;= 6 \left( y_n + h y_n ' + \frac{h^2}{2} y_n '' + \frac{h^3}{6} y_n ''' + \frac{h^4}{24} + O(h^5) \right) \\
-y_n &amp;= -y_n \\
6 h f(t_{n+2}, y_{n+2}) &amp;= 6h \left( y_n' + 2h y_n '' + \frac{(2h)^2}{2} y_n ''' + \frac{(2h)^3}{6} y_n '''' + O(h^4) \right)
\end{align*}</p>

<p>and matching the coefficients of the scheme reveals truncation error $O(h^4)$.</p>

<p>To check if it converges, we look to the root condition (i.e. apply to the test equation $y'=f(t,y)=0$, and see if the scheme is zero-stable.
$$
2 y_{n+3} = -3 y_{n+2} + 6 y_{n+1} - y_n \implies 2 r^3 = -3 r^2 + 6r -1
$$</p>

<p>One of the roots is larger than 1 in absolute value, so by Dahlquist Equivalence Theorem, the scheme does not converge (since it doesn't satisfy the root condition)</p>
"
"2376340","2376343","<p>Yes it is possible to find the inverse of a Taylor Series. You can use series reversion. It can be used to find the inverse of Taylor Series given the forward function. Here is a URL: <a href=""http://mathworld.wolfram.com/SeriesReversion.html"" rel=""nofollow noreferrer"">http://mathworld.wolfram.com/SeriesReversion.html</a></p>
"
"2376341","2376369","<p>Yes, your approach is correct. Note that $|A_i|=4\cdot 3!=24$, $|A_iA_j|=6$, $|A_iA_j A_k|=2$ and $|A_1A_2A_3A_4|=1$. Hence
$|A_{1} \cup A_{2} \cup A_{3} \cup A_{4}|$ is equal to
$$\binom{4}{1}\cdot |A_i|-\binom{4}{2}\cdot |A_iA_j|+\binom{4}{3}\cdot |A_iA_j A_k|-\binom{4}{4}\cdot |A_1A_2A_3A_4|.$$
which implies that the number of those permutations is $|U|-|A_{1} \cup A_{2} \cup A_{3} \cup A_{4}|$:
$$5!-(4\cdot 24-6\cdot 6+4\cdot 2-1)=120-(96-36+8-1)=53.$$</p>

<p>Alternative approach. Let $p(n)$ be the number of permutations of $[1,\dots,n]$ having no substring $[k,k+1]$ with $1\leq k\leq n-1$. Then  $p(1) = 1$, 
$p(2) =1$ and, for $n\geq 2$ it satisfies the recurrence 
$$p(n+1) = n p(n) + (n-1)p(n-1).$$ 
Hence $p(3)=2p(2)+p(1)=3$, $p(4)=3p(3)+2p(2)=11$ and $$p(5)=4p(4)+3p(3)=44+9=53.$$</p>
"
"2376357","2376361","<p>Suppose $e_1$, $e_2,e_3,\ldots$ is a complete orthonormal sequence in a Hilbert
space. Let $f\in H$. When is $f$, $e_2,e_3,\ldots$ also a complete orthonormal sequence?</p>

<p>We can write $f=\sum_{n}a_n e_n$ for some square-summable sequence $(a_n)$.
If $a_1=0$ then $f$, $e_2,e_3,\ldots$ isn't a complete orthonormal sequence, as its closed span avoids $e_1$. But if $a_1\ne0$
then it is a complete orthonormal sequence, as its closed span
contains $a_1e_1=f-\sum_{n=2}^\infty a_ne_n$.</p>
"
"2376359","2376390","<p>Constraints :</p>

<p>(1) going right or up and starting lower left and finishing upper right.</p>

<p>(2) going through the lower left ""$\dots$"".</p>

<p>(3) going through the upper right ""$\vdots$"".</p>

<p>Let :</p>

<p>$X$ = set of paths verifying (1).</p>

<p>$A$ = set of paths verifying (1) and (2). ($A\subset X$)</p>

<p>$B$ = set of paths verifying (1) and (3). ($B\subset X$)</p>

<p>$C$ = set of paths verifying (1) but not (2) and not (3). ($C = (A\cup B)^c = A^c \cap B^c \subset X$)</p>

<p>Then</p>

<p>$|C| = (A\cup B)^c \\= |X| - |A\cup B| \\= |X| - (|A| + |B| - |A\cap B|) \\= |X| - |A| - |B| + |A\cap B|$</p>

<p>Now we need to count. A path can be described as a word made out of the letters $u$ (up) and $r$ (right).</p>

<p>For X we have 10 u's and 10 r's for a total of 20 letters. So $|X| = 20!/(10! 10!)$.</p>

<p>Then $|A| = (9!/(4!5!))(10!/(5!5!))$</p>

<p>Then $|B| = (14!/(7!7!))(5!/(3!2!))$</p>

<p>Then $|A\cap B| = (9!/(4!5!))(4!/(2!2!))(5!/(3!2!))$</p>

<p>So I get </p>

<p>$|C| = 20!/(10! 10!) \\ -(9!/(4!5!))(10!/(5!5!)) \\- (14!/(7!7!))(5!/(3!2!)) \\+ (9!/(4!5!))(4!/(2!2!))(5!/(3!2!))$</p>

<p>(sorry for the typing mess)</p>

<p>Yes, I believe your answer is good.</p>
"
"2376371","2376378","<p><strong>Approach A.</strong></p>

<p>With (1) and (2) you can use induction.</p>

<p><strong>Approach B.</strong></p>

<p>Without (1) and (2) consider for a subinterval $I$ of a partition,  </p>

<p>$$\sup_{x,y \in I}\left|\sum_{j=1}^n c_j f_j(x) -  \sum_{j=1}^n c_j f_j(y)\right| \leqslant \sup_{x,y \in I}\sum_{j=1}^n |c_j| |f_j(x) - f_j(y)| \\ \leqslant \sum_{j=1}^n |c_j|(  \sup_{x,y \in I}|f_j(x) - f_j(y)| )$$</p>

<p>Recall that</p>

<p>$$U(P,f_j) - L(P,f_j) = \sum_{k=1}^n\sup_{x,y \in [x_{k-1},x_k]}|f_j(x) - f_j(y)| (x_k - x_{k-1}) $$</p>

<p>Now what can you say about $U(P, \sum_j c_jf) - L(P, \sum_j c_jf_j)$?</p>

<p><strong>Approach C.</strong></p>

<p>If you must use Riemann sums, note that</p>

<p>$$S(P, \sum_j c_j f_j) = \sum_{k=1}^m \sum_{j=1}^n c_j f_j(\xi_k)(x_k - x_{k-1})= \sum_{j=1}^n c_j\sum_{k=1}^mf_j(\xi_k)(x_k - x_{k-1})\\ = \sum_{j=1}^n c_j S(P, f_j)$$</p>

<p>and</p>

<p>$$\left|S(P, \sum_jc_j f_j) - \sum_{j=1}^n c_j \int_a^b f_j\right| \leqslant  \sum_{j=1}^n |c_j| \,\left|\, S(P, f_j) - \int_a^b f_j\right| $$</p>
"
"2376374","2376376","<p>This is just unique factorisation in $k[x,y]$. The elements $f=ax+by$ and $g=cx+dy$
are irreducible and non-associate as long as $ad-bc\ne0$. Therefore if
$f$ and $g$ both divide a polynomial $h$, then $fg$ also divides $h$.</p>
"
"2376377","2376564","<p>$f(x)=-x+2+\sqrt{3 x^2+4 x+5}$</p>

<p>has a minimum at $x=0.115$</p>
"
"2376379","2376447","<p>Suppose you want to find the opposite: the number of combinations that have at least one digit that occurs at least three times. Let $p_i$ be the set of 5-digit combinations that satisfy the property that the digit $i$ is repeated at least thrice. You can see that to find the opposite, you need to find $$|p_1 \cup p_2 \cup p_3 \cup p_4 \cup p_5|.$$ You can use the inclusion-exclusion principal on this. It seems intimidating, but notice that it's impossible for two or more distinct properties to occur at the same time (do you see why?) This will cut down on the computations significantly.</p>

<p>Once you find that, the answer you seek will be $$|S| -| p_1 \cup p_2 \cup p_3 \cup p_4 \cup p_5|,$$</p>

<p>where $S$ is the total amount of 5 digit combinations with no restriction.</p>
"
"2376381","2376473","<p>As iterated integrals it is abundantly clear that</p>

<p>$$\int_0^1 \left(\int_0^x f(x,y) \, dy \right)\, dx \neq \int_0^1 \left(\int_0^1 f(x,y) \, dy \right)\, dx, $$</p>

<p>the left being the volume of the prism with triangular base and the right being the larger volume of the prism with the square base.</p>

<p>If you are bringing up Fubini here, it would appear you are trying to reconcile the iterated integrals with the Riemann integral over a general region in $\mathbb{R}^2$. The basic definition of the Riemann integral is restricted to rectangular regions. It is extended for our triangular base $T \subset [0,1]^2$, say,  using the indicator function $\chi_T$ according to </p>

<p>$$\int_T f = \int_{[0,1]^2} f \chi_T$$</p>

<p>Now Fubini's theorem implies</p>

<p>$$\int_T f = \int_0^1 \left(\int_0^1 f(x,y) \chi_T\, dy \right)\, dx = \int_0^1 \left(\int_0^x f(x,y) \, dy \right)\, dx $$</p>
"
"2376384","2376504","<p>Let $z\in Z$, with $\lVert z\rVert=1$ for simplicity. Pick any $\varepsilon&gt;0$,
and let $w_0\in W$ with $\lVert w_0\rVert&lt;(1+\varepsilon)$ and $\lVert Qw_0-z_0\rVert&lt;\varepsilon$. Let $z_1=z_0-Qw_0$, so $\lVert z_1\rVert&lt;\varepsilon$.</p>

<p>Now, for $k=1$, $2$, â¦, assume that $z_k$ is given, and pick $w_k\in W$ with $\lVert w_k\rVert&lt;2\lVert z_k\rVert$ and $\lVert Qw_k-z_k\rVert&lt;2^{-k}\varepsilon$. Let $z_{k+1}=z_k-Qw_k$.</p>

<p>Note that $\lVert z_k\rVert&lt;2^{1-k}\varepsilon$, and so $\lVert w_k\rVert&lt;2^{2-k}\varepsilon$, for all $k\ge1$. We find therefore</p>

<p>$$z=z_0=Qw_0+z_1=Qw_0+Qw_1+z_2=\ldots=Q\sum_{k=0}^\infty w_k $$
where
$$\Bigl\lVert\sum_{k=0}^\infty w_k\rVert\le \sum_{k=0}^\infty\lVert w_k\rVert
 &lt; (1+\varepsilon)+\sum_{k=1}^\infty 2^{2-k}\varepsilon=1+5\varepsilon.$$
It follows that $Q$ is surjective, and $\lVert z\rVert\ge\inf\{\lVert w\rVert\in W\colon Qw=z\}$ for all $z\in Z$. The opposite inequality follows from $\lVert Q\rVert=1$.</p>
"
"2376388","2376834","<p>Define</p>

<p>$$h_n(y) = f(\lfloor ng(y)\rfloor/n, y).$$</p>

<p>Then $h_n(y) \to h(y)$ everywhere. (Like your proof that $f$ is measurable, this follows from the hypothesis that each $f^y$ is continuous.) So it suffices to show each $h_n$ is Lebesgue measurable. Now observe</p>

<p>$$\tag1 h_n(y)=f(\lfloor ng(y)\rfloor/n, y) = \sum_{m\in \mathbb Z}f(m/n,y)\chi _{g^{-1}([m/n,(m+1)/n)}\,(y).$$</p>

<p>Each $f(m/n,y)$ is Lebesgue measurable by hypothesis, as is each $\chi _{g^{-1}([m/n,(m+1)/n)}$ because $g$ is Lebesgue measurable. Thus each summand in $(1)$ is Lebesgue measurable, hence so is the sum. </p>
"
"2376389","2376392","<p>$$ 2x+5 =x-3$$ is the same equation as $$ -(2x+5)=-(x-3)$$</p>

<p>and $$(2x+5)=-(x-3) $$ is the same equation as $$-(2x+5)=x-3. $$</p>

<p>In each case, one is obtained from the other by multiplying both sides by $-1.$</p>
"
"2376391","2376487","<p>We prove that any odd prime works if the product is allowed to be prime up to a sign, but that the product being positive (as prime numbers are) ensures the achievable prime values are those of the form $4k+1$ as you conjectured.</p>

<p>Setting three factors to $\pm 1$ and the fourth to $\pm p$ with $p\in\mathbb{P}$ is equivalent to solving $M\mathbf{x}=\mathbf{y}$, where the vector $\mathbf{y}$ can take one of $16$ values (we can place $p$ in one of four positions and apply one of two signs freely to two entries so the other sign is then determined by the requirement that the entries' product is positive so as to be prime; we don't consider cases where an entry is $-p$, since if this works we can multiple the whole vector by $-1$). The matrix $M$ satisfies $M^{-1}=\frac{1}{4}M$, so $\mathbb{x}=\frac{1}{4}M\mathbb{y}$. We thus get a solution iff each entry of $M\mathbb{y}$ is a multiple of $4$.</p>

<p>The first such constraint is $4|y_1+y_2+y_3+p$ if without loss of generality $y_4\ne 1$, but since each $y_1=\pm 1$ we can set $\sum_{i=1}^3 y_i$ to any odd integer from $-3$ to $+3$, so any odd $p$ can be made to work for a suitable choice of $\mathbb{y}$ (whereas $p=2$ cannot). Relative to this constraint, each other constraint subtracts twice the sum of some two $y_i$, requiring only that each such pair have the same parity, which certainly works for any odd $p$.</p>

<p>Thus the only remaining difficulty in achieving a given odd prime value for the product is ensuring we don't get $-p$ instead. For $y_1=y_2=y_3=1,\,y_4=p\in\mathbb{P}\backslash\{ 2\}$ each of the constraints checks either $p+3$ or $1-p$ or $p-1$ for divisibility by $4$, so that's the $p=4k+1$ case ticked off. With $p=4k-1$ we can't make it work, since the first constraint requires either $4|p+3$ or $4|p-1$.</p>
"
"2376394","2376398","<p>If $f$ is differentiable it is continuous. Of course, the derivative need not be continuous.</p>

<p>Note that </p>

<p>$$\lim_{t \to x} \lim_{h \to 0} \frac{f(t+h) - f(t)}{h} = \lim_{t \to x} f'(t)$$</p>

<p>need not exist nor equal</p>

<p>$$\lim_{h \to 0} \lim_{t \to x} \frac{f(t+h) - f(t)}{h} = \lim_{h \to 0}\frac{f(x+h) - f(x)}{h} = f'(x) $$</p>

<p>Read about double sequences and double limits in a real analysis book.  The truth lies there.</p>
"
"2376396","2376406","<p>Hint. Let $z_1$ and $z_2$ be the roots of equation $z^2 +\alpha z+\beta =0 $. Then
$$z^2 +\alpha z+\beta =(z-z_1)(z-z_2)=z^2-(z_1+z_2)z+z_1z_2$$
Hence if $z_1$ and $z_2$ lie on $|z|=1$ then $|\beta|=|z_1z_2|=1$.
It follows that each options (1), (2) and (3) implies that $\mbox{Im}(\alpha)=0$.</p>

<p>Now consider the counterexample $(z-i)(z-1)=z^2-(1+i)z+i$. Then $\alpha=-(1+i)$ and $\mbox{Im}(\alpha)=-1$.</p>
"
"2376400","2376404","<p>There is an inclusion map $\iota:S^1\to D_2$ embedding $S^1$
as the ""boundary"" of $D^2$. Then the continuous $f:S^1\to X$
extends to the continuous $g:D^2\to X$ if $f=g\circ \iota$.</p>
"
"2376408","2376531","<p>Your guess is correct.</p>

<p>Generally if you take a groups $H$ and $K$, and a group homomorphism $f: K \rightarrow \operatorname{Aut}(H)$, you can use $(H,K,f)$ to form the semidirect product $G = H \rtimes_f K$. </p>

<p>Then some basic facts you can check using the definitions:</p>

<ul>
<li>$H' = \{(h,1) : h \in H \}$ is a normal subgroup of $G$, isomorphic to $H$.</li>
<li>$K' = \{(1,k) : k \in K \}$ is a subgroup of $G$, isomorphic to $K$.</li>
<li>Every element $g \in G$ induces a automorphism of $H'$ via conjugation, hence an automorphism of $H$.</li>
<li>For $k \in K$, conjugation by $(1,k)$ induces an automorphism of $H'$ which corresponds to the automorphism $f(k)$ of $H$.</li>
</ul>

<p>In particular if you take $K = \operatorname{Aut}(H)$ and $f$ to be the identity map, the semidirect product $G$ is (by definition) the holomorph. It should be immediate that in this semidirect product $G$, conjugation by elements of $K'$ gives you every automorphism of $H$.</p>
"
"2376414","2376422","<p>Let $f(x)=(x+4)(x+2)(x-3)+(x+3)(x+1)(x-5)$. Then</p>

<ul>
<li>since $-27=f(-4)&lt;0&lt;6=f(-3)$, there is a root in $(-4,-3)$;</li>
<li>since $7=f(-2)&gt;0&gt;-12=f(-1)$, there is a root in $(-2,-1)$;</li>
<li>since $-48=f(3)&lt;0&lt;13=f(4)$, there is a root in $(3,4)$.</li>
</ul>

<p>Since it is a cubic equation, it can have no more roots. Therefore, all roots are real.</p>
"
"2376417","2376439","<p>Consider the map $F : [0,1]\times [-1,1]\to S^1\times [-1,1]$ such that
$ F(t,x)=(e^{2\pi it},x)$. Note that $F(1,x) = F(0,x)$ for every $x\in [0,1]$. Now consider the map $G : S^1\times [-1,1] \to S^2$ such that
$G(z,x) =  (\sqrt{1-x^2}\,z,x)\in S^2$. In this way you obtain a map $H = GF : [0,1] \times [-1,1]\to S^2$ with the property that</p>

<p>$$H(1,x) = H(0,x) \text{ for each $x\in [-1,1]$}$$</p>

<p>$$H(t,1) = (0,0,1) \text{ for each $t\in [0,1]$}$$</p>

<p>$$H(t,-1) = (0,0,-1) \text{ for each $t\in [0,1]$}$$</p>

<p>This means $H$ factors through the quotient of $[0,1]\times [-1,1]$ where we identify $0\times [-1,1]$ with $1\times [-1,1]$ and where we collapse $[0,1]\times 1 $ and $[0,1]\times -1$ to a point. </p>

<p>If you prove this map, which is continuous, is a bijection, and that the quotient space obtained above is compact you are done, because $S^2$ is Hausdorff, and any continuous bijection from a compact space to a Hausdorff space is a homeomorphism. </p>
"
"2376418","2384142","<p>You are interested in finding a $K$ such that </p>

<p>$$
A - B\,K = V\,\Lambda\,V^{-1}, \tag{1}
$$</p>

<p>with $A,\Lambda,V \in \mathbb{R}^{n \times n}$, $B \in \mathbb{R}^{n \times m}$, $K \in \mathbb{R}^{m \times n}$ and $\Lambda$ a diagonal matrix (the right hand side is the <a href=""https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Eigendecomposition_of_a_matrix"" rel=""nofollow noreferrer"">eigendecomposition</a> of the desired closedloop matrix). I will assume that the pair $(A,B)$ is controllable so $\Lambda$ can be chosen freely, furthermore the rank of $B$ is assumed to be equal to $m$. If this last assumption is not the case then it won't change the possible $V$ that could be chosen when using a new $B$ matrix instead, which has the same span as the original $B$ but does satisfy this condition.</p>

<p>When looking at the constrains and degrees of freedom it is possible to get an idea of how many columns of $V$ (the eigenvectors) can be chosen freely. Namely the number of degrees of freedom are equal to $n$ times $m$. Choosing the eigenvalues ($\Lambda$) adds $n$ constraints. Choosing a column of $V$ adds $n-1$ constraints, namely the length of the eigenvector does not matter (as long as it is none zero). So from this it can be concluded that the largest number of eigenvectors $p$ that can be chosen freely should equal</p>

<p>$$
p \leq \frac{n (m - 1)}{n - 1}. \tag{2}
$$</p>

<p>When $m = n$ it is trivial to show that all nonsingular $V$ are allowed. The matrix $K$ can then be found using</p>

<p>$$
K = B^{-1} (A - V\,\Lambda\,V^{-1}). \tag{3}
$$</p>

<p>For $m &gt; n$ then $B$ can be reduced to a $m=n$, however it is also possible to use the <a href=""https://en.wikipedia.org/wiki/Inverse_element#Matrices"" rel=""nofollow noreferrer"">right inverse</a>, which will minimize the 2-norm of $u$.</p>

<p>Now comes the more difficult and interesting case, namely when $m &lt; n$ and thus not all columns of $V$ can be chosen freely. However it can be noted that if $m = 1$ then all degrees of freedom of $K$ need to be used to satisfy the constraints for $\Lambda$, so then for a given $\Lambda$ the matrix $V$ will be fixed. By multiplying equation $(1)$ on the right hand side by $V$ and define a new temporary matrix $\Omega = K\,V$ then the following linear matrix equation can be obtained</p>

<p>$$
A\,V - B\,\Omega = V\,\Lambda. \tag{4}
$$</p>

<p>Equation $(4)$ can be reduced to a <a href=""https://en.wikipedia.org/wiki/System_of_linear_equations#Matrix_equation"" rel=""nofollow noreferrer"">linear system of equations</a>. In order to do this equation $(4)$ has to be reshaped from a matrix of $n \times n$ into a vector of $n^2 \times 1$. One way of achieving this would be</p>

<p>$$
\begin{bmatrix}
A\,V_{\bullet,1} - B\,\Omega_{\bullet,1} - \Lambda_{1,1} V_{\bullet,1} \\
A\,V_{\bullet,2} - B\,\Omega_{\bullet,2} - \Lambda_{2,2} V_{\bullet,2} \\
\vdots \\
A\,V_{\bullet,n} - B\,\Omega_{\bullet,n} - \Lambda_{n,n} V_{\bullet,n}
\end{bmatrix} = 0, \tag{5}
$$</p>

<p>where $X_{\bullet,i}$ denotes the $i$th column of matrix $X$. Next all constraints have to be defined. In order to constrain the length of the of the columns of $V$ which are not chosen freely one can set for example the first element of each of these columns to one. However it can be possible that later this will yield a singular systems of equations, if so you might have to try to set a different element of a column to one. For now I do not have a better solution than trial and error, however I suspect that this should not happen that often. The $p$ chosen columns of $V$ and the elements equal to one of the remaining columns can be substituted into equation $(5)$. This can then be rewritten as linear system of equations, since all unknown parameters only occur linearly. It can be noted that since $V_{\bullet,i}$ and $\Omega_{\bullet,i}$ only occur in the $i$th set of $n$ rows from equation $(5)$, therefore it can also be rewritten as $n$ linear systems of equations.</p>

<p>This formulated system of linear equations however will always be singular when an entire column of $V$ is specified by the user. This is because the only remaining unknown parameters related to that column of $V$ will be pre-multiplied by the $B$ matrix. So in order to be able to solve for the unknown parameters $\Omega_{\bullet,i}$ then $(A-\Lambda_{i,i} I)V_{\bullet,i}$ would have to lie within the span of $B$. Or if $\Lambda_{i,i}$ is not an eigenvalue of $A$, then $V_{\bullet,i}$ would have to lie within the span of $(A-\Lambda_{i,i} I)^{-1} B$. Therefore $\Lambda_{i,i}$ and $V_{\bullet,i}$ can never be chosen entirely freely.</p>

<p>As stated in the previous paragraph the allowed choices for columns of $V$ are limited to a $m$ dimensional span. Choosing a vector from this span will only add $m-1$ constraints per chosen column of $V$ instead of $n-1$, so equation $(2)$ will become $p=n$. This would imply that all columns of $V$ can then be chosen, as long as they lie within a certain span. According to equation $(1)$ $V$ has to be none singular, so all columns of $V$ have to be linearly independent of each other. This implies that the eigenvalues of $\Lambda$ can have at most an multiplicity of $m$, since it is impossible to choose more then $m$ independent columns from a span of dimension $m$. If a higher multiplicity is desired then the structure of $\Lambda$ would have to be changed to the <a href=""https://en.wikipedia.org/wiki/Jordan_normal_form"" rel=""nofollow noreferrer"">Jordan form</a>, but the number of associated Jordan blocks to the same eigenvalue should still be at most be $m$. The constraint on the multiplicity of the eigenvalues of $\Lambda$ also holds for the place() command in MATLAB, so I suspect that it uses a similar method of solving this problem (at least based on eigenvalue decomposition instead of the more general Jordan decomposition).</p>

<p>Once all desired constraints are applied then each $n$ dimensional linear system of equations can be solved. If $V_{\bullet,i}$ is only constrained in its length, then there will be more unknowns then equations and again the right inverse could be used to solve for $\Omega_{\bullet,i}$ and the rest of $V_{\bullet,i}$. Once all linear systems of equations are solved, then all solution can be rewritten to form $V$ and $\Omega$. The controller gain can be obtained through $K = \Omega\,V^{-1}$.</p>
"
"2376421","2376429","<p>Let $u = \sec(x)$. Then, you're trying to solve $u(2u^2-1)\geq 0$, which has solutions $$-\frac{1}{\sqrt{2}}\leq u\leq 0\text{ or } u\geq \frac{1}{\sqrt{2}}$$ As either $\sec(x)\leq -1 &lt; -\frac{1}{\sqrt{2}}$ or $\sec(x)\geq 1 &gt; \frac{1}{\sqrt{2}}$, the above inequality will be satisfied exactly when $\sec(x)$ is positive: $$\sec(x) = \frac{1}{\cos(x)} &gt; 0 \Leftrightarrow \cos(x) &gt; 0$$ Therefore, all we have to do is consider where on the unit circle $\cos(x)$ is positive: $0^{\circ}\leq x &lt; 90^{\circ}$ and $270^{\circ} &lt; x\leq 360^{\circ}$.</p>
"
"2376423","2377311","<p>They are introducing a Verma module implicitly via an adjunction. To be concrete, suppose $M$ and $N$ are $\mathfrak g$- and $\mathfrak b$-modules, respectively. Because $\mathfrak b$ is a subalgebra of $\mathfrak g$, $M$ can be made into a $\mathfrak b$-module, denote it by $UM$. It then makes sense to consider the hom-set $\hom_{\mathfrak b}(N,UM)$. </p>

<p>Similarly, we can consider the extended $\mathfrak g$-module obtained from $N$ as $U(\mathfrak g)\otimes_{U(\mathfrak b)} N$, denote this by $EN$, and we can consider the hom-set $\hom_{\mathfrak g}(EN,M)$. There is a natural isomorphism
$\hom_{\mathfrak g}(EN,M) \to \hom_{\mathfrak b}(N,UM)$
which the author is considering to be an identification. </p>

<p>Explicitly, this maps a $\mathfrak g$-linear map $f : EN\to M$ to the $\mathfrak b$-linear map $g : N\to UM$ such that $g(n) = f(1\otimes n)$. The inverse assigns a map $g$ to the map $f$ such that $f(a\otimes n) = a g(n)$, and it is readily checked this defines the desired natural isomorphisms.</p>
"
"2376425","2376432","<p>Let $\tau$ be in the upper half-plane. Some authors use $q$ to denote
$\exp(2\pi i\tau)$. Others write $q$ for $\exp(\pi i\tau)$. This means
that to translate between them, you sometimes have to replace $q$ by $q^2$ and <em>vice versa</em>.</p>
"
"2376434","2376446","<p>It can help to sub in two values of p, 1 and 2 (although you can choose can be anything >1/2).</p>

<p>The area of the rectangle you can find by using (base)(height), or p(f(p)). So A(1)=1, and A(2)=2/3.</p>

<p>Then you have two simultaneous equations:</p>

<p>A(1)=a+b(f(1))=1, or a+b=1</p>

<p>A(2)=a+b(f(2))=2/3, or 3a+b=2</p>

<p>Then you can solve simultaneously to find a and b.</p>
"
"2376437","2378157","<p>I can totally understand where you are coming from. Being  used to a proof system where every line has to be a sentence (i.e no free variables allowed!) I too find proof systems where free variables are involved 'disturbing' and somewhat 'ugly' ... though no doubt much of that is just what you are used to!</p>

<p>In the comments it is suggested that 'x' is nice to work with as a linguistic reference to some 'arbitrary' object, and that certainly makes sense, but you and I simply use some 'arbitrary' constant instead ... again, much of what makes 'sense' here is simply what we are used to.</p>

<p>Still, I think there actually <em>is</em> a good reason to be able to do logical inferences on general formulas. To see this, note that we can extend notions of logical equivalence, implication, etc. to formulas in general. E.g. $P(x) \lor \neg P(x)$ is a logical tautology in the sense that given any <em>variable assignment</em> and interpretation of $P$, the statement comes out true. </p>

<p>And something like logical equivalence between formulas is actually a really important notion, as it allows us to claim that, for example, that the sentences $\forall x (P(x) \rightarrow Q(x))$ and $\forall x (\neg Q(x) \rightarrow \neg P(x))$ are logically equivalent by pointing out that their bodies are logically equivalent <em>formulas</em> and using the 'Substitution of logically equivalent formulas Principle' that claims that substituting logically equivalent formulas in larger formulas results in logically equivalent formulas formulas (this Principle is itself a generalization of the Substitution of logically equivalent sentences Principle of course).</p>

<p>Thus, there is some real use to being  able to generalise important logical concepts to formulas, rather than just  sentences. And so it would make sense to have a proof system that is able to prove those logical  properties and relationships for formulas in general.</p>
"
"2376442","2376486","<p>First we compute the inner integral
$$\int_{\partial D}{1\over |x-y|}\&gt;dx$$
when $y:=(0,0,1)$ is the north pole. Here $dx$ denotes the surface element on the sphere. Introduce spherical coordinates with $\theta=0$ at $(0,0,1)$. Then
$$\int_{\partial D}{1\over |x-y|}\&gt;dx=\int_0^\pi{1\over2\sin{\theta\over2}}\&gt;2\pi\sin\theta\&gt;d\theta=2\pi\int_0^\pi\cos{\theta\over2}\&gt;d\theta=4\pi\ .$$
Due to rotational symmetry the inner integral is in fact independent of $y$. We therefore obtain
$$\int_{\partial D}\int_{\partial D}{1\over |x-y|}\&gt;dx\&gt;dy=4\pi\cdot 4\pi\ .$$</p>
"
"2376443","2376451","<p>If $|f|$ is Riemann integrable on any bounded interval this coincides with the Lebesgue integral.  We have by the monotone convergence theorem,</p>

<p>$$\int_0^\infty |f(x)| \, dx = \lim_{c \to \infty} \int_0^c |f(x)| \, dx = \lim_{c \to \infty} \int_{[0,c]} |f| = \lim_{c \to \infty} \int_{[0, \infty)} |f| \chi_{[0,c]} = \int_{[0,\infty)}|f|$$</p>

<p>Here I use $\int_a^b g(x) \, dx$ to denote a Riemann integral and $\int_{[a,b]} g$ to denote a Lebesgue integral.</p>
"
"2376448","2376488","<p>We need to prove that
$$\sum_{cyc}\frac{b^2c}{a^2(\sqrt{3(a^2+b^2+c^2)}c+ab)}\geq\frac{3}{4}\sqrt{\frac{3}{a^2+b^2+c^2}}$$
for positives $a$, $b$ and $c$. 
$$\sum_{cyc}\frac{b^2c}{a^2(\sqrt{3(a^2+b^2+c^2)}c+ab)}\geq\frac{3}{4}\sqrt{\frac{3}{a^2+b^2+c^2}}$$ or
$$\sum_{cyc}\frac{6b^2c}{a^2(2\sqrt{3(a^2+b^2+c^2)}3c+6ab)}\geq\frac{3}{4}\sqrt{\frac{3}{a^2+b^2+c^2}}.$$ 
Now, by AM-GM and C-S we obtain
$$\sum_{cyc}\frac{6b^2c}{a^2(2\sqrt{3(a^2+b^2+c^2)}3c+6ab)}\geq\sum_{cyc}\frac{6b^2c}{a^2(3(a^2+b^2+c^2)+9c^2+6ab)}=$$
$$=2\sum_{cyc}\frac{b^2c}{a^2(4c^2+a^2+b^2+2ab)}=2\sum_{cyc}\frac{b^4c^4}{a^2b^2c^2(4c^3+a^2c+b^2c+2abc)}\geq$$
$$\geq\frac{2(a^2b^2+a^2c^2+b^2c^2)^2}{a^2b^2c^2\sum\limits_{cyc}(4a^3+a^2b+a^2c+2abc)}.$$
Id est, it's enough to prove that
$$8(a^2b^2+a^2c^2+b^2c^2)^2\sqrt{\frac{a^2+b^2+c^2}{3}}\geq3 a^2b^2c^2\sum\limits_{cyc}(4a^3+a^2b+a^2c+2abc).$$
Let $a+b+c=3u$, $ab+ac+bc=3v^2$ and $abc=w^3$.</p>

<p>Hence, we need to prove that
$$9(9v^4-6uw^3)^2\sqrt{3u^2-2v^2}\geq3w^6(4(27u^3-27uv^2+3w^3)+9uv^2-3w^3+6w^3$$ or $f(w^3)\geq0$, where
$$f(w^3)=3(3v^4-2uw^3)^2\sqrt{3u^2-2v^2}-(36u^3-33uv^2+5w^3)w^6,$$
which is decreasing function, which says that it's enough to prove the last inequality </p>

<p>for a maximal value of $w^3$, which happens for equality case of two variables.</p>

<p>Since the last inequality is homogeneous, we can assume $b=c=1$, which gives
$$8(2a^2+1)^2\sqrt{\frac{a^2+2}{3}}\geq3a^2(4(a^3+2)+2(a^2+a+1)+6a)$$
or $g(a)\geq0$, where
$$g(a)=2\ln(2a^2+1)+\frac{1}{2}\ln(a^2+2)-2\ln{a}-\ln(2a^3+a^2+4a+5)+2\ln2-\frac{3}{2}\ln3.$$
But
$$g'(a)=\frac{(a-1)(2a^5+2a^4+29a^3+17a^2+44a+20)}{a(a^2+2)(2a^2+1)(2a^3+a^2+4a+5)},$$
which gives $a_{min}=1$ and since $g(1)=0,$ we are done!</p>
"
"2376452","2376453","<p>There is a very handy method, namely Gaussian elimination and the reduced row echelon form:
\begin{align}
\begin{bmatrix}
-2 &amp; -2 &amp; 2 \\
-2 &amp; -5 &amp; -1 \\
2 &amp; -1 &amp; -5
\end{bmatrix}
&amp;\to
\begin{bmatrix}
1 &amp; 1 &amp; -1 \\
-2 &amp; -5 &amp; -1 \\
2 &amp; -1 &amp; -5
\end{bmatrix}
&amp;&amp; R_1\gets-\tfrac{1}{2}R_1
\\[6px] &amp;\to
\begin{bmatrix}
1 &amp; 1 &amp; -1 \\
0 &amp; -3 &amp; -3 \\
0 &amp; -3 &amp; -3
\end{bmatrix}
&amp;&amp;\begin{aligned} R_2&amp;\gets R_2+2R_1\\ R_3&amp;\gets R_3-2R_1\end{aligned}
\\[6px] &amp;\to
\begin{bmatrix}
1 &amp; 1 &amp; -1 \\
0 &amp; 1 &amp; 1 \\
0 &amp; -3 &amp; -3
\end{bmatrix}
&amp;&amp; R_2\gets -\tfrac{1}{3}R_2
\\[6px] &amp;\to
\begin{bmatrix}
1 &amp; 1 &amp; -1 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 0
\end{bmatrix}
&amp;&amp; R_3\gets R_3+3R_2
\\[6px] &amp;\to
\begin{bmatrix}
1 &amp; 0 &amp; -2 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 0
\end{bmatrix}
&amp;&amp; R_1\gets R_1-R_2
\end{align}
Once you have the reduced row echelon form you can directly read that
$$
x_3=-2x_1+x_2
$$
This happens because row operations don't change the linear relations between the columns; in the last matrix, it is clear that the last column can be written as $-2$ times the first column plus the second column, so the same happens for the original columns.</p>
"
"2376456","2376460","<p>If $x \in A \cap B$, then $x \in A$ but $x \notin A \cap C$ so $x \notin C$.  Therefore $x \in (A \cap B) - C$, and the latter is not empty.</p>
"
"2376464","2376466","<p>This is not correct. Look at
$$\begin{pmatrix}1&amp;0&amp;0\\0&amp;1&amp;0\\1&amp;1&amp;0\end{pmatrix}$$</p>
"
"2376470","2376476","<p>Use taylor series$$\lim_{x\to 0}\frac{e^x-x-1}{cosx-1}=\\\lim_{x\to 0}\frac{(1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+...)-x-1}{(1-\frac{x^2}{2}+\frac{x^4}{4!}-...)-1}=\\$$can you go on  ?
$$\lim_{x\to 0}\frac{\frac{x^2}{2!}+\frac{x^3}{3!}+...}{(-\frac{x^2}{2}+\frac{x^4}{4!}-...)}=\\
\lim_{x\to 0}\frac{x^2(\frac{1}{2!}+\frac{x^1}{3!}+...)}{x^2(-\frac{1}{2}+\frac{x^2}{4!}-...)}=\\$$simplify $x^2$
$$\lim_{x\to 0}\frac{(\frac{1}{2!}+\frac{x^1}{3!}+...)}{(-\frac{1}{2}+\frac{x^2}{4!}-...)}=\\\frac{\frac12}{-\frac12}=-1$$</p>
"
"2376474","2376481","<p>You have $f(n+1) = f(n) \frac{(N-n-1)p}{(n+1)q}$ correctly. So</p>

<p>$$|f(n) - f(n+1)|  = \left|1 - \frac{(N-n-1)p}{(n+1)q}\right|f(n)$$</p>

<p>So you don't want to show that $\frac{(N-n-1)p}{(n+1)q} \ll 1$, you want to show that $\left|1 - \frac{(N-n-1)p}{(n+1)q}\right| \ll 1$, which is quite different. Indeed, if we replace $n+1$ with $n$:</p>

<p>$$1 - \frac{(N-n)p}{nq} = \frac{nq + np - Np}{nq} $$
and we want this to be approximately 0, we must have
$$\frac{nq + np - Np}{nq} = \epsilon \implies n(q+p) - Np = n \epsilon q$$
$$\implies n = \frac{Np}{q+p+\epsilon q}$$
So, your desired property only holds in the vicinity of $ n \approx N\frac{p}{q+p}$! But this also happens to be the peak of the distribution. </p>

<p>So the bottom line is that, near the peak, it can be well approximated by a continuous function; far away from the peak, the distribution is actually dropping a factor of 2 or 10 or 100 from one point to the next, so integrating a continuous function in that region is pretty inaccurate. But since your $f(n)$ is mostly useful as a probability distribution, nearly all of the distribution is near the peak (which actually gets very very narrow for large $N$), so in all of the region that actually contributes to any expectation values or medians, it is well-approximated by a continuous function.</p>
"
"2376496","2376499","<p>This is equivalent to $$e\ge\left(1+\frac1k\right)^k.$$
To prove this, you could prove that $\log(1+x)\le x$ for all $x\ge1$.</p>
"
"2376517","2376518","<p>The left eigenvalues of a matrix are the zeroes of its minimal polynomial.</p>

<p>The right eigenvalues of a matrix are the zeroes of its minimal polynomial.</p>
"
"2376520","2376538","<p>You're right.</p>

<p>It's not a contradiction because if $p, q, r$ are true, the statement is true; it's not a tautology because if $q$ is true and $p, r$ are false, the statement is false.</p>
"
"2376526","2376541","<p>First off, I think the notation you want to use is $$\lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x}$$</p>

<p>$\partial$ is usually used for partial derivatives, which doesn't make much sense in the way you're using it. </p>

<blockquote>
  <p>So, my question is, why is it only $\lim_{\partial x\to 0}$
  and not $\lim_{\partial y\to 0}$ too?</p>
</blockquote>

<p>$\Delta y$ does indeed tend to $0$. However, this isn't assumed, since if $\Delta x \to 0$, $\Delta y \to 0$ as well, since continuity is weaker than differentiability. Or, algebraically, if you consider </p>

<p>$$\frac{f(x)}{g(x)}$$</p>

<p>and the denominator $g(x)$ tends to $0$ as $x \to c$, then the numerator $f(x)$ must also tend to $0$ as $x \to c$ for the ratio to have a <em>finite</em> limit. </p>

<blockquote>
  <p>However, if we include both $\lim_{\partial x\to 0}$ and $\lim_{\partial y\to 0}$, wouldn't the gradient always be $\approx 1$?</p>
</blockquote>

<p>No, because $$\lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x}$$ is a $\frac{0}{0}$ indeterminate. It can be any number, including $Â± \infty$.</p>

<p>Geometrically, this is because tangent lines to curves can be arbitrarily steep. </p>
"
"2376527","2376529","<p>I like <a href=""https://www.geogebra.org/"" rel=""nofollow noreferrer"">GeoGebra</a>.  It is free and does all this easily.</p>
"
"2376533","2376647","<p>You're mistaking complement at two different levels.</p>

<p>Consider a family </p>

<p>$$\mathcal{A} = \{ U \subseteq X : U \subseteq A \ \&amp; \ U \text{ is open } \}$$</p>

<ol>
<li><p>The complement of this family (in $\mathcal{P}(X)$) is </p>

<p>$$\mathcal{A}^c = \{ U \subseteq X : U \not \subseteq A \text{ or } U \text{ is not open }\}.$$</p>

<p>It is the set of those $U$ which do not belong to $\mathcal{A}$.<br>
<br></p></li>
<li><p>The set of complements of sets in this family is </p>

<p>$$\{ U^c : U \in \mathcal{A} \} = \{ F \subseteq X : F \supseteq X \setminus A \ \&amp; \ F \text{ is closed } \}.$$</p>

<p>It is the set of those $F$ whose complement $F^c$ belong to $\mathcal{A}$.<br>
<br></p></li>
</ol>

<p>Those are two different notions. You're thinking of the first while the proof goes with the second:</p>

<p>$$\bigcap_{U \in \mathcal{A}} (X \setminus U) = \bigcap \{ U^c : U \in \mathcal{A} \} = \bigcap \{ F \subseteq X : F \supseteq X \setminus A \ \&amp; \ F \text{ is closed } \}.$$</p>
"
"2376537","2376547","<p>The recurrence relation is $$a_{n+1}=\frac{a_n-5}{a_n-4},$$ and for this kind of recurrence we have a trick to calculate the general formula.</p>

<p>The equation $$\frac{x-5}{x-4}=x$$ is called the <em>characteristic equation</em> of the recurrence relation and let us denote its roots (called <em>characteristic roots</em>) by $\alpha$ and $\beta$. Then after some (ugly) calculations we can get $$\frac{a_{n+1}-\alpha}{a_{n+1}-\beta}=k\frac{a_n-\alpha}{a_n-\beta},$$ where $k$ is some constant. Now this is a geometric progression and can be easily solved.</p>
"
"2376540","2376767","<p>For the set given $(\{11, 13, 15, 17, 19\})$, $11$ cannot be done as both $4$ and $11$ have to be sent to $5$ and all the rest can be done like so:</p>

<p>$13$: $$\begin{array}{c|lcr}
k &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 &amp; 11 &amp; 12 &amp; 13\\
\hline
a_{k} &amp; 8 &amp; 2 &amp; 13 &amp; 12 &amp; 11 &amp; 10 &amp; 9 &amp; 1 &amp; 7 &amp; 6 &amp; 5 &amp; 4 &amp; 3\\
\end{array}$$</p>

<p>$15$: $$\begin{array}{c|lcr}
k &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 &amp; 11 &amp; 12 &amp; 13 &amp; 14 &amp; 15\\
\hline
a_{k} &amp; 15 &amp; 14 &amp; 13 &amp; 12 &amp; 11 &amp; 10 &amp; 9 &amp; 8 &amp; 7 &amp; 6 &amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1\\
\end{array}$$</p>

<p>$17$: $$\begin{array}{c|lcr}
k &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 &amp; 11 &amp; 12 &amp; 13 &amp; 14 &amp; 15 &amp; 16 &amp; 17\\
\hline
a_{k} &amp; 3 &amp; 7 &amp; 6 &amp; 5 &amp; 4 &amp; 10 &amp; 2 &amp; 17 &amp; 16 &amp; 15 &amp; 14 &amp; 13 &amp; 12 &amp; 11 &amp; 1 &amp; 9 &amp; 8\\
\end{array}$$</p>

<p>$19$: $$\begin{array}{c|lcr}
k &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 &amp; 11 &amp; 12 &amp; 13 &amp; 14 &amp; 15 &amp; 16 &amp; 17 &amp; 18 &amp; 19\\
\hline
a_{k} &amp; 8 &amp; 7 &amp; 6 &amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 16 &amp; 15 &amp; 14 &amp; 13 &amp; 12 &amp; 11 &amp; 10 &amp; 9 &amp; 19 &amp; 18 &amp; 17\\
\end{array}$$</p>

<p>For $13$ and $15$ there may be other examples but I haven't checked. For $17$ there is only this one (which amazingly contains a $5$ cycle) and it's inverse. For $19$ there are many others but I gave this one as it has the nice property that the squares the numbers add up to increase or stay the same moving from left to right.</p>

<p>In general (and for much bigger numbers) I have very little idea how to show whether it's possible or not but I suspect as there are more possibilities for each number to be sent to the probability increases as the numbers get bigger.</p>
"
"2376551","2376799","<p>\begin{align}
r_c&amp;=3\mu\frac{b-a}{b+a} \tag{1}\label{1}
\\
(b+a)^3&amp;=27\mu^2(b-a) \tag{2}\label{2}
\end{align}</p>

<p>To get the expression for <code>b</code> from \eqref{1}, 
\begin{align}
b-a&amp;=\frac{r_c}{3\mu}(b+a)
,
\end{align}
combined with \eqref{2},
\begin{align}
	(b+a)^3&amp;=27\mu^2\frac{r_c}{3\mu}(b+a)
	,\\
	(b+a)^2&amp;=9\mu{r_c}
	,\\
	b&amp;=3\sqrt{\mu{r_c}}-a
	.
\end{align}</p>

<p>From \eqref{2}</p>

<p>\begin{align}
3\,\mu\,\frac{b-a}{b+a}
&amp;=\frac{(b+a)^2}{9\,\mu}=r_c
\tag{3}\label{3}
.
\end{align}</p>

<p>Now we have two expressions for $r_c$.
One has a factor $\frac{\mu}{(b+a)}$,
the other has its reciprocal $\frac{(b+a)}{\mu}$,
and they both are begging to be canceled.
When we multiply them, we'll get a nice simplified 
expression for $r_c^2$:</p>

<p>\begin{align}
r_c^2&amp;=
\tfrac13\,(b+a)(b-a)
,\\
r_c^2&amp;=\sqrt{\mu\,r_c}(3\,\sqrt{\mu\,r_c}-2\,a)
,\\
\left(\frac{r_c}{\mu}\right)^2
&amp;=
\sqrt{\frac{r_c}{\mu}}
\left(
3\,\sqrt{\frac{r_c}{\mu}}-\frac{2\,a}{\mu}
\right)
,\\
\left(\sqrt{\frac{r_c}{\mu}}\right)^3
-
3\,\sqrt{\frac{r_c}{\mu}}
&amp;=
-\frac{2\,a}{\mu}
,\\
4\,\left(\tfrac12\sqrt{\frac{r_c}{\mu}}\right)^3
-
3\,\left(\tfrac12\sqrt{\frac{r_c}{\mu}}\right)
&amp;=
-\frac{a}{\mu}
\end{align}</p>

<p>Recall that</p>

<p>\begin{align}
4\,\cos^3 x-3\,\cos x=\cos3x.
\end{align}</p>

<p>So, we have </p>

<p>\begin{align}
\cos3x&amp;=-\frac{a}\mu
;\\
3x&amp;=\arccos\left(-\frac{a}\mu\right)+2\,\pi k,\quad k=0,1,2
;\\
x&amp;=\tfrac13\arccos\left(-\frac{a}\mu\right)+\tfrac23\,\pi k,\quad k=0,1,2
;\\
\end{align}</p>

<p>Hence</p>

<p>\begin{align}
\cos x=
\tfrac12\sqrt{\frac{r_c}{\mu}}
&amp;=
\cos\left(
\tfrac13\,\arccos\left( -\frac{a}{\mu} \right)
+\tfrac23\,\pi\,k
\right)
,\quad k=0,1,2
;\\
\tfrac12\frac{r_c}{\mu}
&amp;=
2\,
\cos^2\left(
\tfrac13\,\arccos\left( -\frac{a}{\mu} \right)
+\tfrac23\,\pi\,k
\right)
,\quad k=0,1,2
;\\
r_c&amp;=2\,\mu
\left(
1+\cos\left(
\tfrac23\,\arccos\left( -\frac{a}{\mu} \right)
+\tfrac43\,\pi\,k
\right)
\right)
,\quad k=0,1,2
.
\end{align}</p>
"
"2376552","2376571","<p>Let $\gamma_1$ and $\gamma_2$ be two be two paths from $P_1$ to $P_2$ then $\gamma_1\cup \gamma_2^{-}$ is a closed path ($\gamma_2^{-}$ is the path $\gamma_2$ with the orientation reversed). If $f$ is analytic in $\mathbb{C}$ (like your $e^{2z}$), by the <a href=""https://en.wikipedia.org/wiki/Cauchy%27s_integral_theorem"" rel=""nofollow noreferrer"">Cauchy's Theorem</a>,
$$0=\int_{\gamma_1\cup \gamma_2^{-}} f(z)dz=\int_{\gamma_1} f(z)dz-\int_{\gamma_2} f(z)dz$$
which implies that
$$\int_{\gamma_1} f(z)dz=\int_{\gamma_2} f(z)dz,$$
that is the integral is independent of the path.</p>

<p>To compute the integral, note that $\frac{d}{dz}(e^{2z}/2)=e^{2z}$ and for any path $\gamma$ from $P_1$ to $P_2$,
$$\int_{\gamma} e^{2z}dz=\left [\frac{e^{2z}}{2}\right]_{1â\pi i}^ {2+3\pi i}=\frac{e^{4+6\pi i}-e^{2â2\pi i}}{2}=\frac{e^{4}-e^{2}}{2}.$$</p>
"
"2376560","2376608","<p>If there were no restrictions, we could arrange the eight cards in the set $$\{Q\clubsuit, K\clubsuit, \color{red}{Q\diamondsuit}, \color{red}{K\diamondsuit},\color{red}{Q\heartsuit}, \color{red}{K\heartsuit}, Q\spadesuit, K\spadesuit\}$$ in $8!$ ways.  </p>

<p>We must exclude those arrangements in which there are one or more suits in which cards of the same suit are adjacent.  </p>

<p><em>One suit in which the cards are adjacent</em>: Suppose the two clubs are adjacent.  Place them in a box marked clubs.  We then have seven objects to arrange, the box and the other six cards.  They can be arranged in $7!$ ways.  The $Q\clubsuit$ and $K\clubsuit$ can be arranged within the box in $2!$ ways.  Hence, the number of arrangements in which $Q\clubsuit$ and $K\clubsuit$ are adjacent is $7!2!$.  By symmetry, there are an equal number of arrangements in which two hearts, two diamonds, or two spades are adjacent.  Thus, there are 
$$\binom{4}{1}7!2!$$
arrangements in which there is a suit in which two cards of the same suit are adjacent.</p>

<p>However, if we subtract $\binom{4}{1}7!2! = 8!$ from the total, we will have subtracted those cases in which there is more than one suit in which the cards are adjacent more than once.</p>

<p><em>Two suits in which cards of the same suit are adjacent</em>:  Suppose both the two clubs are adjacent and the two diamonds are adjacent.  Place the two clubs in a box marked clubs and the two diamonds in a box marked diamonds.  We then have six objects to arrange, the two boxes and the other four cards.  They can be arranged in $6!$ ways.  Within each box, the cards can be arranged in $2!$ ways.  Hence, there are $6! \cdot (2!)^2$ arrangements in which the two clubs are adjacent and the two diamonds are adjacent.   By symmetry, there are an equal number of arrangements for any pairing of suits.  Hence, there are 
$$\binom{4}{2}6!(2!)^2$$
arrangements in which there are two suits in which the cards of the same suit are adjacent.</p>

<p><em>Three suits in which cards of the same suit are adjacent</em>:  Suppose the two clubs are adjacent, the two diamonds are adjacent, and the two hearts are adjacent.  Place the clubs in a box marked clubs, the diamonds in a box marked diamonds, and the hearts in a box marked hearts.  We have five objects to arrange, the three boxes and the other two cards.  They can be arranged in $5!$ ways.  The cards within each box can be arranged in $2!$ ways.  Hence, there are $5!(2!)^3$ arrangements in which two clubs are adjacent, two diamonds are adjacent, and two hearts are adjacent.  By symmetry, there are an equal number of arrangements for any trio of suits.  Hence, there are 
$$\binom{4}{3}5!(2!)^3$$<br>
arrangements in which there are three suits in which cards of the same suit are adjacent.</p>

<p><em>Four suits in which cards of the same suit are adjacent</em>:  We place the clubs in a box marked clubs, the diamonds in a box marked diamonds, the hearts in a box marked hearts, and the spades in a box marked spades.  We have four objects to arrange.  Within each box, we can arrange the cards in $2!$ orders.  The number of such arrangements is 
$$\binom{4}{4}4!(2!)^4$$</p>

<p>By the Inclusion-Exclusion Principle, the number of permissible arrangements is 
$$8! - \binom{4}{1}7!2! + \binom{4}{2}6!(2!)^2 - \binom{4}{3}5!(2!)^3 + \binom{4}{4}4!(2!)^4$$</p>

<p>Thus, the desired probability is 
$$\frac{8! - \binom{4}{1}7!2! + \binom{4}{2}6!(2!)^2 - \binom{4}{3}5!(2!)^3 + \binom{4}{4}4!(2!)^4}{8!}$$</p>
"
"2376562","2376592","<p>Let's count the forbidden strings. They cannot contain more than one run of length $\geq3$. This leaves the following types, where $b$ denotes the bad digit, $x$ is placeholder for digits $\ne b$, and $y$ can be any digit:
$$bbbxy,\quad yxbbb,\quad xbbbx,\quad bbbbx,\quad xbbbb, \quad bbbbb\ .$$
The first two can be realized in $10\cdot 9\cdot10$ ways each, the next in $10\cdot9^2$ ways, the next two in $10\cdot9$ ways each, and the last one in $10$ ways. It follows that there are $2800$ forbidden strings, hence $10^5-2800=97\,200$ admissible strings.</p>
"
"2376585","2376615","<p>If the aim is to study the stability of the equilibrium point $x^*$ one can proceed with a qualitative study of the differential equation $x'=f(x)$ with $f(x)=-x^3+\cos(x)$.</p>

<p>We already know that there is only one equilibrium point, that is $x^*&gt;0$. Moreover, since $f\in C^1$, the equation verifies local existence and uniqueness of a solution for every choiche of initial values $x(t_0)=x_0$. Also we have: $f(x)&gt;0$ if and only if $x&lt;x^*$, and $f(x)&lt;0$ if and only if $x&gt;x^*$. Hence for every choice of initial conditions $(t_0,x_0)$ with $x_0&lt;x^*$, the solution will be increasing tending to $x^*$ as $t$ increases; while for every choice of initial conditions $(t_0,x_0)$ with $x_0&gt;x^*$, the solution will be decreasing tending to $x^*$ as $t$ increases. We conclude that $x^*$ is an asymptotically stable equilibrium point.</p>
"
"2376594","2376603","<p><strong>Hint:</strong> $2003$ is coprime to $10^6$, so there is exactly one solution to $ 2003 x \equiv 555555 \pmod{10^6}$.</p>
"
"2376602","2376625","<p>Due to holder inequality:$$\int|f||g|dx\leq\left(\int|f|^pdx\right)^{1/p}\left(\int|g|^qdx\right)^{1/q}$$
So if  $p= 3$ and $q= \frac{3}{2}$, we get 
$$\int|f_n-f||f_n-f|dx\leq\left(\int|f_n-f|^{3}dx\right)^{1/3}\left(\int|f_n-f|^{3/2}dx\right)^{2/3}.$$
Since $f_n$ is bounded in $L^3(\mathbb{R})$, $\Vert f_n-f\Vert_{L^3(\mathbb{R})}(\leq\Vert f_n\Vert+\Vert f\Vert)$ is bounded too.
In the meantime, $f_n\to f$ in $L^{3/2}(\mathbb{R})$, so $$\left(\int|f_n-f|^{3/2}\right)^{2/3}dx\to0 \quad as \quad n\to\infty$$
All in all, we'll see$$\int|f_n-f||f_n-f|dx\to0 \quad as \quad n\to\infty$$</p>

<p>P.S. Thanks to @zhw.</p>

<p>since $f_n\to f$ in $L^{3/2}(\mathbb{R})$ , then we have $f_n\xrightarrow{m} f$ in $\mathbb{R}$. According to Riesz theorem, there is a subsequence of $\{f_n\}_n$, named as $\{f_{n_k}\}_k$, that converges to $f$ almost everywhere in $\mathbb{R}$.</p>

<p>Due to Fatou's lemma $$\int|f|^3dx=\int \varliminf|f_{n_k}|^3dx\leq \varliminf\int|f_{n_k}|^3dx&lt;\infty$$</p>
"
"2376618","2376680","<p>Please keep in mind that what follows is basically an expansion of the remark provided by Peyton.</p>

<p>Suppose that $f|_{B(0,r)}$ is not injective. Then, for some $a\in\mathbb C$, the equation $f(z)=a$ has two distinct solutions in $B(0,r)$. Now, let $\gamma\colon[0,2\pi]\longrightarrow\mathbb C$ be the loop defined by $\gamma(t)=r'e^{it}$, for some $r'\in(r,1)$ chosen such that the equation $f(z)=a$ has no roots with absolute value equal to $r'$. Then, by the argument principle and since $f$ has no poles, $\frac1{2\pi i}\int_\gamma\frac{f'(z)}{f(z)-a}\mathrm dz$ is a natural number greater than $1$. On the other hand, if $\Gamma=f\circ\gamma$, then$$\frac1{2\pi i}\int_\gamma\frac{f'(z)}{f(z)-a}\mathrm dz=\frac1{2\pi i}\int_\Gamma\frac{\mathrm dz}{z-a}=\operatorname{Ind}_\Gamma(a).$$So, the winding number of $\Gamma$ with respect to $a$ is greater than $2$. So, $\Gamma$ loops around $a$ twice (at least). But then it must intersect itself, and therefore the restriction of $f$ to the image of $\gamma$ cannot be injective.</p>
"
"2376621","2376635","<p>Let $w$ be the vector from the <em>point</em> $c$ to the point $v$, and let $n$ be the normal vector to the plane (so $n = a \times b$.)  Let $u$ be the projection of $w$ onto $n$.  Then you have a right triangle with $w$ as the hypotenuse and $u$ as one of the legs.   The other leg $w-u$ is the vector from $c$ to the point you want.  So the answer is the <em>point</em> $c+w-u$.</p>
"
"2376628","2376674","<p>I think you're totally on right track. First, let's solve the special case where your open cover $\{V_i \}$ consists only of two sets, $V_1$ and $V_2$.</p>

<p>In this special case, there exists a $t_1 \in \mathcal F(V_1)$ and a $t_2 \in \mathcal F(V_2)$ such that $$\varphi(t_1) = s|_{V_1}, \ \ \ \ \ \ \varphi(t_2) = s|_{V_2},$$
and your difficulty is that $t_1$ and $t_2$ do not glue together.</p>

<p>However, you showed that there exists a $a_{12} \in \mathcal F'(V_1 \cap V_2)$ such that $$\psi(a_{12}) = t_1 - t_2$$
and using the fact that $\mathcal F'$ is flasque, you also argued that there exists a $b_2 \in \mathcal F'(V_2)$ such that
$$ b_2 |_{V_1 \cap V_2} = a_{12}.$$
So let's define $$\widetilde t_1=t_1 \in \mathcal F(V_1), \ \ \ \ \ \ \ \ \ \widetilde t_2 = t_2 + \psi(b_2)\in \mathcal F(V_2),$$</p>

<p>What's nice about $\widetilde t_1$ and $\widetilde t_2$ is that: </p>

<ul>
<li><p>Just like the original sections $t_1$ and $t_2$, they map onto $s|_{V_1}$ and $s|_{V_2}$ under $\varphi$:
$$ \varphi(\widetilde t_1) = s|_{V_1}, \ \ \ \ \ \varphi(\widetilde t_2) = s|_{V_2} + \varphi \circ \psi(b_2) = s|_{V_2} + 0 = s|_{V_2}.$$</p></li>
<li><p>And moreover, $\widetilde t_1$ and $\widetilde t_2$ agree on $V_1 \cap V_2$:
$$ \widetilde t_2|_{V_1 \cap V_2} =  t_2|_{V_1 \cap V_2} + \psi(a_{12}) = t_2|_{V_1 \cap V_2} + (t_1 - t_2)|_{V_1 \cap V_2} = t_1|_{V_1 \cap V_2}. $$
So $\widetilde t_1$ and $\widetilde t_2$ glue together to form a section $\widetilde t \in \mathcal F(U)$ that maps onto $s$ under $\varphi$.</p></li>
</ul>

<hr>

<p>Now let's discuss the general case. We're going to deduce this from the special case, using Zorn's lemma.</p>

<p>Consider the set of tuples $(V, t)$ where $t \in \mathcal F(V)$ and $\varphi(t) = s|_V$, and define an ordering on this set of tuples by declaring that $(V, t) \leq (V', t')$ iff $V \subseteq V'$ and $t'|_{V} = t$. It is not hard to see that every chain $\{ (V_\alpha, t_\alpha) \}$ in our set of tuples has an upper bound: to construct this upper bound, we take $V = \bigcup_\alpha V_\alpha$, and we use the sheaf axioms to glue the $t_\alpha$ together to form a section $t \in \mathcal F(V)$. So we may apply Zorn's lemma to conclude that our set of tuples has a maximal element, $(V_{\rm max}, t_{\rm max})$.</p>

<p>The only thing left to check is that $V_{\rm max} = U$. Suppose that $x$ is a point in $U \ \backslash \ V_{\rm max}$. Then there exists an open neighbourhood $V_{\rm extra}$ of $x$, and a section $t_{\rm extra} \in \mathcal F(V_{\rm extra})$ such that $\varphi(t_{\rm extra}) = s|_{V_{\rm extra}}$. Using our proof of the special case of the theorem from earlier, with $V_{\rm max}$ and $V_{\rm extra}$ in place of $V_1$ and $V_2$, we deduce that there exists a section $t \in \mathcal F(V_{\rm max} \cup V_{\rm extra})$ extending $t_{\rm max}$ such that $\varphi(t) = s|_{V_{\rm max} \cup V_{\rm extra}}$. By maximality, it must be the case that $V_{\rm max} = V_{\rm max} \cup V_{\rm extra}$, contradicting our assumption that $x \notin V_{\rm max}$.</p>

<p>(Note that $t$ does not extend $t_{\rm extra}$, because we have to modify $t_{\rm extra}$ by adding on $\psi(b_{\rm  extra})$ so that it glues with $t_{\rm max}$ on $V_{\rm max} \cap V_{\rm extra}$. But this doesn't matter!)</p>
"
"2376631","2376633","<p>You are right that $|\mathbb R|=\aleph_0\cdot 2^{\aleph_0}$.</p>

<p>What you are missing is that $\aleph_0\cdot 2^{\aleph_0}$ is <em>the same cardinality</em> as $2^{\aleph_0}$ itself.</p>

<p>(Multiplication of infinite cardinal numbers behaves weirdly: Under the axiom of choice $\kappa\cdot\lambda=\max(\kappa,\lambda)$ for all infinite cardinals $\kappa$ and $\lambda$, and it may be weirder yet -- though not in the particular case of $\aleph_0\cdot 2^{\aleph_0}$ -- if we <em>don't</em> assume the axiom of choice).</p>
"
"2376641","2376650","<p>When you successively apply $f$ to a value, it must be divided by $2$ at least one step out of two (because when you add $3$, you get an even number). Let $n$ be an odd number strictly greater than $3$. Then $f^2(n) = \frac{n+3}{2} &lt; n$. We continue to apply $f$ until we get an other odd number, which is then strictly less than $n$.</p>

<p>So every number leads to an odd number less than or equal to $3$. The two possibilities are $1$ and $3$, leading to the two only cycles.</p>
"
"2376642","2378205","<p>It is easy to come up with counterexamples for non-compact or non-Hausdorff spaces. I will argue that the answer is yes for compact Hausdorff $X$.</p>

<p>Consider a point $x$ not in the eventual image, so $F^{-N}(\{x\})=\emptyset$ for some $N\geq 1$. We need to construct a neighbourhood $U$ of $x$ with $F^{-n}(U)\cap U=\emptyset$ for all $n\geq 1$.</p>

<p>Take $N$ to be minimal ($F^{-n}(\{x\})\neq \emptyset$ for $n&lt;N$). The image of $F^N$ is compact and hence closed, so there is an open neighbourhood $U'$ of $x$ with $F^{-N}(U')=\emptyset$. Since $X$ is locally compact, there are neighbourhoods $U''\subseteq C\subseteq U'$ of $x$ with $U''$ open and $C$ closed. Define</p>

<p>$$U=U'' \setminus \bigcup_{n\geq 1} F^{-n}(C).$$</p>

<p>The sets $F^{-n}(C)$ are closed, and they're empty for $n\geq N$, so $U$ is open. And since $U\subseteq C$, we have $F^{-n}(U)\subseteq F^{-n}(C)$, which implies that $F^{-n}(U)$ is disjoint from $U$. Since $N$ was minimal, $U$ still contains $x$ (suppose for contradiction $x\in F^{-n}(C)$ with $n\geq 1$, then $F^{-(N-n)}(\{x\})\subseteq F^{-N}(C)=\emptyset$).</p>
"
"2376648","2376651","<p>$$I=\int\frac{x^2}{\sqrt{1-x^2}}dx=\int\frac{1-(1-x^2)}{\sqrt{1-x^2}}dx=\int\frac1{\sqrt{1-x^2}}dx-\int\sqrt{1-x^2}\,dx.$$</p>

<p>Then by parts</p>

<p>$$\int\sqrt{1-x^2}\,dx=x\sqrt{1-x^2}+\int\frac{x^2}{\sqrt{1-x^2}}dx=x\sqrt{1-x^2}+I,$$ so that</p>

<p>$$I=\frac12\int\frac1{\sqrt{1-x^2}}dx-\frac12x\sqrt{1-x^2}.$$</p>

<p>Now if you don't recognize the derivative of the $\arcsin$ in $\dfrac1{\sqrt{1-x^2}}$, you absolutely can't avoid the substitution with $\sin\theta$.</p>
"
"2376657","2376660","<ol>
<li>The chain rule works the same in this context.</li>
<li>This function is a rational function, and therefore a $C^\infty$ function. This is so because $A^{-1}=\frac1{\det A}\operatorname{adj}(A)$, where $\operatorname{adj}(A)$ is the <a href=""https://en.wikipedia.org/wiki/Adjugate_matrix"" rel=""nofollow noreferrer"">adjugate matrix</a> of $A$.</li>
</ol>
"
"2376665","2376671","<p>It is not always prime: $ f(13) = 13 \times 2834329 $.  For the primes $ 2 \leq p \leq 997 $, the only time $ f(p) $ is (probably) prime is for $ p = 5, 7, 11, 29, 773 $.</p>
"
"2376668","2376686","<p>For a, you have two points on the line, $(x,0)$ and $(1,2)$.  You have two uses of $x$ in your work, the point where the line hits the axis and the coordinate you use for the line, which leads to confusion.  If we make the point where the line hits the axis $(b,0)$ the line is then $y=\frac{-2(x-1)}{b-1}+2$.  Now find the intersection with the $y$ axis by plugging in $x=0$ and compute the length of the hypotenuse.</p>
"
"2376670","2376677","<p><strong>hint</strong></p>

<p>Let $$f (t)=\frac {1}{t+\sqrt {t}} $$
and
 $$F (x)=\int_x^{x^2} f (t)dt.$$</p>

<p>then</p>

<p>$$F'(x)=2xf (x^2)-f (x) $$</p>

<p>solve for $n $ the equation $$F'(n)=0$$</p>

<p>which can be written as</p>

<p>$$\frac {2}{n+1}=\frac {1}{n+\sqrt {n}} $$</p>
"
"2376681","2376689","<p>For each $m\in\mathbb N$, you know that there is a family $(U_{k,m})_{k\in\mathbb N}$ of open sets in $\mathbb{R}^n$ such that $A\subset\bigcup_{k\in\mathbb N}U_{k,m}$ and that $\sum_{k=1}^\infty l(U_{k,m})&lt;\mu^*(A)+\frac1m$. Define $B_m=\bigcup_{k\in\mathbb N}U_{k,m}$. Then $A\subset B_m$ and$$\mu^*(A)\leqslant\mu(B_m)&lt;\mu^*(A)+\frac1m.$$ Now, define, $B=\bigcap_{m\in\mathbb N}B_m$. Then $B$ is Borelian (and therefore measurable) and $\mu(B)=\mu^*(A)$.</p>
"
"2376683","2376697","<p><a href=""https://i.stack.imgur.com/Arhv6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Arhv6.png"" alt=""enter image description here""></a></p>

<p>Make sure first you understand what the following integrals mean geometrically:
$$
I=\int_0^1f(x)\,dx,\quad J=\int_0^1g(x)\, dx.
$$</p>

<p>Then take the difference: $|I-J|$.</p>
"
"2376702","2376703","<p>Consider the linear map defined by $T(1,0) := (0,0)$ and $T(0,1) = (1,0)$. In matrix form,</p>

<p>$$ \begin{bmatrix}0 &amp; 1 \\0 &amp; 0\end{bmatrix}.$$</p>
"
"2376720","2376760","<p>Method 1:  $\frac ab x + \frac cd x = gy$</p>

<p>Multiple by <em>both</em> $b$ and $d$.  Multiplying by $bd$ will <em>always</em> eleminate everything.  so $bd\frac abx + bd \frac cd x = adx + bc x = bdgy$.</p>

<p>Ex: $\frac 23 a^2 - \frac 49 a^2 = 8a$</p>

<p>$3*9\frac 23 a^2 -3*9\frac 49 a^2 = 3*9*8a$</p>

<p>$18a^2 - 12a^2 = 216 a$</p>

<p>But notice that was over kill. Every term is a multiple of 3.</p>

<p>Method 2: Do it one term at a time:</p>

<p>$\frac ab x + \frac cd x = gy$</p>

<p>$ ax + \frac {bc}dx = by$</p>

<p>$ dax + bcx = bdy$.</p>

<p>Ex:</p>

<p>$\frac 23 a^2 - \frac 49 a^2 = 8a$</p>

<p>$3\frac 23 a^2 - 3\frac 49a^2 = 2a^2 - \frac 43 a^2 = 3*8a = 24a$</p>

<p>$3*2a^2 - 3\frac 43 a^2 = 6a^2 - 4a^2 = 3*24a = 72a$</p>

<p>Notice in this case you avoided the overkill by factoring out the excessive $3$ as it appeared.  Had you started with $9$ it wouldn't have come up at all.</p>

<p>$\frac 23 a^2 - \frac 49 a^2 = 8a$</p>

<p>$9*\frac 23 a^2 - 9*\frac 49 a^2 = 9*8a$</p>

<p>$3*2a^2 - 4a^2 = 72a$.</p>

<p>Method 3:  The conclusion we can reach. Multiply by the least common multiple.</p>

<p>$\frac ab x + \frac cd x = gy$</p>

<p>$\text{lcm}(bd)\frac ab x + \text{lcm}(bd) \frac cd = \text{lcm}(bd)gy$</p>

<p>$d'a x + b'c x = \text{lcm}(bd) gy$.</p>

<p>Example: $\frac 23 a^2 - \frac 49 a^2 = 8a$.  $3=3$ and $9 = 3^2$ so $\text{lcm}(3,9) = 9$.  </p>

<p>$9 \frac 23 a^2 + 9\frac 49 a^2 = 9*8a$</p>

<p>$6a^2 - 4a^2 = 9*8a$.</p>

<p>But notice:  You not only should get rid of the denominaters you should <em>also</em> get rid of the common factor two.</p>

<p>Final  $\frac ab x + \frac cd x = \frac gh y$.  Multiply both sides by $\frac {\text{lcm}(b,d,h)}{\gcd(a,c,g)}$</p>

<p>So for $\frac 23 a^2 - \frac 49 a^2 = 8a$.  $\text{lcm}(3,9) = 3$ and $\gcd(2,4,8) = 2$.  </p>

<p>So multiply everything by $\frac 92$.</p>

<p>$\frac 92\frac 23 a^2 - \frac 92\frac 29 a^2 = \frac 928a$</p>

<p>$3a^2 - 2a^2 = 36a$.</p>
"
"2376721","2376733","<p>By Vasc's inequality $(x^2+y^2+z^2)^2\geq3(x^3y+y^3z+z^3x)$ and by AM-GM we obtain:
$$\sum_{cyc}\frac{a}{ab+1}=a+b+c+\sum_{cyc}\left(\frac{a}{ab+1}-a\right)=$$
$$=3-\sum_{cyc}\frac{a^2b}{ab+1}\geq3-\sum_{cyc}\frac{a^2b}{2\sqrt{ab}}=3-\frac{1}{2}\sum_{cyc}\sqrt{a^3b}\geq3-\frac{3}{2}=\frac{3}{2}.$$
A proof of the Vasc's inequality.
$$(x^2+y^2+z^2)^2-3(x^3y+y^3z+z^3x)=\frac{1}{2}\sum_{cyc}(x^2-y^2-xy-xz+2yz)^2\geq0.$$</p>
"
"2376723","2376725","<p><strong>hint</strong></p>

<p>$$g (x)=\arctan (2x) $$
$$g'(x)=\frac {2}{1+4x^2} $$</p>

<p>check that near $0$, </p>

<p>$$|g'(x)|&gt;1$$</p>

<p>To use Newton's method, replace $g (x) $ by
$$G (x)=x-\frac {x-g (x)}{1-g'(x)} $$</p>
"
"2376727","2376732","<p>I give you the formula for $p_0$, the other points are calculated similarly. First calculate the midpoint $m$ of $v_1,v_2$, which is simply $m=(v_1+v_2)/2$. Then $p_0$ lies on the line from $v_0$ through $m$, i.e. $p_0=v_0+\lambda\cdot\frac{m-v_0}{||m-v_0||}$, where $\lambda$ is the desired distance from $p_0$ to $v_0$. Thus, $\lambda=||m-v_0||+d$. Our final result should be
\begin{align*}
p_0=v_0+\frac{||m-v_0||+d}{||m-v_0||}\cdot(m-v_0)=v_0+\left(1+\frac{d}{||m-v_0||}\right)(m-v_0).
\end{align*}
Here, $||x||$ is the Euclidian norm of $x$, which is for $x=(x_1,x_2,x_3)\in\mathbb R^3$ just $||x||=\sqrt{x_1^2+x_2^2+x_3^2}$.</p>

<p>Here is an example:
Let's say $v_0=(1,0,1),v_1=(1,1,-2),v_2=(3,-1,4),d=3$. Then $m=(v_1+v_2)/2=(4,0,2)/2=(2,0,1)$. Then $m-v_0=(1,0,0)$, hence $||m-v_0||=\sqrt{1^2+0^2+0^2}=1$. Finally,
\begin{align*}
p_0=v_0+\left(1+\frac{d}{||m-v_0||}\right)(m-v_0)=(1,0,1)+4(1,0,0)=(5,0,1).
\end{align*}</p>
"
"2376729","2376748","<p>Since 
\begin{align*}
\left(\frac{x+2}{x+4}\right)^2 &amp;= \frac{x^2+4x+4}{(x+4)^2} \\
&amp;= \frac{4}{(x+4)^2}+\frac{x(x+4)}{(x+4)^2} \\
&amp;= \frac{4}{(x+4)^2}+\frac{x}{x+4} \\
&amp;=f'(x)+f(x), \\ 
\end{align*}
let 
$$
\color{blue}{f(x)=\frac{x}{x+4}}.
$$  </p>

<p>Now for $C$ constant, since 
$$
\dfrac{d}{dx}(e^x f(x)+C) = (e^x f(x)+C)' = e^x f'(x)+e^x f(x), 
$$
we have 
$$
\int \left(e^x f'(x)+e^x f(x)\right)dx = e^x f(x)+C. 
$$
So for the above indefinite integral, 
$$
\begin{align*}
\color{green}{\int e^x\left(\frac{x+2}{x+4}\right)^2 dx} 
&amp;= \int e^x \left(f'(x) + f(x)\right) dx  \\ 
&amp;= \int \left(e^x f'(x)+e^x f(x)\right) dx \\
&amp;= e^x f(x)+C \\ 
&amp;= \color{green}{\frac{x\: e^x}{x+4} + C}.
\end{align*}
$$</p>
"
"2376730","2376739","<p>First of all note that $$A=\begin{bmatrix}a&amp;b\\ c&amp;d\end{bmatrix}.$$</p>

<p>So you need to solve $$ \begin{bmatrix}a&amp;b\\ c&amp;d\end{bmatrix}\begin{bmatrix} 0.6 &amp; 0.2 &amp; 0.2 \\ 0.5 &amp; 0.3 &amp; 0.1 \end{bmatrix} = \begin{bmatrix} 0.9 &amp; 0.05 &amp; 0.05 \\ 1 &amp; 0 &amp; 0  \end{bmatrix}.$$ That is,</p>

<p>$$\begin{cases}0.6a+0.5b= 0.9\\ 0.2a+0.3b=0.05\\0.2a+0.1b=0.05\\0.6c+0.5d= 0.9\\ 0.2c+0.3d=0.05\\0.2c+0.1d=0.05\end{cases}$$</p>

<p>In this case there is no solution. How we know it? Substract to the first equation the second one and the double of the third one. We get a contradiction.</p>

<p><strong>Edit</strong></p>

<p>Another way to solve it is as follows. If we have </p>

<p>$$ A\begin{bmatrix} 0.6 &amp; 0.2 &amp; 0.2 \\ 0.5 &amp; 0.3 &amp; 0.1 \end{bmatrix} = \begin{bmatrix} 0.9 &amp; 0.05 &amp; 0.05 \\ 1 &amp; 0 &amp; 0  \end{bmatrix}$$
then
$$A\begin{bmatrix} 0.6 &amp; 0.2 &amp; 0.2 \\ 0.5 &amp; 0.3 &amp; 0.1 \end{bmatrix} \begin{bmatrix} 0.6 &amp; 0.5\\ 0.2 &amp; 0.3 \\ 0.2&amp; 0.1 \end{bmatrix}= \begin{bmatrix} 0.9 &amp; 0.05 &amp; 0.05 \\ 1 &amp; 0 &amp; 0  \end{bmatrix}\begin{bmatrix} 0.6 &amp; 0.5\\ 0.2 &amp; 0.3 \\ 0.2&amp; 0.1 \end{bmatrix}.$$ </p>

<p>That is</p>

<p>$$A\begin{bmatrix}0.44 &amp; 0.38 \\ 0.38&amp;0.35\end{bmatrix}=\begin{bmatrix} 0.56&amp;0.47\\0.6&amp;0.5\end{bmatrix}.$$ Thus, if $A$ exists, it must be </p>

<p>$$A=\begin{bmatrix} 0.56&amp;0.47\\0.6&amp;0.5\end{bmatrix}\begin{bmatrix}0.44 &amp; 0.38 \\ 0.38&amp;0.35\end{bmatrix}^{-1}.$$ Now, we have to check if this is a solution of the original system or not.</p>
"
"2376745","2377005","<p>Regarding the homology $H^{i,p}_k$ for the filtration<br>
$$0 = K^0 \subseteq K^1 \subseteq \cdots \subseteq K^m =K $$we look at the persistent complex around $K^i_k$: $$\begin{array}{cccccccccc}
 &amp; &amp; \Big\downarrow \small\partial^i_{k+2} &amp; &amp; \Big\downarrow \small\partial^{i}_{k+2} &amp; &amp; &amp; &amp; \Big\downarrow \small\partial^{i+p}_{k+2} &amp; &amp; &amp;\\
\cdots &amp; \xrightarrow{\eta^{i-1}} &amp; K^{i}_{k+1} &amp; \xrightarrow{\eta^i} &amp; K^{i+1}_{k+1} &amp; \xrightarrow{\eta^{i+1}} &amp; \cdots &amp; \xrightarrow{\eta^{i+p-1}}&amp; K^{i+p}_{k+1} &amp; \xrightarrow{\eta^{i+p}} &amp; \cdots\\
 &amp; &amp; \Big\downarrow \small\partial^i_{k+1} &amp; &amp; \Big\downarrow \small\partial^{i+1}_{k+1} &amp; &amp; &amp; &amp; \Big\downarrow \small\partial^{i+p}_{k+1} &amp; &amp; &amp;\\
\cdots &amp; \xrightarrow{\eta^{i-1}} &amp; K^{i}_{k} &amp; \xrightarrow{\eta^i} &amp; K^{i+1}_{k} &amp; \xrightarrow{\eta^{i+1}} &amp; \cdots &amp; \xrightarrow{\eta^{i+p-1}}&amp; K^{i+p}_{k} &amp; \xrightarrow{\eta^{i+p}} &amp; \cdots\\
 &amp; &amp; \Big\downarrow \small\partial^i_{k} &amp; &amp; \Big\downarrow \small\partial^{i+1}_{k} &amp; &amp; &amp; &amp; \Big\downarrow \small\partial^{i+p}_{k} &amp; &amp; &amp;\\
\cdots &amp; \xrightarrow{\eta^{i-1}} &amp; K^{i}_{k-1} &amp; \xrightarrow{\eta^i} &amp; K^{i+1}_{k-1} &amp; \xrightarrow{\eta^{i+1}} &amp; \cdots &amp; \xrightarrow{\eta^{i+p-1}}&amp; K^{i+p}_{k-1} &amp; \xrightarrow{\eta^{i+p}} &amp; \cdots\\
 &amp; &amp; \Big\downarrow \small\partial^i_{k-1} &amp; &amp; \Big\downarrow \small\partial^{i+1}_{k-1} &amp; &amp; &amp; &amp; \Big\downarrow \small\partial^{i+p}_{k-1} &amp; &amp; &amp;\\
\end{array} $$
If we fix $i$ we can take the usual homology on the complex $K^i_*$ with $H^i_k=Z^i_k/B^i_k$ where we have $$Z^i_k \approx ker \, \partial^i_k \,\, \text{and} \,\, B^i_k \approx Im \, \partial^i_{k+1}$$
Intuitively this is, $Z^i_k \subset K^i_k$ are the cycles in $K^i_k$ and $B^i_k \subset K^i_k$ are the elements of $K^i_k$ which are the boundaries of some $\gamma \in K^i_{k+1}$. Thus $H^i_k$ are the elements of $K^i_k$ which are cycles but not boundaries of elements in $K^i_{k+1}$.</p>

<p>Similarly for $H^{i,p}_k \approx Z^i_k / (B^{i+p}_k \cap Z^i_k)$ $Z^i_k$ are cycles in $K^i_k$ and $B^{i+p}_k$ are the boundaries of elements $\gamma \in K^{i+p}_{k+1}$ and consequently boundaries of $\sigma \in K^i_{k+1}$. So elements of $H^{i+p}_k$ are cycles i $K^i_k$ which are not the boundaries for any element $\gamma \in K^j_{k+1}; \forall j \leq i+p$. In this way $H^{i,p}_k$ is a further restriction on $H^i_k$.</p>

<p>Regarding your second question. As stated in the paper the above filtered complex is a persistent complex. To obtain the desired persistent module for fixed $i$ we compute $H^i_k$ (not the persistent homology) for all $k$ then take the sum $\bigoplus_{k=0}^\infty H^i_k$ and with the inclusion maps $\eta^{i}: \bigoplus_{k=0}^\infty H^i_k \rightarrow \bigoplus_{k=0}^\infty H^{i+1}_k$ we have the desired persistence module. As to how this module is connected to the persistent homology group, see the section of the source <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.116.2471&amp;rep=rep1&amp;type=pdf"" rel=""nofollow noreferrer"" title=""3.3 Interpretation"">3.3 Interpretation</a> in which the authors ask ""when $e+B^l_k$ is a basis element for the persistent groups $H^{l,p}_k$.""</p>
"
"2376747","2377040","<p>If I understand your question correctly, you have a mapping from $(0,1,2,3)$ to $(0\%,33\%,66\%,99\%)$, respectively, and this is a linear mapping. That implies that  the mapping preserves averages. For example, if you had instead, three numbers $(1,2,3)$, the average is $2$ which maps to $66\%$ and this is also the average of $(33\%,66\%,99\%)$.</p>
"
"2376749","2376755","<p>Suppose on the contrary that $T$ is invertible.</p>

<p>Let $x$ be a non-zero vector, $ x \neq 0$.</p>

<p>Since $T^n=0$, $T^n(x)=0$, that is $T(T^{n-1}(x))=0$, this implies that $T^{n-1}(x)=0$.</p>

<p>Similarly, we can show that $T^{n-2}(x)=0$ and so on and eventually $x=0$ which is a contradiction.</p>

<p>Hint for the second part:</p>

<p>$$(I+T)\sum_{i=0}^\infty (-T)^i=I$$</p>

<p>Note that $\sum_{i=0}^\infty (-T)^i$ is actually a polynomial function of $T$</p>
"
"2376765","2377114","<p>Since the curl of a gradient is zero, $\vec{B}$ has to satisfy $\nabla \times \vec{B} = 0$ for this equation to make sense (take the curl of both sides to see this). This condition is actually the key to finding the inverse.</p>

<p>Suppose we have a simply-connected open region $\Omega \subseteq \mathbb{R}^3$ where $\nabla a = \vec{B}$. Define
$$ A(\gamma) = \int_{\gamma} \vec{B} \cdot d\vec{x}, $$
where $\gamma:[0,1] \to \Omega$ is a smooth curve joining the fixed point $\vec{x}_0 \in \Omega$ to $\vec{x}$. Now, we can show that $A$ in fact only depends on the endpoint: if $\gamma'$ is another smooth curve joining $\vec{x}_0$ to $\vec{x}$,
$$A(\gamma)-A(\gamma') = \int_{\Gamma} \vec{B} \cdot d\vec{x}, $$
where $\Gamma$ is a piecewise-smooth closed curve, which we assume does not intersect itself. Since $\Omega$ is simply-connected, $\Gamma$ bounds a two-dimensional surface, and so Stokes's Theorem implies that the right-hand side is zero because the curl of $\vec{B}$ is zero. Hence $A=A(\vec{x})$.</p>

<p>One can also show that $\nabla A = \vec{B}$:
$$ A(\vec{x}+\vec{h})-A(\vec{x})-\vec{h} \cdot \vec{B}(\vec{x}) = \int_{\vec{x}}^{\vec{x}+\vec{h}} (\vec{B}(\vec{x}')-\vec{B}(\vec{x})) \cdot d\vec{x}', $$
and since $\vec{B}$ is continuous (or it wouldn't be differentiable), the integral is $o(\lVert h \rVert)$, so $\nabla A =\vec{B}$.</p>

<p>Therefore $A$ is always a solution to the original equation.</p>

<p>(The same result holds in higher dimensions using the exterior derivative instead of curl, and the more general version of Stokes.)</p>
"
"2376768","2378009","<p>The result that you are trying to prove is known as <a href=""https://en.wikipedia.org/wiki/Abelian_and_tauberian_theorems#Tauberian_theorems"" rel=""nofollow noreferrer"">Tauber's theorem</a>. By adding and subtracting $p(x)=\sum_1^\infty a_nx^n$ we get
$$\left|\sum_{k=0}^na_n-A\right|\leq \left|\sum_{k=0}^na_k(1-x^k)\right|+\left|\sum_{k=n+1}^\infty a_kx^k\right|+|p(x)-A|$$
The main idea is now to use the inequality $1-x^k\leq k(1-x)$ together with $\lim_{n\to\infty}na_n=0$ to dominate the first term, and the rest should not be so difficult.</p>

<p>A classical reference is Titchmarsh: <a href=""http://rads.stackoverflow.com/amzn/click/0198533497"" rel=""nofollow noreferrer"">The Theory of Functions</a>, p. 10. </p>
"
"2376770","2376787","<p>You have to write $$(-x)^n=(-1)^{n}\cdot x^n$$ If you integrate that you have $${(-1)}^n\cdot \frac{x^{n+1}}{n+1}$$ Shifting gives $${(-1)}^{n-1}\cdot \frac{x^n}{n}=(-1)^{n+1}\cdot \frac{x^n}{n}$$ So your approach was correct, but you forgot to seperate the sign before integrating.</p>
"
"2376773","2376777","<p>A code is more powerful if it has a greater distance, as there is room for more errors between valid codewords.</p>

<p>When you correct an error, you simply jump to the nearest valid neighbor. If the distance is close to the code distance, you can suspect an unrecoverable error.</p>
"
"2376778","2382581","<p>I spoke to my supervisor who helped me find an answer to this question, so I will share for any future readers who have the same question.</p>

<p>Assume for the sake of contradiction that there are two minimal generating sets for the cone: $x_1,...,x_n$ and $v_1,...,v_m$, and that $x_1$ does not lie in the same line as any $v_j$.</p>

<p>Since each $x_i$ is in the cone, it can be generated by $v_1,...,v_n$. Therefore we can write $x_i=\sum a_{ij}vj$ for some $a_{ij}\in \mathbb{R}_{\ge 0}$. Similarly each $v_j$ can be written as $\sum b_{ij} x_i$ for some $b_{ij}\in \mathbb{R}_{\ge 0}$.</p>

<p>Combining these we get $x_i=\sum_{i=1}^n a_{ij}b_{ij}x_i$.</p>

<p>Therefore, renaming our constants and collecting like terms,  there exists $c_i\in \mathbb{R}$ such that $c_1x_1=\sum_{i=2}^n c_ix_i$. Here $c_1=1-a_{11}b_{11}\in \mathbb{R}$ and $c_i=a_{1i}b_{1i}\in \mathbb{R}_{\ge 0}$. We can assume that $c_2,...,c_n$ are not all zero as this would only be possible if $x_1$ was in the same line as some $v_j$.</p>

<p>There are three cases: $c_i&gt;0$, $c_i=0$ and $c_i&lt;0$.</p>

<p>If $c_i&gt;0$, then $x_1=\sum_{i=2}^n\frac{c_i}{c_1}x_i$. Since $\frac{c_i}{c_1}\in \mathbb{R}_{\ge 0}$, this implies $x_1\in \text{Cone}(x_2,...,x_n)$ which contradicts the fact that $x_1,...,x_n$ is a minimal generating set.</p>

<p>If $c_i=0$, then $0=\sum_{i=2}^nc_ix_i$. Not all of the $c_i$ are zero, so without loss of generality we can assume $c_2\ne 0$. Then $-c_2=\sum_{i=3}^n\frac{c_i}{c_2}x_i\in \text{Cone}(x_1,...,x_n)$, which contradicts the strong convexity of the cone.</p>

<p>Finally if $c_i&lt;0$ then $x_1=\sum_{i=2}^n\frac{c_i}{c_1}x_i$, which again contradicts the strong convexity of the cone.</p>

<p>Therefore, up to scalar multiples, $v_1,...,v_n$ is the only minimal generating set.</p>
"
"2376782","2376810","<p>Let $K$ be a midpoint of the arc $BC$ and $\gamma&gt;\beta$.</p>

<p>If $\gamma=\beta$ then the lines $AM$, $AK$ and $AH$ are the same line and your statement is wrong.</p>

<p>We have:  $\measuredangle AMK=\frac{\alpha}{2}+\beta$ and since $\measuredangle MKA=90^{\circ}$, we obtain $\measuredangle MAK=90^{\circ}-\frac{\alpha}{2}-\beta$.</p>

<p>In another hand, $\measuredangle MAH=\measuredangle BAH-\measuredangle BAM=90^{\circ}-\beta-\left(90^{\circ}-\gamma\right)=\gamma-\beta$.</p>

<p>Thus, we need to prove that
$$2\left(90^{\circ}-\frac{\alpha}{2}-\beta\right)=\gamma-\beta$$ or
$$\alpha+\beta+\gamma=180^{\circ}.$$
Done!</p>
"
"2376788","2376801","<p>Let us define these sets,  </p>

<p>S1 = Relations that are symmetric<br>
S2 = Relations that are anti-symmetric</p>

<p>Total number of relations on a set A with n elements = 2<sup>n<sup>2</sup></sup></p>

<p>Total number of relations that are symmetric = |S1| = 2<sup>n</sup> x 2<sup>n(n-1)/2</sup> = 2<sup>n(n+1)/2</sup></p>

<p>Total number of relations that are anti-symmetric = |S2| = 2<sup>n</sup> x 3<sup>n(n-1)/2</sup></p>

<p>We have to find the total number of relations that are symmetric or anti-symmetric. i.e. |S1 &cup; S2|</p>

<p>So, by principle of inclusion-exclusion<br>
|S1 &cup; S2| = |S1| + |S2| - |S1 &cap; S2|</p>

<p>Total number of relations that are anti-symmetric <strong>and</strong> symmetric = |S1 &cap; S2| = 2<sup>n</sup></p>

<p>So, our answer is = 2<sup>n(n+1)/2</sup> + 2<sup>n</sup>3<sup>n(n-1)/2</sup> - 2<sup>n</sup></p>
"
"2376790","2376803","<p>To compute the integral $I$ on $(x,y)$ in $\mathbb R^2$, first note that, by symmetry of the integrand, $I$ is four times the integral on $x&gt;0$, $y&gt;0$, then define $(x,y)=(u,uv)$ and note that $dx\,dy=u\,du\,dv$ hence $$I=4\iint_{u&gt;0,v&gt;0}\frac{u^2}{(1+u^2)u^3(1+v^2)^{3/2}}\,u\,du\,dv=4KL$$ with $$K=\int_0^\infty\frac{du}{1+u^2}\qquad L=\int_0^\infty\frac{dv}{(1+v^2)^{3/2}}$$ One sees that $K$ and $L$ both converge hence, by Tonelli(-Fubini), $I$ converges as well, and it suffices to compute the values of $K$ and $L$ to deduce the value of $I$ by the formula above. Can you do that?</p>
"
"2376795","2377007","<p>Let $X$ be a smooth curve of genus at least 2, and pick three distinct points $P_1,P_2,Q_1$. Let $D=P_1+P_2-Q_1$. Suppose that $D$ is linearly equivalent to an effective divisor. Since $D$ is of degree one, such an effective divisor must be a point. Call this point $Q_2$.</p>

<p>By the definition of linear equivalence, this means that there's a rational function on $X$ with zeroes at $P_1,P_2$ and poles at $Q_1,Q_2$. But this is the same as a degree 2 morphism $X\to\Bbb P^1$ sending $P_1,P_2$ to the same point. If $X$ is not hyperelliptic (ie does not possess a degree 2 map to $\Bbb P^1$), then this is enough.</p>

<p>In the case that $X$ is hyperelliptic and there exists a degree 2 map to $\Bbb P^1$ taking $P_1,P_2$ to the same point, we have a little more work to do. We'll adjust our choice of $P_2$ so that $P_1$ and $P_2$ no longer are mapped to the same point by any degree 2 morphism to $\Bbb P^1$. In this case, the morphism is given by $|K_X|$. Choose a new $P_2$, called $P_2'$, which is different from our original choice of $P_2$. If $P_1+P_2'\in|K_X|$, then we have $P_2\sim P_2'$, which would imply that $X\cong \Bbb P^1$, and else, we have $P_1+P_2'\notin |K_X|$. </p>
"
"2376816","2376829","<p>$A = \sum_{p=1}^n A_p$ so $AB = \sum_{p=1}^n A_p B$, and $A_p B$ has rank $\le 1$ (hint: what do you know about the rank of a product of two matrices, in terms of the ranks of the two matrices?) </p>
"
"2376821","2376861","<p>The set of optimal solutions is a polyhedron, probably with much lower dimension than the number of variables.  You didn't tell us anything about this except that it is not a single point.  It may still be a line segment, for example.  </p>

<p>One important question may be how those solutions are obtained.  For example, if you're using some version of the simplex method, the solutions you obtain will always be basic solutions (thus extreme points of the set of optimal solutions).</p>

<p>But in any case, I see no reason to believe that the results should follow a normal distribution.</p>

<p>EDIT: It may also be worth mentioning that even if the matrix is under-determined, it is typical to have a unique optimal solution.  The feasible region has a finite number of extreme points, and if you took a random $c$ the probability that two extreme points have the same value of $c^T x$ would be $0$.</p>
"
"2376842","2376892","<p>The fact that $\bar\phi:\mathbb{Z}\rightarrow\mathbb{Z}$ is an isomorphism implies that $\phi(0,1)=(u,1)$ or $\phi(0,1)=(u,-1)$.</p>

<p>Let $x\in\mathbb{Z}^n$, you have $(0,1)(x,0)(0,-1)=(A(x),0)$.
Suppose $\phi(0,1)=(u,1)$</p>

<p>$\phi$ preserves the maximal ideals implies that $\phi$ induces a linear map $P:\mathbb{Z}^n\times\{0\}\rightarrow \mathbb{Z}^n\times\{0\}$. You have: 
\begin{align}\phi((0,1)(x,0)(0,-1))&amp;=\phi(A(x),0),\\&amp;=(PA(x),0),\\&amp;=\phi(0,1)\phi(x,0)\phi(0,-1),\\&amp;=\phi(0,1)\phi(x,0)\phi(0,1)^{-1},\\&amp;=
(u,1)(P(x),0)(-B^{-1}(u),-1),\\&amp;=(BP(x)+u,1)(-B^{-1}(u),-1),\\&amp;=(BP(x),0).
\end{align} We deduce that $BP(x)=PA(x)$.</p>

<p>If $\phi(0,1)=(u,-1)$. The same method shows that $B^{-1}P=PA$.</p>
"
"2376851","2379731","<p>I am not sure, but I think that this is a mistranslation of the original text of Bourbaki or just a minor mistake (even Bourbaki sometimes had mistakes). I can only suspect that Bourbaki meant ""The monoid M <em>generated</em> by S [...]"".</p>

<p>There is a general definition of ""amalgamated product"" found in category theory (which is the same as a ""pushout""), and I think this is what they really meant. You can check the definition here: <a href=""https://en.wikipedia.org/wiki/Free_product#Generalization:_Free_product_with_amalgamation"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Free_product#Generalization:_Free_product_with_amalgamation</a> (it is only for two objects of an arbitrary category, but you can easily generalize it to the case of an arbitrary number of objects).</p>

<p>So the ""intuitive"" idea is that a product $(i,x)\cdot(j,y)$ is either ""calculated"" as you've described above if there is a suitable preimage in $A$, or becomes a ""new"" element of $M$. Or better you can think about it as a free monoid over disjoint union $\bigcup\limits_{i\in I} M_i$ factored by a relation generated by $(u_Î±,v_Î±)$, $(p_Î»,q_Î»)$ and $(Ïµ_i,e)$.</p>
"
"2376852","2377029","<p>Will you look at the <a href=""https://en.wikipedia.org/wiki/Z_function"" rel=""nofollow noreferrer"">definition</a> of $Z(t)$ and $\vartheta(t)$ ?</p>

<hr>

<p>$$\text{For } t \in \mathbb{R}, \ Z(t) \in \mathbb{R}, \qquad \zeta(\tfrac{1}2+it) = Z(t) e^{-i \vartheta(t)} $$
thus
$$\Re(\zeta(\tfrac{1}2+it)) = Z(t) \cos(\vartheta(t)), \qquad Z(t) = \frac{\Re(\zeta(\frac{1}2+it))}{\cos(\vartheta(t))}$$
$$\Im(\zeta(\tfrac{1}2+it)) = - Z(t) \sin(\vartheta(t)) = -\frac{\Re(\zeta(\frac{1}2+it))}{\cos(\vartheta(t))}\sin(\vartheta(t)) =\frac{ -\Re(\zeta(\frac{1}2+it))\sin(2\vartheta(t))}{1+\cos(2\vartheta(t))}$$
where I used $$\frac{\sin(x)}{\cos(x)} =\frac{\cos(x)\sin(x)}{\cos(x)^2}=\frac{\sin(2x)}{1+\cos(2x)}$$</p>
"
"2376854","2376890","<p>It's <a href=""http://math.uww.edu/~mcfarlat/pcolumn.htm"" rel=""nofollow noreferrer"">indeed alright</a> to omit the $z$ column from the simplex tableau:</p>

<blockquote>
  <p>However, during the row operations of the SIMPLEX METHOD, this <strong>z</strong>-COLUMN never changes, and therefore Prof McFarland chooses to omit it to save time and space, and to simplify the appearance of tableaus. Any student may either keep or omit this <strong>z</strong>-COLUMN: the row operations will be unaffected whichever you choose.</p>
</blockquote>

<p>However, there's a caveat:</p>

<blockquote>
  <p>The omission of a <strong>z</strong>-COLUMN has one cost: we must remember that in all simplex tableaus, the value of <strong>z</strong> is found in the lower right corner.</p>
</blockquote>
"
"2376859","2376865","<p>If $H\subset \mathrm{Gal}(K/\mathbb Q)$ then the set elements elements of $K$ fixed by $H$ is a field $k$, and you get: $H\cong\mathrm{Gal}(K/k)$. </p>

<p>But the inverse Galois question is seeking $k$ so that $H\cong \mathrm{Gal}(k/\mathbb Q)$.</p>
"
"2376860","2376885","<p>If we could find out the closest divisor efficiently, choosing the second number as the rounded square root of the given number would either give us a non-trivial factor or it would show that the given number is prime which makes the problem trivial.</p>

<p>Hence, finding the closest divisor must at least be as hard as integer factoring. But we do not know an efficient method for integer factoring, so there will not be an efficient method to find the closest divisor either.</p>
"
"2376862","2376868","<p>To go from $k$ to $k-1$ just pick the last variable as the geometric mean of the other $k-1$.</p>

<p>Suppose that $$ \frac{x_1+...+x_k}{k} \geq \sqrt[k]{x_1...x_k}$$
Now choose $x_k = \sqrt[k-1]{x_1...x_{k-1}}$.</p>

<p>Then you'll get exactly what you want once you note that
$$ \sqrt[k]{x_1...x_{k-1}\sqrt[k-1]{x_1...x_{k-1}}}=\sqrt[k-1]{x_1...x_{k-1}}$$</p>
"
"2376876","2376882","<p>For an analytic function in a neighbourhood of $p$, if $f'(p) \ne 0$ then $f$ is injective in a neighbourhood of $p$, while if $f'(p) = 0$ it is not. Indeed,<br>
$f - f(p)$ has a zero of order $m$ at $p$ if and only if $f'$ has a zero of order $m-1$ there (no zero at all in the case $m=1$).  For $w$ in a deleted neighbourhood of $f(p)$, the zeros of $f - w$ in a neighburhood of $p$ are all simple, and the number of them is equal to $m$ by the Argument Principle. </p>
"
"2376879","2381422","<p>You might try to proceed greedily. Suppose we want to find a number that is ""near"" (but less than) $x$ that is a sum of two squares. Maybe one of these squares will be the largest square less or equal to $x$, which I'll denote by $a$. (So $a = \lfloor \sqrt x \rfloor^2$), where $\lfloor \cdot \rfloor$ is the ""floor"" function, or the ""greatest integer below function""). In this case, the second square, which I'll call $b$, should be chosen to be the greatest square less than or equal to $x - a$. (So $b = \lfloor \sqrt{x - a} \rfloor^2$).</p>

<p>This gives us a candidate sum of squares $a + b$, which is relatively close to $x$. Now we ask, how close does this get us? How large is $x - a - b$?</p>

<p>This really asks, how close is $x$ to the largest square up to $x$? That is, how close is $x$ to $a = \lfloor \sqrt x \rfloor^2$?</p>

<p>Note that
$$ x - \lfloor \sqrt x \rfloor^2 = (\sqrt x - \lfloor \sqrt x \rfloor)(\sqrt x + \lfloor \sqrt x \rfloor) \leq (1) \cdot (2 \sqrt x) \leq 2 \sqrt x.$$
So the largest square up to $x$ is no more than $2 \sqrt x$ away from $x$.</p>

<p>Thus $x - a \leq 2 \sqrt x$. Iterating, we have that 
$$(x - a) - b \leq 2 \sqrt{x - a} \leq 2 (\sqrt {2 \sqrt x}) = 2^{3/2} x^{1/4}.$$</p>

<p>We have shown that given an $x$, the nearest ""greedy"" sum of squares is at most $2^{3/2} x^{1/4}$ away. So we've shown that there is a sum of squares in intervals of the shape $(x - 2^{3/2} x^{1/4}, x]$, which clearly implies the original question.</p>

<hr>

<p><strong>Additional Notes</strong></p>

<p>In comments, Will Jagy noted that the density of numbers which are sums of two squares is of the shape $Cx/\sqrt{\log x}$, where $C$ is the Landau-Ramanujan constant. It is conjectured that the gaps between numbers which are sums of squares shouldn't deviate too far from what is expected from the density, and in particular is bounded by $x^\epsilon$ for any $\epsilon &gt; 0$. In particular it is conjectured that there is a number which is a sum of two squares in the interval $[x, x + x^{\epsilon})$ for any $\epsilon &gt; 0$, for $x$ sufficiently large.</p>

<p>This answer shows that the gap between numbers which are sums of squares is at most $2^{3/2}x^{1/4}$, which is a far cry from $x^{\epsilon}$.</p>

<p>In general, there are not good uniform improvements over what's presented above. But there are remarkable on-average bounds due to Hooley, showing for instance that almost all (in the density sense) gaps between sums of squares is no more than $(\log x) (\log \log x)$.</p>
"
"2376889","2376897","<p>$$19\,|\,10^n+1$$</p>

<p>$\iff$
$$10^n+1\equiv 0 \pmod {19}$$
$\iff$
$$10^n\equiv -1 \pmod {19}$$</p>

<p>Listing powers of $10\pmod{19}$:</p>

<p>$$1,10,100\equiv5,50\equiv12,120\equiv6, 60\equiv3, 30\equiv11, 110\equiv-4,-40\equiv-2,-20\equiv-1$$</p>

<p>Hence $10^9\equiv-1\pmod{19}$.</p>
"
"2376904","2376916","<p>As pointed out in the comments, this is not correct.
However, you can take: $$x = \frac{1}{M+1}$$
So that:
$$\frac{1}{x}= M+1 &gt; M$$
A contradiction.</p>
"
"2376906","2376911","<p>Use $x=y+1$ and Eisenstein's criterion. </p>
"
"2376924","2377124","<p>You do have it right.</p>

<p>There is no essential difference between this example and the example of groups acting on sets, which is usually discussed as a basic example when categories and functors are introduced.</p>

<hr>

<p>Yoneda's lemma can be appropriately stated in the case of <strong>Ab</strong>-valued additive functors &mdash; thus, any small additive category is equivalent to its corresponding category of representable presheaves of abelian groups.</p>

<p>That is, every such module is isomorphic to $\hom(R,R)$.</p>
"
"2376928","2376976","<p>The quotient can be made into a vector space over $A/\mathfrak{m}$. In this case, it is the Zariski Cotangent space (after localization) and is isomorphic to the tangent space for an algebraic variety.</p>

<p>Lets consider curves. one can view the quotient $k[x,y]/\mathfrak m$ as a co ordinate ring (functions on a variety) around a point associated to the maximal ideal (by nullstellensatz. Elements in the maximal ideal are the interesting functions (polynomials) that survive after forming the local ring. Higher degree polynomials are nonlinear, so modding out any higher degree polynomials leaves only the linear functions defined around your point. The space of linear functions through a point defined on a variety is exactly the cotangent space.</p>

<p><a href=""https://finitelygenerated.com/a-geometric-reason-to-care-about-discrete-valuation-rings/"" rel=""nofollow noreferrer"">here</a> is a slightly more involved discussion of regularity conditions in terms of the cotangent space (on a blog post I jotted down some months ago.)</p>
"
"2376936","2376948","<p>If $y(x)=\sum_{n=0}^\infty a_n x^n$, then $y'(x)=\sum_{n=1}^\infty na_n x^{n-1}$ and $y''(x)=\sum_{n=2}^\infty n(n-1)a_nx^{n-2}$. Thus,</p>

<p>$$y''(x)-y'(x)=\sum_{n=0}^\infty (n+1)a_{n+1}x^n+\sum_{n=0}^\infty (n+2)(n+1)a_{n+2}x^n=\sum_{n=0}^\infty \left[(n+1)(a_{n+1}+(n+2)a_{n+2})\right]x^n$$</p>

<p>Now equate the quantity between square brackets to each of the coefficients of the power series expansion of $\sin(x)$. </p>
"
"2376945","2377091","<p>You're right. Interestingly, Marker has in his <a href=""http://homepages.math.uic.edu/~marker/mt-errors.html"" rel=""nofollow noreferrer"">the errata</a> that in the rest of the exercise you're supposed to work with the filter <em>generated by</em> $D$. So, as noted by Alex Kruckman, you just need to show that $D$ has the finite intersection property, which is not very difficult: in the case of the op, $A \cap B$ is not empty, since it contains $\{\phi_1, \phi_2\}$. As noted by Alex in his comments, this clearly generalizes for every finite intersection.</p>
"
"2376947","2376961","<p>Let $v\in N(T)\cap R(T)$. Since $v\in R(T)$, $v=T(w)$ for some $w\in V$. But then$$T^2(w)=T(v)=0,$$since $v\in N(T)$. So, $w\in\ker T^2$, but $\ker T^2=\ker T$ (I will prove it), and therefore $v=T(w)=0$. This proves that $N(T)\cap R(T)=\{0\}$.</p>

<p>The reason why $\ker T=\ker T^2$ is because $\ker T\subset\ker T^2$ and because it follows from the rank-nullity theorem and from the fact that $R(T)$ and $R(T^2)$ have the same dimensions that $\ker T$ and $\ker T^2$ have the same dimensions. Therefore, they are equal.</p>
"
"2376952","2376967","<p>The isomorphism 
$$\iota_n:T_p\mathbb{R}^n \to \mathbb{R}^n$$
is given by $X \mapsto \left(X(\pi_1),\cdots,X(\pi_n)\right)$, where $\pi_i$ are the projections on the $i$-th coordinate.</p>

<p>In the particular case of $\mathbb{R}$, we then have that the isomorphism
$$\iota_1:T_p\mathbb{R} \to \mathbb{R}$$
is given by $X \mapsto X(\mathrm{Id})$, where $\mathrm{Id}$ is the identity. Therefore,</p>

<p>$(\iota_1\circ d_pf)(X)=\iota_1((d_pf)(X))=\iota_1(X (-\circ f))=X(\mathrm{Id} \circ f)=X(f).$</p>
"
"2376965","2376978","<p>Let $ABCD$ be our trapezoid, where $BC||AD$, $AD=a$, $BC=b$, where $a&gt;b$. </p>

<p>$M\in AB$ and $N\in CD$ such that $MN||AD$, $MN=x$ and $AM:MB=n:m$.</p>

<p>Let $K\in AD$ such that $BK||CD$. Also, let $BK\cap MN=\{L\}$.</p>

<p>Thus, $KD=LN=BC=b$ and $$\frac{ML}{AK}=\frac{BM}{AB}$$ or
$$\frac{x-b}{a-b}=\frac{m}{m+n}$$ or
$$x=\frac{ma+nb}{m+n}.$$
Done!</p>
"
"2376970","2377078","<p>Factoring $x^{7} - 1$ is equivalent to factoring $x^{8} - x$.</p>

<p>Now,  $x^{8} - x=0$ is the equation that defines the finite field with $8$ elements.</p>

<p>Therefore, $x^{8} - x$ has at least one irreducible factor of degree $3$ (here we use that $8=2^3$).</p>

<p><a href=""https://math.stackexchange.com/a/32416/589"">This answer</a> gives a simple argument that $x^3 + x^2 + 1$ and $x^3 + x + 1$ are the only irreducible polynomials of degree $3$ over $\mathbb{F}_{2}$. By the uniqueness of finite fields, <em>both</em> polynomials must divide $x^{8} - x$. Indeed, 
we can easily check that $x^{8} - x=x (x + 1) (x^3 + x + 1) (x^3 + x^2 + 1)$ and so
$x^{7} - 1=(x + 1) (x^3 + x + 1) (x^3 + x^2 + 1)$.</p>
"
"2376972","2376982","<p><strong>Hint</strong>
Let $u=\tan(\theta),$ or $\theta=\arctan(u).$ Then the integral becomes</p>

<p>$$\int_{0}^{\infty} \frac{1}{a+\frac{u^2}{1+u^2}} \frac{1}{1+u^2} \ du= \int_{0}^{\infty} \frac{1}{(a+1)u^2+a} \ du,$$</p>

<p>Can you take it from here ? </p>
"
"2376973","2377190","<p>I think it's in the definition of $\psi$. We don't want $\psi(\overline A, \overline B) = \overline{AF + BG}$ to depend on the representative for $\overline A = A + I^n$ or $\overline B = B + I^m$. So if $N \in I^n$ and $M \in I^m$ then we want</p>

<p>$$NF + MG \in I^{m + n}.$$</p>

<p>This works because $m_P(F) = m$ (where $P = (0,0)$) says exactly that $F$ consists of terms of degree $\ge m$ which is exactly the statement $F \in I^m$. Likewise, $G \in I^n$. Thus $NF + MG \in I^{m + n}$.</p>
"
"2376983","2377021","<p>The <em>rank</em> of a linear map $T$ from a finite dimensional vector space $V$ to the   finite dimensional vector space $W$ (possibly the same) is the dimension of its image $T(V)$. </p>

<p>If we choose bases in $V$ and $W$, and represent the linear map $T$ with its matrix $M_T$ relative to these bases, the rank of $T$ is the number of linearly independent columns of $M_T$,  and also the number of linearly independent rows of $M_T$, in other words the rank of $T$ is the rank of its matrix $M_T$.</p>

<p>Similarly the set $S$ of solutions of a matrix equation $MX=0$ corresponds to coordinates of the vectors $u$ in the vector space $V$ such that $T(u)=0$ in $W$, so the <em>nullity</em> of $M_T$  is just the dimension of $\ker T$, and the <em>rank-nullity</em> theoram:
$$\operatorname{rank}(M_T)+ \operatorname{nullity}(M_T)=n$$
corresponds to  the  <em>rank theorem</em>:
$$\dim(\operatorname{Im}T)+\dim(\ker T)=\dim V.$$</p>
"
"2376988","2376992","<p>When $x$ is odd $f(x)=x+3$, so $f(x)=x+3$ is even and $f\circ f(x)=f(f(x))=(x+3)-5=x-2$.</p>

<p>When $x$ is even $f(x)=x-5$, so $f(x)=x-5$ is odd and $f\circ f(x)=f(f(x))=x-5+3=x-2$.</p>

<p>Hence $f\circ f(x)=x-2$.</p>
"
"2376995","2377008","<p>First, letâs gather some general facts. Let $E / F$ be any field extension.</p>

<blockquote>
  <ul>
  <li>(A) For any $Î± â E$, if there is some nonzero $f â F[X]$ with $f(Î±) = 0$, then $F(Î±)$ is a finite extension of $F$ with $[F(Î±):F] â¤ \deg f$.</li>
  <li>(B) For any $Î± â E$, the degree $[F(Î±):F]$ is finite if and only if $Î±$ is algebraic over $F$.</li>
  <li>(C) Whenever $F'$ is an intermediate field of $E / F$, then $[E : F] = [E : F'][F' : F]$ and in particular, $[F' : F] â¤ [E : F]$.</li>
  </ul>
</blockquote>

<p>These are general facts you should get to know by studying Galois theory. Consult your lecture notes or your favorite algebra book.</p>

<p>Now, letâs turn to your special situation:</p>

<ol>
<li><p><em>Why is $E$ a finite extension of $F$?</em> This follows inductively from (B) and (C). Consider the tower of field extensions, each obtained by adjoining an algebraic element $a_{n-1}, â¦, a_0$,
$$E = F(a_0,â¦,a_{n-1}) / F(a_0,â¦,a_{n-2}) / â¦ / F(a_0) / F.$$</p></li>
<li><p><em>Why is $[E(r) : E] â¤ n$?</em> This follows immediately from (A) since $f(r) = 0$ and $f â E[X]$.</p></li>
<li><p><em>Why is $[F(r) : F] â¤ [E(r) : F]$?</em> As $F â E$, $F(r)$ is an intermediate extension of $E(r) / F$, so the result follows by (C).</p></li>
<li><p><em>Why did he conclude that $r$ is algebraic over $F$</em>? By (3), $[F(r) : F]$ is finite, as $[E(r) : F] = [E(r) : E][E : F]$ itself is finite by (1) and (2), using (C). In particular, $F(r) / F$ is a finite extension. By (B), $r$ is algebraic.</p></li>
</ol>
"
"2377001","2377010","<p>Part B:</p>

<p>The formula for confidence interval for the certain problem is: $\bar{x}$ $\pm$ $Z(\frac{Ï}{\sqrt{n}})$.</p>

<p>*Z= $Z_\frac{Î±}{2}$</p>

<p>= 127 $\pm$ 1.96$(\frac{2}{\sqrt{8}})$</p>

<p>= (<strong>125.61</strong>, 128.39)</p>
"
"2377017","2377032","<blockquote>
  <p>(2017/8/6) ""Amusing"" silent downvote, surely for mathematical reasons... :-)</p>
</blockquote>

<p>Because, if $z_n\to\ell$ then $\liminf(y_n+z_n)=(\liminf y_n)+\ell$. Your question is concerned with the case $y_n=\frac1n\sum\limits_{k=N}^nX_k(\omega)$ and $z_n=\frac1n\sum\limits_{k=1}^{N-1}X_k(\omega)$, hence $\ell=0$.</p>
"
"2377028","2377044","<p>A square wave of unit amplitude has the <a href=""http://mathworld.wolfram.com/FourierSeriesSquareWave.html"" rel=""nofollow noreferrer"">Fourier series</a>
$$v(t)=\frac{4}{\pi}\sum_{n=0}^{\infty}\frac{1}{2n+1}\sin\left(\frac{2\pi(2n+1)t}{T}\right)$$
so your function $v$ is a square wave of amplitude $V$ and period $T$. This is easy to integrate:
$$P=\frac{1}{T}\int_{0}^{T}\frac{v(t)^2}{R}dt=\frac{1}{T}\int_{0}^{T}\frac{V^2}{R}dt=\frac{V^2}{RT}\cdot T=\frac{V^2}{R}.$$</p>
"
"2377041","2377106","<p>That is mostly okay, but you are making <em>way too much work</em> for yourself. &nbsp; You are asked to transform variable to find a pdf from a pdf. &nbsp; You don't actually have to <em>perform</em> an integration to a CDF to get there. &nbsp; You can apply the chain rule of differentiation to go straight to goal.</p>

<p>$$\begin{align}U_1&amp;=3Y\\\mathsf P(U_1\leqslant u) &amp;= \mathsf P(3Y\leqslant u)
\\ &amp; = \mathsf P(Y\leqslant u/3)
\\ F_{U_1}(u) &amp; = F_Y(u/3)
\\ f_{U_1}(u) &amp; = \tfrac 13 f_Y(u/3) \\ &amp; = \tfrac 12 (\tfrac u3)^2~\mathbf 1_{-1\leqslant (u/3)\leqslant 1}\\ &amp;=\tfrac 1{18}u^2~\mathbf 1_{(-3\leqslant u\leqslant 3)}\end{align}$$</p>

<hr>

<p>When the transformation $Y\mapsto h(Y)$ has an inverse function $g()$, then the fact that the pdf is the <em>unsigned</em> derivative of the CDF means:</p>

<p>$$F_{h(Y)}(u)=F_Y(g(u)) \implies f_{h(Y)}(u) = \lvert g'(u)\rvert~f_Y(g(u))$$</p>

<p>Thus we do not have to invoke the CDF at all:</p>

<p>$$\begin{align}U_2 &amp; =3-Y\\ f_{U_2}(u) ~&amp;=f_{3-Y}(u) \\ &amp;= \lvert\tfrac{\partial (3-u)}{\partial u}\rvert f_Y(3-u) \\ &amp;= \tfrac {3}{2} (3-u)^2 ~\mathbf 1_{(-1\leqslant 3-u\leqslant 1)}\\ &amp;=\tfrac {3}{2}(u-3)^2~\mathbf 1_{(2\leqslant u\leqslant 4)}\end{align}$$</p>

<hr>

<p>However, complications do arise when the transformation $Y\mapsto U$ is not invertable. In this case $Y\mapsto Y^2$ does not have a single inverse over $[-1;1]$. &nbsp; Taking it carefully we find:</p>

<p>$$\begin{align}U_3&amp;=Y^2 \\\mathsf P(U_3\leqslant u) &amp;= \mathsf P(Y^2\leqslant u)
\\ &amp; = \mathsf P({-}\surd u\leqslant Y\leqslant \surd u)
\\ F_{U_3}(u) &amp; = F_Y(\surd u)-F_Y({-}\surd u)
\\ f_{U_3}(u) &amp; = \tfrac 1{2\surd u} f_Y(\surd u)~+~\tfrac 1{2\surd u} f_Y({-}\surd u)) 
\\ &amp; = \tfrac 3{4\surd u}((\surd u)^2\mathbf 1_{-1\leqslant \surd u\leqslant 1}+({-}\surd u)^2\mathbf 1_{-1\leqslant {-}\surd u\leqslant 1}) 
\\ &amp; = \tfrac 3 2 \surd u~\mathbf 1_{0\leqslant u\leqslant 1}\end{align}$$</p>

<p>$\blacksquare$</p>
"
"2377042","2377056","<p>The so-called delta-method stems from two basic results: </p>

<ol>
<li>By the usual CLT, $\sqrt{n}(\bar X_n-\theta)\to\sigma_\theta Z$ in distribution, where $Z$ is standard normal and $\sigma_\theta^2=\mathrm{var}(X_1)$.</li>
<li>If $h'(\theta)\ne0$, then $h(x)-h(\theta)\sim h'(\theta)(x-\theta)$ when $x\to\theta$.</li>
</ol>

<p>Putting 1. and 2. together, one sees that, if $h'(\theta)\ne0$, then $h(\bar X_n)-h(\theta)\sim h'(\theta)(\bar X_n-\theta)$, which implies:</p>

<blockquote>
  <p>$\sqrt{n}(h(\bar X_n)-h(\theta))\to a_\theta Z$ in distribution with $a_\theta=h'(\theta)\sigma_\theta$.</p>
</blockquote>

<p>This is the result you recall since $a_\theta Z$ is centered normal with variance $a_\theta^2=h'(\theta)^2\theta(1-\theta)$.</p>

<p>Likewise, if $h'(\theta)=0$, our point 2. above becomes:</p>

<ol start=""3"">
<li>If $h'(\theta)=0$ and $h''(\theta)\ne0$, then $h(x)-h(\theta)\sim \frac12h''(\theta)(x-\theta)^2$ when $x\to\theta$.</li>
</ol>

<p>Thus, if $h'(\theta)=0$ and $h''(\theta)\ne0$, then $h(\bar X_n)-h(\theta)\sim \frac12h''(\theta)(\bar X_n-\theta)^2$, which implies: </p>

<blockquote>
  <p>If $h'(\theta)=0$, then $n(h(\bar X_n)-h(\theta))\to b_\theta Z^2$ in distribution with $b_\theta=\frac12 h''(\theta)\sigma_\theta^2$.</p>
</blockquote>

<p>It remains to identify the distribution of $b_\theta Z^2$ if $Z$ is standard normal. Can you do that?</p>
"
"2377043","2377571","<p>No need for $X$ to be a Banach space. For example in Megginson's <em>An introduction to Banach space theory</em>, the Kadets-Klee property is defined for normed spaces (Definition 2.5.26, pg. 220). The same page also provides examples of non Banach spaces which have the Kadets-Klee property:</p>

<p>By a Theorem of Kadets (1959) which is mentioned without proof in the end of this page, every seperable normed space has an equivalent locally uniformly rotund norm. A theorem by Vyborny (1956) states that locally uniformly rotund spaces have the Kadets-Klee property (see Theorem 5.3.7, pg. 463 in the same book). 
Combining these two theorems, you have that every seperable normed space has an equivalent Kadets norm. </p>
"
"2377045","2377058","<blockquote>
  <p>But this doesn't seem to lead anywhere useful.</p>
</blockquote>

<p>Actually, it does. You asked us to show you how to set up the problem, rather than to give a full answer, and that's commendable; but you <em>have</em> set it up correctly, so I think the right thing for me to do here is to indicate to you how to <em>finish</em> it.</p>

<p>The point is that there are lots of ways to find bijections between the sets $\{$infinite sequences of positive integers$\}$ and some subset of $\mathbb{R}$. The first step to doing this is to recognize that this is just a fancy way of saying ""find an injection from $\{$infinite sequences of positive integers$\}$ to $\mathbb{R}$"" (since this is then a bijection from $\{$infinite sequences of positive integers$\}$ to the <em>range</em> of that injection).</p>

<p>So to recap: <strong>Cantor-Shroeder-Bernstein</strong> tells us that all we need to do is find injections both ways. You already know an injection in one direction, so the entire remaining task is: <em>find an injection from $\{$infinite sequences of positive integers$\}$ to $\mathbb{R}$.</em> You (commendably!) don't want spoilers, so I've put one possible way to do this at the end of my answer.</p>

<hr>

<p>Incidentally, although it's often used to mean ""by contradiction,"" I interpret ""indirectly"" here as meaning the weaker ""not constructing an explicit bijection"" - and this in turn is usually, in this context, meant to suggest using Cantor-Shroeder-Bernstein. So I would rephrase the argument as:</p>

<ul>
<li><p>We have an injection from reals to sequences of positive integers (e.g., via not-eventually-all-$1$s binary expansions - but change $0$ to $1$ and $1$ to $2$ so that we actually get a sequence of <em>positive</em> integers).</p></li>
<li><p>We have an injection from sequences of positive integers to reals (e.g., the one above).</p></li>
<li><p>So by CSB we know that the two sets have the same cardinality.</p></li>
</ul>

<p>Note that CSB <a href=""https://mathoverflow.net/questions/123482/is-there-a-constructive-proof-of-cantor-bernstein-schroeder-theorem"">is fundamentally not constructive</a>, although it doesn't use the axiom of choice. So by invoking CSB, the argument above does avoid building an explicit bijection. That said, it is easy to tweak the argument to <em>produce</em> an explicit bijection.</p>

<hr>

<p>Alright, let's wrap up. I wrote above that all that's left to do is to <em>find an injection from $\{$infinite sequences of positive integers$\}$ to $\mathbb{R}$.</em> One way to do this is the following:</p>

<blockquote>
  <p>For an infinite sequence $f$ of positive integers, let $Real(f)$ be the number whose binary expansion consists of $f(1)$-many $1$s, then a $0$, then $f(2)$-many $1$s, then a $0$, and so on.</p>
</blockquote>

<p>As an example, $Real(1, 2,3, 4, 5, 6, ...)$ is just $$0.101101110111101111101111110...$$</p>

<p>It's easy to show that the map $Real$ is injective, so is a bijection between $\{$infinite sequences of positive integers$\}$ and $ran(Real)$ - and the latter is a set of real numbers, so we're done.</p>

<p>There are lots of similar tricks one can do (e.g. using continued fractions is much more natural, but I like the ""algorithmic"" feel of the construction above; also, it is easier to verify).</p>
"
"2377048","2377063","<p>The subdifferential of $f:\mathbb{R}^n \to \mathbb{R}$ at some $x_0 \in \mathbb{R}^n$ is the set of all vectors $g \in \mathbb{R}^n$ (called subgradients) such that
$$f(x_0) + \langle g,x - x_0 \rangle \le f(x), \forall x \in \mathbb{R}^n.$$</p>

<p>If $f$ were differentiable and convex, then the above would hold for $g=\nabla f(x_0)$, since the left-hand side would be the linear approximation of $f$ near $x_0$, which would lie below the function $f$.
If you want to visualize this, in dimension $n=1$ this is saying that the tangent line of a convex function at some $x_0$ lies below the graph of the function. If $n=2$, then the left-hand side is the tangent plane that approximates $f$ at $x_0$, and lies below the graph of $f$.</p>

<p>When $f$ is not differentiable, there may be more than one such $g$ that works. For instance, consider $f(x)=|x|$, which is the $l^1$ norm in one dimension. Then the left-hand side is a line with slope $g \in \mathbb{R}$.</p>

<ul>
<li>If $x_0&gt;0$, the only such $g$ that satisfies the definition is $g=1$, which is the sign of $x$.</li>
<li>Similarly if $x_0&lt;0$, the only such $g$ that satisfies the definition is $g=-1$, which is the sign of $x$.</li>
<li>However, if $x_0=0$, then any $g \in [-1,1]$ will work.</li>
</ul>

<p>[To visualize the above bullet points, just try to draw a line that touches $f$ at $x_0$, but lies entirely below $f$. The valid slopes you can use are the elements of the subgradient.]</p>

<p>Thus, the subdifferential is
$$\partial f(x_0) = \begin{cases} \{\text{sgn}(x_0)\} &amp; x \ne 0 \\ [-1,1] &amp; x = 0\end{cases}$$</p>

<p>A similar argument works for the $l^1$ norm in arbitrary dimension.</p>
"
"2377055","2377066","<p>The point is that given two exact sequences of groups $$ 0 \to \mathbb{Z/3} \to K_1 \to \mathbb{Z/3} \to 0$$ and $$0 \to \mathbb{Z/3} \to K_2 \to \mathbb{Z/3} \to 0$$ such that $K_1\simeq K_2$, this does not necessarily imply that the extensions are equivalent in $H^2(\mathbb Z/3\mathbb Z,\mathbb Z/3\mathbb Z)$. What you need is a commutative diagram </p>

<p>\begin{array}{c}
0 &amp; \to &amp; \mathbb Z/3\mathbb Z&amp; \to&amp; K_1 &amp;\to &amp;\mathbb Z/3\mathbb Z \to &amp;0\\
&amp;&amp;\downarrow&amp;&amp;\downarrow&amp;&amp;\downarrow \\
0 &amp; \to &amp; \mathbb Z/3\mathbb Z&amp; \to&amp; K_2&amp; \to &amp;\mathbb Z/3\mathbb Z  &amp;0  
\end{array}</p>

<p>such that the middle arrow is an <strong>isomorphism</strong> and the arrows on the left and right are the <strong>identity</strong>. </p>

<p>In our particular case of $\mathbb Z/9\mathbb Z$, you have two choices for the map $\mathbb Z_3\hookrightarrow \mathbb Z_9$ either take $x+3\mathbb Z$ to $3x+9\mathbb Z$ or to $6x+9\mathbb  Z $. Notice that they both determine the same subgroup $\{0,3,6\}$ in $\mathbb Z/9\mathbb Z$. Now we want an automorphism of $\mathbb Z/9\mathbb Z$ that restricts to the identity on both copies of $\mathbb Z/3\mathbb Z$ and makes the diagram commute. But we can see that the induced map on the kernel will have to be $ x+3\mathbb Z$ to $2x +3\mathbb Z$ which is not the identity.</p>

<p>This argument generalizes and actually shows that by using the trivial extension and the maps $x+p\mathbb Z \mapsto ix+p^2\mathbb Z$ for $i=1,2\cdots p-1$, the group $H^2(\mathbb Z/p\mathbb Z,\mathbb Z/p\mathbb Z)$ is atleast of size $p$.</p>
"
"2377065","2377075","<p>For the notion of $Î±$-conversion <a href=""https://en.wikipedia.org/wiki/Lambda_calculus#Alpha_equivalence"" rel=""nofollow noreferrer"">I know of</a>, $Î±$-conversion only concerns <em>bound</em> variables. As $x$ and $y$ are both free in $t = (x~x)$ and $s = (y~y)$ respectively, the terms $t$ and $s$ are <em>not</em> $Î±$-convertible.</p>

<p>As a side note: Also, even if they <em>were</em> $Î±$-convertible, any two terms $Î²$-reducing to some same term might still not be $Î²$-convertible. For example, $(Î»x.x)~y$ and $(Î»x. y)~x$ both reduce to $y$, but neither one of them $Î²$-reduces to the other (as far as I can tell â itâs been a while â¦).</p>
"
"2377072","2377094","<p>In any dimension you can use the (hyper-)spherical coordinates, in which $d^n\mathbf{x}=r^{n-1}drd\Omega$, and $\Omega$ is the generalization of solid angle. In this coordinate system your integral becomes
$$
I_n=\int d\Omega\int_{0}^\infty r^{n-1} e^{-r}dr
$$
The latter integral, by definition, is $\Gamma(n)=(n-1)!$, so it remains to calculate $\int d\Omega$ (oh by the way by this integral I mean integral over all solid angles). Let us denote this integral by $\sigma_{n}$.</p>

<p>Note that
$$J=\int_{\mathbb{R}^n} e^{-||\mathbf{x}||^2}d^n\mathbf{x}=\left(\int_{-\infty}^\infty e^{-x^2}dx\right)^n =\pi^{\frac{n}{2}}$$
But one can calculate the same integral in (hyper-)spherical coordinates, then
$$
J=\sigma_{n}\int_{0}^\infty r^{n-1}e^{-r^2}dr\xrightarrow{s=r^2} 
\frac{\sigma_n}{2}\int_{0}^\infty s^{(\frac{n}{2}-1)}e^{-s}ds=\frac{\Gamma(\frac{n}{2})}{2}\sigma_n\Longrightarrow \boxed{\sigma_{n}=\frac{2\pi^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}}
$$
As a result your integral is
$$
\boxed{I_n=2\pi^{\frac{n}{2}}\frac{\Gamma(n)}{\Gamma(\frac{n}{2})}}
$$</p>
"
"2377076","2377096","<p>In you graph the shortest path between node $i$ and node $i$ is 0 (because it is the same node), and these paths have 0 hops (number of nodes of a path, excluding the first node, or number of edges of the path, <em>i.e</em> length of the path).</p>

<p>You have 3 paths of minimum length 1 ($0 \to 1$, $1 \to 2$, $3 \to 4$), and $1$ shortest path of length 2 (the one between nodes $0$ and $2$).</p>

<p>About you edit 2:</p>

<p>If you add a self-loop on the node $0$, you will have several paths from $0$ to $0$, including the trivial path of length $0$ and the path taking the self-loop, on length $1$. However this path of length $1$ is <strong>not</strong> a <strong>shortest</strong> path and therefore is not counted.</p>
"
"2377081","2377513","<p>There are 8 options, but there are only two indistinguishable families of the product. </p>

<p>Suppose we start with $i\times j = k, j\times k = i, k\times i= j$, swapping any orthogonal vector for its negative counterpart will be exactly equivalent to swapping the order of multiplication of two of the cross products. This gives us 4 of the options. Any further sign changes will only convert the cross product into one of these 4 variations. Hence choosing any of these 4 options is equivalent to redefining the coordinate system.</p>

<p>Now let's start with $j\times i = k, k\times j = i, i\times k= j$ or by the anticommutative law, $i\times j = -k, j\times k = -i, k\times i= -j$. Now changing the sign of any basis vector will allow us to convert between the remaining 4 cross products, but we cannot switch to the previous definition.</p>

<p>It is much easier to prove the distributive property of the cross product when we start with the cartesian definition. This argument applies just as well for both methods</p>
"
"2377109","2377254","<p>The easier part is to show that the series is uniformly convergent on any  compact interval $[c_1,c_2]$ that excludes $-1$ and $0$ as well as any integer multiple of $\pi$. This follows from an application of the Dirichlet test for uniform convergence, since  $\sqrt{n}/(n+x)$ converges monotonically and uniformly to $0$ for sufficiently large $n$, and as you observed $\left|\sum_{n=1}^m \sin(nx)\right|$ is uniformly bounded for all $m$ on such intervals.</p>

<p>Convergence is non-uniform on any interval such as $(0,c_2]$ where $0$ is a limit point.  In this case the Cauchy criterion for uniform convergence is violated.</p>

<p>For any $m \in \mathbb{N},$ let $x_m = \pi/(4m)$.  With $m \leqslant n \leqslant 2m$, we have $\pi/4 \leqslant nx_m \leqslant \pi/2$ and $1/ \sqrt{2} \leqslant \sin n x_m \leqslant 1$. </p>

<p>Hence,</p>

<p>$$\left|\sum_{n = m}^{2m} \frac{\sqrt{n}}{n + x_m}\sin (nx_m)\right| \geqslant \frac{1}{\sqrt{2}}\sum_{n=m}^{2m}\frac{\sqrt{n}}{n + \pi/4}.$$</p>

<p>Since the series $\sum_n \sqrt{n}/(n + \pi/4)$ diverges, the RHS cannot be arbitrarily small, regardless of the choice for $m.$ </p>

<p>I shall leave it for you to show that convergence is not uniform on any interval with $-1$ as a limit point. </p>
"
"2377112","2377122","<p>we can use some common sense.</p>

<p>If $n$ is even $16|n^4$.  But if $n$ is odd then $2\not \mid n^4$.  However  $n+1$ and $n-1$ are both even so $2^3|(n-1)(n+1)^2$.  But if $n$ is odd then either $n + 1$ or $n -1$ is divisible by $4$ so $2^4|(n-1)(n+1)^2$.  So $2^4$ will divide all such numbers. </p>

<p>Can any higher powers of $2$ divide it?  Why should they? Example: if $n=2$ then then number is $16*1*3^2$ which is not divisible by any higher power.</p>

<p>But are there any other factors that must divide it?  Either $n$ or $n-1$ or $n+1$ is divisible by $3$ so $3|n^4(n-1)(n+1)^2$.  But if $3|n-1$ then $1$ is the only guaranteed power of $3$ that divides.  (if $n-1 = 3$ then $9 \not \mid n^4(n-1)(n+1)^2$.</p>

<p>So $3*2^4$ will always be a factor.  </p>

<p>Are there any other prime factors that must occur?  Well, if $p$ is a prime $p &gt; 3$ and if we let $n-1 = p+1$ we have $n^4(n-1)(n+1)^2 = (p+2)^2(p+1)*(p+3)^2$ and $p$ is not a factor of that.</p>

<p>So $48 = 3*2^4$ the largest integer that must divide all $n^4(n-1)(n+1)^2$.</p>
"
"2377115","2377121","<p>Write $c=\cos{\psi}$. The equation is
$$ (x^2+y^2)c^2 + z^2 \frac{c^2}{1-c^2} = a^2, $$
since $\cot{t}=\cos{t} / \sin{t}$, and rearranging this gives
$$ (x^2+y^2)c^4-(x^2+y^2+z^2+a^2)c^2 + a^2 = 0, $$
which is a quadratic equation for $c^2$. Solve this for $c^2$, take the square root, and then use $\psi=\arccos{c}$, which is well-defined given the range in which $\psi$ lies.</p>
"
"2377116","2377119","<p>Use the fact that
$
\prod_{k=0}^n {n \choose{k}} ^{\frac{1}{n(n+1)}} = e^{\frac{1}{n(n+1)}\ln(\prod_{k=0}^n{n \choose{k}})}
$ </p>

<p>By the symmetry of the pascal coefficient, it is enough to consider the midpoint, we will assume that $n$ is even.</p>

<p>$
{n \choose 0}{n \choose 1}{n\choose 2}\times ... \times{n\choose n-1}{n\choose n} = \biggr(\frac{[n][n(n-1)][n(n-1)(n-2)]\times...\times[n(n-1)(n-2)\times...\times(n-(\frac{n}{2} -1))]}{[0!][1!][2!][3!]\times...\times[\frac{n}{2}]!}\biggr)^2
$</p>

<p>The last equation is equal to</p>

<p>$n^{\frac{n}{2}}(n-1)^{\frac{n}{2} - 1}\times...\times(n - (\frac{n}{2} - 1))^{\frac{n}{2} - (\frac{n}{2} - 1)}$</p>

<p>If you will consider the limit, the highest exponent is the only term that matters</p>

<p>That term using the Gauss formula for the sum is</p>

<p>$\displaystyle\sum_{i = 0}^{n}\frac{n}{2} - i = \frac{1}{4}[ (n)(n+1) - 4n]$</p>

<p>Those terms are inside the logarithm, then you can see that</p>

<p>$\displaystyle\lim_{n \to \infty}\frac{(\ln(\prod_{k=0}^n{n \choose{k}})}{n(n+1)} = \lim_{n \to \infty}2\frac{\frac{1}{4}[ (n)(n+1) - 4n]}{n(n+1)} = \frac{1}{2}$</p>

<p>Maybe there is a shorter approach, but this was the first that came to my mind. </p>
"
"2377117","2383026","<p>You are trying to solve the PDE $F(Du,u,x)=0$ in $U$, with $u=g$ on $\Gamma\subset\partial U$. If the solution you expect to find is a function $u\in C(\overline{U})\cap C^1(U)$ (for example this happens if $g$ is only continuous), then you are right, you do not expect the solution to satisfy the PDE on $\Gamma$. However, if $g$ is $C^1$ and $F$ is a continuous function defined also for $x\in \partial U$, then you expect/hope to find a solution $u\in C^1(\overline{U})$. In this case, if the PDE is satisfied in $U$ and you consider a point $x_0\in \partial U$, then by continuity,
$F(Du(x_0),u(x_0),x_0)=0.$
Now since $u=g$, if, say $\Gamma=\partial U$ is a smooth manifold, then at every point $x_0\in \partial U$ you have a tangent space and for every tangent vector $t$ to $\partial U$ at $x_0$, $\frac{\partial u}{\partial t}(x_0)=\frac{\partial g}{\partial t}(x_0)$. Since the tangent space at $x_0$ has dimension $n-1$, you have that $\frac{\partial u}{\partial t_i}(x_0)$ are determined for $i=1,\ldots, n-1$, where $t_i$ are linearly independent vectors. So only the derivative of $u$ in the normal direction is missing. To find it, you have to use the equation $F(Du(x_0),u(x_0),x_0)=0.$</p>

<p>We use the same trick to study regularity for elliptic equations, say $\Delta u=f$ in $U$ with Dirichlet boundary conditions $u=g$ on $\partial U$. </p>
"
"2377120","2377174","<p>For simplicity, we denote $a+ib=\mu/(\sigma+i\omega)$.  Assume $y$ is not the trivial solution $y=0$.  Letting $y(t)=\rho(t) e^{i\theta(t)}$ be its polar representation, your equation
$$
\dot y+(a+ib)|y|^2y=0
$$
becomes
$$
e^{i\theta}(i\rho\dot \theta+\dot\rho+(a+ib)\rho^3)=0.
$$
Equating real and imaginary parts to zero gives a system for $\rho(t)$ and $\theta(t)$:
\begin{align}
\dot\rho+a\rho^3&amp;=0,\\
\dot\theta+b\rho^2&amp;=0.
\end{align}
The first equation is decoupled, so the second equation is linear in $\theta$ once $\rho$ is found.</p>
"
"2377163","2377177","<p>$$\Delta=f^{-1}(0)=f^{-1}\Big( \bigcap_{n \in \mathbb{Z}_+}(-\frac{1}{n},\frac{1}{n})\Big)=\bigcap_{n \in \mathbb{Z}_+}f^{-1}((-\frac{1}{n},\frac{1}{n})),$$
and each $f^{-1}((-\frac{1}{n},\frac{1}{n}))$ is open in $X\times X$ by continuity of $f.$</p>
"
"2377165","2377170","<p>No. It is possible to ""patch"" functions together in a differentiable way. For instance, let the domain be $[-1,1]$,
$$
f(x)=\begin{cases}0,&amp;\ x&lt;0 \\ x^2,&amp;\ x\geq0 \end{cases}
$$
Then $f$ is differentiable (you can check that it is differentiable at $0$ by looking at the two side derivatives), constant in part of the domain, but non-constant in the whole. </p>

<p>A variation of this example can achieve the same even if ""infinitely differentiable"" is required. Only when we get to ""analytic"", will ""constant on an interval"" imply ""constant everywhere"". </p>
"
"2377185","2377189","<p>I read the comments, I don't really understand what bothers you. But I guess you did not really understand the notation $\prod$.</p>

<p>To simplify, let us say that $G(\alpha)=\{\beta_1, \beta_2, \beta_3\}$ (just to explain we make the arbitrary assumption that it has 3 elements).</p>

<p>Then $f(x)=\prod_{\beta \in G(\alpha)}(x-\beta)=(x-\beta_1) \cdot (x-\beta_2) \cdot (x-\beta_3)$. So yes, we take <em>each</em> $\beta$ once into account in the product.</p>

<p>It is the same way of saying that $\sum_{n \in \mathbb{N}} a_n = a_0 + a_1 + a_2 + \cdots$. And again, each element $n$ of $\mathbb{N}$ is taken once in the sum.</p>
"
"2377194","2377705","<p>Yes your point (2) looks valid to me but that's not the only thing that is going wrong. </p>

<p>Indeed, you can have a case where the inclusion $R_1 \otimes R_2 \to A \otimes B$ is an injective inclusion which is also integral but the theorem does not hold.</p>

<p>The mistake lies where you choose $f,g$ with constant terms $f_0, g_0$. You may not be able to do that for example if $f = Z^m$. Even otherwise when you plug $x$ into $f$, the expression is defined in a ring which may not be an integral domain (that's what you are trying to prove), so you cannot cancel off the powers and make that assumption. </p>

<p>I will give an example which shows that it may hold that the extension is injective and integral but your result is not true because of the aforementioned point. </p>

<p>Take $k = \mathbb F_p(t)$. $A = B = \mathbb F_p(t^{1/p})$. (This is a $k$ algebra)</p>

<p>Now you can take $R_1 = R_2= k$ because the extension $k \to \mathbb F_p(t^{1/p})$ is integral. Now do the tensor products and you get an extension </p>

<p>$\mathbb F_p(t) \to \mathbb F_p(t^{1/p}) [X]/ (X - t^{1/p})^p$. This is injective  and integral.</p>

<p>But clearly this is not reduced. Also the polynomial annihilator for $X - t^{1/p}$ in $\mathbb F_p(t)$ is $Z^p$. </p>
"
"2377199","2377202","<p>From $1$ to $999$ each odd digit will appear 100 times in the place of hundreds, 100 times in the place of tens and 100 times in the place of ones. To notice that just fix that digit and you have 100 choices for the rest two digits. Now deal the numbers from $1000$ to $1999$ the same, but account for the extra 1 at the beginning. Finally you can take care of the rest 17 numbers, which is quite easy.</p>
"
"2377201","2377212","<p>I would say there are 3 cases, that the tens digit is a zero, the hundreds digit is a zero, and all 4 digits are non-zero.</p>

<p>For each of the first two cases, the number of possible integers is</p>

<p>$$8 \times 7 \times 1 \times 5$$</p>

<p>For the third case, the number of possible integers is</p>

<p>$$8 \times 7 \times 6 \times 5$$</p>

<p>Twice the first number plus the second number gives</p>

<p>$$8 \times 7 \times 8 \times 5$$</p>

<hr>

<p>Merging the three cases, as suggested by @stewbasic:</p>

<p>Consider the unit digit, there are $5$ possible digits from the odd condition.</p>

<p>Consider the thousands digit, there are $8$ possible digits, which is one fewer for the zero and one fewer for the odd unit digit than $10$.</p>

<p>For the remaining hundreds and tens digit, there are no restrictions and there are $8\times 7$ possible digit pairs.</p>

<p>So the answer is $8 \times 8 \times 7 \times 5$.</p>
"
"2377208","2378188","<p><strong>HINT</strong>: Using the chain rule, write what the Frenet equations tell you for a non-arclength parametrized curve. In particular, using prime to denote differentiation with respect to $t$, if you let $\upsilon = \|r'\|$, you'll have
$$T' = \frac{dT}{ds}\frac{ds}{dt} = \kappa\upsilon N.$$
Start with $r' = \upsilon T$.  Differentiate this twice, using the chain rule where appropriate, and inserting the Frenet equations. For example,
$$r'' = \upsilon'T + \upsilon T' = \upsilon' T + \upsilon^2 \frac{dT}{ds} = \dots.$$</p>
"
"2377209","2377214","<p>If you only write ""let $R$ be a local ring"" then when you need to talk about the maximal ideal of $R$ you need to also say ""let $\mathfrak m$ be the maximal ideal of $R$"" because otherwise $\mathfrak m$ isn't defined. Saying ""let $(R, \mathfrak m, k)$ be a local ring"" lets one introduce three symbols at once and because one often wants to talk about $\mathfrak m$ and $k$ it is useful to have an effective means of introducing them rather than the long winded ""let $R$ be a local ring with maximal ideal $\mathfrak m$ and residue field $k = R/\mathfrak m$"".</p>

<p>It's the same reason as one might say at the top of their paper ""in this paper $U, V, W$ will denote vector spaces over a fixed field $\mathbf{F}$"" rather than having to say ""let $V$ be a vector space over a field $\mathbf{F}$"" everywhere.</p>
"
"2377211","2377217","<p>Show that if $f$ is of degree $n$ and if $g$ is of degree $m$, then $f(g)$ is of degree $mn$. To this end, note that if $f = \lambda X^n + \cdots$ and if $g = \mu X^m+\cdots$ then the unique term of degree $mn$ in $f(g)$ is $\lambda \mu^m X^{mn}$, and this is nonzero since $\lambda\mu^m\neq 0$. </p>
"
"2377220","2377231","<p>Let $P_1(x_1,y_1)$, $P_2(x_2,y_2)$ and $D$ is a midpoint of $P_1P_2$.</p>

<p>Thus, $$\vec{P_1P}=\vec{P_1D}+\vec{DP}=\frac{1}{2}(x_2-x_1,y_2-y_1)\pm\frac{1}{2}\cot\frac{\theta}{2}(y_2-y_1,x_1-x_2),$$
which says
$$P\left(\frac{1}{2}\left(x_2-x_1\pm\cot\frac{\theta}{2}(y_2-y_1\right)+x_1,\frac{1}{2}\left(y_2-y_1\pm\cot\frac{\theta}{2}(x_1-x_2\right)+y_1\right)$$ or
$$P\left(\frac{1}{2}\left(x_2+x_1\pm\cot\frac{\theta}{2}(y_2-y_1\right),\frac{1}{2}\left(y_2+y_1\pm\cot\frac{\theta}{2}(x_1-x_2\right)\right)$$</p>
"
"2377225","2377228","<p><a href=""https://en.wikipedia.org/wiki/Riemann%E2%80%93Lebesgue_lemma"" rel=""noreferrer"">Riemann-Lebesgue lemma</a>.  Note that $\sin^3(nx) = \frac{3}{4} \sin(nx) - \frac{1}{4} \sin(3nx)$.</p>
"
"2377233","2377298","<p>If the radius is $r$,
the equation of the sphere is
$x^2+y^2+z^2 = r^2$.</p>

<p>If $r = \sqrt{N}$,
this becomes
$x^2+y^2+z^2 = N$.</p>

<p>Therefore
you are looking for
the number of ways
$N$ can be represented as
the sum of 3 squares.</p>

<p>Since there are 8 octants
(for all possible signs
of $x, y, z$),
the representations
for non-zero $x, y, z$
are counted up to 8 times.</p>

<p>Look up
""sums of three squares"".</p>

<p>For a more advanced discussion,
see
www.ams.org/tran/1951-071-01/S0002-9947-1951-0042438-4/S0002-9947-1951-0042438-4.pdf
(""ON THE REPRESENTATIONS OF A NUMBER AS THE
SUM OF THREE SQUARES""
BY
PAUL T. BATEMAN)</p>
"
"2377235","2378954","<p><a href=""https://eudml.org/doc/213209"" rel=""nofollow noreferrer"">Sikorski</a> (1950) call these spaces as $\omega_\mu$-additive spaces.</p>
"
"2377241","2377255","<p>Using the integral test, for a set $a$, we see
$$\lim_{b\to\infty}\int_{a}^{b}\frac{1}{x\log(x)^{c}}dx=\lim_{b\to\infty}\left(\frac{\log(b)^{1-c}}{1-c}-\frac{\log(a)^{1-c}}{1-c}\right)$$
which goes to infinity if $c\leq 1$ and converges if $c&gt;1$. Thus,
$$\sum_{n=2}^{\infty}\frac{1}{n\log(n)^c}$$
converges if and only if $c&gt;1$. I discarded $n=1$ since $1*\log(1)^c=0$ which introduces dividing by $0$.</p>
"
"2377257","2377264","<p>Try manipulating the integrand into a geometric series:</p>

<p>$$ \dfrac{x^{a-1}}{b+x^n} = \frac{x^{a-1}}{b}\dfrac{1}{1+x^n/b} = \frac{x^{a-1}}{b}\sum\limits_{k=0}^\infty(-1)^k\left(\frac{x^n}{b}\right)^k = \frac{1}{b}\sum\limits_{k=0}^\infty(-1)^k\frac{1}{b^k}x^{nk+a-1}  $$</p>

<p>Now integrate, which we can do inside the interval of convergence. </p>
"
"2377265","2377273","<p>The definition <a href=""https://en.wikipedia.org/wiki/Axiom_of_choice"" rel=""nofollow noreferrer"">here</a> expresses the axiom of choice as</p>

<p>$$ \forall X \left[ \emptyset\not\in X \implies \exists f:X\to\bigcup X\quad
   \forall A\in X\ (f(A) \in A) \right]. $$</p>

<p>By taking $X:=\{A_n \mid n\in\mathbb{N}\}$, since each $A_n$ is nonempty, we are given a function $f:X\to\bigcup X$ such that $f(A) \in A$ for every $A \in X$. Now set $x_n:=f(A_n)$ for each $n\in\mathbb{N}$ to obtain the desired sequence.</p>

<p>In fact, this only requires countable choice because in our case $X$ is countable.</p>
"
"2377274","2377275","<p>Hint: Consider separately the cases where $k$ is even and when $k$ is odd.</p>

<p>A full answer is hidden below.</p>

<blockquote class=""spoiler"">
  <p> If $k$ is even, then $4k$ is divisible by $8$, and hence so is $(2k+1)(4k)(k+1)$.  If $k$ is odd, then $k+1$ is even and thus together with the factor of $4$ in $4k$ we again find that $(2k+1)(4k)(k+1)$ is divisible by $8$.</p>
</blockquote>
"
"2377284","2377301","<p>Suppose the line $y=m(x-m)$ touches the curve at the point $(a,b)$. Then the slope of the tangent at $(a,b)$ to the curve $(1-2x)y=1$ must be $m$ as the slope of the line $y=m(x-m)$ is $m$. So by your calculation $$\frac{2}{(1-2a)^2}=m.$$ But $(a,b)$ is a point on the line $y=m(x-m)$, whence $b=m(a-m)$. Now you have two unknowns and two equations. So find $(a,b)$.</p>
"
"2377285","2377383","<p>The integral is not convergent as $t\to\infty$. </p>

<p>See below the explicit expression of $u(x,t)$ which tends to infinity when $t\to\infty$.</p>

<p><a href=""https://i.stack.imgur.com/MQSBI.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MQSBI.jpg"" alt=""enter image description here""></a></p>
"
"2377290","2377587","<blockquote>
  <p>$P_{X,Y,Z}(x,y,z) = P_{X}(x)\cdot P_{Y}(y)\cdot P_{Z}(z)$ for all $ x,y,z$</p>
  
  <p>Why is it that we only have to check i) for 3 discrete r.v instead of all i), ii), iii), iv) ?</p>
</blockquote>

<p>If it is true for all $x$ in $X(\Omega)$ (aka the support for $P_X$), then by the Law of Total Probability, for all $y,z$ we can show that it infers:</p>

<p>$\mathsf P_{Y,Z}(y,z) {= \sum_{x\in X(\Omega)} \mathsf P_{X,Y,Z}(x,y,z) \\ = \sum_{x\in X(\Omega)} \mathsf P_X(x)\cdot \mathsf P_Y(y)\cdot \mathsf P_Z(z) \\ = \mathsf P_Y(y)\cdot \mathsf P_Z(z) }$</p>

<p>And so forth for the other combinations by symmetry.  There is also an analogous proof for continuous random variables.  Thus for <em>random variables</em> joint independence guarantees pairwise independence.</p>
"
"2377291","2377350","<p>You should be careful to work consistently within one of the equivalent frameworks for defining the Riemann integral (Darboux sums or  Riemann sums) and where convergence to the integral is based on partition mesh or refinement.</p>

<p>You started along one path correctly and then fell short of the mark.  Let's proceed as you did starting with  the definition of the integral over $[a,c]$, that for every $\epsilon &gt; 0$ there is a $\delta &gt; 0$ such that  if $P$ is a partition with $\|P\| &lt; \delta$ then </p>

<p>$$\left|S(P,f;[a,c]) - \int_a^cf(x)\, dx\right| &lt; \epsilon.$$</p>

<p>I have introduced the notation $S(P,f;[a,c])$ to indicate the interval on which the partition and sum are defined.</p>

<p>Given $\epsilon &gt; 0$ we have $\delta_1, \delta_2$ such that $\|P_i\| &lt; \delta_i$ for $i = 1,2$ implies</p>

<p>$$\left|S(P_1,f;[a,b]) - \int_a^bf(x)\, dx\right| &lt; \epsilon/3, \\ \left|S(P_2,f;[b,c]) - \int_b^cf(x)\, dx\right| &lt; \epsilon/3, $$</p>

<p>Let $M$ be an upper bound for $|f|$ and take $\delta = \min(\delta_1, \delta_2, \epsilon/6M)$.</p>

<p>Let $P = (x_0,x_1, \ldots, x_{j-1},x_j, \ldots , x_n)$ be a partition of $[a,c]$ with $\|P\| &lt; \delta.$</p>

<p>In general $P$ will not include the point $b$  (if it does the proof is easy).  I will assume WLOG that $x_{j-1} &lt; b &lt; x_j$.  </p>

<p>In this case,</p>

<p>$$S(P,f;[a,c]) = \sum_{k=1}^{j-1}f(\xi_k)(x_k - x_{k-1}) + f(\xi_j)(x_j - x_{j-1}) + \sum_{k=j}^{n}f(\xi_k)(x_k - x_{k-1}),$$</p>

<p>where $\xi_k \in [x_{k-1},x_k]$. Again WLOG I assume $x_{j-1} \leqslant \xi_j &lt; b &lt; x_j$.</p>

<p>Now we can write $f(\xi_j)(x_j - x_{j-1}) = f(\xi_j)(b- x_{j-1}) + (f(\xi_j) - f(b))(x_j - b) + f(b)(x_j - b), $ and</p>

<p>$$\begin{align}S(P,f;[a,c]) &amp;= \underbrace{\sum_{k=1}^{j-1}f(\xi_k)(x_k - x_{k-1}) + f(\xi_j)(b- x_{j-1})}_{S(P_1,f;[a,b])}\\ &amp;+  (f(\xi_j) - f(b))(x_j - b) \\ &amp;+ \underbrace{\sum_{k=j}^{n}f(\xi_k)(x_k - x_{k-1}) + f(b)(x_j- b)}_{S(P_2,f;[b,c])}\end{align}$$</p>

<p>Hence, since $|f(\xi_j) - f(b)|(x_{j} - b) &lt; 2M\delta &lt; \epsilon/3$,</p>

<p>$$\left|S(P,f;[a,c]) - \left(\int_a^b f(x) \, dx + \int_b^c f(x) \, dx\right)  \right| \\ \leqslant \left|S(P_1,f;[a,b]) - \int_a^b f(x) \, dx  \right|+ |f(\xi_j) - f(b)|(x_{j} - b) + \left|S(P_2,f;[b,c]) - \int_b^c f(x) \, dx  \right| \\ &lt; \epsilon. $$</p>

<p>Thus, $f$ is integrable over $[a,c]$ and</p>

<p>$$\int_a^c f(x) \, dx = \int_a^b f(x) \, dx  + \int_b^c f(x) \, dx. $$</p>
"
"2377300","2377369","<p>Let $G$ be a group, and let $H$ be the set containing group elements with finite conjugacy class. </p>

<p>To show that $H$ is a subgroup, consider the product $ab^{-1}$ of arbitrary elements of $H$. For any $g\in G$, we have the equality $$g(ab^{-1})g^{-1}=(gag^{-1})(gb^{-1}g^{-1}).$$ This implies that any conjugate of $ab^{-1}$ is just a product of conjugates of $a$ and $b$, of which there are only finitely many. Since there are only finitely many products, $ab^{-1}$ has a finite conjugacy class. Hence, $H$ is a subgroup.</p>

<p>Now, let $\varphi$ be some automorphism of $G$, and $a$ be an element of $H$. For any $g\in G$, there exists some $h\in G$ such that $\varphi(h) = g$. Consequently, it follows that we have the following equality for conjugation: $$g\varphi(a)g^{-1}=\varphi(h)\varphi(a)\varphi(h^{-1})=\varphi(hah^{-1}).$$ This implies that any conjugate of $\varphi(a)$ is the image of a conjugate of $a$, of which there are only finitely many. Hence, $\varphi(a)$ has finitely many conjugates, so $\varphi(a)\in H$. Therefore, $H$ is characteristic. </p>
"
"2377304","2377310","<p>An idempotent symmetric matrix of trace $n$ has eigenvalue $1$
with multiplicity $n$ and eigenvalue $0$ with multiplicity $m-n$.
Its image is a $n$-dimensional subspace of $\Bbb R^m$. This gives
one direction of the correspondence.</p>

<p>For the other it may help to note that any subspace of $\Bbb R^m$
has an orthogonal basis.</p>
"
"2377313","2377457","<p>You made two errors:</p>

<ol>
<li>You did not account for the fact that there are only $9$ choices for the leading digit when neither number in the pair of consecutive equal even digits is in the thousands place.</li>
<li>You have subtracted numbers in which there are three or more consecutive even digits more than once.</li>
</ol>

<p>First, we observe that there are $9 \cdot 10 \cdot 10 \cdot 10 = 9000$ four-digit positive integers.</p>

<p>Let $A_1$ be the set of four-digit positive integers in which the thousands place and hundreds place contain equal even digits.  Let $A_2$ be the set of four-digit positive integers in which the hundreds place and tens place contain equal even digits.  Let $A_3$ be the set of four-digit positive integers in which the tens place and units place contain equal even digits.  Then $A_1 \cup A_2 \cup A_3$ is the set of four-digit positive integers that do contain consecutive equal even digits.  By the <a href=""https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle"" rel=""nofollow noreferrer"">Inclusion-Exclusion Principle</a>, the number of four-digit positive integers that do contain consecutive even digits is 
$$|A_1 \cup A_2 \cup A_3| = |A_1| + |A_2| + |A_3| - |A_1 \cap A_2| - |A \cap A_3| - |A_2 \cap A_3| + |A_1 \cap A_2 \cap A_3|$$</p>

<p>$|A_1|$:  Since we cannot use zero, there are four ways of choosing the even digit that occupies both the thousands and hundreds place.  There are ten choices for each of the remaining digits.  Hence, $|A_1| = 4 \cdot 10 \cdot 10 = 400$.</p>

<p>$|A_2|$:  Since we cannot use zero, the thousands place can be filled in nine ways.  There are five ways to choose the even digit that fills both the hundreds and tens places.  There are ten ways to fill the units place.  Hence, $|A_2| = 9 \cdot 5 \cdot 10 = 450$.</p>

<p>$|A_3|$:  We can fill the thousands place in nine ways and the hundreds place in ten ways.  There are five ways to choose the even digit that occupies both the tens and units places.  Hence, $|A_3| = 9 \cdot 10 \cdot 5 = 450$.</p>

<p>$|A_1 \cap A_2|$:  Since we cannot use zero, there are four ways to choose the 
even digit that occupies the thousands, hundreds, and tens places.  There are ten ways to fill the units place.  Hence, $|A_1 \cap A_2| = 4 \cdot 10 = 40$.</p>

<p>$|A_1 \cap A_3|$:  There are four ways to choose the even digit that occupies both the thousands and hundreds places and five ways to choose the even digit that occupies both the tens and units places.  Hence, $|A_1 \cap A_3| = 4 \cdot 5 = 20$.</p>

<p>$|A_2 \cap A_3|$:  There are nine ways to fill the thousands place.  There are five ways to choose the even digit that occupies the hundreds, tens, and units places.  Hence, $|A_2 \cap A_3| = 9 \cdot 5 = 45$.</p>

<p>$|A_1 \cap A_2 \cap A_3|$:  Since we cannot use zero, there are four ways to choose the even digit that occupies the thousands, hundreds, tens, and units places.  Hence, $|A_1 \cap A_2 \cap A_3| = 4$. </p>

<p>Therefore, the number of four-digit positive integers that do contain consecutive even equal digits is 
\begin{align*}
|A_1 \cup A_2 \cup A_3| &amp; = |A_1| + |A_2| + |A_3| - |A_1 \cap A_2| - |A \cap A_3| - |A_2 \cap A_3| + |A_1 \cap A_2 \cap A_3|\\
                        &amp; = 400 + 450 + 450 - 40 - 20 - 45 + 4\\
                        &amp; = 1199
\end{align*}
Therefore, the number of four-digit positive even integers that do not contain consecutive equal even digits is 
$9000 - 1199 = 7801$.</p>
"
"2377314","2377317","<p>By <a href=""https://math.stackexchange.com/questions/209268/using-generating-functions-find-the-sum-13-23-33-dotsb-n3"">this</a> post, the generating function for $n^3$ is
$$
\sum_{n\ge 0}n^3x^n = \frac{x(1+4x+x^2)}{(1-x)^4}. 
$$
This was obtained by the standard trick of applying $(xD)^3$ (differentiating and multiplying $x$) to the generating function $\sum_{n\geq0}x^n = \frac1{1-x}$. Then, we can just evaluate this at $1/2$.</p>
"
"2377324","2377328","<blockquote>
  <p>I get a complicated expression</p>
</blockquote>

<p>That's still a symmetric expression in $\alpha,\beta,\gamma$ so you can express it in terms of the elementary symmetric polynomials $\alpha+\beta+\gamma=-72/36\,$, $\alpha\beta+\alpha\gamma+\beta\gamma=23/36\,$, $\alpha\beta\gamma=5/36\,$.</p>

<blockquote>
  <p>is there an easier way to solve this</p>
</blockquote>

<p>Let $\,y = 6x - 1\,$ and substitute $\,x = (y+1)/6\,$ into the original equation, then expand and collect. The resulting polynomial in $y$ will have precisely the roots $6\alpha-1, 6\beta-1, 6\gamma-1\,$.</p>
"
"2377335","2377341","<p>You're correct. Here is some facts that my help you in the future: arbitrary intersection of closed sets is closed, and arbitrary union of open sets is open. Therefore
$$ \bigcap_{n \in \mathbb{Z}_+}[x,a_n]$$
should be closed, and equals $[x,y]$ if $a_n &gt; y$ for all $n$ and $a_n \to y.$ Similarly
$$ \bigcup_{n \in \mathbb{Z}_+}[x,b_n)=[x,y)$$
if $x&lt;y,$ $b_n &lt;y$ and $b_n \to y.$ (The above is only half-open, but the argument works and show that the union should be half-open as well.)</p>

<p>As you note, arbitrary intersections of open sets need not be open, and arbitrary unions of closed sets need not be closed.</p>
"
"2377361","2377387","<p>A nice way to do this: </p>

<p>First note that the number of ways to pick two disjoint subsets $A, B \subseteq \{ 1, 2, \ldots, n \}$ is $3^n$, because for every $i \in \{ 1, 2, \ldots, n \}$ we choose whether it belongs to $A$ or $B$ or neither. If we want the sets to be distinct, we should subtract $1$ for the case $A = B = \varnothing$, so we get $3^n - 1$.</p>

<p>Now to pick $A, B \subseteq U$ such that $|A \cap B| = 2$ we can:</p>

<ul>
<li>First select $I \subseteq U$ of size $2$ to be the intersection - we can do this in $\binom{5}{2} = 10$ ways,</li>
<li>Then select disjoint and distinct $A \setminus I, B \setminus I \subseteq U \setminus I$ - we can do this in $3^3 - 1 = 26$ ways.</li>
</ul>

<p>So the total number of ways is $10 \cdot 26 = 260$. </p>
"
"2377368","2377429","<p>You want to show that $\overline{\bigcap A_i} \subseteq \bigcup \overline{A_i}$. As @MauroALLEGRANZA says in the comments, to do this you should pick $x \in \overline{\bigcap A_i}$ and show that it follows that $x \in \bigcup \overline{A_i}$. However, your first statement is ""Let $x \in \bigcap A_i$""; that is, you've picked $x \notin \overline{\bigcap A_i} $.  This is kind of like trying to show that all dogs are mammals by saying, ""Let's first take an arbitrary bird...""</p>

<p>The straightforward approach to this is to allow $x \in \overline{\bigcap A_i}$.  This simply means $x$ is not in every $A_i$.  See if you can take it from here. If not, see below.</p>

<blockquote class=""spoiler"">
  <p> \begin{align} x\in \overline{\bigcap A_i} &amp;\implies  x \notin \bigcap A_i \\ &amp; \implies \exists \: A_i \text{ such that } x \notin A_i\\  &amp; \implies \exists \: A_i \text{ such that } x\in \overline{A_i}\\ &amp;\implies x\in \bigcup \overline{A_i}\end{align}</p>
</blockquote>

<p>Hope this helps!</p>
"
"2377370","2377390","<p>I think it can be solved in this way:</p>

<p>We have the condition $n^n&lt;10^G$</p>

<p>Since we are talking about a large number, it is safe to do:
$\log_{10}(n^n) &lt; \log_{10} (10^G)$</p>

<p>Simplifying a little gives us $n\log_{10}(n) &lt; 10^{100}$ since $G = 10^{100}$</p>

<p>Taking log again on both sides will give us: $\log_{10}(n\log_{10}(n)) &lt; 100$ which can be written as $\log(n) + \log(\log(n)) &lt; 100\ \ ...(1)$</p>

<p>Now let $x$ be the number of digits in the number $n$, then $x \approx \log_{10}(n)$</p>

<p>We can rewrite equation 1 above as $x + \log(x) &lt; 100$ for which the largest value of x that satisfies the equation is 98.</p>

<p>Edit: (correction as suggested by Arthur)</p>

<p>The number then would be $n \approx 1.02 \times 10^{98}$ which will then have 99 digits. So the answer would be 99</p>
"
"2377381","2377463","<p>By <a href=""https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle"" rel=""nofollow noreferrer"">sieving</a>, show that $$\sum_{n \le x, \text{lpf}(n) &gt; k} 1 = \sum_{m \in A_k} \mu(m) \lfloor x/m \rfloor$$ where $\text{lpf}$ is the least prime factor and $A_k$ is the set of integers whose largest prime factor is $\le k$. </p>

<p>Then note $$\sum_{n \le x, \text{lpf}(n) &gt; \sqrt{x}} 1 = \pi(x)-\pi(\sqrt{x})$$</p>

<p>The <a href=""https://en.wikipedia.org/wiki/M%C3%B6bius_function"" rel=""nofollow noreferrer"">MÃ¶bius function</a> is defined by $\mu(m) = (-1)^j$ if $m$ is a product of $j$ different primes, $\mu(m) = 0$ otherwise.</p>
"
"2377382","2377437","<p>Yes, there are infinitely many positive integer solutions with $x = b(b+1)^2, y = b^2 (b+1), n=3$.  To be fair, I don't think Wolfram Alpha actually claims to find all solutions.</p>
"
"2377389","2377764","<p>Convert to polar form $r^2=\dfrac{16}{\cos 4 t+3}$</p>

<p>The area is $$A=\frac{1}{2} \int_0^{2 \pi } \frac{16}{\cos 4 t+3} \, dt=32 \int_0^{\frac{\pi }{2}} \frac{1}{\cos 4 t+3} \, dt$$</p>

<p>$\cos 4t=\cos^2 2t-\sin^2 2t=2\cos^2 2t -1$</p>

<p>so we have to integrate</p>

<p>$\int \dfrac{dt}{2(\cos^2 2 t+1)}=\dfrac{1}{2}\int \dfrac{dt}{1+\cos^2 2t}$</p>

<p>Remember the identity $\cos 2t =\dfrac{1-\tan^2 t}{1+\tan^2 t}$</p>

<p>with the substitution $\tan t=u;\;t=\tan^{-1}u;\;dt=\dfrac{du}{1+u^2};\;\cos 2t = \dfrac{1-u^2}{1+u^2}$ </p>

<p>Limits of integration become $t=0\to u=0;\;t=\pi/2\to u=\infty$</p>

<p>$$\dfrac{1}{2}\int \dfrac{dt}{1+\cos^2 2t}=\dfrac{1}{2}\int \dfrac{\dfrac{du}{1+u^2}}{1+\left(\dfrac{1-u^2}{1+u^2}\right)^2}$$
Which simplifies to
$$\dfrac{1}{2}\int \frac{u^2+1}{ u^4+1} \, du=\frac{\tan ^{-1}\left(\sqrt{2} u+1\right)-\tan ^{-1}\left(1-\sqrt{2} u\right)}{4 \sqrt{2}}$$
Area is
$$A=32\left[\frac{\tan ^{-1}\left(\sqrt{2} u+1\right)-\tan ^{-1}\left(1-\sqrt{2} u\right)}{4 \sqrt{2}}\right]_0^{\infty}=4 \sqrt{2} \pi$$</p>
"
"2377391","2377448","<p>I claim that $\{g(n,x):n\in\mathbb N\}$ is a base for the neighborhood system of $x.$ Otherwise, there is a neighborhood $U$ of $x$ such that $g(n,x)\not\subseteq U$ for all $n\in\mathbb N.$ Thus, for each $n\in\mathbb N,$ we can choose a point $x_n\in g(n,x)\setminus U.$ Then $x$ is not a cluster point of the sequence $\{x_n\},$ contrary to (C).</p>
"
"2377392","2377396","<p>If there is no typo in the expression (you typed what you meant), then note that $\int x dt = tx$ modulo constants (i.e. $\int x dt$ having the form $tx + K$ where $K \in \mathbb{R}$) by the definition of primitive, where $x$ is supposed to be a given number. Then you surely can do the rest re-writing-in-another-form job. If there is some typo in the expression, most likely you mean $\int x dx$ instead of $\int x dt$, then note that $\int x dx = x^{2}/2$ modulo constants by the definition of primitive. You can do the rest.</p>
"
"2377399","2377400","<p>$$f(x) = (x - \frac{7}{4})g(x)$$
is the same as $$f(x) = \frac{1}{4}(4x - 7)g(x)$$</p>

<p>Thus $4x - 7$ is a factor of $f(x)$.</p>
"
"2377405","2377411","<p>In statistics a sum of a similar form may have appeared when studying covariance. To make it simple, consider the sum $\sum_{1 \leq i &lt; j \leq n}a_{i}a_{j}$. You can fix every $i$ and let $j$ run to get the sum, which is $=$
$$
(a_{1}a_{2} + a_{1}a_{3} + \cdots + a_{1}a_{n}) + (a_{2}a_{3} + a_{2}a_{4}+\cdots +a_{2}a_{n}) + \cdots + (a_{n-1}a_{n}).
$$</p>
"
"2377406","2377423","<p>The Residue Theorem is usually used to evaluate integrals in terms of residues, but here you'll use it to write a residue as an integral (let's say around a circle around $z_0$ within $\Omega$).  The circle is a compact set, so $f_n$ converges uniformly to $f$ on it.  Therefore...</p>
"
"2377410","2377465","<p>The lower limit and the upper limit of $x_2$ are $-8$ and $\frac12x_1-8$ respectively. That means that $\frac12x_1-8\geq -8\Rightarrow \frac12x_1 \geq 0\Rightarrow x_1\geq 0$ Therefore the calculation is </p>

<p>$$P(8- \frac{1}{2}x_{1}+x_{2}&lt;0)=\int_{0}^8 \frac{1}{16} \frac{x_1}{32} \, dx_1 $$</p>
"
"2377412","2377433","<p>$$\lim_{x\to \infty} {\ln x \over x} =\lim_{x\to \infty} \ln x^{1/x} = \ln\left( \lim_{x\to \infty}  x^{1/x}\right) = \ln 1 = 0$$</p>

<p>Limit used : <a href=""https://math.stackexchange.com/questions/115822/how-to-show-that-lim-n-to-infty-n-frac1n-1"">How to show that $\lim_{n \to +\infty} n^{\frac{1}{n}} = 1$?</a></p>
"
"2377426","2377451","<p>Every bounded ball in the dual space is $w^{*}$ compact by the Banach-Alaoglu theorem, in particular, every bounded net has a $w^{*}$-convergent subnet. The state $\omega_n$ is bounded as an average of states, and so it has a convergent subnet in the $w^{*}$ topology, which implies that the sequence has a limit point in the $w^{*}$ topology. I don't think you can assert that the sequence converges itself, though.</p>
"
"2377427","2377455","<p>If ""optimised"" means ""the day with fewest employees at work should have as many employees as possible"", and vacations are required to be three consecutive weeks, then I think your best bet is to make four overlapping batches (one starting in the beginning of the first week, one in the beginning of the second or third, one beginning in the beginning of the fourth or fifth, and one beginning in the beginning of the sixth week). Then send three employees on vacation each batch.</p>

<p>That way, only two batches overlap at any time, and you have at least half of your employees on call each day (not counting sick leave or similar). I don't think you can get five batches without triple overlap somewhere.</p>

<p>If you can distribute single weeks, then you can get more people on site at any time. For instance, consider having a list of the employees, and the first week give vacation to the first five on the list. The next week to the next five, and so on, looping around when you reach the bottom. Then you have seven employees on call every day, and the last of the eight weeks you even have eleven employees on call, so you can make up for that by giving only four employees vacation some of the weeks (four of the weeks, in fact, with five employees the other four).</p>

<p>Of course, once you have used this scheme to decide who takes vacation simultaneously each week, you can shuffle the weeks themselves around so that the employees get more consecutive vacations (you can guarantee at least one two-week vacation to all of your employees, and a three-week to some).</p>

<p>Since there are a total of $3\cdot 5\cdot 12 = 180$ vacation days you need to give to your employees, and only $8\cdot 5 = 40$ days to distribute them on, you cannot get away with only giving vacation to four employees at a time, even by giving single days of vacation. You need at least five employees to have vacation simoltaneously some of the time (otherwise you can only reach a total of $160$ vacation days). So this is the best possible.</p>
"
"2377445","2377538","<p>The easiest way for two permutations to commute is to have disjoint support. (To use different symbols.) So, once you've got $a$ and $b$ figured out, with support $\{1,2,3\}$, and you want $c$ to commute, the easiest way to do that is to set $c=(45)$.</p>
"
"2377452","2377696","<p>Note that
$$
\begin{align}
\sum_{k=1}^{n-1}\left[\log\left(1+\frac{z}{k}\right)-\frac{z}{k}\right]
&amp;=\log\left(\frac{\Gamma(n+z)}{\Gamma(n)\Gamma(1+z)}\right)-zH_{n-1}\\
&amp;=\log\left(\frac{\Gamma(n+z)}{\Gamma(n)\Gamma(1+z)}\right)-z\log(n)-z\gamma+O\!\left(\frac{z}{n}\right)
\end{align}
$$
Taking the limit as $n\to\infty$, and using <a href=""https://math.stackexchange.com/a/103028"">Gautschi's Inequality</a>, we get
$$
\sum_{k=1}^\infty\left[\log\left(1+\frac{z}{k}\right)-\frac{z}{k}\right]
=-\log(\Gamma(1+z))-z\gamma
$$
Therefore,
$$
\frac1{\Gamma(1+z)}=e^{z\gamma}\prod_{k=1}^\infty\left(1+\frac{z}{k}\right)e^{-\frac{z}{k}}
$$
The product on the right hand side converges for all $z$ and is $0$ at the negative integers.</p>

<p>Thus, $\frac1{\Gamma(1+z)}$ is an entire function and $\Gamma(1+z)=z!$ is never $0$.</p>
"
"2377454","2377478","<p>There are infinitely many ""tensor irreducible"" representations of any group. Just take the direct sum of $p$ copies of the trivial module, for any prime $p$.</p>

<p>One-dimensional representations give non-unique tensor decompositions in trivial ways (if $U$ and $V$ are representations with $U$ one-dimensional, then $U\otimes U^*\otimes V\cong V$), so you probably want uniqueness up to one-dimensional factors.</p>

<p>But even then, the alternating group $A_5$ has no one-dimensional representation apart from the trivial one, but it has two three-dimensional representations $U$ and $U'$ and a five-dimensional representation $V$ with $U\otimes V\cong U'\otimes V$.</p>
"
"2377456","2377460","<p>$\frac{23}{12} \pi = 2\pi - \frac{\pi}{12}$, and $\frac{13}{12} \pi = \pi + \frac{\pi}{12}$. So you have:</p>

<p>$$\sin^4 \frac{\pi}{12} - \cos^4 \frac{\pi}{12}$$</p>

<p>which surprisingly gives the same answer you reached.</p>
"
"2377458","2377462","<p><strong>NO</strong>.</p>

<hr>

<p>Take $X=\mathbb R$ and $Y=\mathbb R^2$.</p>

<p>Now, define $T(x)=(x,0)$ and define $S((x_1,x_2))=x_1$</p>

<p>Then, for all $x\in X$, you have $$S(T(x)) = S((x,0)) = x$$</p>

<p>However, it is not true that $T(S(y))=y$ is true for all $y\in Y$. For example, for $y=(1,1)$ you have $$T(S(y)) = T(S((1,1))) = T(1) = (1,0)\neq (1,1)=y$$</p>
"
"2377464","2377604","<p>If $x$ is a frontier point of $I^2$ and $F(x)$ is not a frontier point of $D$, then there is an open set around $F(x)$ contained in $\mbox{Int } D$.  The inverse image of that open set has to contain $x$, but is in $\mbox{Int } I^2,$ contradiction. </p>

<p>Since the Jacobian is positive, we know the orientation is preserved.  </p>
"
"2377466","2377484","<p>Here is a lesser known approach to these things, which sometimes helps</p>

<p>Let $xy+yz+zx=p$ so that $x^2+y^2+z^2=(x+y+z)^2-2p=25-2p$.</p>

<p>Set $S_3=a^3+b^3+c^3$</p>

<p>Then clearing fractions in the second equation gives $abc=5p$.</p>

<p>We have also that $a,b,c$ are roots of the polynomial equation $$q(x)=x^3-5x^2+px-5p=0$$</p>

<p>Now observe that $0=q(a)+q(b)+q(c)=S_3-5(a^2+b^2+c^2)+p(a+b+c)-15p$ so that $$S_3-125+10p+5p-15p=0$$ and $S_3=125$</p>

<hr>

<p>Observe also that we have, with $$S_r=a^r+b^r+c^r$$$$a^{n-3}q(a)+b^{n-3}q(b)+c^{n-3}q(c)=0=S_n-5S_{n-1}+pS_{n-2}-5pS_{n-3}$$which gives us the sums of powers in terms of $p$ using the recurrence relation (note that $S_0=3, S_{-1}=\frac 15$).</p>
"
"2377468","2377483","<p>For a set $A=\{a_n~:~n\in\mathbb N\}$ the supremum and infimum is not the same as the limes superior or limes inferior of the sequence $(a_n)_n$. You might compute a few elements 
\begin{align}
A&amp;=\left\{1-\frac11,1+\frac12,1-\frac13,1+\frac14,1-\frac15,1+\frac16,1-\frac17\ldots\right\}\\
&amp;=\left\{0,\frac32,\frac23,\frac54,\frac45,\frac76,\frac67,\ldots\right\}
\end{align}
The supremum of $A$ is the smallest upper bound of $A$. You can see that it might be $\frac32$ since it seems that all other elements are less then $\frac32$. But you need to prove it. This is given by
$$
\frac32=1+\frac12\geq 1+\frac1n\geq 1+\frac{(-1)^n}{n} \text{ for all }n\geq 2
$$
and $\frac32&gt;0=1+\frac{(-1)^1}1$. Therefore $\frac32$ is an upper bound of $A$ and since $\frac32\in A$ it is the smallest upper bound.<p>
Same way you can argue why $0$ is the biggest lower bound of $A$.</p>
"
"2377469","2377723","<p>As mentioned in some of the comments, those diagrams take place in the category of sets and relations, $\operatorname{Rel}$. $\operatorname{Rel}$ isn't ""just"" a category: it's enriched over posets. That is, there isn't just a <em>set</em> of relations between sets $A$ and $B$, there's a <em>poset</em> of them (ordered by $\subseteq$).</p>

<p>The inequalities in the diagrams mean that they aren't intended to be commutative diagrams. Instead, they mean that the composition of relations on the left of the $\leq$ sign is contained as a subset of the composition of relations on the right (rather than equal to).</p>
"
"2377470","2377481","<p>But $Int(X_n)=X_n$, because for $1$ there is an open neighbourhood namely $X_n$ with $X_n\subseteq X_n$.</p>
"
"2377475","2377839","<p>I'm going to deal with the general case for any odd number of vertices and any starting sum (with noted exceptions), because I like giving general solutions :)</p>

<p>Let there be $n$ vertices (where $n$ is odd), labelled $0$ through $n-1$, with vertex $r$ having an associated integer $x_{r}$, and with $\Sigma x = \sum_{r=0}^{n-1}x_{r}$. In your case, $n=5$ and $\Sigma x=2011$.</p>

<p>I also presume that $x_{r}$ can be negative, since the question merely states ""integer"", and because otherwise there are some states that are unwinnable - for example, in your scenario, any state with one vertex having $2010$ and any other having $1$ is unwinnable if $x_{r}\ge0$ is enforced.</p>

<h2>Method:</h2>

<ol>
<li><p>Find some function $I$ of $\{x_{r}\}$ that is invariant under the allowed transformations of $\{x_{r}\}$. These transformations take the form:
$$\begin{align}x_{r} &amp;\rightarrow x_{r}-m\\x_{r+1} &amp;\rightarrow x_{r+1}-m\\x_{r+k+1} &amp;\rightarrow x_{r+k+1}+2m\end{align}$$
where $n=2k+1$ and indices are considered modulo $n$. Try building a quantity that is invariant modulo $n$ - you don't need to get too complicated, a linear combination of $\{x_{r}\}$ will suffice. This invariant will never change throughout the game, so it's a useful quantity to know.</p></li>
<li><p>Consider the $n$ possible winning states (one for each vertex), and the corresponding values of your invariant - $I_{r}$ being the value of the invariant for the win state where only $x_{r}$ is non-zero. If there are no repeated members in $\{I_{r}\}$, then any initial state can have at most one winning state, since there is at most one winning state you can reach. If $I \notin \{I_{r}\}$, then you cannot win from this state, since you can only reach states that have invariant $I$, and no winning state has invariant $I$. To answer the question, we need $\{I_{r}\}$ to comprise distinct elements, and to contain every possible value of $I$ - hence every state leads to one and only winning state.</p></li>
<li><p>Now plug in values to solve for the specific case (admittedly, I solved for the specific case first then generalised, you might find it easier to do the process with specific values of $n$ and $\Sigma x$ in mind).</p></li>
</ol>

<h2>Answer in spoilers below:</h2>

<p>Part 1 (picking an invariant):</p>

<blockquote class=""spoiler"">
  <p> Choose the function $I=(\sum_{r=0}^{n-1}rx_{r})\ (\mathrm{mod}\ n)$, which transforms to $I - rm - (r+1)m + (r+k+1)2m\ (\mathrm{mod}\ n) = I + (2r+2k+2-2r-1)m\ (\mathrm{mod}\ n) = I + (2k+1)m\ (\mathrm{mod}\ n) = I$<br>and is hence invariant since it didn't matter what $r$ or $m$ was. It doesn't matter ultimately which vertex is vertex $0$ - while $I$ will depend on the labelling of vertices, it will still indicate the same winning vertex - proof after part 2.</p>
</blockquote>

<p>Part 2 (using invariant to deduce possible winning states):</p>

<blockquote class=""spoiler"">
  <p> The function's codomain is the set $\mathbb{Z}_{n}$, the integers modulo $n$, and we have $I_{r} = r\Sigma x\ (\mathrm{mod}\ n)$. For any $\Sigma x$ coprime with $n$, $\{I_{r}\}$ will be a set equivalent to $\mathbb{Z}_{n}$; the winning state is then the $I_{r}$ that equals $I$ - i.e. the winning vertex is $r = I\cdot(\Sigma x)^{-1} \ (\mathrm{mod}\ n)$, where $(\Sigma x)^{-1}$ is the multiplicative inverse of $\Sigma x$ modulo $n$ - hence why we require $\Sigma x$ coprime to $n$ as otherwise $(\Sigma x)^{-1}$ does not exist.</p>
</blockquote>

<p>Proof why labelling doesn't matter (uses results from parts 1 and 2):</p>

<blockquote class=""spoiler"">
  <p> To prove why the choice of which vertex is vertex $0$ is irrelevant, increment all indices by some amount $a$ (and then reduce modulo $n$). $I$ will then increase by $a\Sigma x\ (\mathrm{mod}\ n)$, thus the winning vertex's index will increase by $a\Sigma x(\Sigma x)^{-1}\ (\mathrm{mod}\ n) = a$, so the winning vertex is the same. As long as you label the vertices consecutively, it doesn't matter which vertex you start from.</p>
</blockquote>

<p>Part 3 (solution for specific case):</p>

<blockquote class=""spoiler"">
  <p> Now we have the general solution, in your specific case, all we need show is that $2011$ is coprime with $5$; it clearly is, and in fact $2011 = 1\ (\mathrm{mod}\ 5)$, which makes life very easy as its multiplicative inverse is just $1$. Now just compute $I = x_{1}+2x_{2}+3x_{3}+4x_{4}$, and then $I\ (\mathrm{mod}\ 5)$ gives you the winning vertex.</p>
</blockquote>
"
"2377480","2377501","<p>If $D$ is a simply connected domain, which contains no zero of $\sin$, then $1/\sin z$ has an antiderivative on $D$.</p>
"
"2377485","2377695","<p>Assuming that your expression contains a typo :)</p>

<p>Actually we have to simplify
$$\frac{5 \sqrt{5}}{4\left(\sqrt{3\sqrt{5}}-\sqrt{2\sqrt{5}}\right)}- \frac{4 \sqrt{5}}{\sqrt{3\sqrt{5}}-\sqrt{2\sqrt{5}}}$$</p>

<p>Notice that both fractions can be simplified multiplying numerator and denominator by </p>

<p>$\left(\sqrt{3\sqrt{5}}+\sqrt{2\sqrt{5}}\right)$</p>

<p>Indeed 
$$\left(\sqrt{3\sqrt{5}}-\sqrt{2\sqrt{5}}\right)\left(\sqrt{3\sqrt{5}}+\sqrt{2\sqrt{5}}\right)=3\sqrt{5}-2\sqrt{5}=\sqrt{5}$$
the expression becomes
$$\frac{\left(5 \sqrt{5}\right)\left(\sqrt{3\sqrt{5}}+\sqrt{2\sqrt{5}}\right)}{4\sqrt 5}- \frac{4 \sqrt{5}\left(\sqrt{3\sqrt{5}}-\sqrt{2\sqrt{5}}\right)}{\sqrt 5}=\\=\frac{5 \left(\sqrt{3\sqrt{5}}+\sqrt{2\sqrt{5}}\right)}{4}- 4\left(\sqrt{3\sqrt{5}}-\sqrt{2\sqrt{5}}\right)=\frac{21 \sqrt{2 \sqrt{5}}}{4}-\frac{11 \sqrt{3 \sqrt{5}}}{4}$$</p>

<p>If the $\sqrt{7}$ is not a typo the numerator is a bit more complicated, but the basics are always the same </p>
"
"2377487","2377498","<p>You can't get rid of the square. You can write your function as
$$f(x)=\frac{A}{1-x}+\frac{B}{1+x}+\frac{C}{(1+x)^2},$$ and use
$$\frac{1}{(1+x)^2}=-\frac{d}{dx}\frac{1}{1+x}.$$
Does that help?</p>
"
"2377494","2377504","<p>$$\frac{|a|}{1+|a|}+\frac{|b|}{1+|b|}\geq\frac{|a|}{1+|a|+|b|}+\frac{|b|}{1+|a|+|b|}=$$
$$=\frac{|a|+|b|}{1+|a|+|b|}=1-\frac{1}{1+|a|+|b|}\geq1-\frac{1}{1+|a+b|}=\frac{|a+b|}{1+|a+b|}$$</p>
"
"2377497","2377514","<p>It might help if you think about a slightly simpler problem - how many pairs of positive integers $b,c$ satisfy $bc \le n$ for $b \le c$ and $n$ being some other positive integer. You should be able to express this as a sum - find the number of possible values of $c$ for a given $b$, then sum over all possible values of $b$. Note that $b^{2} \le bc \le n$.</p>

<p>Then you just go the next step up - sum this over all possible values of $a$, with $n = 1000/a$. $a$ can only take one of a few values, which you can again find since $a^{3} \le abc \le 1000$. You should get a nested sum, which I believe you'd then just have to compute (but it might simplify).</p>
"
"2377506","2377520","<p>Let $X$ be a non-negative non (almost surely) constant random variable whose expectation is positive. Then there exists a positive $\varepsilon$ such that 
$$\mathbb P\left\{\frac X{\mathbb E\left[X\right]} \geqslant 1+\varepsilon             \right\}\gt 0 .$$ 
Otherwise, we would have for any $n$ that 
$$\mathbb P\left\{\frac X{\mathbb E\left[X\right]} \geqslant 1+\frac 1n            \right\}= 0$$
hence 
$$\mathbb P\left\{\frac X{\mathbb E\left[X\right]} \gt 1            \right\}= 0.$$
As a consequence, defining $Y:= \mathbb E\left[X\right]-X$, the random variable $Y$ is non-negative and its expectation is zero. Therefore, $X$ would be almost surely equal to its expectation, which is excluded.</p>

<p>Now, since $X/\mathbb E\left[X\right]$ is non-negative, it follows that<br>
$$\mathbb E\left[\left( \frac X{\mathbb E\left[X\right]}\right)^n\right]   \geqslant \mathbb E\left[\left( \frac X{\mathbb E\left[X\right]}\right)^n\mathbf 1  \left\{\frac X{\mathbb E\left[X\right]} \geqslant 1+\varepsilon             \right\}\right] \geqslant \left(1+\varepsilon\right)^n\mathbb P \left\{\frac X{\mathbb E\left[X\right]} \geqslant 1+\varepsilon             \right\}  \to +\infty.    
    $$ </p>
"
"2377533","2377616","<p>As I understand it, this is partially a notation question about set
builder notation. If there is no specified set which the
expression of the notation is iterating over (such as
the positive integers in $\{x \in \mathbb Z_{&gt;0}: x &lt; 10\}$)
the notation is frankly ambiguous.</p>

<p>So supposing there is some universal set $S$, this is how I read your definition of $B|_a$.</p>

<p><em>For a fixed $a$,
for all $b$ in $S$,
$b \in B|_a$ if
and only if  $a \cdot b \in A$.</em></p>

<p>So if $A=\{1, 1\cdot 1,1\cdot 2,1\cdot 3\}$, and $S = \{1,2,3\}$,
then $B|_1$ will be precisely $\{1,2,3\}$.</p>

<p>The other part of your question, if you wish to describe a set $B'$ such that</p>

<p><em>For a fixed $a$, if $b \in B'$ then
$a \cdot b \in A$.</em></p>

<p>You are then not defining a single set but a collection of sets.
The property that defines them is $B' \subseteq B|_a$.</p>
"
"2377534","2377539","<p>From additivity: $T(0) + T(0) = T(0+0) = T(0)$, so $T(0) = 0$.</p>

<p>For any positive integer $n$: </p>

<ul>
<li><p>$T( n \cdot v ) = T( \underbrace{v+\ldots+v}_n ) = \underbrace{T(v)+\ldots+T(v)}_n = n \cdot T(v)$</p></li>
<li><p>$T( -n \cdot v ) + T( n \cdot v ) = T( -nv + nv ) = T(0) = 0$, so $T( -n \cdot v ) = -T(n \cdot v) = -n \cdot T(v)$.</p></li>
</ul>

<p>Thus $T(k \cdot v) = k \cdot T(v)$ for every integer $k$.</p>

<ul>
<li><p>By the above $T(v) = T\left( n \cdot \frac{1}{n} v \right) = n \cdot T \left( \frac{1}{n} v \right)$, therefore $T\left( \frac{1}{n} v \right) = \frac{1}{n} T(v)$</p></li>
<li><p>Finally $T \left( \frac{m}{n} v \right) = T \left( m \cdot \frac{1}{n} v \right) = m \cdot T \left( \frac{1}{n} v \right) = \frac{m}{n} \cdot T(v)$, </p>

<p>i.e. $T(\alpha v) = \alpha T(v)$ for each $\alpha \in \mathbb{Q}$.</p></li>
</ul>
"
"2377543","2377559","<p>Consider any grid $\{a_1,\ldots,a_n\}$ over $[0,1]$. Then</p>

<p>$$ \frac{1}{n} \sum \log a_i = \log (\sqrt[n]{\Pi a_i}) \le \log (\frac{1}{n}\sum a_i) $$</p>

<p>Where the inequality follows from the AM-GM and $\log$ being a monotonically increasing function.</p>
"
"2377545","2377568","<p>You seem to have misunderstood what the ""oblique manifold"" is, at least according to <a href=""http://manopt.org/manifold_documentation_oblique.html"" rel=""nofollow noreferrer"">this source</a>.  Until today, I hadn't heard of the term ""oblique manifold"".</p>

<p>In any case, a notable thing about the Euclidean sphere
$$
S^{n-1} = \{x \in \Bbb R^n : x^Tx = 1\}
$$
is that it is invariant under orthogonal changes of basis.  A consequence, as it turns out, is that $\{x \in \Bbb R^n : x^Tx = 1 \wedge v^Tx = 0\}$ will be ""the same manifold"", no matter which non-zero $v$ is chosen.</p>

<p>With that in mind, taking $v = e_n = (0,\dots,0,1)$ gives us the set 
$$
\{x \in \Bbb R^n : x^Tx = 1 \wedge e_n^Tx = 0\} = \{(x_1,\dots,x_{n-1},0): x_i \in \Bbb R, x_1^2 + \cdots + x_{n-1}^2 = 1\}
$$
In other words: we may regard the above set as an embedding of $S^{n-2}$ in the hyperplane orthogonal to $e_n$.  Similarly, your set is an embedding of $S^{n-2}$ in the hyperplane orthogonal to $\mathbf 1$.</p>
"
"2377565","2377584","<p>Given any $x &gt; 1$, the author would like to show that there is some integer $n \geq 1$ such that $x &gt; 1+1/n^{2}$. Note that $x &gt; 1 + 1/n^{2}$ if $x-1 &gt; 1/n^{2}$, if $n^{2} &gt; 1/(x-1)$, and if $n &gt; 1/\sqrt{x-1}$. The number $\lceil 1/\sqrt{x-1} \rceil$ is an integer $\geq 1$ and is $&gt; 1/\sqrt{x-1}$. So it is intuitive to choose the number.</p>
"
"2377566","2377576","<p>The multiplication on the naturals is associative and commutative :</p>

<ul>
<li>Associative :
$$a*(b*c)=(a*b)*c$$</li>
<li>Commutative :
$$a*b=b*a$$</li>
</ul>

<p>So you get $$(c*d)*(c*d)=c*d*c*d=c*c*d*d=c^2*d^2$$</p>
"
"2377567","2377591","<p>I assume $E(-)$ denotes injective envelopes, and $I \leq R$ means $I$ is a left ideal of $R$.</p>

<p>Recall Lemma 25.2 on page 290 in Anderson-Fuller. It says $M$ cogenerates $R/I$ iff $I$ is the annihilator of $M$. We apply this to our situation.</p>

<p>Take any indecomposable injective $E$, and let $I \leq R$ be its annihilator. 
Then $E$ cogenerates $R/I$ by the lemma. 
That implies there is a monomorphism $\iota \colon R/I \to E^n$ for some $n \in \mathbb{N}$. 
Take a projection $\pi \colon E^n \to E$ such that $\pi \iota \neq 0$. 
Then the composition induces a monomorphism $(R/I) /\ker(\pi \iota) \to E$. 
Since $E$ is the injective envelope of all of its submodules, this shows $E$ is isomorphic to the injective envelope of $(R/I) /\ker(\pi \iota)$, which is a factor module of $R$.</p>
"
"2377569","2377586","<p><strong>Hint:</strong> Noting that $A^Tx = x^TA \in \Bbb R$, we can rewrite 
$$
A^TxxA^T = (A^Tx)^2
$$
an appropriate ""chain rule"" will work here.</p>
"
"2377570","2377585","<p>Here's a counterexample: let $\Omega=\mathbb{R}$, $A=\{(-\infty,t]|t\in\mathbb{R}\}$ and $B=\{(a,b)|a,b\in\mathbb{R}\}\cup\{\emptyset\}$ - then $A$ and $B$ are closed under intersection and disjoint from each other but they generate the same $\sigma$-algebra (i.e. the standard Borel $\sigma$-algebra on $\mathbb{R}$).</p>
"
"2377574","2379775","<p>Edit: The answer is that $\mathbb{Z_n}$ does not form a group with multiplication as 0 has no inverse element.</p>
"
"2377589","2377597","<p>The second equation you need to solve is $(A-\lambda I)m = n$, which is</p>

<p>$\begin{bmatrix}0 &amp; 2 \\0 &amp; 0 \end{bmatrix}m = \begin{bmatrix}1 \\ 0 \end{bmatrix}.$</p>

<p>The first line gives $2m_2=1$, so $n=\begin{bmatrix}0 \\ 1/2\end{bmatrix}$ works.</p>
"
"2377590","2377596","<p>The statement is false. The In $\mathbb Z_3$, you have $2^3=2$ and $2+2=1\neq 0$.</p>
"
"2377594","2377653","<p>If I understood well you want that the area between the line and the parabola is $\dfrac{1}{8}$ of the area between the two parabolas.</p>

<p>We first compute the area between the two parabolas
$$A=\int_0^1 \left(\sqrt{x}-x^2\right) \, dx=\left[\frac{2 x^{3/2}}{3}-\frac{x^3}{3}\right]_0^1=\dfrac{1}{3}$$</p>

<p>Now we want to find a line $y=mx$ such that the area between the line and the parabola is $\dfrac{1}{8}\,A=\dfrac{1}{24}$.</p>

<p>First we find the intersection point between the line and the parabola</p>

<p>$x^2=mx\to x=m$ so we must have</p>

<p>$$\int_0^m \left(m x-x^2\right) \, dx=\frac{1}{24}\to \frac{m^3}{6}=\frac{1}{24}\to m=\frac{1}{\sqrt[3]{4}}$$</p>

<p>In a similar way we find  the other line $y=px$ which intersects the other parabola in $x= \frac{1}{m^2}$ </p>

<p>$$\int_0^{\frac{1}{m^2}} \left(\sqrt{x}-m x\right) \, dx=\frac{1}{24}\to \frac{1}{6 m^3}=\frac{1}{24}\to m=\sqrt[3]{4}$$</p>

<p>Hope this helps</p>

<p><a href=""https://i.stack.imgur.com/ZzuXg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZzuXg.png"" alt=""enter image description here""></a></p>
"
"2377598","2377632","<p>I think the best way to understand abstractions intuitively is to study lots of examples. To understand group actions, write some down. Then think about whether each is transitive, whether it's primitive, look at the stabilizers of elements. Verify that the theorem is true; try to understand why in each particular case. Look for examples where some hypothesis fails and see whether (and why) the conclusion fails.</p>

<p>That learning strategy reproduces (in part) the explorations that led to the useful abstractions of group theory. Mathematicians studying the symmetries of various geometrical objects realized that they could reason about the symmetry of just about anything by inventing an abstract language whose definitions captured the essence of the properties of symmetries.</p>

<p>Unfortunately, often the abstractions - the definitions and theorems - take center stage in teaching and learning. Students find it hard to understand what's going on without the examples. I would tell my students that for me the category of groups consisted of those groups with which I was personally familiar. I assigned homework that called for working out examples at least as much as proving theorems.</p>

<p>In your own discipline (computer science) there are analogous historical trends leading from examples to abstractions: the concepts of object oriented or functional programming languages, the development of abstract tools to reason about databases.</p>
"
"2377601","2377622","<p>If $B$ is a complex Banach space and $T:B \to B$ is a bounded linear operator, then the spectrum $\sigma(T)$ has the following properties:</p>

<p>$\sigma(T) \ne \emptyset$ and $\sigma(T)$ is compact (hence closed).</p>
"
"2377603","2377630","<p>a) Let $x_n \to x$. Apply the dominated convergence theorem to the sequence $f_n(y) := \mathbb{1}_{B(x_n, |x_n|/2)}(y)f(y)$.</p>

<p>b) Suppose that the assertion weren't true. Then, for some $\epsilon &gt; 0$, there would be a sequence $x_n$ with $|x_n | \to \infty$ such that $g(x_n) &gt; \epsilon$. You may assume wlog that the $B(x_n, |x_n |/2)$ are disjoint. Edit: Because these sets are disjoint, for all $n \in \mathbb{N}$ we have that</p>

<p>$$\int_{\mathbb{R}^n} f(y)~\mathrm{d}y \geq \sum_{i=1}^{n}\int_{B(x_i,\vert x_i\vert/2)}f(y)~\mathrm{d}y. $$ </p>

<p>c) If $g(x) = 0$ for all $x \in \mathbb{R}^n$ the assertion is trivially true. Now let $y \in \mathbb{R}^n$ be such that $g(y) \neq 0$. By b), there is a $R &gt; 0 $ such that $g(x) &lt; g(y)$ for all $|x| &gt; R$. Consider the restriction of $g$ to $[-R,R]$ and use part $a)$ to conclude.</p>
"
"2377607","2377615","<p>I assume that $p$ is a prime number.</p>

<p>Then $\mathbb F_p$ and $\mathbb Z_p$ both denote the set of classes of integers mod $p$.</p>

<p>The notation $\mathbb F_p$ is typically used when the emphasis is on the fact that it is a field: $\mathbb F_p$ is the finite field with $p$ elements.</p>

<p>The underlying set in $(\mathbb F_p)^3=\mathbb F_p \oplus \mathbb F_p\oplus\mathbb F_p$ is the Cartesian product of three copies of $\mathbb F_p$. Now consider this: if $A$ is a set with $n$ elements, how many elements are there in the set $A \times A \times A$ ?</p>
"
"2377624","2377626","<p>$$E\left(\frac{1}{n}\sum_{i=1}^n X_i\right) = \frac1nE\left(\sum_{i=1}^n X_i\right) = \frac{1}{n}\sum_{i=1}^nE\left( X_i\right)$$</p>

<p>by linearity (i.e. $E(\alpha X)=\alpha E(X)$ and $E(X+Y)=E(X)+E(Y)$).</p>

<p>Then, use the fact that $X_i$ are equally distributed, which means $E(X_1)=E(X_2)=\cdots=E(X_n)$</p>
"
"2377628","2377633","<p>Let $f(x)=x^2+\frac{1}{x^2}$. </p>

<p>Thus, $f''(x)&gt;0$. </p>

<p>Thus, by Jensen
$$x^4+\frac{1}{x^4}+y^4+\frac{1}{y^4}\geq2\left(\left(\frac{x^2+y^2}{2}\right)^2+\frac{1}{\left(\frac{x^2+y^2}{2}\right)^2}\right)=\frac{17}{2}.$$
The equality occurs for $x=y=\sqrt2$, which says that we got a minimal value.</p>

<p>Done!</p>
"
"2377629","2377991","<p>$\int_{-\infty}^{\infty}\frac{1}{x}\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)\,\mathrm{d}x\quad$
I suppose that you raises the question of convergence for the singular point $x=0$. This would be a serious problem if the lower bound of the integral was $0$. But in the present case, $0$ isn't a bound. The Integral is convergent in the sens of Cauchy Principal Value : <a href=""http://mathworld.wolfram.com/CauchyPrincipalValue.html"" rel=""nofollow noreferrer"">http://mathworld.wolfram.com/CauchyPrincipalValue.html</a></p>

<p>$\tag{4}
\frac{\partial}{\partial a}\left[\exp\left(\tfrac{1}{2}a^{2}\right)\,I(a)\right]
= \exp\left(\tfrac{1}{2}a^{2}\right)\,.$
Integrating this equation with respect to variable $a$ leads to equation $(5)$, which, on my opinion should be better written with a dummy variable $\alpha$ instead of $a$ :</p>

<p>\begin{equation}
\tag{5}
\exp\left(\tfrac{1}{2}a^{2}\right)\,I(a)
=\int_{0}^{a}\exp\left(\tfrac{1}{2}\alpha^{2}\right)\,\mathrm{d}\alpha\,
\end{equation}</p>
"
"2377638","2377666","<p>You can indeed construct an additive, non-linear function $T:\Bbb R\to \Bbb R$ (as vetor spaces over $\Bbb R$, since additive maps of $\Bbb Q$-vector spaces necessarily become linear automatically) by starting with what you have: $T(p) = p$ for $p\in \Bbb Q$, and $T(r) = 0$ for all $r$ with $r/\sqrt2 \in \Bbb Q$. However, constructing it is not easy, and requires the so-called <em>axiom of choice</em> in order to be finalised.</p>

<p>So, we start out with what we already have: some values that $T$ sends to themself, and some values that $T$ sends to $0$. By additivity, this forces $T$ to be defined for a slightly greater class of numbers, namely $T(p + q\sqrt2) = p$, for all rational $p, q$.</p>

<p>But we haven't defined $T$ for all real numbers yet.  Note that whatever else we define $T$ to be, it already cannot be linear. So we just build on it as best we can and ensure that it stays additive. We do this by picking a number for which $T$ isn't already defined, like $\pi$ or $\sqrt 3$ or $\ln 2$ or $e$ or anything else. I'll pick $\pi$ for now. We can make $T(\pi)$ to be whatever we want it to be. I'll pick $3$ because I like $3$. By additivity, $T$ is now forcibly defined to be $T(p+q\sqrt2+r\pi) = p+3r$ for any rational $r$.</p>

<p>However, we are not done definining $T$ yet; there are still many more real numbers to cover. And this is where the axiom of choice comes in: there is no finite way, or even countably infinite way to finalise this definition of $T$. We just have to keep going and going, picking one new real number each step, until we've covered the entire real number line, which requires an uncountably infinite number of steps. The axiom of choice is exactly what allow us to say that even though we cannot finalise our $T$, some finalised $T$ does indeed exist somewhere out there.</p>
"
"2377644","2377782","<p>I don't think you understand Mean Value Theorem (MVT) correctly (as far as I understood your question).</p>

<p>For $f(x)$ and $g(x)$ MVT gives us the following:</p>

<p>$\exists$ $c_1 \in [a, b]: f^{'}(c_1) = \dfrac{f(b) - f(a)}{b - a}$</p>

<p>$\exists$ $c_2 \in [a, b]: g^{'}(c_2) = \dfrac{g(b) - g(a)}{b - a}$</p>

<p>$\exists$ $c_3 \in [a, b]: \dfrac{f^{'}(c_3)}{g^{'}(c_3)} = \dfrac{f(b) - f(a)}{g(b) - g(a)}$</p>

<p>$c_1, c_2, c_3$ are not related in any way, and MVT gives us no information about the actual values of $c_1, c_2, c_3$, we only know that these points do exist. In addition, $c_1, c_2, c_3$ don't have to be unique.</p>
"
"2377654","2380367","<p>First, you have to make a decision: Do you want to use the ""real"" alexnet (with the grouping) or what most frameworks use as AlexNet (without grouping).</p>

<p>In case you choose without grouping, you might want to have a look at <a href=""https://arxiv.org/pdf/1707.09725.pdf#page=105"" rel=""nofollow noreferrer"">Table D2 of my masters thesis</a> for a better overview over the layers. Especially the output size / number of filters / stride. Don't take the number of FLOPs too seriously, it is rather a ballpark-estimate.</p>

<p>Then you have to ask you the following:</p>

<ul>
<li>How would you implement convolution?</li>
<li>How would you implement max pooing?</li>
<li>How would you implement a fully connected layer?</li>
</ul>

<p>I will not give you the answer directly here, but recommend to have a look at the implementation of a framework of your choice. Or you could search for ""pure numpy cnn implementation"" or something similar, e.g.</p>

<p><a href=""https://github.com/llSourcell/Convolutional_neural_network/blob/master/app/model/alpha_cnn_predict.py#L72-L98"" rel=""nofollow noreferrer"">https://github.com/llSourcell/Convolutional_neural_network/blob/master/app/model/alpha_cnn_predict.py#L72-L98</a></p>
"
"2377664","2377688","<p>In the finite-dimensional case, here's another proof to add to the mix. The product $AB$ is equal to the matrix </p>

<p>$$[Ab_1~|~Ab_2~|\dots|~Ab_n]$$</p>

<p>where the $b_i$ are the columns of $B$. Since $AB$ is invertible, it follows that these columns are linearly independent. That is, no linear combination of $Ab_i$ equals the zero vector. Therefore,</p>

<p>$$c_1(Ab_1) + \dots + c_n(Ab_n) = A(c_1b_1 + \dots + c_nb_n)$$</p>

<p>will never be zero, implying that the columns of $B$ are linearly independent. Moreover, they form a basis. Consequently, $A$ must also be invertible, otherwise some choice of $c_i$ could send the sum to zero which cannot happen.</p>
"
"2377669","2377692","<p><strong>Hint</strong>: By symmetry, the angle between a chord and the tangent at the end of the chord is the same at both ends of the chord.</p>

<p>So if BA and CD are the tangents at the two ends of BC ...</p>

<hr>

<p>If CB and CD are <em>not</em> supposed to be tangents to the circles at C, then you don't have enough information in your drawing to conclude anything. Angle C could be <em>anything</em> in that case -- no matter how you choose points B, C, D, you would be able to find an A somewhere such that the angle at A is 80Â°, and <em>then</em> draw small circles tangent to BA and DA that both go through C.</p>
"
"2377674","2377817","<p>Your older brother is probably right that if you just want to pass your class, you don't need to understand everything perfectly. But it sounds like you want to, and there's absolutely no reason you can't attain a really good grasp of Calculus even in high school, if you're willing to put in the effort. </p>

<p>Let me address the second part of your question first: Why doesn't $u\lim_{\delta x\to0}\frac{\delta v}{\delta x}$ go to $0$? </p>

<p>As $\delta x$ goes to $0$, it's certainly true that $\delta v$ goes to $0$, so at first glance, we might want to say that the whole thing goes to $0$. But since the denominator goes to $0$ as well, that is not necessarily true. We have a very small quantity divided by another very small quantity, and as they both approach $0$, the ratio can approach something nonzero. In fact, the $\frac{\delta v}{\delta x}$ approaches $\frac{dv}{dx}$. Since this is multiplied by $u$, that whole term approaches $u\frac{dv}{dx}$. Similarly with the second term.</p>

<p>Now what is the difference in the third term? Recall the product rule for limits: the limit of a product is the product of the limits (contrast this with the derivative of a product, which is NOT the product of the derivatives, as this very discussion shows). So then: $\lim_{\delta x\to0}\frac{\delta u}{\delta x}\delta v=\lim_{\delta x\to0}\frac{\delta u}{\delta x}\cdot\lim_{\delta x\to0}\delta v=\frac{du}{dx}\cdot0$.</p>

<p>What happened here? The $\delta v$ was not divided by another small quantity, since the $\delta x$ was busy keeping the $\delta u$ in check. So the $\delta v$ just goes to $0$, bringing the whole term to $0$. Note that we could have also written this term as $\delta u\cdot\frac{\delta v}{\delta x}$. In that case, the $\frac{\delta v}{\delta x}$ would have gone to $\frac{dv}{dx}$, but the $\delta u$ would have gone to $0$. The point is that the $\delta x$ can only take care of one thing at a time.</p>

<p>Another way to look at it is this: Suppose $\frac{du}{dx}=5$. (or any other number). Eventually, with $\delta x$ small enough, we will have $\frac{\delta u}{\delta x}$ really close to $5$. Say we make $\delta x$ small enough that $\frac{\delta u}{\delta x}$ is between $4.9$ and $5.1$. Then we can also make $\delta x$ small enough that $\delta v$ is close to $0$. Say we make $\delta x$ small enough that $\delta v$ is between $-.00001$ and $.00001$. Then $\frac{\delta u}{\delta x}\delta v$ is at most $5.1$ times $.00001$, which is really close to $0$. You can probably see how making $\delta x$ even smaller makes this quantity even closer to $0$. In fact, you can make the quantity as close to $0$ as you want, just by making $\delta x$ small. And that's exactly what it means for the limit of the whole term to be $0$.</p>
"
"2377685","2377693","<p>By the definition of ceiling function,</p>

<p>$$\log_p n \le f(n) &lt; 1 + \log_p n$$</p>

<p>For the case $p &gt; 1$, for large $n &gt; p$, the upper bound is</p>

<p>$$1+\log_p n = \log_p pn &lt; \log_p n^2 = 2\log_p n = \frac{2}{\log p}\log n$$</p>

<hr>

<p>For the case $0 &lt; p &lt; 1$, for large enough $n&gt;\dfrac1p$, $f(n)$ and $(1+\log_p n)$ are negative. So considering the absolute values,</p>

<p>$$|1 + \log_p n| &lt; |f(n)| \le |\log_p n|$$</p>

<p>The upper bound is</p>

<p>$$|\log_p n| = -\log_p n = \frac1{-\log p}\cdot\log n$$</p>

<p>Additionally, for the interest of big-$\Theta$ notation, the lower bound is</p>

<p>$$|1 + \log_p n| = -1-\log_p n = -\log_p pn = \frac{\log pn}{-\log p}$$</p>

<p>For large $n &gt; \frac1{p^2}$, i.e. $p &gt; \frac 1{\sqrt n}$,</p>

<p>$$\frac{\log pn}{-\log p} &gt; \frac{\log \sqrt n}{-\log p} = \frac1{-2\log p} \cdot \log n$$</p>

<p>($-\log p$ is a positive constant)</p>
"
"2377689","2377979","<p>I think your identifications more obfuscate whatâs going on than simplify the problem.</p>

<ul>
<li>If $Î±_2$ is to make both the left and the right square commute, in the reduced case it must be an inclusion â as all other arrows of the left square are inclusions as and the vertical arrow in the right square is assumed to be the identity as well.</li>
<li>Thus, the question becomes: <em>Does $N_2 / M_1 = M_2 / M_1$ imply $N_2 = M_2$?</em></li>
</ul>

<p>And yes, it does. Let me rephrase it as: <em>Does $A / C = B / C$ imply $A = B$?</em> The premise says that for any element $x$ of either $A$ or $B$, there is some $y$ in either $B$ or $A$ respectively such that $x + C = y + C$, so in particular $x â B + C$ and $y â A + C$. Since $C â A$ and $C â B$, it follows that $A = B$.</p>

<p><em>But</em>, I think itâs less complicated to do the diagram chase and conclude this fact above as an easy corollary.</p>
"
"2377690","2377718","<p>It is a geometric series of the form $$\sum a^n $$
which, you should know, converges
$$\iff |a|&lt;1$$
the sum is then $\frac {1}{1-a} $.</p>

<p>so, your series converges
$$\iff |1-\frac {\sqrt {x}}{3}|&lt;1$$
$$\iff -1 &lt;1-\frac {\sqrt {x}}{3}&lt;1$$
$$\iff 0 &lt;\frac {\sqrt {x}}{3}&lt;2$$
$$\iff 0&lt;\sqrt{x}&lt;6$$
$$\iff 0 &lt;x &lt;36$$</p>
"
"2377691","2377703","<p>Suppose $n-m^2$ is a prime power for every integer $m$ with $0&lt;m&lt;\sqrt{n}$. Then the differences
$$(n-m^2)-(n-(m+1)^2)=2m+1,$$
are odd, so precisely one of the differences is a positive power of $2$. That means $n-m^2$ is a positive power of $2$ either for all odd $m$, or for all even $m$. Neither
$$(n-3^2)-(n-7^2)=40\qquad\text{ nor }\qquad(n-4^2)-(n-6^2)=20,$$
is a difference of two positive powers of $2$, so $n\leq49$.</p>
"
"2377702","2377711","<p>You have
$$a_{n+1}=c_{n}a_{n}$$
Devide by $\prod_{k=0}^{n}c_{k}$ to give
$$\frac{a_{n+1}}{\prod_{k=0}^{n}c_{k}}=\frac{a_{n}}{\prod_{k=0}^{n-1}c_{k}}$$
Let
$$\frac{a_{n}}{\prod_{k=0}^{n-1}c_{k}}=A_{n}$$
Then
$$A_{n+1}=A_{n}$$
Hence $A_{n}=A$, where $A$ is constant
$$A=\frac{a_{n}}{\prod_{k=0}^{n-1}c_{k}}$$
So
$$a_{n}=A\prod_{k=0}^{n-1}c_{k}$$</p>
"
"2377716","2377798","<p>Yes, you are correct, and probably the easiest way to see that is to actually write out the definition of the euclidean norm squared and take the gradient.
$$\nabla_\vec{x}\|\vec{x}\|^2=\nabla_\vec{x} (\sqrt{\sum_i x_i^2}^2) = \nabla_\vec{x} (\sum_i x_i^2)=\{2x_1,2x_2,\dots 2x_n\}=2\vec{x}$$
So it is exactly as you said.</p>

<p>I suggest for relatively simple matrix- or vector-argument functions to write out the function in summation notation if possible, then take derivatives. It can help a lot. If you really want to get into calculus with matrices, I recommend Jan Magnus' ""Matrix Differential Calculus with Applications in Statistics and Econometrics"", which is available free online as a pdf.</p>
"
"2377719","2377722","<p>If $a&lt;b$, then $\frac a2&lt;\frac b2$.</p>

<p>Now, take the inequality $\frac a2&lt;\frac b2$ and try to add some number to both sides. Try to add such a number that one side becomes $a$.</p>
"
"2377726","2377895","<p>Note that $H/Z$ has a subgroup $K/Z$ of order $p$ with $K$ normal in $G$. (That follows easily from the fact that $H/Z$ has nontrivial intersection with $Z(G/Z)$.)</p>

<p>If $K  \cong C_p \times C_p$ then we are done. Otherwise $K \cong C_{p^2}$, and in that case $|\ker f| &lt; p^3$, so we are not in either of the two cases that you have not been able to handle. </p>
"
"2377727","2377950","<p>Let $S:V\to H$ be the inclusion, and set $T=S^*$.  Then for all $\varphi\in H^*$ and all $v\in V$ we have 
$$T\varphi(v)=\varphi(Sv)=\varphi(v),$$
which is precisely $(1)$.</p>
"
"2377728","2377737","<p>The notation</p>

<p>$$\{ x \mid \forall A. (A \in F \rightarrow x \in A)\}$$</p>

<p>is an example of the commonly used notion of <em>set abstraction</em>; we build the set of elements that satisfy a given condition. $x$ is a bound variable here that ranges over <em>elements</em>.</p>

<p>The intersection of $F$ is the set $Z$ such that $x$ belongs to $Z$ if for every set $A$ in the family $F$, $x$ is an element of $A$. </p>
"
"2377731","2377801","<p>The idea: the first assumption tells you that $f$ is not very big around $0$ and the second assumption tells you that it's not very big around $\infty$. </p>

<p>So: let $$A_0 = \{ x \in \mathbb{R}^d : \| x \| \leqslant 1 \ \&amp; \ |f(x)| \leqslant 1 \} \\[1ex] 
A_1 = \{ x \in \mathbb{R}^d : \| x \| \leqslant 1 \ \&amp; \ |f(x)| &gt; 1 \}.$$</p>

<p>Then </p>

<p>$$\int \limits_{\| x \| \leqslant 1} |f(x)| \, \mathrm{d} \lambda(x) = \int \limits_{A_0} |f(x)| \, \mathrm{d} \lambda(x) + \int \limits_{A_1} |f(x)| \, \mathrm{d} \lambda(x)$$ </p>

<p>and </p>

<p>$$\begin{align*}
\int \limits_{A_0} |f(x)| \, \mathrm{d} \lambda(x) &amp; \leqslant \int \limits_{A_0} 1 \, \mathrm{d} \lambda(x) = \lambda( A_0 ) &lt; \infty \\[1ex]
\int \limits_{A_1} |f(x)| \, \mathrm{d} \lambda(x) &amp; \leqslant \int \limits_{A_1} |f(x)|^2 \, \mathrm{d} \lambda(x) &lt; \infty \end{align*}$$</p>

<p>where the finiteness of the second integral follows from the assumption $f(x) \in L^2(\mathbb{R}^d)$.</p>

<p>The second part goes similarly: let</p>

<p>$$B_0 = \left\{ x \in \mathbb{R}^d : \| x \| &gt; 1 \ \&amp; \ |f(x)| \leqslant \frac{1}{\|x\|^{d+1}} \right\} \\[1ex] 
B_1 = \left\{ x \in \mathbb{R}^d : \| x \| &gt; 1 \ \&amp; \ |f(x)| &gt; \frac{1}{\|x\|^{d+1}} \right\}.$$</p>

<p>Then</p>

<p>$$\int \limits_{\| x \| &gt; 1} |f(x)| \, \mathrm{d} \lambda(x) = \int \limits_{B_0} |f(x)| \, \mathrm{d} \lambda(x) + \int \limits_{B_1} |f(x)| \, \mathrm{d} \lambda(x)$$ </p>

<p>and </p>

<p>$$\begin{align*}
\int \limits_{B_0} |f(x)| \, \mathrm{d} \lambda(x) &amp; \leqslant \int \limits_{B_0} \frac{1}{\|x\|^{d+1}} \, \mathrm{d} \lambda(x) &lt; \infty \\[1ex]
\int \limits_{B_1} |f(x)| \, \mathrm{d} \lambda(x) &amp; \leqslant \int \limits_{B_1} |f(x)|^2 \| x \|^{d+1} \, \mathrm{d} \lambda(x) &lt; \infty \end{align*}$$</p>

<p>where the finiteness of the second intergral follows from the assumption $\| x \|^{\frac{d+1}{2}} f(x) \in L^2( \mathbb{R}^d )$.</p>
"
"2377745","2377758","<blockquote>
  <p>after that how to do to find individual limits on $x$ and $y$?</p>
</blockquote>

<p>You can't get individual limits because what one variable is allowed to be depends on what the other variable's value is.  This can happen with multivariable functions.  Sometimes it isn't possible to get limits for each individual variable.</p>

<p>You've actually already finished the problem, and you approached it correctly as well.  Anything we take the arcsine of must be in the interval $[-1,1]$.  Since we are taking the arcsine of $xy$, we will think of $xy$ as one entity, so for simplicity we can call it $z$.  Thus $z \in [-1,1]$.  Thus $xy \in [-1,1]$.  So, the <em>only</em> restriction on $x$ and $y$ is that their product $xy$ is in the interval $[-1,1]$.</p>

<p>Individually, $x$ can be any real number as long as $y$ is a real number such that $xy \in [-1,1]$.  Or you can ""approach through $y$ first"" and say $y$ can be any real number as long as $x$ is a real number such that $xy \in [-1,1]$. And the simplest way to say all this is simply $\{ (x,y) \in \Bbb R^2 : xy \in [-1,1] \}$.</p>
"
"2377752","2377795","<p>The other users basically already answered your question, I just want to recommend that you try showing this by proving that the preimage of any open set is open. This would save you a lot of time.  You could also prove this by taking the limits and showing they exist. Both of these are much shorter.</p>
"
"2377753","2377789","<p>If you take out a vertex from the first graph, leaving $G-x$, but you can't find any isomorphism between $G-x$ and $H-y$ for any $y \in V(H)$, then $G$ and $H$ are definitely not isomorphic. (If there were an isomorphism $\phi$, then $H - \phi(x)$ would be isomorphic to $G-x$.)</p>

<p>Ulam's conjecture addresses the converse of this principle, and you don't need it to answer your question.</p>
"
"2377759","2377815","<p>Write it in lowest terms; what is its denominator?</p>

<p>Since it is $\frac1{n+1}\binom{2n}n$, the denominator must divide $n+1$.</p>

<p>Since it is $\frac{1}{2n+1}\binom{2n+1}n$, the denominator must divide $2n+1$.</p>

<p>Since $n+1$ and $2n+1$ are coprime, the denominator must be $1$.</p>
"
"2377761","2377769","<p>Recall that the sum of two cubes can be factored as
$$a^3+b^3=(a+b)(a^2-ab+b^2)$$</p>
"
"2377762","2377784","<p>$x$ must be an element of $X$ to ensure that $x_n-x$ has meaning and that this is again an element of $X$ so we can look at its norm: $\lVert x_n-x\rVert$</p>

<p>However, if the space $X$ is a subspace of a normed space $Y$ then it is possible for the limit to be outside of $X$. We then say that he sequence doesn't converge in $X$ but it does converge in $Y$.</p>

<p>A subspace $X$ for which every convergent sequence has it's limit still in $X$ is called a closed subspace</p>
"
"2377763","2377778","<p>First off, you forgot the factor of $2$ in the transformation, i.e.</p>

<p>$$\int_{0}^{4} x^2 dx = 2\int_{-1}^{1}(2x+2)^2 dx$$</p>

<p>Secondly your calculation is wrong, you should get</p>

<p>$$\left(\frac{2}{\sqrt{3}} + 2\right)^2 + \left(-\frac{2}{\sqrt{3}} + 2\right)^2 = \frac{32}{3}$$</p>

<p>Finally, $4^3 = 64$. If you correct all your mistakes you'll get the correct answer of $64/3$ using both methods.</p>
"
"2377770","2378365","<p>It's a bit tricky to formulate a duality between them, since the largest inscribed ball is not always unique (e.g. a rectangle in the plane), whereas the smallest enveloping ball is always unique (in a reflexive space, including all finite-dimensional spaces).</p>

<p>However, there is something we can do, but first let's formulate the problem. Let $X$ be a real Hilbert Space (in finite dimensions, $\mathbb{R}^n$ with inner product $\langle x, y \rangle := x.y$; the dot product). Let $\iota_{x, r}$ be the operation of inversion in the sphere $S_X[x; r] := \lbrace y \in X : \|y - x\| = r \rbrace$. For a subset $\emptyset \neq Q \subseteq X$, we'll denote by $c(Q)$ the centre of the ball of minimal radius that envelopes $Q$ (where $Q$ is bounded), and denote by $C(Q)$ the set of centres of balls of maximal radius that are contained in $Q$. Furthermore, we denote $r(Q)$ and $R(Q)$ to be the respective radii of these balls.</p>

<p>Let us consider the case where the interior of $Q$ is non-empty and $C(Q) \neq \emptyset$ (otherwise the problem becomes somewhat silly). So, in particular, $R(Q) &gt; 0$. Let $y \in C(Q)$, consider the image of the set $X \setminus Q$ under the operation $\iota_{y, R(Q)}$, and call it $A_y$. Then $y = c(A_y)$.</p>

<p>How does this work? Proving it requires some tedious algebra, but can be understood well enough with some geometry. When in an inner product space (or indeed, a Hilbert space), the operation $\iota_{x, r}$ preserves ""generalised spheres"", meaning spheres and hyperplanes. So, every sphere maps to a sphere or hyperplane (and the same for hyperplanes). A sphere will only map to a hyperplane if the sphere goes through $x$, the centre of the sphere in which you are inverting. Note also that $\iota_{x,r}$ is an involution, meaning that $\iota_{x, r}^2 = I$.</p>

<p>Moreover, if you map a ball containing $x$ in its interior under $\iota_{x, r}$, then the image will be the (unbounded) complement of another ball. If the ball does not contain $x$ in its interior, then it will map to another ball (not of the same centre). If $x$ is on the sphere of the ball, then the ball maps to a half-space.</p>

<p>Here's something to note about $A_y$: it is contained in $B[y; R(Q)]$. This is because we inverted in the sphere the set $X \setminus Q$, which was entirely contained outside the sphere, so the resulting set $A_y$ lies in the ball. From this, we conclude that $r(A_y) \le R(Q)$.</p>

<p>Another fact about $A_y$ is that, given any ball $B$ containing $A_y$, we must have $y \in \operatorname{int}(B)$. This can be deduced by the geometric facts above. It therefore follows that any ball containing $A_y$ will map to the complement of a ball, where the ball is contained in $Q$. It takes some formulae to prove formally, but the smaller the ball containing $A_y$, the larger the ball inside $Q$. Where they achieve their common optimisation is at $y$, with radius $R(Q)$.</p>

<p>So, in conclusion, we can take any Chebyshev centre (of one type) $y \in C(Q)$, perform an involution on the set (mapping under an inversion, then taking the complement) to recover a set $A_y$ whose (unique) Chebyshev centre (of the other type) is also $y$. It's not much of a duality, but it's something.</p>

<p>Aside: These techniques were used to make some of the most significant progress on the Chebyshev conjecture, and originally attributed to F. A. Ficken. Using these techniques, a non-convex Chevyshev set (a set for which each point has a unique closest point in the set) could be turned into a uniquely remotal set (a set for which each point has a unique furthest point in the set).</p>
"
"2377783","2377792","<p>Assume that you have a continuous surjective map from $[0,1]$ to $[0,1]\cup[2,3]$ and use the intermediate value theorem to derive a contradiction from that.</p>

<p>(This is of course the same idea as in the earlier answer.)</p>
"
"2377796","2377811","<p>the equation that will be arrived is
$$x=\frac{29a}{5593}-\frac{b}{16779}$$ plugging this in your first equation we get (after simplifications)
$${\frac {{b}^{2}}{281534841}}-{\frac {58\,ba}{93844947}}+{\frac {841\,{
a}^{2}}{31281649}}-{\frac {1200\,a}{5593}}+{\frac {1970\,b}{16779}}=0
$$</p>
"
"2377803","2381718","<p>The second assertion follows from the first because, by Menger's Theorem, for any nonempty $S \subsetneq V(X)$, $|\partial S| \geq \kappa_1(X)$.</p>

<p>Here's another way to think about it. If either of $\partial (A \cup B)$ or $\partial (A \cap B)$ had size different from $\kappa_1 (X)$, then one of the two would have to be less than $\kappa_1 (X)$, which contradicts $\kappa_1 (X)$ being the edge connectivity, since neither $A \cap B$ or $A \cup B$ is $V(X)$ or empty.</p>
"
"2377804","2377822","<p>What you suggest would normalize the vector of eigenvalues, but since you are not manipulating the matrix itself this couldn't effect the eigenvalues of the matrix.</p>

<p>One way to normalize a matrix such that it's largest eigenvalue is equal to 1 is to divide the matrix by that eigenvalue. If I write the normalized matrix $U$ as $\hat{U}$, this is accomplished by
$$\hat{U} = \frac{U}{\lambda_{max}}$$
This corresponds to what you have found regarding normalization because $\lambda_{max}=\|U\|$, the induced matrix 2-norm. Hence your normalized $\hat{U}$ could be written $$\hat{U}=\frac{U}{\|U\|}$$ which matches the form of the normalization process that you found.</p>
"
"2377805","2378132","<p>Putting $a=-3/2$ gives
$$
A=\left[\begin{array}{rrr|r}
2 &amp; 0 &amp; \frac{25}{4} &amp; \frac{5}{4} \\
0 &amp; 1 &amp; -\frac{19}{4} &amp; \frac{11}{4} \\
0 &amp; 0 &amp; 6 &amp; 0
\end{array}\right]
$$
Now, consider the row-reductions
\begin{align*}
\left[\begin{array}{rrr|r}
2 &amp; 0 &amp; \frac{25}{4} &amp; \frac{5}{4} \\
0 &amp; 1 &amp; -\frac{19}{4} &amp; \frac{11}{4} \\
0 &amp; 0 &amp; 6 &amp; 0
\end{array}\right]
&amp;\xrightarrow{R_1\to (1/2)\cdot R_1}\left[\begin{array}{rrr|r}
1 &amp; 0 &amp; \frac{25}{8} &amp; \frac{5}{8} \\
0 &amp; 1 &amp; -\frac{19}{4} &amp; \frac{11}{4} \\
0 &amp; 0 &amp; 6 &amp; 0
\end{array}\right] \\
&amp;\xrightarrow{R_3\to(1/6)\cdot R_3}\left[\begin{array}{rrr|r}
1 &amp; 0 &amp; \frac{25}{8} &amp; \frac{5}{8} \\
0 &amp; 1 &amp; -\frac{19}{4} &amp; \frac{11}{4} \\
0 &amp; 0 &amp; 1 &amp; 0
\end{array}\right] \\
&amp;\xrightarrow{R_1\to R_1+(-25/8)\cdot R_3}\left[\begin{array}{rrr|r}
1 &amp; 0 &amp; 0 &amp; \frac{5}{8} \\
0 &amp; 1 &amp; -\frac{19}{4} &amp; \frac{11}{4} \\
0 &amp; 0 &amp; 1 &amp; 0
\end{array}\right] \\
&amp;\xrightarrow{R_2\to R_2+(19/4)\cdot R_3}\left[\begin{array}{rrr|r}
1 &amp; 0 &amp; 0 &amp; \frac{5}{8} \\
0 &amp; 1 &amp; 0 &amp; \frac{11}{4} \\
0 &amp; 0 &amp; 1 &amp; 0
\end{array}\right]
\end{align*}
Hence
$$
\DeclareMathOperator{rref}{rref}\rref\left[\begin{array}{rrr|r}
2 &amp; 0 &amp; \frac{25}{4} &amp; \frac{5}{4} \\
0 &amp; 1 &amp; -\frac{19}{4} &amp; \frac{11}{4} \\
0 &amp; 0 &amp; 6 &amp; 0
\end{array}\right]=\left[\begin{array}{rrr|r}
1 &amp; 0 &amp; 0 &amp; \frac{5}{8} \\
0 &amp; 1 &amp; 0 &amp; \frac{11}{4} \\
0 &amp; 0 &amp; 1 &amp; 0
\end{array}\right]
$$</p>
"
"2377812","2377875","<p>This is wrong even in the case where $\xi_n=1$ almost surely. Take for example $\eta_n$ such that $\mathbb P\left(\eta_n=n^2\right)=1/n$, $\mathbb P\left(\eta_n=1\right)=1-1/n$. </p>
"
"2377813","2377844","<p>Induction on two variables is fairly common.  The general structure is to nest one induction proof inside another.  For example, in order to prove a statement $P[m,n]$ is true for all $m,n \in \mathbb{N}$, one might proceed as follows:</p>

<p><strong>Induction on $n$:  Base Case, $n = 0$</strong></p>

<p>We need to prove $P[m,0]$.  To do this, we have a sub-proof by induction on $m$:</p>

<p><em>Induction on $m$:  Base case, $m = 0$</em></p>

<p>We prove that $P[0,0]$ is true.</p>

<p><em>Induction on $m$:  Inductive step.</em></p>

<p>We prove that if $P[m,0]$ is true, then $P[m+1,0]$ is true as well.</p>

<p>This concludes the subproof, which establishes the $n=0$ case for the main argument.</p>

<p><strong>Induction on $n$:  Inductive step</strong></p>

<p>We need to prove that $P[m, n] \implies P[m, n+1]$.  To do this, we will have another sub-proof by induction on $m$:</p>

<p><em>Induction on $m$:  Base case, $m = 0$</em></p>

<p>We prove that $P[0, n] \implies P[0, n+1]$.</p>

<p><em>Induction on $m$:  Inductive step.</em></p>

<p>We prove that if $P[m, n] \implies P[m, n+1]$, then also $P[m+1, n] \implies P[m+1, n+1]$.</p>

<p>This concludes the subproof, which establishes the inductive step for the main argument, which concludes the main proof as well.</p>
"
"2377816","2378482","<blockquote>
  <ol>
  <li>can someone generalize it, so as to make my understanding more clear? Say for $n$ events? </li>
  </ol>
</blockquote>

<p>If $(B_k)_n$ is a sequence of $n$ events that partition the sample space (or if at least $(B_k\cap A_1)_n$ partitions $A_1$) then, $\mathsf P(A_2\mid A_1) = \sum_{k=1}^n \mathsf P(A_2\mid A_1\cap B_k)\mathsf P(B_k\mid A_1)$</p>

<blockquote>
  <ol start=""2"">
  <li>Also, in $P(A_2|A_1)=P(A_2|\color{red}{AA_1})P(\color{red}{A|A_1})+P(A_2|\color{magenta}{A^cA_1})P(\color{magenta}{A^c|A_1})$, I feel red colored stuff should be same and pink colored stuff should be same, as in case of simple form law of total probability. </li>
  </ol>
</blockquote>

<p>They are <em>not</em> the same in the case of the simple form. So why should they be?</p>

<p>Where $\Omega$ is the entire sample space, then:</p>

<p>$${{\mathsf P(A_2)~}{= \mathsf P(A_2\mid \Omega)\\=\mathsf P(A_2\mid \color{red}{A}, \Omega)P(\color{red}{A}\mid \Omega)+\mathsf P(A_2\mid \color{magenta}{A^c}, \Omega)\,\mathsf P(\color{magenta}{A^c}\mid \Omega)\\=\mathsf P(A_2\mid \color{red}{A})P(\color{red}{A})+\mathsf P(A_2\mid \color{magenta}{A^c})\,\mathsf P(\color{magenta}{A^c})}}$$</p>

<blockquote>
  <ol start=""3"">
  <li>I felt it should be $P(A_2|\color{red}{(A_1|A)})P(\color{red}{A_\,\mathsf 1|A})+P(A_2|\color{magenta}{(A_1|A^c)})P(\color{magenta}{A_1|A^c})$. Am I absolutely stupid here? </li>
  </ol>
</blockquote>

<p>:) Well, I would not say <em>absolutely</em>. &nbsp; But seriously, it is a rather common misunderstanding.</p>

<p>The conditioning bar is <em>not</em> a set operation. &nbsp; It seperates the <em>event</em> from the <em>condtion</em> that the <em>probability function</em> is being measured over. &nbsp; There can only be one inside any probability function; they do not nest.</p>

<blockquote>
  <ol start=""4"">
  <li>For a moment I felt its related to:$P(E_1E_2E_2...E_n)=P(E_1)P(E_2|E_1)P(E_3|E_1E_2)...P(E_n|E_1...E_{n-1})$. Is it so?</li>
  </ol>
</blockquote>

<p>Yes, this is so. &nbsp; Specifically $\mathsf P(A_2,A,A_1)=\mathsf P(A_2\mid A,A_1)\mathsf P(A\mid A_1)\mathsf P(A_1)\\ \mathsf P(A_2,A^\mathsf c,A_1)=\mathsf P(A_2\mid A^\mathsf c,A_1)\mathsf P(A^\mathsf c\mid A_1)\mathsf P(A_1)$</p>

<p>$$\begin{align}\mathsf P(A_2\mid A_1) 
~ &amp; = \mathsf P((A\cup A^\mathsf c){\cap} A_2\mid A_1) &amp;&amp; \text{Union of Complements}
\\[1ex] &amp; = \mathsf P((A{\cap}A_2)\cup(A^\mathsf c{\cap}A_2)\mid A_1) &amp;&amp; \text{Distributive Law}
\\[1ex] &amp; = \mathsf P(A{\cap}A_2\mid A_1) + \mathsf P(A^\mathsf c{\cap}A_2\mid A_1) 
&amp;&amp; \text{Additive Rule for Union of Exclusive Events}
\\[1ex] &amp; = \dfrac{\mathsf P(A{\cap}A_1{\cap}A_2)+\mathsf P(A^\mathsf c{\cap}A_1{\cap}A_2)}{\mathsf P(A_1)} &amp;&amp; \text{by Definition}
\\[1ex] &amp; = \dfrac{\mathsf P(A_2\mid A{\cap}A_1)\,\mathsf P(A{\cap}A_1)+\mathsf P(A_2\mid A^\mathsf c{\cap}A_1)\,\mathsf P(A^\mathsf c{\cap}A_1)}{\mathsf P(A_1)} &amp;&amp; \text{by Definition}
\\[1ex] &amp; = {\mathsf P(A_2\mid A{\cap}A_1)\,\mathsf P(A\mid A_1)+\mathsf P(A_2\mid A^\mathsf c{\cap}A_1)\,\mathsf P(A^\mathsf c\mid A_1)} &amp;&amp; \text{by Definition of Conditional Probability}
 \end{align}$$</p>
"
"2377831","2377918","<p>This is a <a href=""https://en.wikipedia.org/wiki/Double_factorial#Alternative_extension_of_the_multifactorial"" rel=""nofollow noreferrer"">multifactorial</a>, specifically with</p>

<p>$$(1-m)(1-2m)\dots(1-nm)=\frac{(1-m)!^{(m)}}{(1-(n+1)m)!^{(m)}}$$</p>

<p>As the Wikipedia shows,</p>

<p>$$z!^{(\alpha)}=\alpha^{(z-1)/\alpha}\frac{\Gamma\left(\frac z\alpha+1\right)}{\Gamma\left(\frac1\alpha+1\right)}$$</p>

<p>Thus, we get</p>

<p>$$(1-m)!^{(m)}=m^{-1}\frac{\Gamma\left(\frac1m\right)}{\Gamma\left(\frac1m+1\right)}=1$$</p>

<p>$$(1-(n+1)m)!^{(m)}=m^{-(n+1)}\frac{\Gamma\left(\frac1m-n\right)}{\Gamma\left(\frac1m+1\right)}$$</p>

<p>Using the Gamma function's reflection formula,</p>

<p>$$\Gamma(z+1)=z\Gamma(z)$$</p>

<p>We may deduce that</p>

<p>$$(1-m)(1-2m)\dots(1-nm)=m^{n+1}\frac{\Gamma(\frac1m+1)}{\Gamma\left(\frac1m-n\right)}$$</p>

<p>Where $\Gamma$ is the Gamma function, which may be evaluated using the recursive definition/formula.</p>

<p>For example, at $n=2$, we have</p>

<p>$$(1-m)(1-2m)=m^3\frac{\Gamma\left(\frac1m+1\right)}{\Gamma\left(\frac1m-2\right)}$$</p>

<p>We know that:</p>

<p>$$\Gamma\left(\frac1m+1\right)=\frac1m\Gamma\left(\frac1m\right)=\frac1m\left(\frac1m-1\right)\Gamma\left(\frac1m-1\right)\\=\frac1m\left(\frac1m-1\right)\left(\frac1m-2\right)\Gamma\left(\frac1m-2\right)$$</p>

<p>Thus,</p>

<p>$$(1-m)(1-2m)=m^3\frac1m\left(\frac1m-1\right)\left(\frac1m-2\right)$$</p>

<p>which checks out.</p>

<p>If you want it in terms of seemingly more elementary functions, we may use the factorial to get</p>

<p>$$(1-m)(1-2m)\dots(1-nm)=m^{n+1}\frac{\left(\frac1m\right)!}{\left(\frac1m-n-1\right)!}$$</p>

<p>Or in terms of the binomial coefficients:</p>

<p>$$(1-m)(1-2m)\dots(1-nm)=m^{n+1}(n+1)!\binom{1/m}{n+1}$$</p>
"
"2377847","2377870","<p>Let's study the values of $f$:</p>

<p>if $n=4k$, then $f(n)= (2k)(4k-1)= 0 \mod 2$.</p>

<p>if $n=4k+1$, then $f(n)= (4k+1)(2k) = 0 \mod 2$.</p>

<p>if $n=4k+2$, then $f(n)= (2k+1)(4k+1)= 1 \mod 2$.</p>

<p>if $n=4k+3$, then $f(n)= (4k+3)(2k+1)= 1 \mod 2$.</p>
"
"2377852","2378001","<p>You can construct a counterexample by transfinite induction.  Let us say the game is that the two players take turns saying either $0$ or $1$ forever, so the set of terminal histories is $\{0,1\}^{\mathbb{N}}$.  We will build a subset $\tau\subset\{0,1\}^{\mathbb{N}}$ such that player 1 has no strategy to guarantee an outcome in $\tau$ and player 2 has no strategy to guarantee an outcome in $\tau^C$.</p>

<p>First, note that there are $\mathfrak{c}$ different strategies for each player; fix an enumeration $(f_\alpha)_{\alpha&lt;\mathfrak{c}}$ of all strategies for player 1 and an enumeration $(g_\alpha)_{\alpha&lt;\mathfrak{c}}$ of all strategies for player 2.  By induction on $\alpha&lt;\mathfrak{c}$, we construct two sequences $(t_\alpha)_{\alpha&lt;\mathfrak{c}}$ and $(u_\alpha)_{\alpha&lt;\mathfrak{c}}$ of elements of $\{0,1\}^\mathbb{N}$, such that the sets $\{t_\alpha\}_{\alpha&lt;\mathfrak{c}}$ and $\{u_\alpha\}_{\alpha&lt;\mathfrak{c}}$ are disjoint. You can think of the $t_\alpha$ as the histories we have decided will be in $\tau$ so far, and the $u_\alpha$ as the histories we have decided must not be in $\tau$ so far.</p>

<p>Having defined $t_\beta$ and $u_\beta$ for all $\beta&lt;\alpha$, we define $t_\alpha$ and $u_\alpha$ as follows.  Let $A\subset\{0,1\}^{\mathbb{N}}$ be the set of all possible outcomes that can be obtained if player 1 follows strategy $f_\alpha$.  Note that $A$ has cardinality $\mathfrak{c}$, since any sequence of player 2's moves is possible for elements of $A$.  In particular, there exists some element of $A$ which is not equal to $t_\beta$ for any $\beta&lt;\alpha$.  Pick some such element and define it to be $u_\alpha$.  Similarly, let $B$ be the set of all outcomes that can be obtained if player 2 follows strategy $g_\alpha$, and pick some element of $B$ which is not equal to any $u_\beta$ for any $\beta\leq\alpha$ and define it to be $t_\alpha$.</p>

<p>Now define $\tau=\{t_\alpha\}_{\alpha&lt;\mathfrak{c}}$.  Then player 1 has no strategy guaranteeing an outcome in $\tau$, since for any strategy $f_\alpha$ of player 1, $u_\alpha$ is one of the possible outcomes of this strategy, and $u_\alpha\not\in\tau$ (since we constructed the $u$'s and $t$'s to be disjoint sets).  Similarly, player 2 has no strategy guaranteeing an outcome in $\tau^C$, since for any strategy $g_\alpha$, $t_\alpha$ is a possible outcome of the strategy and $t_\alpha\in\tau$.</p>
"
"2377859","2377863","<p>Let $f(x)=\ln(1+x)-x+\frac{x^2}{2}$.</p>

<p>Hence, $f'(x)=\frac{1}{x+1}-1+x=\frac{x^2}{x+1}&gt;0.$</p>

<p>Thus, $f(x)&gt; f(0)=0$.</p>

<p>Let $g(x)=x-\ln(1+x)$.</p>

<p>Thus, $g'(x)=1-\frac{1}{x+1}=\frac{x}{x+1}&gt;0$ and</p>

<p>$g(x)&gt;g(0)=0$.</p>

<p>Done!</p>
"
"2377861","2378088","<p>Heuristically what I would do is, for each polygonal approximation:</p>

<ol>
<li><p>Decide on a value for both the <em>direction</em> and a <em>curvature</em> at each vertex, by fitting a circle through it and its neighbor vertices on each side.</p></li>
<li><p>Along each of the straight segments, assign a curvature as a quadratic function of arc length, such that (a) it agrees with the values a the endpoints that you have already decided on, and (b) its integral over the entire segment is exactly the different in assumed <em>direction</em> between the vertices.</p></li>
</ol>

<p>If your vertices always lie <em>on</em> the original smooth curve, as is the case in your illustration, this ought to converge nicely (that is, locally uniformly) towards the true curvature.</p>

<p>For some applications it may work just as well simply to interpolate linearly in step 2. Using a quadratic correction has the advantage that you can integrate it to find a continuously varying <em>direction</em> at each point on the segment, which can be used to simulate shading, bouncing off the curve, and the like.</p>
"
"2377865","2377909","<p>First off, let me preface this post with the very important note that the notion that Cantor discusses has <em>nothing</em> to do with the term $\infty$ that appears in calculus, nor with the various geometric notions of ""at infinity"" such as seen in inversive geometry or projective geometry.</p>

<hr>

<p>Cantor's notion of <em>cardinal number</em> is about quantifying <em>sets</em> up to <em>bijection</em>. We call this quantity the <em>cardinality</em> of a set.</p>

<p>A bijection is a one-to-one correspondence between the elements of the sets: for example, we can write down a bijection between the natural numbers and the primes as follows:</p>

<p>$$ \begin{matrix} 
0 &amp; 1 &amp; 2 &amp; 3 &amp; \ldots
\\ \updownarrow &amp; \updownarrow &amp; \updownarrow &amp; \updownarrow &amp; 
\\ 2 &amp; 3 &amp; 5 &amp; 7 &amp; \ldots
 \end{matrix} 
$$</p>

<p>Formally, a bijection from a set $X$ to a set $Y$ is a function $f$ from $X$ to $Y$ with the property that $f(x) = f(y)$ implies $x=y$. (here, ""function"" is meant in the precise set-theoretic sense)</p>

<p>When there exists a bijection between two sets, we say they are <em>equipollent</em>. (or <em>bijective</em> or <em>isomorphic</em>)</p>

<p>In the case of <em>finite sets</em>, it's pretty easy to see that two sets are equipollent if and only if they have the same size &mdash; we can just list out the elements side by side. Thus, we can quantify the finite sets by using natural numbers.</p>

<p>Cantor's key insight is that this notion applies equally well to infinite sets: e.g. the set of natural numbers and the set of primes described above are equipollent, as shown by the correspondence indicated above.</p>

<p>Thus, these two sets are quantified by the same cardinal number; we call it  $\aleph_0$. </p>

<p>The ""surprise"" turns out to be that $\aleph_0$ doesn't quantify <em>all</em> infinite sets &mdash; there are infinite sets that are not equipollent to the natural numbers! Consequently, there are many different infinite cardinal numbers in addition to the familiar $\aleph_0$.</p>

<p>With the benefit of hindsight this maybe shouldn't be so surprising &mdash; it's just that people have spent millenia with just a single word for quantifying non-finite sets, so it's suddenly shocking when it's discovered there are more refined things you can say!</p>
"
"2377872","2377913","<p>If $n$ and $m$ are relatively prime, $n$ is invertible in $â¤/mâ¤$, say with an inverse represented by $c â â¤$, that is: $nc \equiv 1 \bmod mâ¤$. Then, consider the <em>maps</em>
$$Î± \colon K â K,~k â¦ k^n\quad\text{and}\quad Î² \colon K â K, ~kâ¦k^c.$$
For all $k â K$, $(k^c)^n = k^{nc} = k^1 = k$ as $nc \equiv 1 \bmod mâ¤$ and $k^m = 1$ in $K$ by Lagrange. Hence $Î±$ is a left inverse to $Î²$, hence $Î±$ is surjective, hence $\{k^n;~k â K\} = \operatorname{img} Î± = K$.</p>
"
"2377877","2377882","<p>No by definition. Two functions are called equal iff they share the same domain and have the same functional value everywhere on the domain. Since $f$ and $g$ thus defined have different domains, they are not the same function. But you may say that $g$ restricted on $]0, +\infty[$ is $f$, for then they have the same domain and the same functional values. </p>
"
"2377883","2377891","<p>Because $$\frac{\binom{2n-1}{n-1}}{2n-1}=\frac{\binom{2n-2}{n-1}}{n}$$ and
$gcd(2n-1,n)=1$.</p>
"
"2377898","2377947","<p>Let $v_1$ satisfy $xv_1=\delta_0$ and $v_2$ satisfy $xv_2=\delta_2$. Then
$x(v_1-v_2) = xv_1-xv_2 = \delta_0-\delta_2.$</p>
"
"2377899","2377907","<p>As you pointed out...</p>

<p>$$
\frac{\sqrt{|h|}}{h}=\frac{\text{sgn}(h)\sqrt{|h|}}{\sqrt{|h|}\sqrt{|h|}}=\frac{\text{sgn}(h)}{\sqrt{|h|}},
$$
and the limit as $h\to 0$ from the right is $+\infty$, while the limit as $h\to 0$ from the left is $-\infty$. </p>
"
"2377900","2377948","<p>Your proof is hard to follow and according to my view indeed overly complicated.</p>

<p>This alternative proof might convince you that things can be done more easily.</p>

<hr>

<p>Let $x\in\bigcup F$. </p>

<p>Then (by definition) $x\in y\in F$ for some $y$. </p>

<p>Then on base of $F=\wp(B)$ we conclude that $y\in\wp(B)$ or equivalently $y\subseteq B$.</p>

<p>Then from $x\in y\wedge y\subseteq B$ it follows that $x\in B$.</p>

<p>Proved is now that $x\in\bigcup F\implies x\in B$ or equivalently $\bigcup F\subseteq B$, as requested.</p>
"
"2377902","2377957","<p>Your result is correct. The result from Mathematica is correct as well.</p>

<p>What is mysterious ? </p>

<p>Nothing because simplifying the Mathematica formula leads exactly to your formula.</p>

<p>$u(x,t)=\frac{1}{4}\left(4t^3\ln(x)-6t^2\ln^2(x)+4t\ln^3(x)-\ln^4(x)+\left(\ln(e^{-t}x)\right)^4 +4\Phi(e^{-t}x) \right)$</p>

<p>$\left(\ln(e^{-t}x)\right)^4 = \left(-t+\ln(x)\right)^4 = t^4-4t\ln^3(x)+6t^2\ln^2(x)-4t\ln^3(x)+\ln^4(x)$</p>

<p>After simplification :</p>

<p>$u(x,t)=\frac{1}{4}\left(t^4 +4\Phi(e^{-t}x) \right) = \frac{t^4}{4}+\Phi(\frac{x}{e^t})$</p>
"
"2377910","2377933","<p>No.</p>

<p>Define $\lambda_n$ by stating that $\lambda_{2^n}=2^{-n}$ and $\lambda_k=2^{-k}$ for other values of $k$.</p>

<p>Then $2^n\lambda_{2^n}=1$ so there is no convergence to $0$.</p>

<p>It is evident however that $\sum_n\lambda_n&lt;\infty$.</p>
"
"2377923","2377935","<p>I'm assuming you're coming from single variable calculus. 
You can rearrange the equation of the line to be $y = -x - 7$. You know the slope is of the tangent line is $-1$. Now you simply need to find the point at which the slope is equal to $-1$. Taking the derivative of the ellipse, we have $\frac{2x}{4} +\frac{2y}{9}\frac{dy}{dx} = 0$. Now see if you can finish it. </p>
"
"2377926","2378058","<p>This follows from the deep result of John Thompson that the kernel of a finite Frobenius group is nilpotent. Equivalently, a finite group that admits a fixed-point-free automorphism of prime order is nilpotent.</p>

<p>The original reference is: Thompson, John G. (1960), ""Normal p-complements for finite groups"", Mathematische Zeitschrift, 72: 332â354.</p>

<p>There is also a complete proof in the book ""Permutation Groups""  by D. Passman.</p>
"
"2377930","2377951","<p>Base $KC_1$ of triangle $KB_1C_1$ is ${3\over4}$ of $KC$, and its altitude $B_1N_1$ is ${2\over3}$ of the altitude $BN$ in triangle $KBC$ (by similar triangles). 
Thus the area of triangle $KB_1C_1$ is ${2\over3}\cdot{3\over4}={1\over2}$ of the area of $KBC$. </p>

<p>Height $A_1H_1$ from $A_1$ to the plane of face $KBC$ is ${1\over2}$ of height $AH$ from $A$ to $KBC$ (by similar triangles). </p>

<p>Hence the volume of $KA_1B_1C_1$ is ${1\over2}\cdot{1\over2}={1\over4}$ the volume of $KABC$.</p>

<p><a href=""https://i.stack.imgur.com/caPeY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/caPeY.png"" alt=""enter image description here""></a></p>
"
"2377934","2377943","<p>$AB = I \implies A, B$ are non-singular matrices.</p>

<p>Proof by contradiction:</p>

<p>Suppose $A$ is singular. $\det(A) = 0.$  </p>

<p>Since $\det(AB) = \det(A)\det(B),$ if $\det(A) = 0$ then $\det(AB) = 0.$ </p>

<p>$\det(I)  =1.$
$\det(AB) \ne \det(I) \implies AB\ne I$ </p>

<p>This violates the given condition that $AB= I.$  </p>

<p>$A$ is non-singular.</p>

<p>Same logic can be applied for $B.$</p>
"
"2377938","2378794","<p>For 1) $\langle P^t(v),P^t(v)\rangle=\langle v,PP^t(v)\rangle=\langle v,P^tP(v)\rangle=\langle P(v),P(v)\rangle$ is not sufficient to deduce $P^t = P$ for example if we P was a unitary matrix ($P^tP = PP^t = I$) we would have $\langle P^t(v),P^t(v)\rangle=\langle v,PP^t(v)\rangle=\langle v,P^tP(v)\rangle=\langle P(v),P(v)\rangle$ but $P \neq P^t$.</p>

<p>For 2) $\langle P(v)-P^t(v),P(v)-P^t(v)\rangle=2(\langle P(v),P(v)\rangle-\langle P(v),P^t(v)\rangle)$ this formula combined with the two formulas $\langle P(v),P(v)\rangle=\langle P(x),P(x)\rangle$ and $\langle P(v),P^t(v)\rangle=\langle P(x),P(x)\rangle$ prove that $\langle P(v)-P^t(v),P(v)-P^t(v)\rangle = 0$ which implies that $P(v) = P^t(v)$. </p>
"
"2377942","2378167","<p>Not always.  Suppose $P(X=\nu)=1$, where $\nu$ is a constant, different from $\mu;$ in other words $F=\delta_\nu.$  Since $F$ is an extreme point in the space of probability measures, both $G$ and $H$ must equal $F$.  But the expectation of $G$ is then wrong.</p>

<p>More elaborately, let $F= (\delta_0 + \delta_1)/2$ put half its mass at $0$ and half at $1$.  Then $G$ and $H$ must also be supported on $\{0,1\}$, and hence $P(Y=1) = \mu$ and thus $P(Y=0)=1-\mu$.  Already we know that $\mu$ must satisfy $0\le\mu\le1$.
Similarly, $P(Z=1)$ must then solve $1/2 = p\mu + P(Z=1)$ which implies $$0 \le \frac{1/2-\mu p}{1-p} \le 1.$$</p>

<p>If I can think of a simple condition involving $F$, $p$, and $\mu$ alone, allowing a decomposition, I'll post it.</p>

<p>ADDED 2 hours later.  </p>

<p>Suppose $F$ is continuous, with support $\mathbb R$.  Since $F$ is a non trivial mixture of $G$ and $H$, we have $G\ll F$ and $H\ll F$, and thus $G$ and $H$ have Radon-Nikodym derivatives with respect to $F:$ call them
$\gamma$ and $\eta$.  The desired decomposition  thus requires $p\gamma(x) +(1-p)\eta(x)=1$ for almost all (w.r.t. $F$) values of $x$,
and $\int_{\mathbb R} \gamma(x) F(dx)=\int_{\mathbb R} \eta(x) F(dx) = 1$ and $\int_{\mathbb R} x \gamma(x) F(dx)=\mu$.  And, of course, $\gamma(x)\ge 0$ and $\eta(x)\ge 0$, for almost all $x$.This is, in a sense, a linear programming problem (or feasibility problem).  The unknown function $\eta$ can be eliminated from this setup by noting that given $\gamma$, we find $\eta=(1-p\gamma)/(1-p)$, and the non-negativity constraint on $\eta$ can be transferred to $\gamma$ by requiring $0\le \gamma(x) \le 1/p$.</p>

<p>I think the following sketch shows when such a $\gamma$ exists.  For each $t\in [0,1-p]$ let $A(t) = F^{-1}(1-p-t)$ and $B(t) = F^{-1}(1-t)$.  Thus, $p$ of $X$'s mass lies between $A(t)$ and $B(t)$, and $t$ of the mass lies above $B(t)$.  Now consider the decreasing  function $m(t) = \int_{A(t)}^{B(t)} x F(dx)/p$.  If $\mu = m(t)$ for some $t$, we are done: let $\gamma=I_{[A(t),B(t)]}/p$.  If there is no such $t$, I <em>think</em> the problem is infeasible.  Which could happen if $\mu$ is too big.</p>
"
"2377962","2377977","<p>It's correct, though you haven't really justified that the limit of $y'$ is $+\infty$ rather than $-\infty$.</p>

<p>Multiply your implicit solution by $y$, and you get a quadratic you can solve explicitly for $y$. </p>
"
"2377963","2377971","<p>It is false. Evaluating $\,f = rp+sr $ at $x=0,y=1$ yields $\,1 = r\cdot 0 + s\cdot 0 = 0,\,$ contradiction.</p>
"
"2377966","2378134","<p>The answer might be a bit more complicated, as it depends a little on what you actually want to solve. In particular, is this a functional equation on the real variable $x$ or is $x$ restricted to be an  integer? In any case the solution is not unique without specifying more details.</p>

<p>If the latter is the case, we are simply dealing with a recurrence relation:
$$
f(n) = k f(n-1) - f(n-2)
$$
with the characteristic polynomial
$$
x^2 - k x + 1 =0
$$
which has the roots
$$
\lambda_\pm = \frac{k \pm \sqrt{k^2-4}}{2}
$$
and the general solution
$$
f(n) = c_+ \lambda_+^n + c_- \lambda_-^n
$$
for some constants $c_+$ and $c_-$. Their values would follow from two given functional values, which need to be stated in order to have a unique solution. Note that for $k^2 &lt; 4$ the roots are complex and that there is a special case for $k=\pm 2$ where things are slightly modified.</p>

<p>If the problem is actually a functional equation that is to be solved for $x \in {\mathbb R}$, it gets more interesting and there are many different types of solutions that can be continuous and/or discontinous.</p>

<p>For instance we can find the solution
$$
f(x) = g(x) ~\left(\lambda_+\right)^x 
$$
which is valid for any periodic function $g(x)=g(x+1)$. This is easier to see if we introduce constants $\alpha_\pm$ such that $\lambda_\pm = e^{\alpha_\pm}$, because inserting this in the functional equations gives:
$$
f(x-1) + f(x+1) = g(x-1)e^{\alpha_+ (x-1)} + g(x+1)e^{\alpha_+ (x+1)} \\= g(x) e^{\alpha_+ (x-1)} \left( 1 + e^{2 \alpha_+} \right) \\= g(x) e^{\alpha_+ (x-1)} \left( k e^{\alpha_+} \right) \\= k f(x)
$$
A whole family of real, continuous solutions is for instance given by
$$
f(x) = \cos(2 \pi k x + \delta) \left( e^{\alpha_+ x} + e^{\alpha_- x} \right)
$$
for any constant $\delta$ and integer $k$. This also means that there is no unique answer unless additional restrictions to the solution are given.</p>

<p>As to solving these type of problems that depends a bit on whether it is a sequence of numbers or a problem on functions. For functional equations is often looking for fixed points $f(c)=c$ or symmetries such as $f(x)=-x$. Sometimes it is possible to take suitable derivatives or special combinations in multi-variable equations. Another approach is to make an expansion in the vicinity of a fixed point that might reveal a clue to the solution. In general I am inclined to say that these problems are rather tough and that there is no generic strategy that will work, but that it requires creativity to solve them and of course practice helps a lot. </p>
"
"2377985","2377990","<p>Consider some topological fields such as $\Bbb R$ and $q=2$</p>
"
"2377992","2378005","<p>Your final expansion should flip the intersections into logical disjunctions (use De Morgan's)</p>

<p>Thus, we would go from</p>

<p>$$(a,a) \notin (R_1 \cap \overline{R_2}) \land (a,a) \notin (\overline{R_1} \cap R_2)$$</p>

<p>to</p>

<p>$$((a,a) \notin R_1 \lor (a,a) \in R_2) \land ((a,a) \in R_1 \lor (a,a) \notin R_2)$$</p>

<p>which is always true.</p>
"
"2378003","2378121","<p>$(a_i,b_i)$ are solutions of $\, L(a,b) =  (1,-\lambda)\cdot (a,b) = a-b\lambda = 0.\,$ Since $\,L\,$ is <em>linear,</em> the solutions form a vector space $V$, i.e solutions are closed under addition and scalings, therefore $$\,(a_i,b_i)\in V\ \Rightarrow\ \sum_i \lambda_i (a_i,b_i)\, =\, \left(\sum_i \lambda_i  a_i,\, \sum_i \lambda_i b_i\right ) \in V$$</p>
"
"2378007","2378049","<p>Suppose there is such an $E.$ Since $x\to m([0,x]\cap E)$ is continuous, we actually have $m([0,x]\cap E)=x/3$ for all $x\in [0,1].$ Verify that this implies $m([x,y]\cap E)= (y-x)/3$ for all $[x,y]\subset [0,1].$</p>

<p>Now $m(E)=m(E\cap [0,1]) = 1/3.$ So there are closed intervals $I_1,I_2,\dots $ such that $E \subset \cup I_n$ and $\sum m(I_n) &lt; 1/2.$ Thus</p>

<p>$$\frac{1}{3} = m(E) \le \sum m(E\cap I_n) = \sum\frac{m(I_n)}{3} = \frac{1}{3}\sum m(I_n) &lt; \frac{1}{3}\cdot \frac{1}{2} = \frac{1}{6}.$$</p>

<p>That's a contradiction, proving that such an $E$ does not exist.</p>
"
"2378010","2378035","<p>For $n = 1$, $\dfrac{1}{2} &lt; 1$, so it is true. Assume it is true for $n = k \ge 1$, you show it is also true for $n = k+1$. You have:
$\dfrac{1}{2}+\dfrac{1}{4} +\cdots +\dfrac{1}{2^{k+1}}= \dfrac{1}{2}+\dfrac{1}{2}\left(\dfrac{1}{2}+\dfrac{1}{4}+\cdots+\dfrac{1}{2^k}\right)&lt; \dfrac{1}{2}+\dfrac{1}{2}\cdot 1 = 1$ by the inductive step $n=k$, so it is true for $n = k+1$, and hence is true for all $n \ge 1$.</p>
"
"2378011","2378161","<p>Since both, left hand side and right hand side of
\begin{align*}
(1 + i) z - 2 \overline{z} = -11 + 25i\tag{1}
\end{align*}
represent a complex number, we can equate real and  imaginary part  of them.  We  set $z=x+iy$ with real part $x$ and imaginary part $y$.</p>

<blockquote>
  <p>We obtain
  \begin{align*}
\Re\left((1+i)(x+iy)-2(x-iy)\right)&amp;=(x-y)-2x\\
&amp;=-x-y\tag{2}\\
\Im\left((1+i)(x+iy)-2(x-iy)\right)&amp;=(x+y)+2y\\
&amp;=x+3y\tag{3}
\end{align*}</p>
  
  <p>Equating real part (2) and imaginary part (3) with the RHS of (1) we get
  \begin{align*}
-x-y&amp;=-11\\
x+3y&amp;=25
\end{align*}
  and finally obtain 
  \begin{align*}
z&amp;=x+iy\color{blue}{=4+7i}
\end{align*}</p>
</blockquote>
"
"2378014","2378020","<p>Neither of these relations is reflexive: Reflexivity means that for <strong>all</strong> elements $a \in A, (a,a) \in R$. In this case, $(3,3) \notin R_1$ and $(1,1) \notin R_2$.</p>
"
"2378030","2381519","<p>Too long for a comment. </p>

<p>Banach-Tarski theorem shows that some conditions imposed on the volume (or mass) cannot hold simultaneously. I think that the answer to your Question 2 depends on which properties mass should have. Do we require that each set have a mass? Is mass translation and rotation invariant? That is, is mass $m(B)$ of a copy $B$ of a set $A$, which is shifted and rotated with respect to the set $A$ the same as mass $m(A)$ of the set $A$? Ignoring mass invariance we can simply define it via delta function, choosing a point $x_0$ and setting $m(A)=1$ if $A$ contains $x_0$ and $m(A)=0$, otherwise. </p>

<p>Which additivity properties should have a mass? I mean the following. If $A$ is a union of a disjoint  family $\{A_i: i\in I\}$ then $A$ have mass and $m(A)=\sum \{m(A_i): i\in I\}$. Usually it is required that this property takes place for each finite $I$ (additivity) of each countable $I$ ($\sigma$-additivity). I remark that since the group $\Bbb R^3$ is abelian, it is so-called amenable and it admits finitely additive (but not necessarily $\sigma$-additive) translation invariant (but not necessarily rotation invariant) non-trivial mass (that is $m(\Bbb R^3)=1$) which is defined on <em>each</em> subset $A$ of $\Bbb R^3$. </p>

<p>A demand that mass additivity holds for uncountable cardinality of the set $I$ may lead to the following paradox. Let $A$ be an uncountable set. If uncountably many points of $A$ has positive mass, how we sum these values to obtain mass of $A$? Conversely, if each point of $A$ has mass $0$, then how can mass of $A$ be non-zero? Even mass $\sigma$-additivity with its translation invariance can lead to contradictions, similarly to that used in a construction of a non-measurable <a href=""https://en.wikipedia.org/wiki/Vitali_set"" rel=""nofollow noreferrer"">Vitali</a> set.</p>

<p>The above paradoxes may lead to a conclusion that the mass is a property not of each set and it is not concentrated in points, similarly to other physical notions. I recall Zeno âarrowâ aporia on motion. When a flying arrow moves if at each moment it has a fixed position? Also how time can have duration if it is composed of moments none of which has duration? </p>
"
"2378032","2378046","<p>Your descriptions of the intersection and the difference are incorrect.  When intersecting two events, you should think ""$A\cap B$ is the event that both $A$ and $B$ occur.""  In this case, you select at least one odd number (event $A$), and at most one odd number (event $B$).  This means that you have selected exactly one odd number, thus
$$ A\cap B : \text{exactly one odd number is selected}.$$</p>

<p>For the difference, think ""$A\setminus B$ is the event that $A$ happens, but not $B$.""  In this case, you select at least one odd number (event $A$), but not 0 or 1 odd numbers (the event ""not $B$"").  Thus
$$ A \setminus B : \text{at least two odd numbers are selected}.$$</p>

<p>Finally, $B\setminus A$ is the event that you select at most one odd number (event $B$), but fewer than 1 odd numbers (the event ""not $A$"").  Hence
$$ B \setminus A : \text{no odd numbers are selected}.$$</p>
"
"2378051","2378060","<p>We really should only care about case 2, since case 1 doesn't help provide a value for $y$ that works for <strong>every</strong> real number $z$.</p>

<p>In case 2, we can use the binomial formula to write $$(x+z)^2 - (x^2 + z^2) = (x^2 + 2xz + z^2) - x^2 - z^2 = 2xz$$. Thus, if we set $y = 2x$, we always satisfy $(x+z)^2 - (x^2 + z^2) = (2x)z = yz$.</p>
"
"2378052","2379074","<p>Let $f$ be your map, i.e.:
$$f(p) := \frac{1}{1+e^{-\beta\left(u_1-u_2+J(2p-1)\right)}}.$$</p>

<p>Suppose your state $p$ is close to the fixed point $p^*$, namely it is $p := p^*+Îµ$. Then you can estimate how quickly the distance to the fixed point shrinks via the following quantity:</p>

<p>$$
Î
:= \left | \frac{f(p) - p^*}{p-p^*}\right |
 = \left | \frac{f(p^*+Îµ) - f(p^*)}{Îµ}\right |
 â \left | f'(p^*) \right |.
$$</p>

<p>With more iterations, the distance behaves like $Îµ Î^t$ (where $t$ is the number of iterations). The quantity $Î» := \ln(Î)$ is also known as the local <a href=""//en.wikipedia.org/wiki/Lyapunov_exponent"" rel=""nofollow noreferrer"">Lyapunov exponent</a> at the point $p^*$. For more details on the exponential behaviour, see this <a href=""https://math.stackexchange.com/q/2222090/65502"">question</a>.</p>

<p>So to compare convergence behaviours close to fixed points, you only need to take a look at the derivative of your map. For multidimensional systems, things are a bit more complicated: There are multiple Lyapunov exponents and the one you need is the largest one, which in turn you can obtain from eigenvalues of the Jacobian of $f$.</p>
"
"2378065","2378069","<p>Do you know about geometric series?</p>

<p>In this problem, you have
$$ \sum_{k=0}^{\infty} \frac{(-3)^k}{4} = \sum_{k=0}^{\infty} \left( \frac{-3}{4} \right)^k = \frac{1}{1-\left(\frac{-3}{4}\right)}, $$
and
$$ \sum_{k=0}^{\infty} \frac{5}{4^k} = 5 \sum_{k=0}^{\infty} \left( \frac{1}{4} \right)^k = 5 \cdot \frac{1}{1-\frac{1}{4}}$$
Simplify and add to get what you want.</p>

<hr>

<p>A quick, not entirely rigorous justification:  suppose that $|r| &lt; 1$, and that we want to sum
$$ \sum_{k=0}^{\infty} r^k. $$
Since $|r|&lt;1$, we know that this series converges by the ratio test.  So, suppose that the limit is $S$; that is
$$ S := \sum_{k=0}^{\infty} r^k = 1 + r + r^2 + r^3 + \cdots. $$
Multiplying by $r$, we get
$$ rS = r + r^2 + r^3 + r^4 + \cdots .$$
Subtracting, we have
\begin{align}
&amp;S - rS = (1+r+r^2+\cdots) - (r+r^2+r^3+\cdots) \\
&amp;\qquad\implies (1-r)S = 1 + (r-r) + (r^2-r^2) + \cdots = 1 \\
&amp;\qquad\implies S = \frac{1}{1-r},
\end{align}
which is the result used above.  Note that the second line is justified, as series involved all converge absolutely, and so we may rearrange the terms without being too careful.</p>
"
"2378073","2378095","<p>Let $k&lt;m+1$. Then $k-1&lt;m$</p>

<p>Using definition and inductive hypothesis,</p>

<p>$$f^{m+1}:=f\circ f^m=f\circ (f^{k-1}\circ f^{m-k+1})=(f\circ f^{k-1})\circ f^{m+1-k}:=f^k\circ f^{m-k+1}$$</p>

<p>Third equality comes from associativity, rest are definitions and inductive hypothesis</p>
"
"2378075","2379376","<p>Yes, there is such a state.</p>

<p>At least <a href=""http://www.randelshofer.ch/rubik/vcube6/X900.01.html"" rel=""nofollow noreferrer"">http://www.randelshofer.ch/rubik/vcube6/X900.01.html</a> claims to present two sequences of moves that each will create one.</p>

<p>Here they are (but I'm not sure how to read the notation):</p>

<blockquote>
  <p>Algorithm Michael Z. R. Gottlieb 2008<br>
  F R' B' D2 Â· F' B2 Â· D2 L D' F R' F' U' F' R' B R U2
  TF TR' TB' TD2 Â· TF' TB2 Â· TD2 TL TD' TF TR' TF' TU' TF' TR' TB TR TU2
  T3B T3L T3U2 T3B2 T3L' T3B T3U T3L' T3U2 T3L' T3B' (47 btm)</p>
</blockquote>

<p>First</p>

<p><a href=""https://i.stack.imgur.com/8WB9j.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8WB9j.jpg"" alt=""part1""></a></p>

<p>Second</p>

<p><a href=""https://i.stack.imgur.com/LuSMl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LuSMl.jpg"" alt=""part2""></a></p>

<p>Final</p>

<p><a href=""https://i.stack.imgur.com/XoKmz.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XoKmz.jpg"" alt=""part3""></a></p>

<blockquote>
  <p>Algorithm Walter Randelshofer 2014<br>
  U SF SR SD' F2 SR2 B U SF2 D2 WR2 U WR2
  TU S2F S2R S2D' TF2 S2R2 TB TU S2F2 TD2 M2R2 TU M2R2
  T3U2 T3B T3L2 T3U' T3L2 T3U T3B2 T3L' (34 btm)</p>
</blockquote>
"
"2378089","2378096","<p>This is the right idea, and then you want to use the monotone convergence theorem:</p>

<p>$$\sum m(E_n)=\int\sum1_{E_n}\,\mathrm dm.$$</p>

<p>If $x$ appears in exactly $3$ of the $E_n$, what is the value of $\sum1_{E_n}(x)$?</p>
"
"2378092","2378100","<p>Note that $$\angle CPD + \angle PCD+\angle PDC = \pi$$</p>

<p>$$\angle CQD +\angle QCD+\angle QDC = \pi$$</p>

<p>and $$\angle QCD =\angle QCP + \angle PCD\quad \&amp; \quad \angle QDC = \angle QDP +\angle PDC$$</p>

<p>The desired result follows at once.</p>
"
"2378101","2378111","<p>Your answer to 1) is fine, but your answer to 2) is incorrect. This is because the events are not independent.</p>

<p>It's easiest here to think of this entirely in terms of subsets. We want the ratio of the number of subsets with exactly $k$ defective to the total number of subsets. The total is $\binom{20}4$. To count the desired subsets, note that a set with exactly $k$ defective consists of one set of $k$ defective elements and one set of $4-k$ good elements. Compute these two numbers. The result is their product, representing all possibilities.</p>
"
"2378105","2378117","<p>Consider, in $\mathbb{R}^2$, the following counterexample:</p>

<p>$v_1 = (1, 0) \\
v_2 = (0, 1) \\
w = (-\frac{1}{2}, -\frac{1}{2})$</p>

<p>Then you have that:</p>

<p>$a + w + b+w = 0$ so they are not linerly independent.</p>

<p>It is true, however, when w is linearly independent from the elements of the list.</p>
"
"2378122","2378159","<p>Let $f(t) = \mu \{ t \}$. </p>

<p>Suppose $\mu \{ x \} = 0$ and $f'(x) \neq 0$ (by which we mean $f'(x)$ might not exist). Then there is $\varepsilon &gt; 0$ such that arbitrarily close to $x$ there exists $y$ with $\mu \{ y \} \geqslant \varepsilon |y-x|$. </p>

<p>There is at most countably many $y$ with $\mu \{ y \} &gt; 0$ - if there is finitely many, of course $f$ is a.e. differentiable, otherwise arrange all such $y$ into a sequence $y_m$. Then $$\sum_{m=1}^{\infty} \mu \{ y_m \} = \mu( \{ y_m : m \geqslant 1 \} ) \leqslant 1.$$ </p>

<p>Now, for $\varepsilon &gt; 0$ let $I_m^{\varepsilon} = \{ x \in \mathbb{R} : \mu \{ y_m \} \geqslant \varepsilon |y_m-x| \}$ so $I_m^{\varepsilon}$ is an interval of radius $\frac{\mu \{ y_m \}}{\varepsilon}$ centered at $y_m$. By the remark in the beginning</p>

<p>$$\{ x \in \mathbb{R} : \mu \{ x \} = 0 \ \&amp; \ f'(x) \neq 0 \} \subseteq \bigcup_{\varepsilon &gt; 0} \bigcap_{n=1}^{\infty} \bigcup_{m=n}^{\infty} I_m^{\varepsilon}.$$</p>

<p>Now, for a fixed $\varepsilon &gt; 0$ and $n \geqslant 1$ we have </p>

<p>$$\lambda \left( \bigcup_{m=n}^{\infty} I_m^{\varepsilon} \right) \leqslant \sum_{m=n}^{\infty} \lambda( I_m^{\varepsilon} ) = \frac{2}{\varepsilon} \sum_{m=n}^{\infty} \mu \{ y_m \}.$$</p>

<p>As the series $\displaystyle \sum_{m=1}^{\infty} \mu \{ y_m \}$ converges, the above tends to $0$ as $n \to \infty$. Therefore</p>

<p>$$\lambda \left( \bigcap_{n=1}^{\infty} \bigcup_{m=n}^{\infty} I_m^{\varepsilon} \right) = 0$$</p>

<p>and since the union over $\varepsilon &gt; 0$ can be made countable by taking it over a sequence $\varepsilon_k = \frac{1}{k}$, we obtain </p>

<p>$$\lambda \big( \{ x \in \mathbb{R} : \mu \{ x \} = 0 \ \&amp; \ f'(x) \neq 0 \} \big) = 0.$$</p>

<p>Finally, if $\mu \{ x \} \neq 0$, then $x = y_m$ for some $m$, so </p>

<p>$$\lambda \big( \{ x \in \mathbb{R} : \mu \{ x \} \neq 0 \} \big) = 0.$$</p>

<p>Therefore</p>

<p>$$\lambda \big( \{ x \in \mathbb{R} : f'(x) \neq 0 \} \big) = 0,$$</p>

<p>i.e. $f'(x) = 0$ almost everywhere.</p>

<h2>Example</h2>

<p>Recall the construction of the <a href=""https://en.wikipedia.org/wiki/Cantor_set"" rel=""nofollow noreferrer"">Cantor set</a>:</p>

<p>$$\mathcal{C} = \bigcap_{n=1}^{\infty} C_n$$ </p>

<p>where each $C_n$ consists of $2^n$ intervals of total length $\left( \frac{2}{3} \right)^n$. Arrange them all in a sequence $I_m$ and let $y_m$ be the middle of $I_m$. Note that $y_m$ are pairwise distinct.</p>

<p>Now define a measure $\mu$ on $\mathbb{R}$ to be $\frac{1}{2}|I_m|$ on $y_m$ and $0$ on $\mathbb{R} \setminus \{ y_m : m \geqslant 1 \}$. Note that $\mu$ is a probability measure and each $I_m$ is exactly $I_m^1$ from the proof above. Since each $a \in \mathcal{C}$ belongs to intervals $I_m^1$ of arbitrarily small length, $f$ is not differentiable in any point $a \in \mathcal{C}$. </p>

<p>The construction can be slightly modified to replace the Cantor set with a dense $G_{\delta}$ set.</p>

<h2>Summary</h2>

<p>For arbitrary probability measure $\mu$ on $\mathbb{R}$, the set of points where $t \mapsto \mu \{ t \}$ is not differentiable:</p>

<ul>
<li><p>must have measure zero;</p></li>
<li><p>may contain the Cantor set;</p></li>
<li><p>may contain a dense $G_{\delta}$ set. </p></li>
</ul>

<p>Also, as the OP has noted, wherever the derivative exists, it must be zero. </p>
"
"2378124","2378138","<p>The extremum of a quadratic function happens at the half-sum of its roots (whether real of complex), thus at $x=1$, so $F(1)=-3$.</p>

<p>Note this function is equal to
$$F(x)=a(x-1)^2-3 \quad\text{for some} \,a\in\mathbf R.$$</p>
"
"2378125","2378131","<p>It is possible to have a set $S$ which satisfies (1), (2), and (3').  Indeed, let $A$ be any nonempty set.  Recursively define $A_0=A$ and $A_{n+1}=A_n\cup\{\{a_1,a_2\}:a_1,a_2\in A_n\}$ for each $n\in\mathbb{N}$.  Then $S=\bigcup_{n\in\mathbb{N}} A_n$ is a set which contains $A$ and satisfies (3'), since for any $a_1,a_2\in S$, there are $m,n\in\mathbb{N}$ such that $a_1\in A_m$ and $a_2\in A_n$, and then $\{a_1,a_2\}\in A_{\max(m,n)+1}\subseteq S$.  It follows that $S$ also satisifes (2) by taking $a_1=a_2=a$.</p>

<p>More generally, a similar construction works for any version of (3) where you put a bound on the cardinality of the indexing set $I$, by iterating this process by a transfinite induction whose length has cofinality greater than $|I|$. For instance, if you wanted $S$ to actually contain all finite subsets of itself, you could do the same construction with $A_{n+1}=A_n\cup\{F:F\subseteq A_n\text{ and $F$ is finite}\}$.  Or if you wanted $S$ to contain all countable subsets of itself, you could do an induction of length $\omega_1$ where $A_{\alpha+1}=A_\alpha\cup\{C:C\subseteq A_\alpha\text{ and $C$ is countable}\}$.</p>
"
"2378133","2379515","<p>The interpolation norm seems to derive its name from the fact that it is sort of an `interpolation' between the $l_2$ and $l_1$ norms. In particular, if $s=1$ then $$||\mathbf{x}||_{1,2,1}=\sqrt{\sum_{l=1}^n{|x_l|^2}}=||\mathbf{x}||_2.$$ On the other hand, when $s=n$,.$$||\mathbf{x}||_{1,2,n}=\sqrt{{||\mathbf{x}||_1^2}}=||\mathbf{x}||_1.$$ </p>

<p>In MATLAB, all you need is to first sort the elements in a decreasing order of magnitude. Then you partition the $n$ elements into $\lceil n/s \rceil$ sets (you will have to append $s \lceil n/s \rceil-n$ zeros at the end of $\mathbf{x}$. Afterwards, you compute the $l_1$ norm of each partition and add their squared values before taking the square root.</p>
"
"2378136","2378193","<p>Let $I$ denote the identity function, $I(x) = x$. There's no reason to expect that <em>any</em> local condition (such as continuity, differentiability, or real-analyticity) guarantees that if $(f_{n}) \to I$ uniformly on $[-a, a]$ for all $a &gt; 0$, then $(f_{n}) \to I$ uniformly on $\mathbf{R}$.</p>

<p>The sequence $(f_{n})$ defined by
$$
f_{n}(x) = x + \tfrac{1}{n} x^{2}
$$
confirms the suspicion.</p>
"
"2378149","2378375","<p>Let me formalize and generalize the problem. You have a sequence of $n$
random variables, $X_1, X_2, \ldots, X_n,$ such that $X_i \sim U[0,1]$ for each $i.$ Let $X = \sum_{i=1}^n X_i.$
That is, $X$ is a <em>uniform sum distribution</em> with parameter $n.$</p>

<p>Then the number of points scored is $1 + Y,$ where
$$
 Y =
\begin{cases}
1 &amp; \text{if } X &lt; n/2,
\\ 
0 &amp; \text{if } X \geq n/2.
\end{cases}
$$</p>

<p>That is, $Y=1$ if the mean of the $n$ uniform random variables is less than $\frac12,$ and $Y=0$ otherwise.</p>

<p>In order to keep the discussion relatively simple, I'll assume the values of $X_1, X_2, \ldots, X_n$ are revealed one at a time, and that the ""point value"" of a number $q$ at any given time is the amount you should be willing to ""pay"" in order to replace the next random variable with the number $q.$</p>

<p>You can also apply formulas based on this interpretation to a situation in which you have some kind of opportunity to choose the values of one or more of $X_1, X_2, \ldots, X_n$ and then allow the remaining variables to be assigned randomly.</p>

<p>Now suppose the values of $h$ of the variables are known
(already randomly revealed or chosen by you), that is,
we know that $X_1 = x_1, \ldots, X_h = x_h,$
and suppose you have the opportunity to set an additional $k$ variables
to a certain sequence of values,
$X_{h+1} = x_{h+1}, \ldots, X_{h+k} = x_{h+k}.$
The ""point value"" of the sequence $(x_{h+1}, \ldots, x_{h+k})$
under those conditions is the difference between the expected value of $1+Y$ given that all $h+k$ of these variables are known to have the specified values and the expected value of $1+Y$ given only the first $h$ variables.
But the (conditional) expected value of $1+Y$ is just $1$ plus the (conditional) expected value of $Y$, so
\begin{multline}
V(x_{h+1}, \ldots, x_{h+k}) =
E(Y \mid X_1 = x_1 \land \cdots\land X_{h+k} = x_{h+k}) \\
 - E(Y \mid X_1 = x_1 \land \cdots\land X_h = x_h)
\end{multline}
In other words, the point value of the $k$ additional numbers is the amount by which we increase the conditional expectation of $Y$ when we include those $k$ numbers in the condition.</p>

<p>For example, with $h=0$ and $k=1$ we are assessing the point value of choosing the value of $X_1$ before any of the values are known.
The point value of the numerical value $x_1$ in those circumstances is
$$ V(x_1) = E(Y \mid X_1 = x_1 ) - E(Y) . $$</p>

<p>The conditional expectation $E(Y \mid X_1 = x_1)$ is the probability that after the remaining $n-1$ variables are chosen randomly, the total
(including $x_1$) will not exceed $n/2$; that is,
$$ E(Y \mid X_1 = x_1) = P\left(X_2+\cdots+X_n &lt; \frac n2 - x_1\right).$$</p>

<p>The probability on the right-hand side depends on a uniform sum distribution with parameter $n-1.$
For convenience, let $F_m$ be the cumulative distribution function of
a uniform sum distribution with parameter $m.$
Then
$$
P\left(X_2+\cdots+X_n &lt; \frac n2 - x_1\right) 
= F_{n-1}\left(\frac n2 - x_1\right).
$$
Of course 
$$E(Y) = P\left(X_1+\cdots+X_n &lt; \frac n2\right)
 = F_n\left(\frac n2\right) = 0.5.$$</p>

<p>The following table gives some approximate values of this probability when $n=5$ (as in the original question):</p>

<p>\begin{array}{lc}
x_1 &amp; E(Y \mid X_1 = x_1) \\
0   &amp; 0.7995\\
0.2 &amp; 0.6920\\
0.4 &amp; 0.5663\\
0.5 &amp; 0.5000\\
0.6 &amp; 0.4337\\
0.8 &amp; 0.3080\\
1   &amp; 0.2005\\
\end{array}</p>

<p>That is, in these circumstances the point value of $0$ is approximately $0.2995$ and the point value of $1$ is approximately $-0.2995,$
that is, we should desire to be paid at least $0.2995$ before we would be willing to choose to set $X_1 = 1.$</p>

<p>Going back to the general case in which $h$ variables have already been set
to the values $x_1, \ldots, x_h,$ we can show that
$$
V(x_{h+1}, \ldots, x_{h+k}) =
F_{n-h-k}\left(\frac n2 - (x_1 + \cdots + x_{h+k})\right)
 - F_{n-h}\left(\frac n2 - (x_1 + \cdots + x_h)\right).
$$</p>

<p>This is a general formula covering all cases of this kind;
the fly in the ointment is that the formula for $F_m$ can be a bit
annoying to work with. For the first few uniform sum distributions,
the cumulative probability functions are:</p>

<p>\begin{align}
F_1(x) &amp;= \begin{cases}
0 &amp; \text{if }x&lt;0,\\
x &amp; \text{if }0\leq x\leq 1, \\
1 &amp; \text{if }x &gt; 1,
\end{cases} \\
F_2(x) &amp;= \begin{cases}
0 &amp; \text{if }x&lt;0,\\
\frac12x^2 &amp; \text{if }0\leq x&lt;1,\\
1 - \frac12 (2 - x)^2 &amp; \text{if }1\leq x\leq 2,\\
1 &amp; \text{if }x &gt; 2,
\end{cases} \\
F_3(x) &amp;= \begin{cases}
 0 &amp; \text{if } x&lt;0,\\
\frac16x^3 &amp; \text{if } 0\leq x&lt;1,\\
-\frac13 x^3 + \frac32 x^2 - \frac32 x + \frac12 &amp; \text{if } 1\leq x&lt;2,\\
1 - \frac16 (3 - x)^3 &amp; \text{if } 2\leq x\leq 3,\\
1 &amp; \text{if }x&gt;3,
\end{cases} \\
F_4(x) &amp;= \begin{cases}
 0 &amp; \text{if } x&lt;0,\\
\frac{1}{24}x^4 &amp; \text{if } 0\leq x&lt;1,\\
-\frac18 x^4 + \frac23 x^3 - x^2 + \frac23 x - \frac16 
&amp; \text{if } 1\leq x&lt;2,\\
\frac18 (4 - x)^4 - \frac23 (4 - x)^3 + (4 - x)^2 - \frac23 (4 - x) + \frac76 
&amp; \text{if } 2\leq x&lt;3,\\
1 - \frac{1}{24} (4 - x)^4 &amp; \text{if } 3\leq x\leq 4,\\
1 &amp; \text{if } x &gt; 4.
\end{cases}
\end{align}</p>

<p>(Source: <a href=""http://www.wolframalpha.com/input/?i=uniform+sum+distribution+with+n%3D4"" rel=""nofollow noreferrer"">Wolfram Alpha</a>.)</p>
"
"2378155","2378294","<p>The characteristics method works well: the charateristics curves for the second problem are:</p>

<p>$$\begin{cases}
 u=c_1\\
 x^2/2-t=c_2
\end{cases}$$</p>

<p>Being the general solution:</p>

<p>$$u=f(x^2/2-t)$$</p>

<p>with $f$ some function.</p>

<p>Imposig the initial conditions, </p>

<p>$$f(x^2/2)=\cos x$$</p>

<p>$$f(y)=\cos\sqrt{2y}$$</p>

<p>Then, the particular solution must fulfill the condition:</p>

<p>$$u(x,t)=\cos\sqrt{x^2-2t}$$</p>

<p>But observe that the square root is not defined for values such that $2t\gt x^2$. This gives us a hint about what's going on here. The characteristics are parabolas, $x^2/2-t=c_2$ for which $u$ is constant and the boundary conditions we are imposing ($u(x,0)=\cos x$) say nothing about the parabolas not crosing the $x$ axis, e.g. $x^2/2-t=-1$, so, we can give any value to $u$ along such parabolas and we will not contradict the initial conditions. Further, such arbitrary assignation for $u$ along that family of parabolas can be done in a continuous and differentiable way, so, there are infinitely many solutions.</p>

<p>I don't get why you say that the line $x=0$ is characteristic. It is not. In fact, the line along which the boundary conditions are given is not a characteristic curve, and this is a necessary condition for the problem to have a single solution, but as we can see, it is not suficient.</p>

<p><strong>Added</strong>
The general solution for first problem would have to be very similar to the found for the second,</p>

<p>$$u(x,t)=\sin\sqrt{x^2-2t}$$</p>

<p>and it miss too determine the value of $u$ for curves with $2t\gt x^2$, but the point to consider here is different: the solution cannot be differentiable at $(0,0)$.</p>

<p>Consider the partial derivative along the line $x=0$ that, because the boundary conditions, for any solution it must coincide with the partial derivative for the function we have found:</p>

<p>$$\left.\frac{\partial u}{\partial t}\right|_{x=0}=\frac{t\cos\sqrt{-2t}}{\sqrt{2}(-t)^{3/2}}$$</p>

<p>that is not defined at $t=0$.</p>
"
"2378164","2378185","<p>Try induction on $m$:</p>

<p>Assume $f\colon [m]\to[n]$ bijective. If $m=0$, then $f$ is bijective iff $n=0$. </p>

<p>Assume $m&gt;0$ and $m=m'+1$. Clearly, $n&gt;0$, so $n=n'+1$.
Now $f\restriction_{[m']}$ is a bijection $[m']\to[n]\setminus\{f(m)\}$. By a simple transposition, we find a bijection $g\colon [n]\setminus\{f(m)\}\to [n']$ and hence a bijection $g\circ f\restriction_{[m']}\colon [m']\to[n']$. By induction hypothesis, $m'=n'$ and hence $m=n$.</p>
"
"2378165","2378181","<p>In one sense -- in the end, where you draw branch cuts is kind of arbitrary, and really just up to you. In the other sense -- you may want to create branch cuts in places that are meaningful, or that follow some kind of convention (because typically those conventions are useful).</p>

<p>The latter would suggest one of two things to me: convention for $\sqrt(z)$ is to have a branch cut usually either where $Im(z) = 0, z &gt; 0$, or where $Im(z) = 0, z &lt; 0$. The former would mean -- in your case -- having four branch cuts from your 4 poles going in towards zero. (Equivalently, two branch cuts, that cross at the origin.) The latter would mean having 4 branch cuts that go out from your poles to infinity.</p>

<p>Which is better? Depends what you want to do. If you want to integrate around all 4 poles, then the former is clearly better. If you want to move freely inside the circle where $|z| &lt; 1$, then the latter is clearly better.</p>

<p>What if you want some of both? Then, as you said, you need to allow even multiples but no single poles. So, pair them up! You can have one branch cut reaching in a straight line from $z=1$ to $z=i$, and another branch cut reaching in a straight line fro $z=-1$ to $z-i$. For example.</p>
"
"2378168","2378186","<p>Recognize that all the other values, that is, $z_j$ for $j\neq i$, are constants; and that they only appear in the sum. Then define
$$q = \sum_{j\neq i} z_j$$
and then we get just
$$f(z) = \frac{z_i}{q + z_i}$$
$$f'(z) = \frac{q}{(q+z_i)^2} = \frac{\sum_{j\neq i} z_j}{\left(z_i + \sum_{j\neq i} z_j\right)^2} = \frac{\sum_{j\neq i} z_j}{\left(\sum_{j} z_j\right)^2}$$</p>

<p>Note that I, like S. Ong, got a different expression than you asked for: no factor of $z_i$ on top. I don't know if that's a mistake in the book, in your copying, or in our interpretation.</p>
"
"2378169","2378172","<p>This fact is use to show that $g(0)=g(0e^{\theta})=f_0(e^{i\theta})$ is well defined and does not depend of $\theta$.</p>
"
"2378173","2378551","<p>Assuming that $b$ is significantly larger than $d$, you could generate a table $\left[x_i,F(x_i) \right]$ over the range of interest and try to curve fit the points using as a model $$F(x)=e\sin(x-\theta)+\color{red}{f}$$ but the result will just be an approximation.</p>

<p>For illustration purposes, I used $a=5$, $b=6$, $c=7$, $d=1$, $\phi=\frac \pi 3$ and generated $25$ equally spaced values in the range $0\leq x \leq 2\pi$.</p>

<p>A standard nonlinear regression lead to $$F(x)=0.92056 \sin (x-0.10377)+0.75191$$ to which corresponds $R^2=0.9978$ and the following results
$$\begin{array}{clclclclc}
 \text{} &amp; \text{Estimate} &amp; \text{Standard Error} &amp; \text{Confidence Interval} \\e &amp; 0.92056 &amp; 0.01452 &amp; \{0.89037,0.95075\} \\
 \theta &amp; 0.10377 &amp; 0.01519 &amp; \{0.07218,0.13536\} \\
 f &amp; 0.75191 &amp; 0.01008 &amp; \{0.73095,0.77287\} \\
\end{array}$$</p>

<p><strong>Edit</strong></p>

<p><em>I do not know where we should go with the following.</em></p>

<p>Assuming, as you wrote in comments, that, in the model $$F(x)=\frac{a + b\sin(x)}{c - d\sin(x-\phi)}$$ $c$ is much larger than $d$. So, by Taylor, $$F(x)\approx G(x)=\frac{(a+b \sin (x)) (c+d \sin (x-\phi ))}{c^2}$$ Now computing $$\Psi=\int_0^{2\pi}\left(G(x)-e\sin(x-\theta)-f\right)^2\,dx$$ which is doable, compute $$\frac{d\Psi}{de}\qquad \frac{d\Psi}{d\theta}\qquad \frac{d\Psi}{df}$$ and try to make them equal to zero.</p>

<p>The formulae are too long to type and I shall go to the results (obtained by successive eliminations) $$f=\frac{2 a c+b d \cos (\phi )}{2 c^2}$$ $$e=\frac{a d \cos (\theta -\phi )+b c \cos (\theta )}{c^2}$$ and we end with an equation for $\theta$
$$a d (a d \sin (2 (\theta -\phi ))+2 b c \sin (2 \theta -\phi ))+b^2 c^2 \sin (2
   \theta )=0$$ that we need to solve using a numerical method such as Newton's.</p>

<p>Using the same numbers as before, we get $$e=\frac{5}{49} \sin \left(\theta +\frac{\pi }{6}\right)+\frac{6 }{7}\cos (\theta )\qquad \qquad f=\frac{73}{98}$$ and the equation to be solved is $$1764 \sin (2 \theta )-25 \sin \left(2 \theta+\frac{\pi}{3} \right)-420 \cos
   \left(2 \theta +\frac{\pi }{6}\right)=0$$ the first solution of which being $\theta\approx 0.0970008$, making $e=0.912453$ and $f=0.744898$ which is quite close to what was obtained by nonlinear regression.</p>

<p><strong>Edit</strong></p>

<p>For sure, we could continue the initial Taylor expansion and use $$G(x)=\frac{(a+b \sin (x)) \left(c^2+c d \sin (x-\phi )+d^2 \sin ^2(x-\phi )\right)}{c^3}$$ and use the same procedure as above.</p>

<p>This would lead to $$f=\frac{2 a c^2+a d^2+b c d \cos (\phi )}{2 c^3}$$ $$e=\frac{d (4 a c \cos (\theta -\phi )+b d \cos (\theta -2 \phi ))+2 b \left(2
   c^2+d^2\right) \cos (\theta )}{4 c^3}$$ leaving us with the equation $$d \left(d \left(4 \sin (2 (\theta -\phi )) \left(4 a^2 c^2+b^2 \left(2
   c^2+d^2\right)\right)+b d (8 a c \sin (2 \theta -3 \phi )+b d \sin (2 (\theta -2
   \phi )))\right)+16 a b c \left(2 c^2+d^2\right) \sin (2 \theta -\phi )\right)+4
   b^2 \left(2 c^2+d^2\right)^2 \sin (2 \theta )=0$$ to be solved for $\theta$.</p>

<p>Using the same values as before, $\theta\approx 0.100410$, making $e=0.919354$ and $f=0.752187$ which is still closer to what was obtained by nonlinear regression.</p>
"
"2378184","2378479","<p>No, it's possible for two entirely different sequences in $\beta\omega$ to have the same limit with respect to some ultrafilter $p$. A rather silly example is obtained by taking any sequence $(s_n)$ of distinct points in $\beta\omega$ and any non-principal $p\in\beta\omega$, and then letting $(t_n)$ be the constant sequence with all $t_n$ equal to the $p$-limit of $(s_n)$.</p>

<p>But there are also serious examples. Among the most important of these are those arising from idempotent ultrafilters.  A nonoprincipal ultrafilter $p$ on $\omega$ is called idempotent if the sequence of translates $(p+n)_{n\in\omega}$ has $p$-limit equal to $p$ itself.  (Here $p+n$ is defined as the ultrafilter generated by the $n$-translates $A+n=\{a+n:a\in  A\}$ of all the sets $A\in p$.)  Note that any $p\in\beta\omega$ is the $p$-limit of the sequence $(n)_{n\in\omega}$ of principal ultrafilters.  So an idempotent ultrafilter $p$ is simultaneously the $p$-limit of a sequence of principal ultrafilters and of a sequence of nonprincipal ultrafilters.</p>

<p>The existence of idempotent ultrafilters is a nontrivial result (a special case of a theorem of Ellis about existence of idempotent elements in compact semi-topological semigroups). The proof can be found in many places, for example in a survey paper of mine available at <a href=""http://www.math.lsa.umich.edu/~ablass/ufdyn.pdf"" rel=""nofollow noreferrer"">http://www.math.lsa.umich.edu/~ablass/ufdyn.pdf</a> (see Theorem 2 and its corollary).  For a great deal more information about idempotent and related ultrafilters and their applications, see the book ""Algebra in the Stone-Cech Compactification"" by Neil Hindman and Dona Strauss.</p>
"
"2378190","2378204","<p>Just note that $$f\left(\frac {g(x)}{h(x)}\right)=\frac{\lambda_ng(x)^n+\lambda_{n-1}g(x)^{n-1}h(x)+\ldots+\lambda_1g(x)h(x)^{n-1}+\lambda_0h(x)^n}{h(x)^n}. $$
For $n\ge1$ nad $\lambda_n\ne0$, the numerator is not a multiple of $h(x)$ (and even less so a multiple of $h(x)^n$) because the first summand is not and all others are.</p>
"
"2378198","2378824","<p>Let $A$ be a matrix whose columns are the coordinates of the points being fitted, relative to the centroid (that is, every column of $A$ is a point being fitted minus the coordinates of the centroid).  From the Eckhart-Young thoeorem, we find that if $A$ has singular value decomposition $A = U \Sigma V^T$ where
$$
\Sigma = \pmatrix{\sigma_1\\&amp;\sigma_2 \\ &amp; &amp; \sigma_3}, \qquad \sigma_1 \geq \sigma_2 \geq \sigma_3
$$
Then the coordinates of the projection of the columns on the best fit plane are the columns of the matrix $\tilde A = U \tilde \Sigma V^T$, where
$$
\tilde \Sigma = \pmatrix{\sigma_1\\&amp;\sigma_2 \\ &amp; &amp; 0}
$$
Now, let $\|A\|$ denote the Frobenius norm, which is to say that $\|A\|^2 = \sum_{i,j}|a_{ij}|^2$.  We find that the square of the minimized error is given by
$$
\|A- \tilde A\|^2 = \|U(\Sigma - \tilde \Sigma)V^T\|^2 = \|\Sigma - \tilde \Sigma\|^2 = \sigma_3^2
$$</p>
"
"2378239","2378271","<p>I will apply the standard techniques of quantum field theory.  Begin with,
$$
I^\mu(x^\mu) = \frac{\partial}{\partial x_\mu} \int \frac{d^dp}{(2\pi)^d} \frac{e^{-ix\cdot p} }{p^2} \delta^{(d)}(p).
$$
(I denote a vector with an index notation basically because I am used to that from physics.)  Introduce a so-called Schwinger parameterisation,
$$
I^\mu(x^\mu) = \frac{\partial}{\partial x_\mu} \int \frac{d^dp}{(2\pi)^d} \int_0^\infty d\tau e^{-ix\cdot p - \tau p^2} \delta^{(d)}(p).
$$
Interchange(!) the order of integration which allows one to use the $\delta$-function,
$$
I^\mu(x^\mu) = \frac{1}{(2\pi)^d}\frac{\partial}{\partial x_\mu}\int_0^\infty d\tau=0.
$$
As is common with this technique, the integral is regularised to zero.  I have not checked in detail here but generally speaking: It is permitted to interchange the order of integrals by completing the square of the exponential and moving to spherical polar coordinates then analytically continuing $d$ into the complex plane.</p>
"
"2378252","2378256","<p>You basically want to prove that for however small $\delta$, you can find two points at with distance less than $\delta$ such that distance between their images remains large(greater than $1$). So it suffices to prove it for $\delta$ less than a fixed quantity, say $\frac 12$.</p>

<p><strong>Edit:</strong> Mathematically, suppose one has proved that for every $0&lt;\delta&lt;\frac 12$ there exists $x,y \in (0,1)$ such that $$|x-y|&lt;\delta \text{ and yet } |f(x)-f(y)|\geq1$$</p>

<p>Now, let $\delta \geq \frac12$. </p>

<p>By above hypothesis, there exists $x,y \in (0,1)$ such that $$|x-y|&lt;\frac14\text{ and yet } |f(x)-f(y)|\geq1$$</p>

<p>As $\delta&gt;\frac14$ hence,  $$|x-y|&lt;\delta\text{ and yet } |f(x)-f(y)|\geq1$$</p>
"
"2378260","2378434","<p>You have misread Mac Lane.  Note that he says $fN=1$, which means that $N$ is contained in the kernel of $f$ but the kernel of $f$ could also be larger than $N$.  So the correct definition is $$H(G') :=
 \{f \in \mathrm{Hom}(G,G') : \mathrm{ker}(f) \color{red}\supseteq N\}.$$  There is now an obvious way to define $H$ on morphisms: given $\varphi:A\to B$ and $f\in H(A)$, define $(H\varphi)(f)=\varphi\circ f$.  I'll leave it to you to verify that this gives an element of $H(B)$ and defines a functor.</p>
"
"2378267","2378279","<p>Let $M$ be a module over a ring $R$.  We say that the <em>module</em> $M$ is Noetherian if and only if every submodule of $M$ is finitely generated.  This is equivalent to saying that there is no strictly ascending chain of submodules of $M$.  From here, it is easy to see that </p>

<blockquote>
  <ol>
  <li><p>Every submodule and homomorphic image of a Noetherian $R$-module is Noetherian.</p></li>
  <li><p>Conversely, if $M$ is an $R$-module, and $N$ is a submodule of $M$ such that both $N$ and $M/N$ are Noetherian, then so is $M$.</p></li>
  </ol>
</blockquote>

<p>The $R$-module $R$ is Noetherian as a ring if and only if it is Noetherian as a module.  Also, if  $R$ is Noetherian, then so is the $R$-module  $\bigoplus\limits_{i=1}^n R$ for any $n \geq 0$.  You have an exact sequence</p>

<p>$$0 \rightarrow \bigoplus\limits_{i=1}^{n-1} R \rightarrow \bigoplus\limits_{i=1}^n R \rightarrow R \rightarrow 0$$</p>

<p>with which you can apply the previous statement and induction.</p>

<p>Now to prove that $R$ is Noetherian if and only if every submodule of a finitely generated $R$-module is finitely generated.</p>

<p>($\Rightarrow$): a finitely generated $R$-module $M$ is a homomorphic image of $\bigoplus\limits_{i=1}^n R$, which is Noetherian.  Hence $M$ is Noetherian, i.e. every submodule is finitely generated.</p>

<p>($\Leftarrow$): $R$ is a finitely generated $R$-module.</p>
"
"2378272","2378303","<p>When solving problems (contest or research or whatever), a good approach is to make conjectures/assumptions and try to prove them (or fail to prove them, giving you more insight).</p>

<p>Coming up with good conjectures which lead to the answers you seek comes with trying to solve various problems.</p>

<p>In this case, the question itself asks for which values of $n$ is $G_n$ connected, so it is natural to consider various values of $n$.</p>

<p>Also intuitively, the more the $n$, the more the chances of connecting the two sets via intermediate disjoints sets.</p>

<p>For instance if $n \ge 6$, then given $u = \{a,b\}$ and $v = \{c,d\}$ we can always pick $w = \{e,f\}$ such that $e,f$ are distinct from any of $a,b,c,d$, and thus there is a path from $u$ to $v$ via $w$.</p>

<p>Now all that remains is to consider $n=2,3,4,5$. For $n=2,3,4$ we can easily figure out if $G_n$ is connected or not.</p>

<p>For $n=5$, we just consider the cases where $a,b,c,d$ are all distinct or not (split into cases). </p>

<p>The proof in the pdf for $n=5$ also carries over for $n=6$, so is just written for $n \ge 5$.</p>

<p>Not sure if this helped.</p>
"
"2378278","2378282","<p>It does not matter if they are primes. </p>

<p>So here we are trying to calculate the number of multiples of $30$ which are not multiples of $16$ below thousand. For this, we take all multiples of $30$ below thousand first, and then subtract from this, all numbers that are multiples of $30$ and $16$ which are below thousand.</p>

<p>Any number is a multiple of $30$ and $16$ if and only if it is a multiple of their least common multiple, which in our case is $240$, which I computed by prime factorization, if you wanted to know how that is done.</p>

<p>So the answer is the number of multiples of $30$ minus the number of multiples of $240$ which are less than thousand. This is then $33-4 = 29$.  </p>
"
"2378300","2378301","<p>It's a fact (or an exercise if you don't know the fact) that the set of all $\omega$ for which the sequence $f_n(\omega)$ converges is measurable. You said you can show the set of all $\omega$ for which $f_n(\omega)$ converges to a rational number is measurable. So, the set of $\omega$ for which $f_n(\omega)$ converges to an irrational number is just the set difference of those two measurable sets, which is of course measurable.</p>
"
"2378321","2378332","<p>Since
$\DeclareMathOperator{Span}{Span}\Span\{v_1,\dotsc,v_k\}\subset\Span\{u_1,\dotsc,u_k\}$,
each $v_i$ is a linear combination of the vectors $\{u_1,\dotsc,u_k\}$. This
means that there are scalars $a_{ij}$ satisfying
\begin{array}{rcrcrcrcrc}
v_1    &amp;=&amp; a_{11}\cdot u_1 &amp;+&amp; a_{21}\cdot u_2 &amp;+&amp; \dotsb &amp;+&amp; a_{k1}\cdot u_k \\
v_2    &amp;=&amp; a_{12}\cdot u_1 &amp;+&amp; a_{22}\cdot u_2 &amp;+&amp; \dotsb &amp;+&amp; a_{k2}\cdot u_k \\
\vdots &amp; &amp; \vdots          &amp; &amp; \vdots          &amp; &amp; \ddots &amp; &amp; \vdots \\
v_k    &amp;=&amp; a_{1k}\cdot u_1 &amp;+&amp; a_{2k}\cdot u_2 &amp;+&amp; \dotsb &amp;+&amp; a_{kk}\cdot u_k \\
\end{array}
Now, let $A=[a_{ij}]$. What is $YA$?</p>
"
"2378322","2378325","<p>Have you seen the proof that the rational numbers are countable, where you arrange them in a grid?</p>

<p>List the elements in each set in a similar grid, with $A_1$ in the first row, $A_2$ in the second row, etc. Then define a similar function in a zig-zag manner through the grid. This should be a bijection between $\mathbb{N}$ and your union.</p>
"
"2378327","2378334","<p>The theorem is false in the Lebesgue setting. Consider
$$f(x) = \begin{cases} 0 &amp; x \in \mathbb{Q} \\ 1 &amp; \textrm{otherwise} \end{cases} .$$
Your very first sentence of your ""proof"" is wrong.</p>

<p>EDIT: I just realized this was meant to be a proof for Riemann-integrable functions, not Lebesgue.</p>
"
"2378335","2378339","<p>For some given eigenvalue, there are many eigen<em>vectors</em> of a matrix. The eigen<em>space</em> is the set of <em>all</em> the eigenvectors, so there's only one eigen<em>space</em>. </p>

<p>In your example, (1,0,-1) and (-1,0,1) are eigenvectors - but so are (5,0,-5), (-1729$\pi$,0,1729$\pi$) and so on. </p>

<p>There's only one eigen<em>space</em>, namely, $\{(t,0,-t)|t\in F\}$ where $F$ is the field your vectors are over. </p>
"
"2378336","2379375","<p>A candidate Lyapunov function should always at least be semi-positive definite, so $V(\vec{x})\geq 0\ \forall\ \vec{x} \in \mathbb{R}^n$. In order to say something about the stability of the system the time derivative of this Lyapunov function should then at least be semi-negative definite, so $\dot{V}(\vec{x})\leq 0\ \forall\ \vec{x} \in \mathbb{R}^n$. One can view the Lyapunov function as a sort of virtual energy of the system, with lowest energy state set as zero. If the Lyapunov function is always greater or equal to zero and its derivative always smaller of equal to zero, then the value of the Lyapunov function should always decrease or stay constant.</p>

<p>Your candidate Lyapunov function is not semi-positive definite, so from that one can already conclude that it will not be a suitable Lyapunov function. However the first candidate Lyapunov function is semi-positive definite</p>

<p>$$
V(x_1,x_1) = \frac{g}{l} (1 - \cos x_1) + \frac12 x_2^2.
$$</p>

<p>The time derivative of this candidate Lyapunov function can be found with</p>

<p>$$
\dot{V} = \frac{\partial V}{\partial x_1} \dot{x}_1 + \frac{\partial V}{\partial x_2} \dot{x}_2 = \frac{g}{l} \sin(x_1)\, x_2 + x_2 \left(-\frac{g}{l} \sin x_1 - \frac{k}{m} x_2\right) = -\frac{k}{m} x_2^2.
$$</p>

<p>The time derivative of the candidate Lyapunov function is thus semi-negative definite (assuming $k,m&gt;0$).</p>

<p>It can be noted that the system in your question has infinitely many equilibriums and that the candidate Lyapunov function is only equal to zero at its stable equilibriums.</p>
"
"2378344","2378379","<p>This is exactly what <a href=""https://en.wikipedia.org/wiki/Mayer%E2%80%93Vietoris_sequence"" rel=""nofollow noreferrer"">Mayer-Vietoris sequences</a> are for.  Suppose $X$ is a space and $U$ and $V$ are open subspaces of $X$ with $U\cup V=X$.  Then there is a long exact sequence of homology groups \begin{align*}\dots &amp; \to H_n(U\cap V)\to H_n(U)\oplus H_n(V)\to H_n(X)
\\
 &amp;\to H_{n-1}(U\cap V)\to H_{n-1}(U)\oplus H_{n-1}(V)\to H_{n-1}(X)\\
&amp;\to\dots
\end{align*}
In this sequence, the map $H_n(U\cap V)\to H_n(U)\oplus H_n(V)$ is given by $x\mapsto (i_*(x),-j_*(x))$ where $i:U\cap V\to U$ and $j:U\cap V\to V$ are the inclusion maps.  The map $H_n(U)\oplus H_n(V)\to H_n(X)$ is given by $(y,z)\mapsto k_*(y)+\ell_*(z)$, where $k:U\to X$ and $\ell:V\to X$ are the inclusion maps.</p>

<p>In your case where $X$ is two tori glued together, you can take $U$ to be a small open neighborhood of the left torus and $V$ to be a small open neighborhood of right torus, so $U$ and $V$ both deformation-retract to the tori and $U\cap V$ deformation-retracts to the intersection of the two tori, which is a disjoint union of two circles.  The inclusion map $U\cap V\to U$ is nullhomotopic (both circles enclose disks in the left torus), but the inclusion map $U\cap V\to V$ sends both generators of $H_1(U\cap V)\cong \mathbb{Z}^2$ to one of the generators of $H_1(V)\cong \mathbb{Z}^2$ (since the two circles in $U\cap V$ are both meridans of the right torus).</p>

<p>We can then compute $H_2(X)$ using the Mayer-Vietoris sequence $$H_2(U\cap V)\to H_2(U)\oplus H_2(V)\to H_2(X)\to H_1(U\cap V)\to H_1(U)\oplus H_1(V)$$ which becomes $$0\to \mathbb{Z}\oplus\mathbb{Z}\to H_2(X)\to \mathbb{Z}^2\to \mathbb{Z}^2\oplus\mathbb{Z}^2.$$  The last map sends both $(1,0)$ and $(0,1)$ to $(0,0,-1,-1)$ by the analysis of the inclusion maps $U\cap V\to U$ and $U\cap V\to V$ of the previous paragraph.  In particular, its kernel is generated by $(1,-1)$ and is isomorphic to $\mathbb{Z}$, so we have a short exact sequence $$0\to\mathbb{Z}\oplus\mathbb{Z}\to H_2(X)\to\mathbb{Z}\to 0$$ which gives $H_2(X)\cong \mathbb{Z}^3$.</p>

<p>I'll leave it as an exercise for you to similarly compute $H_1(X)$ using the Mayer-Vietoris sequence.</p>
"
"2378353","2378357","<p>This isn't quite right - in this case $\rho$ represents the radius of the sphere, so it would be simpler to write:
$$ S = \{\rho, \theta, \phi : \rho = a, 0 \leq \theta \leq 2\pi, 0 \leq \phi \leq \pi\}$$.</p>

<p>Otherwise, your conceptual understanding of set-builder notation seems good. Keep trying to write other surfaces as sets.</p>
"
"2378356","2378453","<p>As Ixion remarked, (c) is false as one can take $f$ to be any even function.</p>

<p>In (a) and (b) a change of variable gives $\int_0^1 f(\sqrt y)y^n\,dy=0$.
Set $F(y)=f(\sqrt y)$. In (a) use the Weierstrass Approximation Theorem to approximate $F$ by a polynomial. Can you adapt this trick for (b)?</p>
"
"2378360","2378362","<p>Hint: This theorem can be proven using an epsilon/delta approach, given the <a href=""https://en.wikipedia.org/wiki/Completeness_(order_theory)"" rel=""nofollow noreferrer"">completeness</a> property of the real numbers.</p>
"
"2378366","2378373","<p><strong>Step 1:</strong>
Suppose that $f = \chi_{[a,b]}$, where $\chi_E$ is the indicator function defined by
$$ \chi_E(x) = \begin{cases} 1 &amp; \text{if}\ x\in E, \\ 0 &amp; \text{otherwise}.
\end{cases}$$
<strong>Step 2:</strong>
Suppose that $f$ is a step function; that is, $f=\sum_{i=1}^nc_i\chi_{[a_i,b_i]}$.</p>

<p><strong>Step 3:</strong>
Use the fact that the step functions are dense in the space of Lebesgue integrable functions on $\mathbb{R}$.</p>

<hr>

<p>To expand on step 3, let $f$ be a Lebesgue integrable function and $\varepsilon&gt;0$. The density of step functions in the space of Lebesgue integrable functions yields a step function $h$ such that $\int |h(x)-f(x)|dx &lt; \varepsilon/2$. Then
$$ \left|\int \cos(xn)f(x)dx\right|
\leq \int |cos(xn)||f(x)-h(x)|dx + \left|\int\cos(xn)h(x)dx\right|.
$$</p>

<p>Can you take it from here?</p>
"
"2378368","2378429","<p>Let $X_{ij}$ be independent random variables with
$$P(X_{ij}=0)=\cos^2\alpha_{ij},$$
$$P(X_{ij}=1)=\sin^2\alpha_{ij}.$$
Let $A$ be the event $(\exists i)(\forall j)(X_{ij}=1)$ and $B$ the event $(\exists j)(\forall i)(X_{ij}=0)$. These are mutually exclusive, so
$$
  P(A\cap B)=0.
$$
We have
$$\begin{eqnarray*}
  P(A)&amp;=&amp;1-P(\neg A)\\
    &amp;=&amp;1-P\left((\forall i)(\exists j)(X_{ij}=0)\right)\\
    &amp;=&amp;1-\prod_iP\left((\exists j)(X_{ij}=0)\right)\\
    &amp;=&amp;1-\prod_i\left(1-P((\forall j)(X_{ij}=1))\right)\\
    &amp;=&amp;1-\prod_i\left(1-\prod_j P(X_{ij}=1)\right)\\
    &amp;=&amp;1-\prod_i\left(1-\prod_j\sin^2\alpha_{ij}\right).
\end{eqnarray*}$$
Similarly
$$
  P(B)=1-\prod_j\left(1-\prod_i\cos^2\alpha_{ij}\right).
$$
Hence
$$
  \prod_j\left(1-\prod_i\cos^2\alpha_{ij}\right)+\prod_i\left(1-\prod_j\sin^2\alpha_{ij}\right)
    =2-P(A)-P(B).
$$
But $1\geq P(A\cup B)=P(A)+P(B)-P(A\cap B)=P(A)+P(B)$, so the RHS is $\geq1$.</p>
"
"2378369","2378377","<p>Let $A = U^TU$ be the cholesky factorization. </p>

<p>Notice that $A_{ii}=\left\|u_i\right\|^2$.</p>

<p>By <a href=""https://en.wikipedia.org/wiki/Hadamard%27s_inequality"" rel=""nofollow noreferrer"">Hadamard inequality</a>,</p>

<p>$$\det(A) =\det(U)^2 \leq \prod_{i=1}^n \|u_i\|^2 =\prod_{i=1}^n A_{ii}$$</p>
"
"2378376","2378431","<ol>
<li><p>They said that the coefficients of the polynomial $h(X)=qg(X)-f(X)$ are zero, not that the coefficients of $g(X)$ and $f(X)$ are zero.  Since $g(X)$ is a nonzero polynomial (it's the denominator of a rational function), it must have some coefficient which is nonzero.</p></li>
<li><p>We are considering $h(X)=qg(X)-f(X)$ as a polynomial with coefficients in $F(q)$.  If $g(X)=\sum_{n=0}^N b_nX^n$ and $f(X)=\sum_{n=0}^N a_n X^n$ (here $N=\max(\deg f, \deg g)$), then $$h(X)=\sum_{n=0}^N(qb_n-a_n)X^n.$$  If $h(X)=0$, that means all its coefficients are $0$, so $qb_n-a_n=0$ for all $n$. </p></li>
<li><p>If $q$ were algebraic over $F$, then $F(q)$ would be algebraic over $F$ (a field extension generated by algebraic elements is algebraic).  Since $F(q)$ is not algebraic over $F$, $q$ cannot be algebraic over $F$.</p></li>
</ol>
"
"2378378","2378560","<p>The triangle $\Delta(v_0,v_1,v_2)$ defines a plane $\Pi$. If the line $\ell$ is not parallel to $\Pi$ it will intersect the plane in exactly one point $p$. Then we can apply three determinants $det(p,v_0,v_1)$, $det(p,v_1,v_2)$, and $det(p,v_2,v_0)$ determine the orientation (sign) and know if $p$ lies inside of $\Delta$, if so then $\ell$ intersects $\Delta$.</p>

<p>To calculate $p$ we can use the parametric form (<a href=""https://en.wikipedia.org/wiki/Line%E2%80%93plane_intersection"" rel=""nofollow noreferrer"">Wikipedia</a>): </p>

<p>$\Pi$: $v_0 + (v_1 - v_0)u + (v_2-v_0)w$</p>

<p>Assuming $\ell$ is defined by two points $p_0$ and $p_1$. Then we can write the point of intersection as:</p>

<p>$p_0 + (p_1-p_0)t = v_0 + (v_1 - v_0)u + (v_2-v_0)w$</p>

<p>$\Leftrightarrow p_0 - v_0 = (p_0 - p_1)t + (v_1 - v_0)u + (v_2-v_0)w$</p>

<p>Three equations (one for each $x$, $y$, and $z$) three unknown variables ($t$, $u$, and $w$) gives us an exact solution. </p>

<p>Then $p$ lies in the plane $\Pi$ and we can see the problem as $2D$ point location problem where we have to find out if $p$ lies inside of $\Delta$. Here we apply the three determines as explained above. </p>

<hr>

<p>EDIT: <em>Coplanarity was given later and only in the comments.</em></p>

<p>In case all points a coplanar we can simply calculate $det(p_0,p_1,v_0)$, $det(p_0,p_1,v_1)$, and $det(p_0,p_1,v_2)$. If one sign is different the line passes trough $\Delta$ otherwise it does not. (This variant was already described by another answer but as it seems it was deleted.)</p>
"
"2378384","2378401","<p>Finding the kernel first is a good idea. (though actually what we need is just to find the nullity)</p>

<p>Construct the augmented matrix (note that the last column is actually not needed):</p>

<p>$$\left[\begin{array}{ccc|c}2 &amp; -1 &amp; 0 &amp; 0\\ 1 &amp; 1 &amp; -3 &amp; 0 \\ -1 &amp; 0  &amp; 1 &amp; 0\end{array}\right]$$</p>

<p>And find its corresponding RREF:
$$\left[\begin{array}{ccc|c}1 &amp; 0 &amp; -1 &amp; 0\\ 0 &amp; 1 &amp; -2 &amp; 0 \\ 0 &amp; 0  &amp; 0 &amp; 0\end{array}\right]$$</p>

<p>See that the third column is not a pivot column, let $c=t$. from the second equation, $b-2c=0$, hence $b=2t$, from the first equation, $a-c=0$, hence $a=t$. </p>

<p>Hence solution to the system $Tx=0$ is $(a,b,c)=t(1,2,1)$.</p>

<p>A basis of the kernel would be $\{ 1+2x+x^2\}$.</p>

<p>As mentioned at the start, the goal is to find the nullity. From the RREF, we can see that $rank(T)=2$ and the the number of columns is $3$, hence $nullity(T)=1 &gt; 0$, hence it is not injective.</p>

<p>Your codomain is $\mathbb{R}^3$, however as you can see from the RREF, $rank(T)=2 &lt; 3$, it does not have enough vectors to span $\mathbb{R}^3$.</p>
"
"2378389","2378430","<p>Hint: For each $z\in \mathbb D,$</p>

<p>$$g_z(w) = \frac{z+w}{1+\bar z w}$$</p>

<p>is an automorphism of the unit disc. You are trying to prove</p>

<p>$$(f\circ g_z)(0) = \frac{1}{2\pi}\int_0^{2\pi} (f\circ g_z)(e^{it})\, dt.$$</p>
"
"2378393","2378403","<p>Unless $n=1$ and $a \neq 0$, it does not have an inverse, the rank of matrix $aa^T$ is rank $1$ if $a \neq 0$.</p>

<p>To see this clearly suppose $a= \begin{bmatrix} a_1 \\ \vdots \\a_n\end{bmatrix}$, $aa^T=\begin{bmatrix} a_1 a &amp; \ldots &amp; a_n a\end{bmatrix}.$ We can see that the columns are multiples of $a$.</p>

<p>Remark: </p>

<p>Now suppose you want to solve $Xa=b$ given $a$ and $b$. if $a=0$ and $b \neq 0$, then there is no solution. if both $a=0$ and $b=0$, then $X$ can be anything.</p>

<p>Suppose $a_i \neq 0$, let $X_i$ be the $i$-th column of $X$, then let $X_i = \frac{b}{a_i}$ and $X_j = 0, \forall j \neq i$, would be a feasible solution. </p>
"
"2378395","2378415","<p>How about  $\cong  $.  It seems to work in these types of situations...  or maybe ~.  I doubt there's much concern about this.   One would also quite likely just write $=$, when there's no confusion. ..</p>
"
"2378416","2378664","<p>The author gives the following hint: </p>

<blockquote>
  <p>Hint:   Conside $B_n(t)X_n(\omega)$  where the random variables $\{B_n\}_{n\geqslant    1}$ are
   coin  tossing or
   Bernoulli  random  variables. Apply  Fubini  on   the space
    of $(t,\omega )$.</p>
</blockquote>

<p>Indeed, the main difficulty lies on the fact that we know that for any $\varepsilon= \left(\varepsilon_n\right)\in\{-1,1\}^n$, there exists $\Omega_\varepsilon$ such that $\mathbb P\left(\Omega_\varepsilon\right)=1$ and for all $\omega\in\Omega_\varepsilon$, the series  $\sum_{n=1}^{\infty}\varepsilon_n X_n\left(\omega\right)$ converges. But it is not clear whether we can find $\widetilde\Omega$ of probability one which works for <em>any</em> choice of sequences. </p>

<p>Denote $\left(\Omega,\mathcal A,\mathbb P\right)$ the original probability space. On an other probability space $\left(\Omega',\mathcal A',\mathbb P'\right)$, we consider a sequence of independent identically distributed random variables $\left(B_n\right)_{n\geqslant 1}$ such that $\mathbb P'\left(B_n=1\right)=\mathbb P'\left(B_n=-1\right)=1/2$. Denote 
$$E:=\left\{\left(\omega,\omega'\right): \sum_{n=1}^{+\infty} B_n\left(\omega'\right)X_n\left(\omega \right)\mbox{ does not converge}                           \right\}.$$
Now appling Fubini's theorem to the indicator function of $E$, we get the existence of $\Omega_0\subset\Omega  $ such that $\mathbb P\left(\Omega_0\right)=1$ and for all $\omega\in\Omega_0$, the series $\sum_{n=1}^\infty B_n\left(\omega'\right)X_n\left(\omega\right)$ converges for almost every $\omega\in\Omega'$. Now, use the three series theorem to show that if for some deterministic sequence $\left(a_n\right)_{n\geqslant 1}$, the        $\sum_n a_nB_n\left(\omega'\right)$ converges for almost every $\omega\in\Omega'$, then $\sum_n a_n^2$ is finite.                              </p>
"
"2378417","2378423","<p>In general, the space of polynomials in $k$ variables having degree $\le n$ has the basis:</p>

<p>$$\{x_1 ^{a_1} \cdots x_k^{a_k}: \sum_{i=1}^k a_i \le n\}$$</p>

<p>which is the same as the set:</p>

<p>$$\{1^{a_0} x_1^{a_1} \cdots x_k ^{a_k}: \sum_{i=0}^k a_i = n\}$$</p>

<p>Using <a href=""https://en.wikipedia.org/wiki/Stars_and_bars_(combinatorics)"" rel=""nofollow noreferrer"">stars and bars</a>, the cardinality of this set (hence the dimensionality of the vector space) is:</p>

<p>$${n + k \choose n}$$</p>
"
"2378426","2378432","<p>If $$A = \begin{bmatrix}x_1&amp;x_2\\x_3&amp;x_4 \end{bmatrix}$$</p>

<p>Then</p>

<p>$$ A\begin{bmatrix}a_{n-1}\\b_{n-1} \end{bmatrix} = \begin{bmatrix}a_n\\b_n \end{bmatrix}$$</p>

<p>$$ A^2\begin{bmatrix}a_{n-1}\\b_{n-1} \end{bmatrix} = \begin{bmatrix}a_{n+1}\\b_{n+1} \end{bmatrix},$$</p>

<p>$$ (\tau A - d I)\begin{bmatrix}a_{n-1}\\b_{n-1} \end{bmatrix} = \begin{bmatrix}a_{n+1}\\b_{n+1} \end{bmatrix},$$
where $\tau = x_1 + x_4$ and $d = x_1 x_4 - x_2 x_3$
and so</p>

<p>$$  a_{n+1} = \tau a_n - d a_{n-1},  $$
$$  b_{n+1} = \tau b_n - d b_{n-1}.  $$</p>

<p>$$ A^n\begin{bmatrix}a_{0}\\b_{0} \end{bmatrix} = \begin{bmatrix}a_n\\b_n \end{bmatrix}$$</p>

<p>$A^n$ can be computed (to give a closed form formula) via diagonalization if that exists.</p>

<p>In other cases, repeated squaring will give fast algorithms. Not sure about a closed form formula.</p>
"
"2378438","2378441","<p>A finite extension of $\mathbb{Z}_p$ is a finite vector space of dimension $n$ over $\mathbb{Z}_p$. It has $p^n$ elements and is the  splitting field of $X^{p^n}-X$, so it is a normal extension and separable extension since $X^{p^n}-X$ is separable so it is a Galois extension.</p>
"
"2378442","2378501","<p>Using HÃ¶lder's inequality is a good idea. There is $t\in (0,1)$ such that 
$$
\frac{1}{p} = \frac{1-t}{r} + \frac{t}{s}
$$
(any number between two others is their convex combination). Write $|f|^p = |f|^{p(1-t)} |f|^{pt}$ and apply the inequality as 
$$
\int|f|^{p(1-t)} |f|^{pt} \le \left(\int |f|^r\right)^{p(1-t)/r}
\left(\int |f|^s\right)^{pt/s}
$$
Raising both sides to power $1/p$ yields
$$
\|f\|_p  \le \|f\|_r^{1-t} \|f\|_s^t \le \max(\|f\|_r,\|f\|_s)
$$</p>
"
"2378443","2378460","<p>a) Handshaking shows $2|E(H)| \ge 4p$. Each spanning tree has $p-1$ edges so $|E(H)| \le 2p-2$. Thus $2p \le 2p-2$ which is a contradiction.</p>

<p>b) Let $G$ be the complete graph on $4$ vertices. Let $T_1$ and $T_2$ be spanning trees with no common edges. Then $H=G$.</p>

<p>c)</p>

<p>Hint 1: The only way for $H$ to have a vertex of degree $2$ is if this vertex is a leaf in both trees. (This is because both trees are spanning and share no edge.)</p>

<p>Hint 2: Each tree has $&gt; p/2$ leaves.</p>
"
"2378448","2378451","<p>What you wrote is comprehensible and has the correct meaning, but it is expressed kind of strangely and there is a much simpler way to write it.  In English, what you wrote is</p>

<blockquote>
  <p>For all even integers $m$ and $n$, there exists $a$ which is equal to $m+n$ such that $a$ is even.</p>
</blockquote>

<p>The second half of this sentence is a pretty convoluted way to say ""$m+n$ is even""!  There is no need to make a variable $a$ and say ""there exists $a$"": $m+n$ is a specific number, not an unspecified number whose existence you are asserting.  So instead you can just say </p>

<blockquote>
  <p>$$\forall m,n\in 2\mathbb{Z}, m+n\in 2\mathbb{Z}.$$</p>
</blockquote>

<p>Or in English:</p>

<blockquote>
  <p>For all even integers $m$ and $n$, $m+n$ is even.</p>
</blockquote>
"
"2378449","2378455","<p>How about $11$? Odd norms from $\Bbb Z[\sqrt 2]$ are $\pm1\pmod 8$
and norms coprime to $3$ from $\Bbb Z[\sqrt 3]$ are $1\pmod 3$.</p>

<p>This argument extends
to show that that no finite set of norms from quadratic rings
covers $\Bbb Z$.</p>
"
"2378456","2378474","<p>The title can be read two ways.  One is that $\emptyset: A \to B$ is not always a function when $A,B$ are non-empty.  Your example proves that.  Another is that $\emptyset: A \to B$ is never a function when $A,B$ are non-empty.  One example cannot prove that.  You would then have to argue along the lines of ""Take $a \in A$, which we can do because $A$ is non-empty.  There is no ordered pair in $F$ with $a$ as a first element, so it fails criterion $1$.""</p>
"
"2378459","2378781","<p>To prove that the limit is $\frac{1}{4}$, we just need to prove that </p>

<p>$\sum_{k=1}^n k \sin ^2 (k) = \frac{n^2}{4} + o(n^2)$. In fact :</p>

<p>$\sum_{k=1}^n k \sin ^2 (k) = \sum_{k=1}^n \frac{k}{2} (1 - \cos(2k)) = \frac{n(n+1)}{4} - \frac{1}{2} \sum_{k=1}^n k\cos(2k)$.</p>

<p>Let now $f$ be the function defined by </p>

<p>$f(x) = \sum_{k=0}^n e^{ikx} = \frac{1 - e^{inx}}{1 - e^{ix}} = \frac{\sin(\frac{n}{2}x)}{\sin(\frac{x}{2})}e^{i\frac{n}{2}x}$ for $x\in\mathbb{R}\backslash 2\pi\mathbb{Z}$. It is clear that  </p>

<p>$\sum_{k=1}^n k \sin ^2 (k) = \frac{n(n+1)}{4} - \Im(f'(1)) = \frac{n^2}{4} + o(n^2)$ 
since $f'(1) = O(n)$</p>
"
"2378462","2378465","<p>$1+\tan^2 (x)=\sec^2 (x) $ thus both answers just differ by a constant hence they are equivalent.</p>
"
"2378463","2378541","<p>$$\alpha=1\qquad\to\qquad (1-x^2)y''-2xy'+2y=0 \tag 1$$
You found a particular solution $y=x$ which is correct.</p>

<p>Or, more general, a family of solutions : $\quad y=C\:x\quad$ where $C$ is a constant.</p>

<p>In order to find the general solution of the ODE, one can use the method of variation of parameter. </p>

<p>In the present case, remplace the parameter $C$ by an unknown function $u(x)$:</p>

<p>$y=u(x)\:x \quad\to\quad y'=xu'+u \quad\to\quad y''=xu''+2u'$</p>

<p>Putting them into $(1)$ leads to :
$$x(1-x^2)u''+2(1-2x^2)u'=0$$
$$\frac{u''}{u'}=2\frac{2x^2-1}{x(1-x^2)}$$
$$\ln|u'|=2\int \frac{2x^2-1}{x(1-x^2)}dx =-\ln|1-x^2|-2\ln|x|+\text{constant}$$</p>

<p>$$u'=\frac{c_1}{x^2(1-x^2)}$$
$$u=c_1\int \frac{dx}{x^2(1-x^2)} = c_1\left(-\frac{1}{x}+\frac{1}{2}\ln|1+x|-\frac{1}{2}\ln|1-x| \right)+c_2$$
The general solution of $(1)$ is :
$$y(x)=c_1\int \frac{dx}{x^2(1-x^2)} = c_1\left(-1+\frac{1}{2}x\ln|1+x|-\frac{1}{2}x\ln|1-x| \right)+c_2x$$</p>
"
"2378467","2379024","<p>It matters not that you're dealing with groups.  One simply needs to argue through the fact that $|A\times B|=|A||B|$ for any two sets $A$ and $B$ and that the cardinality of this direct product is infinite if either $|A|$ or $|B|$ is infinite.</p>

<p>Why is this the case?  Well, $|A\times B|=|\{(a,b)\ :\ a\in A, b\in B\}|$.  Use basic counting principles here.  How many choices for the first coordinate?  exactly $|A|$.  How many choices for the second coordinate?  exactly $|B|$.  Therefore, $|A\times B|=|A|\cdot |B|$.  It is clear that if either $|A|$ or $|B|$ is infinite then $|A\times B|$ is infinite since then there are infinite options for either the first or second coordinate (or both).  No need to monkey with homomorphisms.  Just count.</p>
"
"2378469","2378496","<p>We make use the following result.</p>

<blockquote>
  <p>$f_n\to f$ uniformly on (say) $E$ $\iff$ $||f_n-f||=\sup_{x\in E}|f_n(x)-f(x)|\to0$ as $n\to\infty$</p>
</blockquote>

<p>For (a) you've already shown $$||f_n-f|| \to 1 $$</p>

<p>Thus (a) is not UC.</p>

<p>For (b) we have $$||f_n-f||=1/n\to 0$$</p>

<p>Hence (b) is UC.</p>

<p>For (c) I suggest you to find the supremum by using First Derrivative Test for Maxima. You should get the following  $$||f_n-f||=\frac{1}{1+n^2}\to 0$$</p>

<p>and hence (c) is UC.</p>

<p>Finally for (d) $$||f_n-f||=1\to1$$ </p>

<p>Hence it is not UC. To obtain $||f_n-f|| $ you can follow the same technique as the one i've mentioned in (c)</p>
"
"2378476","2378489","<p>Chose $x = 2n$, $y = 2n+1$, $z = 1$</p>

<p>Then $$\frac{xyz}{x+y+z} = \frac{2n(2n+1)}{4n+2} = n$$</p>
"
"2378490","2378498","<p>Taking convex hull is not even an injective operation. Consider the subsets 
$$A=\{-1,1\} \quad \text{and}\quad B=\{-1,0,1\}$$
of the real line (so, $L=\mathbb R$). These two sets have the same convex hull, namely $[-1,1]$.</p>
"
"2378493","2378608","<p>I spotted a few problems in your attempt (props for posting the details as that makes it easier to gauge where you are in your studies).</p>

<p>The factor $x+1$ can pretty much be ignored as it won't affect neither the splitting fields nor the Galois groups. You are really only looking at the splitting fields of the other factor $p(x)=3x^3+1$. This cubic has as its complex zeros $x_j=\omega^j\root3\of{-1/3}$, where $j=0,1,2$ and $\omega=(-1+i\sqrt3)/2$ is a primitive third root of unity. So a splitting field $L$ of $p(x)$ over $\Bbb{Q}$ is $L=\Bbb{Q}(\root3\of3,\omega)$. In other words, this is the same splitting field that the polynomial $x^3-3$. The Galois group is isomorphic to $S_3$, and $[L:\Bbb{Q}]=6$. The arguments proving those facts are undoubtedly familiar to you from the standard example of the splitting field of $x^3-2$ over $\Bbb{Q}$ so I won't rehash them. You seem to have made the mistake of including $i$ and $\root6\of3$ into the splitting field. That is not unnatural given that $\sqrt{-3}$ and $\root3\of3$ are in there, but you cannot actually get the real sixth root of $3$ by combining those (but you do get a sixth root of $-3$).</p>

<hr>

<p>Over the field $\Bbb{F}_5$ a splitting field $L_2$ is much smaller. You apparently missed (due to numerical errors) that $p(2)=25=0\in\Bbb{F}_5$, so we get a factorization
$$
p(x)=(x-2)(3x^2+x+2)\in\Bbb{F}_5[x].
$$
That quadratic is irreducible over $\Bbb{F}_5$. To see that you can either verify that it has no zeros in $\Bbb{F}_5$, or you can check that the discriminant
$$\Delta=b^2-4ac=1^2-4\cdot3\cdot 2=-23=2$$
has no square root in $\Bbb{F}_5$. I outlined a different way of reaching the same conclusion in the comments: because $\Bbb{F}_5^*$ is cyclic of order $4$ the element $-1/3$ has a single cube root in there.</p>

<p>Anyway, over $\Bbb{F}_5$ the splitting field is thus $L_2=\Bbb{F}_5(\sqrt\Delta)=\Bbb{F}_{25}$. Therefore $Gal(L_2/\Bbb{F}_5$ is cyclic of order two.</p>

<hr>

<p>You were also interested in the relation between these two Galois groups,
$G_1=Gal(L/\Bbb{Q})$ and $G_2=Gal(L_2/\Bbb{F}_5)$. A relation can be described using concepts of algebraic number theory. Let $\mathcal{O}_L$ be the ring of algebraic integers in $L$. It turns out that in that ring the ideal $(5)$ can be written as a product of three prime ideals
$$
(5)=\mathfrak{p}_1\mathfrak{p}_2\mathfrak{p}_3.
$$
All those prime ideals $\mathfrak{p}_j, j=1,2,3,$ have $5$ as an element, and all the quotient rings $\mathcal{O}_l/\mathfrak{p}_j$ are actually isomorphic to $L_2$.  Furthermore, if $\sigma\in G_1$ then $\sigma$ must also permute the three prime ideals. It then turns out that the point stabilizers of that action are all isomorphic to $G_2$. If $\sigma(\mathfrak{p}_1)=\mathfrak{p}_1$, then $\sigma$ induces an automorphism $\overline{\sigma}$ of $\mathcal{O}/\mathfrak{p}_1\simeq L_2$. A non-trivial fact is that the mapping
$\sigma\mapsto \overline{\sigma}$ is surjective onto $G_2$. The kernel of this mapping measures the ramification of a prime. The fact that five is not a factor of the discriminant of $p(x), \Delta=-972,$ is the key here. It implies that $p=5$ is unramified. This allows us to identify $G_2$ as a subgroup of $G_1$.</p>
"
"2378502","2378543","<p>Let $M_{m,n}$ be the space of $m\times n$ matrices endowed with the usual inner product $\langle P,Q\rangle=\mathrm{tr}(P^TQ)$. This induces an inner product on the space of symmetric matrices $\mathrm{Sym}_n\subseteq M_{n,n}$.</p>

<p>Note that $S_n$ induces an isometry from $\mathbb R^s$ to $\mathrm{Sym}_n$, so we can identify $G$ with the map $\mathrm{Sym}(n)\to M_{m,n}$ given by $P\mapsto AP$. Thus $\lambda\geq0$ is a singular value of $G$ iff there exists a nonzero $P\in\mathrm{Sym}_n$ such that $\langle AQ,AP\rangle=\lambda^2\langle Q,P\rangle$ for all $Q\in\mathrm{Sym}_n$. That is,
$$
  \mathrm{tr}(QA^TAP)=\lambda^2\,\mathrm{tr}(QP).
$$
Conjugating by an orthogonal matrix, we may suppose
$$
  A^TA=D=\mathrm{diag}(\sigma_1(A)^2,\ldots,\sigma_n(A)^2).
$$
If $P=E_{ii}$, we have $DP=\sigma_i(A)^2P$, giving a singular value $\sigma_i(A)$.</p>

<p>If $P=E_{ij}+E_{ji}$ then $DP=\sigma_i(A)^2E_{ij}+\sigma_j(A)^2E_{ji}$, so
$$
  \mathrm{tr}(QP)=Q_{ji}+Q_{ij},
$$
$$
  \mathrm{tr}(QDP)=\sigma_i(A)^2Q_{ji}+\sigma_j(A)^2Q_{ij}.
$$
But $Q_{ij}=Q_{ji}$, so
$$
  \mathrm{tr}(QDP)=\frac12(\sigma_i(A)^2+\sigma_j(A)^2)\mathrm{tr}(QP).
$$
This gives a singular value $\sqrt{\frac12(\sigma_i(A)^2+\sigma_j(A)^2)}$.</p>
"
"2378511","2378518","<p>Fix $y &gt; 0$, and $f(a) = \ln(a+y) - \ln y - \dfrac{a}{a+y}, a &gt; 0\implies f'(a) = \dfrac{1}{a+y} - \dfrac{y}{(a+y)^2} = \dfrac{a}{(a+y)^2} &gt; 0 \implies f(a) &gt; f(0) = 0$, and the problem solved.</p>
"
"2378520","2378527","<p>Hint:</p>

<p>$$24^k=(2^33)^k=2^{3k}3^k$$</p>

<p>\begin{align}
13! &amp;= (13)(12)(11)(10)(9)(8)(7)(6)(5)(4)(3)(2)\\
&amp;=2^2(3)(2)(3^2)(2^3)(2)(3)(2^2)(3)(2)M \\
&amp;= 2^{10}3^5M
\end{align}</p>

<p>where $M$ is not divisible by $2$ or $3$.</p>

<p>Edit:</p>

<p>For $24^k$ to divide $13!$, we need $3k \leq 10$ (inspect power of $2$) and $k \leq 5$ (inspect power of $3$).</p>

<p>Hence $k \leq \frac{10}{3}$ and $k \leq 5$.</p>

<p>Hence $0 \leq k \leq 3$.</p>
"
"2378529","2378531","<p>If you include $F$, then there are only $2!$ ways to organise any triple you've chosen for the board, not $3!$. Therefore it should be
$$
2!\binom92 + 3!\binom93 = 576
$$</p>
"
"2378534","2378537","<p>Within the equivalence class, everyone is related to each other including itself. Also, if they are not in the same equivalent class, they are not related.</p>

<p>Hence $$8^2+10^2+12^2$$</p>
"
"2378540","2378557","<p>First, we can solve for $g(x_1)$ explicitly.</p>

<p>If $x_1 &lt; 0$, the domain for the second optimization problem includes origin, hence $g(x_1) =0 $. </p>

<p>If $x_1 \geq 0$, the distance from the origin to the line $u_1+2u_2=x_1$ would be $\frac{|1(0)+2(0)-x_1|}{\sqrt{1+2^2}}.$ Hence $g(x_1)=\frac{x_1^2}{5}$.</p>

<p>In summary, </p>

<p>$$g(x_1)= \begin{cases} \frac{x_1^2}{5} &amp; \text{ if } x_1 \geq 0 \\ 0 &amp; \text{ if } x_1 &lt; 0 \end{cases}$$</p>

<p>(Credit: Rodrigo de Azevedo)</p>

<p>In the first optimization problem, we are interested in the case where $x_1 \geq 0$,</p>

<p>Hence  $f(x_1,x_2)=\frac{x_1^2}{5}-x_1^2+x_2^2=-\frac{4x_1^2}{5}+x_2^2$</p>

<p>Are you able to complete the problem now?</p>
"
"2378544","2378549","<p>This proposition need not hold. for instance $f:\mathbb N\to \{1\}, n\mapsto 1$ is a surjection. $B$ is finite.</p>
"
"2378548","2378584","<p>$$\frac{d}{dt}x(t)=\frac{x^3(t)-x^2(t)}{n}$$
This ODE is separable :
$$dt=n\frac{dx}{x^3-x^2}=n\left(\frac{1}{x-1}-\frac{1}{x}-\frac{1}{x^2} \right)dx$$
$$t=n \left(\ln|x-1|-\ln|x|+\frac{1}{x}\right)+c$$
$x(0)=x_0\quad\to\quad c=-n\left(\ln|x_0-1|-\ln|x_0|+\frac{1}{x_0}\right)$</p>

<p>$$t(x)=n \left(\ln\left|\frac{x-1}{x_0-1}\right|-\ln\left|\frac{x}{x_0}\right|+\frac{1}{x} -\frac{1}{x_0}\right)$$</p>

<p>The solution of the ODE is obtained on the form of $t$ as a function of $x$.
There is no closed form for the inverse function $x(t)$. It has to be solved by numerical calculus.</p>
"
"2378553","2378564","<p>Your result is correct.</p>

<p>Another approach :
$$\frac{dy}{dx}=\frac{dx}{dy}\quad\to\quad \left(\frac{dy}{dx} \right)^2=1\quad\to\quad \frac{dy}{dx}=\pm 1 \quad\to\quad y=\pm x+c$$</p>
"
"2378554","2378670","<p>It is indeed true that $F'\dashv G'$, and not too hard to show once you have the right picture in mind.</p>

<p>The thing to notice is that an object in $D'$ is a cone over $Q$, i.e. an object $y$ of $D$ together with a natural transformation $\beta:\Delta_D(y)\Rightarrow Q$, and an arrow $(y,\beta,Q)\to (y',\beta',Q)$ is just an arrow $g:y\to y'$ in $D$ that commutes with $\beta$ and $\beta'$, in the sense that $\beta_i'\circ g=\beta_i$ for all objects $i$ of $I$. $C'$ can be described similarly.</p>

<p>In particular, an arrow $(x,\alpha,GQ)\to (Gy,G\beta,GQ)$ is just an arrow $h:x\to Gy$ such that $G\beta_i\circ h=\alpha_i$, and an arrow $(Fx,\alpha^{ad},Q)\to (y,\beta,Q)$ is just an arrow $k:Fx\to y$ such that $\beta_i\circ k =\alpha_i^{ad}$; so the bijection induced by the adjunction $F\dashv G$ restricts (by naturality) to a bijection between arrows of one kind and arrows of the other kind, and it is still natural, since $F'$ and $G'$ acts on arrows like $F$ and $G$. Hence $F'\dashv G'$.</p>
"
"2378555","2378570","<p>First you want to put everything over a common denominator, which should be a multiple of all the denominators. Here you would go for $4(k+1)(k+2)(k+3)$.</p>

<p>Now you need to multiply top and bottom of each fraction in turn by the appropriate amount to make the bottom what you are aiming for. So you get
$$\frac{3(k+1)(k+2)(k+3)}{4(k+1)(k+2)(k+3)}-\frac{(2k+3)2(k+3)}{2(k+1)(k+2)2(k+3)}+\frac{4(k+2)}{(k+1)(k+3)4(k+2)}$$
which equals
$$\frac{3(k+1)(k+2)(k+3)-2(2k+3)(k+3)+4(k+2)}{4(k+1)(k+2)(k+3)}.$$
Next you need to expand out each product in the top, and collect like terms. When you've done that you should try to factorise the top and cancel any common factor with the bottom. If you've done it right, you should get a common factor of $(k+1)$.</p>
"
"2378558","2378606","<p>The answer is <strong>No</strong>!</p>

<hr>

<hr>

<p><strong>Lemma(1)</strong>: </p>

<ul>
<li><strong>Part(a)</strong> 
Let $p$ to be an odd prime number. </li>
</ul>

<p>$\ \ \ \ \  $ 
Let $a$ and $b$ to be any integers. 
If $ \  \  \nu_p(ab)=0 \ \ $ and $ \  \ 1 \leq \nu_p(a-b) \ , \ $ then for every $n$ we have:</p>

<p>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ 
\nu_p\big(a^n-b^n\big)=\nu_p\big(n(a-b)\big)=\nu_p(a-b)+\nu_p(n)$. </p>

<ul>
<li><strong>Part(b)</strong> 
Let $p=2$. </li>
</ul>

<p>$\ \ \ \ \  $ 
Let $a$ and $b$ to be any integers. 
If $ \  \  \nu_2(ab)=0 \ \ $ and  $ \  \ 2 \leq \nu_2(a-b) \ , \ $ then for every $n$ we have:</p>

<p>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ 
\nu_2\big(a^n-b^n\big)=\nu_2\big(n(a-b)\big)=\nu_2(a-b)+\nu_2(n)$. </p>

<p><strong>Lemma(2)</strong>: </p>

<ul>
<li><strong>Part(a)</strong> 
Let $p$ to be an odd prime number. </li>
</ul>

<p>$\ \ \ \ \  $ 
Let $a$ and $b$ to be any integers. 
If $ \  \  \nu_p(ab)=0 \ \ $ and  $ \  \ 1 \leq \nu_p(a+b) \ , \ $ then for odd $n$ we have: </p>

<p>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ 
\nu_p\big(a^n+b^n\big)=\nu_p\big(n(a+b)\big)=\nu_p(a+b)+\nu_p(n)$. </p>

<ul>
<li><strong>Part(b)</strong> 
Let $p=2$. </li>
</ul>

<p>$\ \ \ \ \  $ 
Let $a$ and $b$ to be any integers. 
If $ \  \  \nu_2(ab)=0 \ \ $ and  $ \  \ 2 \leq \nu_2(a+b) \ , \ $ then for odd $n$ we have: </p>

<p>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ 
\nu_2\big(a^n+b^n\big)=\nu_2\big(n(a+b)\big)=\nu_2(a+b)+\nu_2(n)$. </p>

<hr>

<hr>

<hr>

<hr>

<hr>

<hr>

<p><strong>Lemma(3)</strong>: 
Let $p$ to be a prime number. 
Assume that the Lemma holds for $n=r$ and arbitrary $a$ &amp; $b$. 
Also asssume that the Lemma holds for $n=r$ and arbitrary $a$ &amp; $b$. 
then this holds for $n=rs$ and arbitrary $a$ &amp; $b$. </p>

<p><strong>Proof</strong>: Let $A:=a^r$ &amp; $B:=b^r$.</p>

<p>$ 
\nu_p\big(a^{rs}-b^{rs}\big)= 
\nu_p\Big((a^r)^s-(b^r)^s\Big)= 
\nu_p\big(A^s-B^s\big) 
\overset
{ \tiny {\text {Lemma for $n=s$} } } 
{=} 
\\ 
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\nu_p\big(s(A-B)\big)= %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\nu_p(A-B)+\nu_p(s)= 
\nu_p(a^r-b^r)+\nu_p(s) 
\overset
{ \tiny {\text {Lemma for $n=r$} } } 
{=} 
\\ 
\big(\nu_p(a-b)+\nu_p(r)\big)+\nu_p(s)= 
\nu_p(a-b)+\big(\nu_p(r)+\nu_p(s)\big)= 
\nu_p(a-b)+\nu_p(rs)$</p>

<hr>

<hr>

<p><strong>Lemma(4)</strong>: </p>

<ul>
<li><strong>Part(a)</strong> 
Let $p$ to be an odd prime number. </li>
</ul>

<p>$\ \ \ \ \  $ 
Let $a$ and $b$ to be any integers. 
If  $ \  \  \nu_p(ab)=0 \ \ $ and $ \  \ 1 \leq \nu_p(a-b) \ , \ $ then we have:</p>

<p>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ 
\nu_p\big(a^p-b^p\big)=\nu_p\big(p(a-b)\big)=\nu_p(a-b)+\nu_p(p)$. </p>

<p><strong>Proof</strong>: Assume that $a-b=lp^{\nu}$, with $gcd(l,p)=1$ 
and $\nu$ is the abbreviation for $\nu_p(a-b)$. 
Then we have: </p>

<p>$\nu_p \big(a^p-b^p\big)= 
\nu_p \big((lp^{\nu}+b)^p-b^p\big)= 
\nu_p \Bigg( \sum_{i=1}^{i=p}\Big(C(i,p)(lp^{\nu})^i b^{(p-i)})\Big) \Bigg)= 
\nu_p \Big(C(1,p)(lp^{\nu})^1 b^{(p-1)}\Big)= 
\nu_p \Big(  p   (lp^{\nu})   b^{(p-1)}\Big)= 
\nu_p \Big(  p  .  p^{\nu}             \Big)= 
1+\nu= 
\nu_p(a-b)+1= 
\nu_p(a-b)+\nu_p(p)
$</p>

<p>So the assertion holds for $n=p$.</p>

<hr>

<hr>

<p><strong>Lemma(5)</strong>: </p>

<ul>
<li><strong>Part(a)</strong> 
Let $p$ to be an odd prime number, and let $q \neq p$ to be another distinct prime number.</li>
</ul>

<p>$\ \ \ \ \  $ 
Let $a$ and $b$ to be any integers. 
If $ \  \  \nu_p(ab)=0 \ \ $ and $ \  \ 1 \leq \nu_p(a-b) \ , \ $ then we have:</p>

<p>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ 
\nu_p\big(a^qp-b^q\big)=\nu_p\big(q(a-b)\big)=\nu_p(a-b)+\nu_p(q)$. </p>

<p><strong>Proof</strong>: Assume that $a-b=lp^{\nu}$, with $gcd(l,p)=1$ 
and $\nu$ is the abbreviation for $\nu_p(a-b)$ . 
Then we have: </p>

<p>$\nu_p \big(a^q-b^q\big)= 
\nu_p \big((lp^{\nu}+b)^q-b^q\big)= 
\nu_p \Bigg( \sum_{i=1}^{i=q}\Big(C(i,q)(lp^{\nu})^i b^{(q-i)})\Big) \Bigg)= 
\nu_p \Big(C(1,q)(lp^{\nu})^1 b^{(q-1)}\Big)= 
\nu_p \Big(  q   (lp^{\nu})   b^{(q-1)}\Big)= 
\nu_p \Big(  q     p^{\nu}             \Big)= 
\nu= 
\nu_p(a-b)+0= 
\nu_p(a-b)+\nu_p(q)
$</p>

<p>So the assertion holds for $n=q$.</p>

<hr>

<hr>

<p><strong>Proof of the lemma(1), Part(a)</strong>. We will prove the Lemma(1) by induction on $n$.
By considering the Lemma(3) it only suffices to prove the lemma for prime exponents, which already we have done in lemma(4) and lemma(5).</p>

<hr>

<hr>

<p><strong>Proof of the lemma(2)</strong>: Let $a^{\prime}=a$, $b^{\prime}=-b$. 
It only suffices to notice that for odd $n$ we have: </p>

<p>$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
  \ \ \ \ \ \ \ \ \ \ \ \  
a^n+b^n=a^{\prime ^ n}-b^{\prime ^ n}$.</p>

<p>Now it can easily derived from lemma(1).</p>

<hr>

<hr>

<p>The answer is <strong>No</strong>!</p>

<p>At first suppose that $p$ is odd, and let $x=p-1$ and $y=2p-1$.</p>

<ul>
<li><p>If $n$ is odd; then we have: 
$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\nu_p(x^n+1)=\nu_p(x+1)+\nu_p(n)=
\nu_p(y+1)+\nu_p(n)=\nu_p(y^n+1)$.</p></li>
<li><p>If $n$ is even; then we have: 
$ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\nu_p(x^n+1)=0=\nu_p(y^n+1)$.</p></li>
</ul>
"
"2378561","2378562","<p>Certainly, since $\det A^2 = (\det A)^2 = (-1)^n$, and squares cannot be negative in $\mathbb R$. </p>
"
"2378565","2378578","<p>A matrix in $M(R)$ is invertible if and only if its determinante is a unit of $R$. <a href=""https://math.stackexchange.com/questions/221681/invertible-matrices-over-a-commutative-ring-and-their-determinants"">(Argument for example here)</a>. If $AB=I$, we get from $$1=\det(I)=\det(AB)=\det(A)\det(B)$$ that $\det(A)$ is a unit of $R$, so $A$ is invertible and so there is a matix $C$ with $CA=I$.</p>
"
"2378585","2378850","<p>Alternatively:
$$x^2+y^2=\left(\frac{-b-\sqrt{b^2-4ac}}{2a}\right)^2+\left(\frac{-b+\sqrt{b^2-4ac}}{2a}\right)^2=\frac{b^2-4ac}{a^2}&lt;0 \Rightarrow$$
$$b^2-4ac&lt;0 \Rightarrow x,y\in \mathbb{C}.$$</p>
"
"2378592","2378607","<p>Note that your dual basis is given by nothing but the evaluation at the $4$ points $-1,0,1,2$. By very definition of dual basis, you want to find $4$ linearly independent vectors in your base space, such that
$$\phi_i(v_j)=\delta_{ij},$$</p>

<p>In other words, you want to find $4$ linearly independent polynomials of degree smaller equal $3$ such that each vanishes at exactly $3$ of the four evaluation points $-1,0,1,2$ and such that the evaluation at the fourth point is $1$.</p>

<p>Writing the candidate polynomial as
$$f(x)=a(x-b)(x-c)(x-d)$$
makes it clear that for $f$ to vanish for example at $-1,0,1$ it must be of the form
$$f(x)=a(x+1)(x)(x-1)$$
and lastly evaluating it at $2$ gives the value of $a$, namely
$$f(2)=a(2+1)(2)(2-1)=6a\overset{!}{=}1\Rightarrow a=\frac16$$
hence the first basis element is
$$f(x)=\frac16(x+1)(x)(x-1).$$
The same reasoning applies for the remaining $3$.</p>

<p>Of course one shall last argue that these are indeed linearly independent. But if $B^\ast$ is known to be a (dual) basis, then this immediate.</p>
"
"2378595","2378765","<p>$\lim_{x\to \infty } \, \dfrac{\frac{x}{\log x}}{\text{Li}(x)}=1$</p>

<p>Use L'Hopital rule
$$\lim_{x\to \infty } \, \frac{\frac{1}{\log (x)}-\frac{1}{\log ^2(x)}}{\frac{1}{\log (x)}}=\lim_{x\to \infty } \, \left(1-\frac{1}{\log (x)}\right)=1$$</p>

<p>Hope this helps</p>
"
"2378597","2378651","<p>If $f(x) = 1_{[0,a]}-1_{[-a,0]},  g(x) = 1_{[0,b]}-1_{[-b,0]}$ then $$h(x) = f\ast g(x), \qquad h'(x) = f \ast g'(x) = 2 f(x)-f(x+b)-f(x-b)$$</p>

<p>$$ h(x) = \int_{-\infty}^x h'(y)dy = 2F(x)-F(x+b)-F(x-b), \\ F(x) = \int_{-\infty}^x f(y)dy = a(|x/a|-1) 1_{|x| &lt; a}$$</p>
"
"2378600","2378646","<p>Simply do$$\begin{pmatrix}2&amp;0\\3&amp;0\\0&amp;-4\end{pmatrix}\begin{pmatrix}1\\1\end{pmatrix}=\begin{pmatrix}2\\3\\-4\end{pmatrix}.$$That's because the coordinates of $\phi$ in the dual of the canonical base are $1$ and $1$, that is, if the canonical basis is $(e_1,e_2)$, then $\phi={e_1}^*+{e_2}^*$.</p>
"
"2378604","2378615","<p>The variable in a <em>relational</em> symbols expression $X^n(v_1, \ldots, v_n)$ are ""place holders"".</p>

<p>Thus, what matters is their ""position"" in the expression: this is the reason wahy we use subscripts to name them.</p>

<p>In an expression:</p>

<blockquote>
  <p>$âX^n \ âv_1 \ldots âv_n \ X^n(v_1, \ldots, v_n)$</p>
</blockquote>

<p>all variables are properly bound.</p>

<p>This does not happens if we have e.g. $âX^3 \ âv_1 âv_3 âv_2 \ X^3(v_1, v_4, v_3)$; thus, we have to take care in using ""dots"".</p>

<p>In general, if we write: $1,2,\ldots, n$, it is implicit tha we are listing <strong>all</strong> numbers between $1$ and $n$ in their proper order.</p>

<hr>

<p>Things are different with a different ""mix"" of quantifiers.</p>

<p>Consider the binary predicate $(v_1 \le v_2)$ (that reads ""$v_1$ is less-or-equal than $v_2$"").</p>

<p>Nothing changes in writing $âv_1 âv_2 (v_1 \le v_2)$ instead of $âv_1 âv_2 (v_2 \le v_1)$, because the second form is the same as $âv_2 âv_1 (v_1 \le v_2)$ and we can ""swap"" the same quantifier.</p>

<p>This is not so for different quantifiers: $\exists v_1 âv_2 (v_1 \le v_2)$ is <em>true</em> in $\mathbb N$, while $\exists v_1 âv_2 (v_2 \le v_1)$ is <em>false</em>.</p>
"
"2378605","2378609","<p>No. You need to remember to use the chain rule here. If $g'(x) = (1-g(x))g(x)$, then $$\frac{\mathrm{d}}{\mathrm{d}x} g(f(x)) = g'(f(x))\cdot f'(x) = (1-g(f(x)))\cdot g(f(x))\cdot f'(x)$$ In this case, $f(x) = 2x^2-x+ab$, so $f'(x) = 4x-1$. Therefore, $$\frac{\mathrm{d}}{\mathrm{d}x}\left[\frac{1}{1+e^{2x^2-x+ab}}\right] = \left(1-\frac{1}{1+e^{2x^2-x+ab}}\right)\frac{4x-1}{1+e^{2x^2-x+ab}}$$</p>
"
"2378616","2378622","<p>This function is <em>the</em> example of a function which is differentiable everywhere, but where the derivative isn't continuous. What is happening here is that $f$ is squeezed between $x^2$ and $-x^2$, so at $0$ the derivative must exist and it must be $0$ (this is true of any function, no matter how ugly and discontinuous: If it is squeezed between $x^2$ and $-x^2$, or something similar, then it is differentiable at $0$ with derivative $0$). If you think of the derivative as the slope of a line that approximates the function, rather than some algebraic limit of a fraction, this is easier to grasp.</p>

<p>However, <em>close</em> to $0$, the function $f$ oscillates up and down ever faster. In fact, it's happening so fast that even though it's squeezed between two parabolas, it gets to be about $1$ steep each time. That means that the derivative goes up and down between (roughly) $\pm 1$ more and more as you get closer and closer to $0$. And that is why $\lim_{x\to 0}f'(x)$ doesn't exist.</p>
"
"2378617","2378619","<p>In this specific case, by symmetry, we can say that all $$\int_{\partial B}{x_i}^2dS(x)$$ are equal ($i=1,2,3$), but their sum is $R^2 vol(\partial B)=4\pi R^4$, so...</p>
"
"2378618","2378620","<p>The blue line (graph of the function $f(x)=x$) is the <strong>derivative</strong> of the red line (graph of the function $g(x)=1$). At each point on the red line, the slope (i.e., the tangent of the angle between the horizontal line and the red line) is equal to $1$ (which is the value of the blue line).</p>

<p>The two lines do not coincide, because the two functions are not the same function.</p>
"
"2378630","2378652","<p>Hint:</p>

<p>If you change the question into:</p>

<p>""Randomly choose (uniformly and independently) $4$ points so that there are $4$ arcs. What is the sum of the expected lengths of the two arcs that are bordered by (e.g.) the first chosen point?"" </p>

<p>then there is no essential difference with the original question. </p>

<p>To understand that be aware of the ""coincidental"" choice for $(0,1)$. Why not just some randomly chosen point here? That would not change things, would it?</p>

<p>This approach will lead easily to $\pi$ as answer on base of linearity of expectation and symmetry.</p>
"
"2378635","2379169","<p>Let's investigate the equation $$\sum_{i=0}^n (n+i)^{x_n}=(2n+1)^{x_n}:$$ As $i$ goes from $0$ to $n$, $n+i$ goes from $n$ to $2n$. If we reorder the sum upside down, and divide both sides by $(2n+1)^{x_n}$, we get
$$\sum_{k=1}^{n+1}\left(\frac{2n+1-k}{2n+1}\right)^{x_n}=\sum_{k=1}^{n+1}\left(1-\frac{k}{2n+1}\right)^{x_n}=1.$$ Using the elementary inequality $$1-\frac{k}{2n+1}\le\left(1-\frac1{2n+1}\right)^k$$ and setting $$\lambda_n=\left(1-\frac1{2n+1}\right)^{x_n},$$ the equation gives $$1\le\sum_{k=1}^{n+1}(\lambda_n)^k&lt;\sum_{k=1}^\infty(\lambda_n)^k=\frac{\lambda_n}{1-\lambda_n}.$$ So $\lambda_n&gt;1/2,$ meaning $$x_n&lt;\frac{\ln 2}{\ln\frac1{1-1/(2n+1)}}&lt;(2n+1)\ln 2,\tag{1}$$ since $\ln\frac1{1-x}&gt;x$ for $1&gt;x&gt;0.$<br>
The lower bound for $x_n$ is somewhat more technical:<br>
We have $$1-\frac{k}{2n+1}=\frac1{1+\frac{k}{2n+1-k}}\ge\left(\frac1{1+\frac{1}{2n+1-k}}\right)^k\ge\left(\frac1{1+\frac{1}{2n+1-\sqrt{n}}}\right)^k$$ for $k\le\sqrt{n}.$ Setting $$\mu_n=\left(\frac1{1+\frac{1}{2n+1-\sqrt{n}}}\right)^{x_n},$$ we obtain the estimate
$$1\ge\sum_{k=1}^{\lfloor\sqrt{n}\rfloor}\left(1-\frac{k}{2n+1}\right)^{x_n}\ge\sum_{k=1}^{\lfloor\sqrt{n}\rfloor}(\mu_n)^k=\frac{\mu_n}{1-\mu_n}\left(1-(\mu_n)^{\lfloor\sqrt{n}\rfloor}\right).$$ Clearly, this means $\mu_n\le1/(2-\epsilon_n),$ where $\epsilon_n=O(c^\sqrt{n})$ with some $c&lt;1.$ This means $$x_n\ge\frac{\ln(2-\epsilon_n)}{\ln\left(1+\frac{1}{2n+1-\sqrt{n}}\right)}&gt;(2n+1-\sqrt{n})\ln(2-\epsilon_n),\tag{2}$$ since $\ln(1+x)&lt;x$ for $x&gt;-1.$<br>
(1) and (2) together imply $$\left(2+\frac1n-\frac1{\sqrt{n}}\right)\ln(2-\epsilon_n)&lt;\frac{x_n}{n}&lt;\left(2+\frac1n\right)\ln2,$$ and thus, $$\lim_{n\rightarrow\infty}\frac{x_n}{n}=2\ln2.$$<br>
In the case of the equation $$\sum_{i=0}^n (n+i)^{x_n}=(2n+2)^{x_n},$$ we get
$$\sum_{k=2}^{n+2}\left(1-\frac{k}{2n+2}\right)^{x_n}=1,$$ so the equation for $\lambda=\lim_{n\rightarrow\infty}\lambda_n$ becomes not $\lambda/(1-\lambda)=1,$ but $\lambda^2/(1-\lambda)=1,$ i.e. $\lambda=1/\phi.$</p>
"
"2378641","2378661","<p>In reference to your first point, by Fourier Law the heat flux $q$ is proportional to the temperature gradient. Due to simmetry, the flux is the same along any point of the outer surface, at fixed $z$. We can conveniently choose a point where the gradient simplifies to a single component, and then 
$$ \lvert F \rvert = - k \frac{\partial T}{\partial x} = - k\, (-\frac{1}{2}) (x^2 + y^2 + z^2)^{-3/2} 2 x $$
Evaluating at $x=a$, and incorporating all the constants in the constant $\alpha$ (without changing its name, as its arbitrary anyhow) you get the stated result.</p>

<p>In referece to your second point, it seems to me perfectly valid.  I wonder if they actually meant the center as the point $(0 , \frac{h}{2})$ in a cylindrical coordinate system with the origin coincinding with the center of one of the bases,and the $z$ axis aligned to the axis, directed towards the other base. The center has to be a point: otherwise, they would have maybe used the expression, ""distance from the axis"". </p>
"
"2378642","2378988","<p>The expression they provide is just a useful measure, but it is not exact. </p>

<p>Let me first recast the problem in a simpler form, maybe easier to handle. The helical geometry is not essential, as the only relevant value is the radius $r$.
So one can translate everything to the plane. </p>

<p>Given a continuous function $f(x)$ such that it is different from zero only on a certain interval, without loss of generality $[-\frac{L}{2};\frac{L}{2}]$, we can ask the question of looking for a quantity that could give us an approximation of the said interval length. </p>

<p>This is analogous to what they do in the paper you cite: they deal with âlocalisedâ helices, such that $r$ is different from $0$ only over an interval (this explains their comment on why the expression they mention is rather insensitive to integration limits).</p>

<p>So, looking for such quantity, first of all we would like it to be dimensionally right: and the expression you quote has the dimension of length (the denominator has dimension $L^6$ and the denominator $L^5$), so we indeed get a length out of it, dimensionally speaking. </p>

<p>Further, we can check how the provided expression works for âboxâ type functions, zero outside an interval (of length, say, $L$) and equal to a constant, which we call $a$, inside it.
Then, the expression you quote simplifies to $$ \frac{(L a^2)^2}{L a^4} = \frac{L^2 a^4}{L a^4} = L $$ as desired.</p>

<p>For a general function $f$, one can use the mean value theorem, and say
$$ \frac{(\int_{-\infty}^{\infty} f^2(x) \mathrm{d}x)^2} {\int_{-\infty}^{\infty} f^4(x)\mathrm{d}x} =
\frac{(\int_{-L/2}^{L/2} f^2(x) \mathrm{d}x)^2} {\int_{-L/2}^{L/2} f^4(x)\mathrm{d}x} =
\frac{(L f^2(\xi_1))^2}{L f^4(\xi_2)} = L \frac{f^4(\xi_1)}{f^4(\xi_2)}
$$ 
And it all boils down to the fact $\xi_1$ and $\xi_2$ are âcloseâ.</p>

<p>We can further check that the answer is not exact for less simple functions: one can try functions such as $ f(x) =\beta \sqrt{cos(x)}$ defined on $[-\pi/2;\pi/2]$ and zero everywhere else, or $ f(x) = \beta \sqrt{1-x^2}$ defined on $[-1;1]$ and zero everywhere else , and verify that the answer is only an approximation, albeit very useful. </p>
"
"2378653","2378719","<p>The continuous image of a countably compact space is countably compact (the proof is like the proof for compact spaces, just that one considers only countable open covers of the image), and a countably compact subspace of $\mathbb{R}$ is compact (since $\mathbb{R}$ is second countable, hence LindelÃ¶f), thus</p>

<p>$$A_k = \{ f(k) : f \in M\}$$</p>

<p>is bounded for each $k \in K$. Therefore $F_k = \overline{A_k}$ is compact and by TÃ­khonov's theorem,</p>

<p>$$\prod_{k \in K} F_k$$</p>

<p>is a compact subset of $\mathbb{R}^K$. Since $M \subset \prod\limits_{k\in K} F_k$, it follows that</p>

<p>$$\overline{M}^{\mathbb{R}^K} \subset \prod_{k\in K} F_k,$$</p>

<p>and as a closed subset of a compact set, it is itself compact.</p>
"
"2378657","2378659","<p>Because actually
$$\sin^4(\alpha)+\cos^4(\alpha)=(\sin^2(\alpha)+\cos^2(\alpha))^2-2\sin^2(\alpha)\cos^2(\alpha)=1-\frac12\sin^2(2\alpha),$$ </p>
"
"2378660","2378752","<p>If we're not required to find the basis $B$, then our job is a lot easier.</p>

<p>Note that $J$ is the Jordan form corresponding to $A$, then $\operatorname{rank}(A^k) = \operatorname{rank}(J^k)$.  Moreover, for $k = 1,2,\dots,n$, $\operatorname{rank}(J^{k-1}) - \operatorname{rank}(J^k)$ is the number of Jordan blocks in $J$ with size at least $k$.  Finally, note that $A$ is nilpotent, which means that $0$ is its only eigenvalue, which means that we only need to worry about blocks for the eigenvalue $0$.</p>

<p>Putting all this together, it suffices to determine $\operatorname{rank}(A^k)$ for all $k \leq m$, where $m$ is the first integer such that $A^m = 0$.</p>

<p>This process is described in Horn and Johnson's <em>Matrix Analysis</em>.</p>
"
"2378662","2378793","<p>Let $n=2m-1$ be odd. The simple function
$$\phi(x):=|x|-|x+1|=\left\{\eqalign{&amp;\ \quad1\qquad\quad(x\leq-1) \cr &amp;-2x-1\quad(-1\leq x\leq0)\cr&amp;\quad-1\qquad\ \ (x\geq0)\cr}\right.$$
is $\equiv1$ for $x\leq-1$, then has a ramp of slope $-2$ in the interval $[{-1},0]$, and is $\equiv-1$ for all $x\geq0$.</p>

<p>Since
$$f_{2m-1}(x)=\sum_{j=0}^{m-1} \phi(x+2j)$$
the function $f_{2m-1}$ is a sum of $m$ copies of $\phi$ translated to the left by amounts $2j\geq0$ $(0\leq j\leq m-1)$. It therefore has $m$ such descending ramps, the leftmost beginning at $-1-2(m-1)=-(2m-1)$. It follows that $f_{2m-1}$ is monotonically decreasing and takes its minimum $-m$ in all points $x\geq0$.</p>

<p>Now 
$$f_{2m}(x)=f_{2m-1}(x)+|x+2m|\ .$$
Here  the term $|x+2m|$ is decreasing with slope $-1$ for $x\leq-2m$ and increasing with slope $1$ for $x\geq2m$. In the interval $[{-2m},0]$ this steady increase of $x\mapsto |x+2m|$ interferes with the cascade of ramps of $f_{2m-1}$, resulting in a horizontal zigzag of period $2$ and amplitude $1$. At $x=0$ we are at the lower end of the last ramp, and from then on $f_{2m}$ will definitely increase with slope $1$. The minimum of $f_{2m}$ can therefore be found by computing $f_{2m}(0)$, and is found to be
$$f_{2m}(0)=f_{2m-1}(0)+|0+2m|=-m+2m=m\ .$$</p>
"
"2378663","2382545","<p>The gradient of $f$ is $\nabla f(x) = 2 A^T(Ax-b)$. To use Nesterov's method we need a Lipschitz constant for $\nabla f$ (not $f$). Notice that
\begin{align}
\| \nabla f(x)-\nabla f(y) \| &amp;= 
\|2 A^T A(x-y) \| \\
&amp;\leq 2 \| A^T A \| \|x-y\| \\
&amp;= 2 \| A\|^2 \|x-y\|^2.
\end{align}
So a Lipschitz constant for $\nabla f$ is 
$$
\beta= 2 \|A\|^2.
$$</p>
"
"2378668","2378690","<p>The answer is A. Since $\lim_{x\to \infty}f'(x)$ exists, suppose by contradiction that $\lim_{x\to \infty}=L\ne 0.$ For some $r$ we have $$x&gt;r\implies |f'(x)-L|&lt;|L|/2\implies |f'(x)|&gt;|L|/2\implies$$ $$\implies \exists y\in (x,x+1)\text { such that }|f(x+1)-f(x)|=\left|\frac {f(x+1)-f(x)}{(x+1)-x}\right|=$$ $$=|f'(y)|&gt;|L|/2.$$ But if $L\ne 0$ then $\lim_{x\to \infty}f(x)$ cannot exist when  $x&gt;r\implies |f(x+1)-f(x)|&gt;|L|/2.$</p>

<p>OR You can eliminate C,D, and E with $f(x)=\tan^{-1}x,$ and eliminate B on the grounds that  there may be arbitrarily large $x$ where $f''(x)$ does not exist.</p>
"
"2378687","2379086","<p>Yes, this is correct. Start with an orthonormal moving frame $e_1,\dots,e_n; e_{n+1}$ with $e_{n+1}$ the normal, and let $\omega_i$ be the dual coframe on $M$. If $e_1,e_2$ is an orthonormal basis for $\Sigma$ and $\Omega_{ij}$ is the curvature $2$-form, we have
$$K(\Sigma) = -\Omega_{12}(e_1,e_2).$$
By the Gauss equation, we also have
$$\Omega_{12} = -\sum_{i,j=1}^n h_{1i} h_{2j} \omega_i\wedge\omega_j,$$
where $(h_{ij})$ is the matrix of the second fundamental form.
In particular, we see that $K(\Sigma) = h_{11}h_{22}-h_{12}h_{21}$, and this is precisely the determinant of $s|_\Sigma$ in the event that $s(\Sigma)\subset\Sigma$. (After all, the matrix of $s_p$ with respect to the basis $e_1,\dots,e_n$ is precisely $(h_{ij})$.)</p>

<p>By the way, here's something else that you may not know relating to the sectional curvature. $K(\Sigma)$ is precisely the Gaussian curvature at $p$ of the surface $\exp_p(\Sigma)$.</p>
"
"2378695","2378718","<p>Let $M=\min\{|e^{3z}-1|\,:\,|z|=2\}$. Clearly, $M&gt;0$. Since $({P_N}^3-1)_{N\in\mathbb{N}}$ converges uniformly to $e^{3z}-1$ in the circle $\{z\in\mathbb{C}\,:\,|z|=2\}$, if $N$ is large enough, then$$\bigl|(e^{3z}-1)-({P_N}^3(z)-1)\bigr|&lt;\frac M2.$$ But this implies that $|{P_N}^3(z)-1|&gt;\frac M2$ and therefore$$\left|\frac1{{P_N}^3(z)-1}\right|&lt;\frac2M.$$So, the sequence$$\left(\frac1{{P_N}^3-1}\right)_{N\in\mathbb{N}}$$is uniformly bounded and therefore it converges uniformly to $\frac1{e^{3z}-1}$ in $\{z\in\mathbb{C}\,:\,|z|=2\}$.</p>
"
"2378698","2378707","<p>This equation is separable. Namely, we can write $$\frac{S'(t)}{q-\mu XY\frac{S(t)}{S(t)+K}} = \frac{(K+S(t))S'(t)}{qK+(q-\mu XY)S(t)} = 1$$ which implies $$\int \frac{(K+S(t))S'(t)}{qK+(q-\mu XY)S(t)}\,\mathrm{d}t = \int 1\,\mathrm{d}t$$ Applying partial fractions and integrating gives us $$\frac{S(t)}{q-\mu XY}-\frac{\mu KXY}{(q-\mu XY)^2}\log(qK+(q-\mu XY)S(t)) = t+C$$ Plugging in $t = 0$ gives us $$C = -\frac{\mu KXY\log(qK)}{(q-\mu XY)^2}$$ so we can move this to the left-hand side of the equation to get $$\frac{S(t)}{q-\mu XY}-\frac{\mu KXY}{(q-\mu XY)^2}\log\left(1+\frac{q-\mu XY}{qK}S(t)\right) = t$$</p>
"
"2378700","2378711","<p>The idea is to resort to <a href=""https://en.wikipedia.org/wiki/Euler%27s_theorem"" rel=""nofollow noreferrer"">Euler's Theorem</a>. You want to solve the equation 
$$x^{98}\equiv 7\,(\text{mod } 18).$$
This, in particular, would imply that $x^{98}=7 + 18\cdot n$ for some $n\in\mathbb{N}$, hence $x$ has to be coprime with $18$ (if $p$ divides $x$ and $18$, then it divides $x^{98}$ and $18$ and so $7$ as well, which is impossible). Thus you are in the hypothesis of Euler's Theorem: $x$ and $18$ are coprime and so
$$x^{\varphi(18)}\equiv 1\,(\text{mod } 18).$$
By repeatedly using this relation, you should be able to simplify your equation to something more handy.</p>
"
"2378702","2378770","<p>I think the easiest way to do this is to count the words with all letters different, then the other cases.</p>

<p>The number of words with all letters different is ${}^6\mathrm P_4=360$, since there are $6$ different letters to choose from.</p>

<p>The number of words with two Rs and other letters all different is ${}^4\mathrm C_2\times {}^5\mathrm P_2=120$, since there are ${}^4\mathrm C_2$ ways to choose the positions of the Rs and then ${}^5\mathrm P_2$ ways to pick two different letters from the remaining five, in order. Similarly there are $120$ words with two Ds and all other letters different.</p>

<p>Finally, the number of words with two Rs and two Ds is ${}^4\mathrm C_2=6$: once we choose two places for the Rs, we know where the Ds have to go.</p>

<p>This gives a total of $606$ words.</p>
"
"2378710","2378733","<p>Here's something <a href=""https://math.stackexchange.com/questions/2007373/basic-geometric-intuition-context-is-undergraduate-mathematics/2007496#2007496"">I wrote about this viewpoint</a> earlier:</p>

<blockquote>
  <p>A <strong>better answer</strong> to your misgivings (in the sense of being closer to the mainstream presentation) is probably simply to jump in with both feet and declare that <strong>a line is not really made of points</strong>.</p>
  
  <p>Decide to think of a line as <em>something that is in principle a different kind of thing from a bunch of points glued together</em>. You can do this and still acknowledge that points <em>exist</em> and some of them are on the line while others are not.</p>
  
  <p>It then turns out that all of the properties of a line segment (or a smooth curve in general) can be <em>recovered</em> from knowing which points lie on it and which don't. This doesn't necessarily mean that the points <em>make up</em> the line, but merely that the points tell us <em>enough</em> about the line.</p>
  
  <p>It is <em>technically convenient</em>, then, to speak about the <em>set of points on the line</em> as a placeholder for the line itself, when we're formalizing our reasoning -- for the pragmatic reason that we have a well-developed common machinery and notation for speaking of sets of things, which means that we don't need to introduce a new formalism for an entirely different kind of things.</p>
  
  <p>Some people are so comfortable with this representation that they happily declare that the line IS its set of points -- but nobody says you have to think of it that way. As long as you agree that the set of points <em>determine</em> the line, you can still communicate with people who prefer the other idea.</p>
</blockquote>

<p>Why would one want to do that, you ask?</p>

<p>One reason is if it happens to be more comfortable philosophically to think that way. It's up to you whether it is or not, of course -- but, for example, the asker I wrote the above text had gotten himself into conceptual trouble trying to imagine how points can have zero length and width, and yet still combine to form a line of nonzero length.</p>

<p>Another reason is that you can <em>avoid talking about sets</em> at all for some purposes. Mathematicians use sets all the time, of course, but set theory does come at a foundational cost. For example, you can't have a <em>formal theory</em> that admits sets in a useful way without GÃ¶del's incompleteness theorem applying, so the theory would necessarily be incomplete.</p>

<p>In particular, plain old <em>Euclidean geometry</em> can be formalized without having any concept of a set of points. That's what Euclid was doing, of course, though he missed a number of ""intuitive"" continuity properties. Hilbert tried to repair that, now explicitly considering ""line"" to be a primitive concept, but he failed to make it completely formal because formal logic was still being invented. (Later Tarski constructed an actual complete first-order theory of geometry, but he did it by omitting <em>any</em> concept of ""line"" at all, speaking instead of ""collinear"" as a relation between three points).</p>
"
"2378714","2379544","<p>(I). We want to show that for any $(p_1,p_2)\in U_1\times U_2$ there exists $r_3&gt;0$ such that  the  open $d_+$ ball $B_{d_+}((p_1,p_2),r_3)$ is a subset of $U_1\times U_2.$</p>

<p>Take $r_1,r_2&gt;0$ such that  $B_{d_1}(p_1,r_1)\subset U_1$  and $B_{d_2}(p_2,r_2)\subset U_2.$ Let $r_3=\min (r_1,r_2).$  Then  $$(p,p')\in B_{d_+}((p_1,p_2),r_3)\implies$$ $$\implies (\;d_1(p,p_1)&lt;r_3\leq r_1\;\land\; d_2(p',p_2)&lt;r_3\leq r_2\;) \implies$$ $$\implies (\; p\in B_{d_1}(p_1,r_1)\subset U_1 \;\land \;p'\in B_{d_2}(p_2,r_2)\subset U_2\;)\implies$$ $$\implies (p,p')\in U_1\times U_2.$$</p>

<p>Since this holds for all $(p,p')\in B_{d_+}((p_1,p_2),r_3),$ we have $B_{d_+}((p_1,p_2),r_3)\subset U_1\times U_2. $ </p>

<p>(II). Metrics $d$ and $d'$ on a set $X$ produce the same topology iff $$(i).\quad  \forall p\in X \;\forall r&gt;0 \;\exists s&gt;0 \;(\;B_d(p,s)\subset B_{d'}(p,r)\;),\text { and }$$ $$ (ii). \quad \forall p\in X \;\forall r&gt;0 \;\exists s'&gt;0\;(\;B_{d'}(p,s')\subset B_d(p,r)\;).$$ </p>

<p>To get an idea of how to use this to show that $d_e$ and $d_{max}$ generate the same topology as $d_+,$ consider the case $X_1=X_2=\mathbb R,$ with $d_1(x,y)=d_2(x,y)=|x-y|.$  Sketch  some pictures of open balls of various radii, centered at some $p\in \mathbb R^2$, with respect to these 3 metrics.</p>

<p>Metrics on a set $X$ that generate the same topology  on $X$ are called equivalent metrics.</p>
"
"2378717","2378739","<p>When submitting a paper for publication, it is not only the editor who looks at it. The editor will (usually) take a first look to determine if the paper looks to be suitable for the journal. If they see that the main results are already well-known this will likely make them reject the paper (especially if the authors have not made any remark on this themselves). </p>

<p>If this is not the case and the paper generally looks fine (this also includes the results actually seeming to be interesting), then the paper will be sent to one or two experts in the field for review. These will (hopefully) take a much closer look at the paper and determine if they think it is suitable for the journal. Once again, this will include trying to determine whether the results are actually new, which is why it is important for the editor to choose reviewers who really are experts, as otherwise they will not be familiar enough with the field to determine this.</p>

<p>Finally, the reviewers will make a recommendation to the editor based on their reading, and using the editor will make a final decision on whether to accept the paper, possibly contingent on certain improvements.</p>
"
"2378758","2378861","<h2><strong>Partial answer:</strong></h2>

<p>We want to find the number $n$ whit decimal expansion $abc$ which
$$10^2a+10b+c=(a+4)(b+4)(c+4)$$
Then the following must hold true:
$$c=abc \mod 2$$
$$2b+c=abc \mod 4$$
If $c$ is odd, $a,b$ must be also odd. Then:
$$c+2=abc \mod 4$$
and
$$3=ab \mod 4$$
Then one must be $1$ and the other $3$ in modulus $4$. It is,</p>

<p>$(13c)$ or $(31c)$</p>

<p>$(17c)$ or $(71c)$</p>

<p>$(53c)$ or $(35c)$</p>

<p>$(57c)$ or $(75c)$</p>

<p>$(93c)$ or $(39c)$</p>

<p>$(97c)$ or $(79c)$</p>

<p>Then, we only need to test it for every $c$ odd number, i.e. 30 cases in total, although we could discard some numbers in advance (759,757,977,..).</p>
"
"2378764","2378936","<p>Introduce new orthogonal coordinates $u$, $v$ by means of 
$$x={u-v\over2},\quad y={u+v\over2}\ .$$
This amounts to a rotation by $45^\circ$ and a scaling. The equation then becomes
$$u^3-3u^2+3v^2+3uv^2=0$$
or
$$v^2={4\over 3(1+u)}-{(u-2)^2\over3}\ .$$
This shows that $|u|$ cannot get arbitrarily large, but $|v|$ can. The latter necessitates $$x+y=u\to-1\ .$$</p>
"
"2378766","2379928","<p>Assuming your problem is convex, KKT theorem ensures that under certain regularity conditions, $x^*$ is a global minimum iff there exists $\lambda^*\geq 0$ satisfying:
\begin{equation}
\nabla\mathcal{L}(x^*,\lambda^*)=0 \\ g_i(x^*)\leq0 \qquad \forall i\\ \lambda_i^*g_i(x^*)=0  \qquad \forall i
\end{equation}
So, for each solution you get by setting the gradient of $\mathcal{L(x,\lambda)} $ to zero, you should check that the other conditions are met.</p>
"
"2378768","2378778","<p>$$P(A|B)=\frac{P(Aâ§B)}{P(Aâ§B)+P(Â¬Aâ§B)}$$</p>

<p>This is true, and is essentially the definition of $P(A|B)$. Note however that the denominator $P(Aâ§B)+P(Â¬Aâ§B)$ is equal to $P(B)$, so we have:</p>

<p>$$P(A|B)=\frac{P(Aâ§B)}{P(B)}$$</p>

<p>From this definition, you can deduce Bayes. Just write the same formula with A and B swapped:</p>

<p>$$P(B|A)=\frac{P(Bâ§A)}{P(A)}$$</p>

<p>Then divide the last two equations, and use the fact that $Aâ§B=Bâ§A$, you get Bayes:
$$\frac{P(A|B)}{P(B|A)}=\frac{P(Aâ§B)}{P(B)}\frac{P(A)}{P(Bâ§A)}=\frac{P(A)}{P(B)}$$</p>
"
"2378783","2378795","<p>Preassuming that we are dealing with expectation of a discrete random variable here (you use the words ""sum up"") I would say that they indeed always sum up to $1$. </p>

<p>If you are calculating something like $\mathbb E[X\mid X\in A]$ then you get: $$E[X\mid X\in A]=\sum_{x\in A} xp'_x$$ where for $x\in A$: $$p'_x:=P(X=x\mid X\in A)=\frac {P(X=x)}{P(X\in A)}=\frac{p_x}{\sum_{x\in A}p_x}$$ </p>

<p>so that: $$\sum_{x\in A} p'_x=1$$</p>
"
"2378790","2378811","<p>Let ${\bf p}=(p_1,p_2,p_3)$ be the center of your sphere and introduce new coordinates $\bar x_i$ by
$$x_i=p_i+\bar x_i\qquad(1\leq i\leq3)\ .$$
Then your integral becomes
$$I=\int_{\partial\bar B}(p_1+\bar x_1)^2\&gt;{\rm d}S(\bar {\bf x})\ .$$
Here $\bar B$ is the ball of radius $R$ centered at the origin of $\bar{\bf x}$-space. The rest is easy.</p>
"
"2378797","2378801","<p>No. Define$$\begin{array}{rccc}f\colon&amp;(0,+\infty)&amp;\longrightarrow&amp;\mathbb R\\&amp;x&amp;\mapsto&amp;\frac1x.\end{array}$$Then $\lim_{x\to+\infty}f(x)=0$ and $f$ has no supremum (in $\mathbb R$).</p>

<p>However, if $f$ is continuous and if the domain is an interval of the type $[a,+\infty)$, then, yes, $f$ has a real supremum. Indeed, since $\lim_{x\to+\infty}f(x)=0$, there is a $M&gt;a$ such that$$x&gt;M\Longrightarrow f(x)&lt;1.$$ On the other hand, since $f$ is continuous and $[a,M]$ is a closed and bounded interval, $f|_{[a,M]}$ is bounded an, in particular, there is a $K&gt;0$ such that $(\forall x\in[a,M]):f(x)&lt;K$. Therefore$$(\forall x\in[a,M]):f(x)&lt;\max\{K,1\}$$and so $\sup f\leqslant\max\{K,1\}$.</p>
"
"2378799","2379203","<p>Your proposal is correct. However, when considering the function $f$ it is best to think of it as $f(t,x)$ rather than $g(t) = f(t, x(t)) $, since this is a different function. </p>
"
"2378803","2378807","<p>Multiply $\sqrt{n^2+n}+n$ top and bottom. In other words, we have:</p>

<p>$$\sqrt{n^2+n}-n=\frac{(\sqrt{n^2+n}-n)\times(\sqrt{n^2+n}+n)}{\sqrt{n^2+n}+n}$$</p>

<p>I think the rest you can figure it out on your own. We usually call such a method as 'rationalizing' the surd. </p>
"
"2378810","2378827","<p>You can use the fact that: $\sum_{i=1}^n i = \frac{n(n+1)}{2}$ to find the accurate value of the upper and lower sum. The fact that we're dealing with infinity can be a little bit tricky, but usually (at least that was the case for me) during the lectures we accept that this formula is true even when $n \to \infty$ without a proof. So:</p>

<p>$$L =\lim_{n \to \infty}  \sum_{r=1}^{n}  \left(2\cdot\left(2 +\frac{(r-1)}{n}\right) + 3 \right) \cdot \frac 1n = \lim_{n \to \infty}\sum_{r=1}^{n}  \left(\frac{2r + 7n-2}{n} \right) \cdot \frac 1n $$
$$ = \lim_{n \to \infty} \left(\frac{n(n+1)+7n^2 - 2n}{n} \right) \cdot \frac 1n = \lim_{n \to \infty} \frac{8n^2 - 2n}{n^2} = 8$$</p>

<p>Similarly:</p>

<p>$$U =\lim_{n \to \infty}  \sum_{r=1}^{n}  \left(2\cdot\left(2 +\frac{r}{n}\right) + 3 \right) \cdot \frac 1n = \lim_{n \to \infty}\sum_{r=1}^{n}  \left(\frac{2r + 7n}{n} \right) \cdot \frac 1n $$
$$ = \lim_{n \to \infty} \left(\frac{n(n+1)+7n^2}{n} \right) \cdot \frac 1n = \lim_{n \to \infty} \frac{8n^2}{n^2} = 8$$</p>

<p>As $U = L = 8$ we have that the function is Riemann Integrable on $[2,3]$.</p>
"
"2378812","2379588","<blockquote>
  <p>If we have two Poisson process $A$ and $B$ with arrival rate $\lambda_{1}$ and $\lambda_{2}$. Now I have question for calculating the probability of an arrival e.g., from process A in the case of merged Poisson process. Which of the approach we should use.</p>
</blockquote>

<p>If you have two <em>independent</em> Poisson processes, then the probabiliy that any particular arrival in the merged process was generated by process $A$ is indeed $\lambda_1/(\lambda_1+\lambda_2)$ .</p>

<p>A Poisson process consists of point events (arrivals) that occur over an interval at a constant average rate but each <em>independently</em> of any other arrival in the process.</p>

<p>Thus in the merged process you have arrivals that occur at a constant joint rate $(\lambda_1+\lambda_2)$ and each <em>independently</em> generated by one from the two <em>independent</em> Possion processes.</p>

<hr>

<p>Your aproach seems to be asking the condtional probability that, in a <em>very small</em> interval, we get one arrival from process A and no arrival from process B, given that we get only one arrival from the merged process in that <em>very small</em> interval. &nbsp; Well...sure.</p>

<p>$$\lim\limits_{\Delta t\to 0^+}\cfrac{~\cfrac{(\lambda_1\Delta t)^1e^{-\lambda_1\Delta t}}{1!}\cdot\cfrac{(\lambda_2\Delta t)^0e^{-\lambda_2\Delta t}~}{0!}}{\cfrac{((\lambda_1+\lambda_2)\Delta t)^1e^{-(\lambda_1+\lambda_2)\Delta t}}{1!}}~=~\dfrac{\lambda_1}{\lambda_1+\lambda_2}$$</p>

<hr>

<p>[Of course, to use this approach you have to first establish that the count of arrivals in the joint process follows a Poisson distribution with the combined average rate.]</p>
"
"2378817","2378943","<p>Set coordinate system as $A(0,1)$, $B(b,0)$ and $C(0,c)$. Then $D(0,0)$ and $P(0,p)$ for some $p$. </p>

<p>Then $X({(p-1)bc\over pc-b},{p(b-c)\over pc-b})$ and so the slope from $D$ through $X$ is $k = {p(c-b)\over (p-1)bc} $. </p>

<p>Also $Y({(p-1)bc\over pb-c},{p(c-b)\over pb-c})$ and so the slope from $D$ through $Y$ is $k' = {p(b-c)\over (p-1)bc} = -k$. </p>

<p>So $\angle ADX = \angle ADY$. </p>
"
"2378818","2378832","<blockquote>
  <p>But, the way we solve 2nd order differential equation is not applicable here, i.e., writing it as two first order differential equations.</p>
</blockquote>

<p>Why not?  Let $z = y'$;  then we have
$$
y'(x) = z(x) \\
z'(x) = \pm \sqrt{- \frac{B(x) y(x) + C(x)}{A(x)}} \\
z(0) = a \\
y(0) = b
$$
This will provide you with a local solution for $y(x)$ so long as $y''(x)$ does not cross zero (i.e., $z'(x)$ never switches sign.)  In fact, the solution in a neighborhood of 0 will not be unique (if it existsâthe quantity under the square root could be negative);  there will be two solutions, one with $z'(0) &gt; 0$ and one with $z'(0) &lt; 0$.</p>
"
"2378822","2379157","<p>This is the best I could get, hope it helps :)</p>

<p>$\dfrac{a}{b}+\dfrac{c}{d}=\dfrac{a+c}{b+d}$</p>

<p>For any $h,\;k,\;j\in\mathbb{Z},\;j\ne 0,h\ne-1$  </p>

<p>Let $\left\{a= k,b= j,c=- h^2 -k,d= h j\right\}$</p>

<p>we get $\dfrac{k}{j}-\dfrac{h k}{j}=\dfrac{k-h^2 k}{h j+j}$</p>

<p>indeed $\dfrac{k-hk}{j}=\dfrac{k(1+h)(1-h)}{j(h+1)}$</p>

<p>and finally $\dfrac{k-hk}{j}=\dfrac{k(1-h)}{j}$</p>

<p><strong>Edit</strong>.</p>

<p>I am not sure that these are <em>ALL</em> the solutions</p>
"
"2378828","2378863","<p>The Bezout GCD identity is $\ \gcd(a,b) = a x + b y\ $ for some integers $\,x,y,\,$ i.e. the gcd can be written as an integral linear combination of its arguments. Here $\,x,y\,$ are the Bezout coefficients.</p>

<p>One common application of Bezout coefficients is computing modular inverses</p>

<p>$$ \gcd(a,b)= 1\,\Rightarrow\, ax+by = 1\,\Rightarrow\,\bmod b\!:\,\ ax\equiv 1\,\Rightarrow\, a^{-1}\equiv x$$</p>

<p>Here are some other interesting related applications.</p>

<ul>
<li><p><a href=""https://math.stackexchange.com/a/68540/242"">Hermite's method</a> of integrating rational functions.</p></li>
<li><p><a href=""https://math.stackexchange.com/a/124378/242"">Factoring $\ ab + x\ $ into power series</a> when $\,\gcd(a,b) = 1$</p></li>
</ul>

<p>By Bezout's GCD identity we may write $\ \color{#c00}{\bf 1} = ad\!-\!bc\,$</p>

<p>Recall the <a href=""https://en.wikipedia.org/wiki/Catalan_number#Proof_of_the_formula"" rel=""nofollow noreferrer"">Catalan series</a> $\rm\, C(x)\in \mathbb Z[[x]]\,$ satisfies $\,\color{#090}{C(x) - x\:C(x)^2\! = 1},\,$ so with $\,e = cd$</p>

<p>$\ \ \   (a - cx\:C(ex))\:(b+dt\:C(ex))\ =\ ab + \color{#c00}{\bf 1}\cdot x\:(\color{#090}{C(ex) - et\:C(ex)^2})\: =\ ab + x$</p>
"
"2378830","2379773","<p>We have to start reading the definition of the set $A = \{ \ x \mid x^2âxâ2=0 \ \}$.</p>

<p>It is the set of <em>roots</em> of the <a href=""https://en.wikipedia.org/wiki/Quadratic_equation"" rel=""nofollow noreferrer"">quadratic equation</a>: $x^2âxâ2=0$.</p>

<p>You have rewritten the equation as: $(xâ2)(x+1)=0$ and thus its roots are: $-1$ and $2$ (see also comment above), that means:</p>

<blockquote>
  <p>$A = \{ -1, 2 \}$.</p>
</blockquote>

<p>Now we have to ""build"" the <a href=""https://en.wikipedia.org/wiki/Power_set"" rel=""nofollow noreferrer"">power set</a> $P=\mathcal P(A)$ (see comment above).</p>

<p>We know that, for a finite set with $n$ elements, its power set has $2^n$ elements.</p>

<p>We know that $A$ has two elements, i.e. $n(A)=2$, and thus we may conclude that:</p>

<blockquote>
  <p>$n(P)=4$.</p>
</blockquote>
"
"2378836","2380372","<p>Following @Mario Gretsas 's thoughts. Please help me check if it's correct.</p>

<p>Suppose otherwise then $m\{f&gt;0\}&gt;0\ or\ m\{f&lt;0\}&gt;0$. Without loss of generality, we assume $m\{f&gt;0\}&gt;0$. Since $\{f&gt;0\}=\lim_{n\to\infty}\{f\geq1/n\}$, and due to continuity of measurement and integrality of the function we have
$$\exists n_0\in\mathbb{N}\quad s.t.\quad 0&lt;m\{f\geq1/n_0\}&lt;\infty$$
For the measurablity of $\{f\geq1/n_0\},\exists\ an\ open\ set\ sequence\ \{G'_n\}_n\ $ such that</p>

<p>$$\bigcap_{n=1}^\infty G_n' \triangleq G\supset\{f\geq1/n_0\}\ \ \&amp;\ \ m(G-\{f\geq 1/n_0\})=0\ \ \&amp;\ \ \forall n,\ m(G_n')&lt;\infty$$
Let $G_n=\bigcap_{k=1}^nG'_k$. Then we get a open set sequence $\{G_n\}$ monotonically decresing to $\{G_\delta\}$. Then $\lim_{n\to\infty}m(G_n-G)=0.$</p>

<p>Because of the uniform continuity of integrals, there is a real number $\delta&gt;0$ such that for all measurable set $A$ satisfies $m(A)&lt;\delta$ we have $|\int_Afdx|&lt;\frac{m(G)}{2n_0}(&gt;0)$.</p>

<p>Since $\lim_{n\to\infty}m(G_n-G)=0$, there is a $N$ such that $m(G_N-G)&lt;\delta$.</p>

<p>$G_N$ is open, so it can be expressed as a countable union of open intervals, then $$\int_{G_N} fdx=0$$</p>

<p>But meanwhile, we have $$\int_G fdx=\int_{A_{n_0}}fdx\geq\frac{m\{f\geq1/n_0\}}{n_0}=\frac{m(G)}{n_0}&gt;0$$</p>

<p>Thus $$\frac{m(G)}{2n_0}\geq|\int_{G_N-G}fdx|=|\int_G fdx-\int_{G_N} fdx|\geq\frac{m(G)}{n_0}$$</p>

<p>In contradiction to our hypothesis.</p>
"
"2378838","2378895","<p>For the first function observe that it is a composition of $(x-1)^2$ and $e^x$ and since both are continuous and moreover, they are differentiable on $\Bbb R$, the composition i.e. $e^{(x-1)^2}$ is differentiable over $\Bbb R$. </p>

<p>So differentiating, $f^/(x) = 2(x-1)e^{(x-1)^2}$ and $$ f^{(2)}(x) = {4(x-1)^2}e^{(x-1)^2} + 2e^{(x-1)^2}$$.</p>

<p>Now equating, $f^/(x) = 0 $ we obtain that x = 1.</p>

<p>Putting x = 1 at  $ f^{(2)}(x) = {4(x-1)^2}e^{(x-1)^2} + 2e^{(x-1)^2}$ we deduce that $f^{(2)}(x) &gt; 0$, so f has a global minima at x = 1 . At that point , f(1) = 1.</p>

<p>So at all other points $f(x)&gt;1$ and since $f$ is continous on a connected interval, it satisfies the IVP and thus answer to your first question becomes $[1,+\infty)$ .</p>

<p>[Actually, you can consider the graph of $e^{(x-1)^2}$ to be that of $e^{x^2}$ with the origin right-shifted at x = 1.]</p>

<p>For the second question observe that $2x^2 + 5x - 5$ is a polynomial and so it is also continuous and also since the set you're considering the pre-image of is a connected set you just need to solve the quadratic for the boundary points of the inetrval i.e. you just need to solve </p>

<p>$$  2x^2 + 5x - 5 = 0$$ and $$2x^2 + 5x - 5 = 3$$ 
The roots you'll obtain probably, $$x = \frac{-5 \pm \sqrt{65}}{4}$$ for the first one and $$x = \frac{-5 \pm \sqrt{89}}{4}$$ for the second one. Now take suitable union and you'll obtain answer . </p>

<p>[$\frac{-5 + \sqrt{89}}{4}$ will replace 28 in your answer and $\frac{-5 - \sqrt{89}}{4}$ will replace -28]</p>
"
"2378839","2378848","<p>Suppose $\log(x)=\frac{f(x)}{g(x)}$ for polynomials $f, g$.</p>

<p>Because $\log(x)$ isn't bounded for $x\to \infty$, we must have $\deg(f)&gt;\deg(g)$. </p>

<p>But we have
$$\log'(x)=\frac{1}{x}=\frac{f'(x)g(x)-g'(x)f(x)}{g(x)^2}$$
and so</p>

<p>$$g(x)^2=xf'(x)g(x)-xg'(x)f(x)$$</p>

<p>and we get the $2\deg(g)=\deg(xf'(x)g(x)-xg'(x)f(x))$.</p>

<p>But $\deg(xf'(x)g(x)-xg'(x)f(x))\leq \deg(g)+\deg(f)$ and so to get no contradiction with $\deg(f)&gt;\deg(g)$, the leading coefficient of $f'g$ and $g'f$ must be the same. But the leading coefficients are the leading coefficients of $fg$ multiplied with $\deg(f)$ in the first case and $\deg(g)$ in the second. And so we also get a contradiction namely $\deg(f)=\deg(g)$.</p>
"
"2378847","2378854","<p>I think it is only the classical terminology of the theory of differential equations. Instead of speaking of the set of all the solutions of the equation, one speaks of the general solution because for simple equations, the general solution can be described by a formula containing a few arbitrary constants.</p>

<p>In the same way, you could say that a general plane in 3 dimensional space has the equation $a x + b y + c z = d$ where $a, b, c, d$ are real constants. It's easier than speaking about the set of all the planes. </p>
"
"2378857","2378946","<p><strong>Solution 1.</strong> Let $a = \sqrt[p]{n}$ and $b = \sqrt[p]{n+1}$. If $p &gt; 1$ is an integer, then</p>

<p>\begin{align*}
\frac{1}{(n+1)\sqrt[p]{n}}
&amp;= \frac{p}{ab \cdot pb^{p-1}} \\
&amp;&lt; \frac{p}{ab \cdot (b^{p-1} + ab^{p-2} + \cdots + a^{p-1})} \\
&amp;= p \cdot \frac{b - a}{ab (b^p - a^p)} \\
&amp;= p \left( \frac{1}{\sqrt[p]{n}} - \frac{1}{\sqrt[p]{n+1}} \right).
\end{align*}</p>

<p>Summing over $n = 1, 2, \cdots$ gives the desired inequality.</p>

<hr>

<p><strong>Solution 2.</strong> Let $p \geq 1$ be real and write</p>

<p>$$ p \left( \frac{1}{n^{1/p}} - \frac{1}{(n+1)^{1/p}} \right)
= \frac{p}{(n+1)n^{1/p}} \left[ (n+1) - n^{\frac{1}{p}}(n+1)^{1-\frac{1}{p}} \right]. $$</p>

<p>By the Jensen's inequality (or by the AM-GM inequality if $p$ is integer), we have</p>

<p>$$n^{\frac{1}{p}}(n+1)^{1-\frac{1}{p}} \leq \frac{1}{p}\cdot n + \left(1-\frac{1}{p}\right)\cdot(n+1) = n+1 - \frac{1}{p} $$</p>

<p>with the equality exactly when $p = 1$. So it follows that</p>

<p>$$ p \left( \frac{1}{n^{1/p}} - \frac{1}{(n+1)^{1/p}} \right) \geq \frac{1}{(n+1)n^{1/p}} $$</p>

<p>with equality if and only if $p = 1$. Summing over $n = 1, 2, \cdots $ gives the desired inequality.</p>
"
"2378865","2378877","<p>To be fair, they do state ""It is by convention every element is
adjoined with the identity of the group $G$."" However, I agree with the comments that you should close the book and find another one to study. After spending a little time reading this one, it is confusing and poorly written to say the least.</p>
"
"2378866","2379009","<p>Everything you say is correct. You have the parametrization
$${\bf r}(\theta,z)=(\cos\theta,\sin\theta,z)\ ,\tag{1}$$
giving
$$|{\bf r}_\theta\times{\bf r}_z|\equiv1\ ,$$
hence the surface element on $S$ is given by ${\rm d}\omega={\rm d}(\theta,z)$. There is no Jacobian determinant here, but a scaling factor from the measure ${\rm d}(\theta,z)$ in the parameter plane to the surface measure ${\rm d}\omega$ in $3$-space. This scaling factor happens to be $1$ in the case at hand. You end up with the integral of your first attempt, obtained using ""geometric intuition"". Given the parametrization $(1)$ the condition  $0\leq z\leq 1+x y$ translates into $0\leq z\leq1+\cos\theta\sin\theta$.</p>
"
"2378869","2378881","<blockquote>
  <p>Finally:
  $$\begin{aligned}
U = y^2x-kyzx+2k^2yz^2+yz^2+C
\end{aligned}$$</p>
</blockquote>

<p>If you compute the gradient of $U$, you'll find that this isn't equal to the initially given $F$, at least not for all $k$.</p>

<p>There's an easy way to check whether a vector field $F$ is conservative, in which case there exists a potential function $U$ such that $\nabla U = F$; namely $F = (F_1,F_2,F_3)$ is conservative if and only if:
$$\color{red}{\frac{\partial F_2}{\partial x} = \frac{\partial F_1}{\partial y}} \quad \mbox{and} \quad
\color{green}{\frac{\partial F_1}{\partial z} = \frac{\partial F_3}{\partial x}} \quad \mbox{and}
\quad\color{blue}{\frac{\partial F_2}{\partial z} = \frac{\partial F_3}{\partial y}}$$
You can easily verify that the green condition is always satisfied, but the red and blue conditions only hold for $k=-1$. Plug this into $F$ and proceed as you did to determine a potential function $U$. Use the condition $U(0,0,0)=0$ to find the extra constant of integration that will pop up.</p>

<p>Can you take it from here?</p>
"
"2378876","2380478","<p>For a degree lexicographically order replace the weighted<br>
quasiorder in the definition of the weighted lexicographically<br>
order with the degree quasiorder, x &lt;=_d y when deg(x) &lt;= deg(y).</p>
"
"2378879","2378927","<p>Another option is to precede the result with the phrase ""by inspection"", meaning the reader can work out the proof themselves.</p>
"
"2378884","2378896","<p>We can rewrite $x=\lfloor x \rfloor + y$ where $ 0 \leq y &lt; 1$.</p>

<p>The equation becomes $$\lfloor n \lfloor x \rfloor + ny \rfloor = n \lfloor x \rfloor$$</p>

<p>But since the first term is an integer, we can pull it out as such:</p>

<p>$$ n \lfloor x \rfloor + \lfloor ny \rfloor = n \lfloor x \rfloor$$</p>

<p>So we see that the equality holds iff $\lfloor ny \rfloor = 0$. However, this is not always the case - consider $y = \frac{1}{2}, n=2$.</p>
"
"2378891","2378914","<p>A minimal set of axioms for vector spaces is:</p>

<p><img src=""https://i.stack.imgur.com/BBlOk.png"" alt=""enter image description here""></p>

<p>This is discussed and proved in this paper:</p>

<blockquote>
  <p><a href=""http://www.jstor.org/stable/3615171"" rel=""nofollow noreferrer"">Independent Axioms for Vector Spaces</a>,
  J. F. Rigby and James Wiegold.
  <em>The Mathematical Gazette</em>
  Vol. 57, No. 399 (Feb., 1973), pp. 56-62.</p>
</blockquote>
"
"2378894","2379796","<p>Supposing that $q=ax$ and $p=\frac{a(y+z)}{1+ax}$ (I didn't check the simajinid'calculus).
$$q=\frac{\partial z}{\partial y}=ax\quad\to\quad z=axy+\phi(x)$$</p>

<p>$$p=\frac{\partial z}{\partial x}=ay+\phi'(x)=\frac{a(y+axy+\phi(x))}{1+ax}$$
$$\phi'(x)=\frac{a\:\phi(x)}{1+ax}$$
$$\phi(x)=b(1+ax)$$
$$z(x,y)=axy+b(1+ax)$$
Of couse, this is only one particular solution, not the general solution of the PDE.</p>
"
"2378897","2379087","<p>Changing notation from $(x,y,z)$ to $(x_1,x_2,x_3)$, the flow map $\boldsymbol{\varphi}$ maps the coordinates $(x_1,x_2,x_3)$ of a fluid particle at time $t= 0$ onto the coordinates $(\varphi_1, \varphi_2,\varphi_3)$  at time $t$.</p>

<p>The Jacobian determinant $|J|$ can be expressed as</p>

<p>$$|J| = \sum_{(j_1,j_2,j_3)}s(j_1,j_2,j_3) \frac{\partial\varphi_1}{\partial x_{j_1}}\frac{\partial\varphi_2}{\partial x_{j_2}}\frac{\partial\varphi_3}{\partial x_{j_3}},$$</p>

<p>where the sum is taken over all permutations $(j_1,j_2,j_3)$ of $(1,2,3)$ and $s(j_1,j_2,j_3)$ is the sign of the permutation.</p>

<p>Since </p>

<p>$$\frac{\partial}{\partial t}\frac{\partial \varphi_k}{\partial x_{j_k}} = \frac{\partial}{\partial x_{j_k}}\frac{\partial \varphi_k}{\partial t} = \frac{\partial v_k}{\partial x_{j_k}}$$</p>

<p>we have</p>

<p>$$\tag{*}\begin{align}\frac{\partial}{\partial t} |J| &amp;= \sum_{(j_1,j_2,j_3)}s(j_1,j_2,j_3) \frac{\partial v_1}{\partial x_{j_1}}\frac{\partial\varphi_2}{\partial x_{j_2}}\frac{\partial\varphi_3}{\partial x_{j_3}} \\ &amp;+ \sum_{(j_1,j_2,j_3)}s(j_1,j_2,j_3) \frac{\partial \varphi_1}{\partial x_{j_1}}\frac{\partial v_2}{\partial x_{j_2}}\frac{\partial\varphi_3}{\partial x_{j_3}} \\ &amp;+ \sum_{(j_1,j_2,j_3)}s(j_1,j_2,j_3) \frac{\partial \varphi_1}{\partial x_{j_1}}\frac{\partial\varphi_2}{\partial x_{j_2}}\frac{\partial v_3}{\partial x_{j_3}}   \end{align}.$$</p>

<p>By the chain rule,</p>

<p>$$\frac{\partial v_1}{\partial x_{j_1}} = \frac{\partial v_1}{\partial \varphi_1}\frac{\partial \varphi_1}{\partial x_{j_1}} + \frac{\partial v_1}{\partial \varphi_2}\frac{\partial \varphi_2}{\partial x_{j_1}} + \frac{\partial v_3}{\partial \varphi_1}\frac{\partial \varphi_3}{\partial x_{j_1}}. $$</p>

<p>Applying this to (*) the first sum on the RHS becomes</p>

<p>$$\begin{align}\sum_{(j_1,j_2,j_3)}s(j_1,j_2,j_3) \frac{\partial v_1}{\partial x_{j_1}}\frac{\partial\varphi_2}{\partial x_{j_2}}\frac{\partial\varphi_3}{\partial x_{j_3}} &amp;= \frac{\partial v_1}{\partial \varphi_1}\sum_{(j_1,j_2,j_3)}s(j_1,j_2,j_3) \frac{\partial \varphi_1}{\partial x_{j_1}}\frac{\partial\varphi_2}{\partial x_{j_2}}\frac{\partial\varphi_3}{\partial x_{j_3}} \\ &amp;+ \frac{\partial v_2}{\partial \varphi_1}\sum_{(j_1,j_2,j_3)}s(j_1,j_2,j_3) \frac{\partial \varphi_2}{\partial x_{j_1}}\frac{\partial\varphi_2}{\partial x_{j_2}}\frac{\partial\varphi_3}{\partial x_{j_3}}  \\ &amp;+  \frac{\partial v_3}{\partial \varphi_1}\sum_{(j_1,j_2,j_3)}s(j_1,j_2,j_3) \frac{\partial \varphi_3}{\partial x_{j_1}}\frac{\partial\varphi_2}{\partial x_{j_2}}\frac{\partial\varphi_3}{\partial x_{j_3}} \\ \end{align} $$</p>

<p>The second and third sums on the RHS of (**) are determinants with identical rows and must vanish.</p>

<p>Hence,</p>

<p>$$\sum_{(j_1,j_2,j_3)}s(j_1,j_2,j_3) \frac{\partial v_1}{\partial x_{j_1}}\frac{\partial\varphi_2}{\partial x_{j_2}}\frac{\partial\varphi_3}{\partial x_{j_3}} =   \frac{\partial v_1}{\partial \varphi_1}\sum_{(j_1,j_2,j_3)}s(j_1,j_2,j_3) \frac{\partial \varphi_1}{\partial x_{j_1}}\frac{\partial\varphi_2}{\partial x_{j_2}}\frac{\partial\varphi_3}{\partial x_{j_3}} =   \frac{\partial v_1}{\partial \varphi_1} |J|$$</p>

<p>Applying the chain rule, similarly, to the second and third sums on the RHS of (*) and adding we obtain</p>

<p>$$\frac{\partial}{\partial t} |J|= \left(\frac{\partial v_1}{\partial \varphi_1} + \frac{\partial v_2}{\partial \varphi_2} + \frac{\partial v_3}{\partial \varphi_3} \right)|J|  = \operatorname{div}(\mathbf{v}) |J|$$</p>
"
"2378900","2379092","<p>In <a href=""https://math.stackexchange.com/a/2377696"">this answer</a>, it is shown that
$$
\frac1{\Gamma(1+z)}=e^{z\gamma}\prod_{k=1}^\infty\left(1+\frac{z}{k}\right)e^{-\frac{z}{k}}
$$
Therefore, we have
$$
z!=\Gamma(1+z)=e^{-z\gamma}\prod_{k=1}^\infty e^{\frac{z}{k}}\left(1+\frac{z}{k}\right)^{-1}
$$
where $\gamma$ is the <a href=""https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant"" rel=""nofollow noreferrer"">Euler-Mascheroni Constant</a>.</p>
"
"2378903","2379206","<p>It's hard to answer such questions when we aren't given any context.  I have several answers in mind, but they each depend on what you already know.  Just taking a stab:  The function $m=-r(r-4)= -r^2+4r$ is quadratic and its graph is a parabola.  Since the leading coefficient is negative, we know it's parabola with arms pointing down.  So the maximum of the function is at the vertex of the parabola.  The usual way to find the vertex is by completing the square, as was done in your example.  </p>

<p>Other ways are by taking the derivative $\frac{dm}{dr} = -2r+4$ and setting it equal to zero.  When you solve, you get $r=2$, which you can plug back into the function to get $m=4.$</p>
"
"2378912","2378924","<p>The gradient of the surface is given by $\nabla = \langle -e^{-x}siny, e^{-x}cosy \rangle $. Evaluating this gradient at $x = 0, y = \frac{\pi}{2}$, you find $\langle (-e^{0})sin(\frac{\pi}{2}), e^{0}cos(\frac{\pi}{2}) \rangle = \langle -1, 0 \rangle$. Therefore the slope of $x = -1, y = 0$. The value of $z$ at this surface is $1$ so you get $z = -x + 1$ or $z + x = 1$ as your tangent plane. </p>
"
"2378918","2379124","<p>Let's define the following: $$ d_N = \det\left(\prod_{k=1}^N A(k)\right) = \prod_{k=1}^N \det(A(k))=\prod_{k=1}^N a_{11}(k)a_{22}(k)-a_{21}(k)a_{12}(k) $$ where $a_{ij}(k)\sim\mathcal{N}(0,\sigma^2)$.
So then the expected value is given by: $$ \mathbb{E}[d_N]=0=:\mu_N $$
Next, define $$ \hat{S}_1(k) = \hat{a}_{11}(k)\hat{a}_{22}(k),\;\;\hat{S}_2(k) = \hat{a}_{21}(k)\hat{a}_{21}(k) $$
where $\hat{a}_{ij}(k)\sim\mathcal{N}(0,1)$.
So, the variance is written: 
\begin{align} 
\mathbb{V}[d_N] &amp;= \mathbb{E}[d_N^2]= \mathbb{E}\left[ \prod_{k=1}^N (a_{11}(k)a_{22}(k)-a_{21}(k)a_{12}(k))^2 \right]\\
&amp;=\prod_{k=1}^N \mathbb{E}\left[ (a_{11}(k)a_{22}(k)-a_{21}(k)a_{12}(k))^2 \right]\\
&amp;= \prod_{k=1}^N \mathbb{E}\left[ (\sigma^2\hat{S}_1(k)-\sigma^2\hat{S}_1(k))^2 \right]\\ &amp;= \sigma^{4N}\prod_{k=1}^N \mathbb{E}\left[ (\hat{S}_1(k)-\hat{S}_1(k))^2 \right]
\end{align}
Now for some random variable algebra. First, note that the $\hat{S}_i$ are <a href=""https://en.wikipedia.org/wiki/Chi-squared_distribution"" rel=""nofollow noreferrer"">chi-squared</a> because they are a product of independent normal RVs (e.g. <a href=""https://math.stackexchange.com/questions/161757/what-is-the-distribution-of-a-random-variable-that-is-the-product-of-the-two-nor?noredirect=1&amp;lq=1"">here</a>): $$ \hat{S}_1(k), \hat{S}_2(k)\sim\chi^2_1 $$<br>
Next, define $\mathfrak{D}_k= \hat{S}_1(k) - \hat{S}_2(k)$. The difference between two chi-squared variables (e.g. <a href=""https://math.stackexchange.com/questions/85249/distribution-of-difference-of-chi-squared-variables"">here</a>, or <a href=""http://www.math.kit.edu/stoch/~klar/seite/veroeffentlichungen/media/note-vg-revision.pdf"" rel=""nofollow noreferrer""><em>A note on gamma difference distributions</em> by
Bernhard Klar</a>) follow a <a href=""https://en.wikipedia.org/wiki/Variance-gamma_distribution"" rel=""nofollow noreferrer"">variance-gamma</a> (generalized Laplace) distribution:
$$ \mathfrak{D}_k\sim\Gamma_\mathcal{V}(\mu,\alpha,\beta,\lambda,\gamma)=\Gamma_\mathcal{V}\left(0,\frac{1}{2},0,\frac{1}{2},\frac{1}{2}\right) $$
This also tells us that:
\begin{align}
\mathbb{E}[\mathfrak{D}_k] &amp;= \mu + \frac{2\beta\lambda}{\gamma^2} = 0 \\ 
\mathbb{V}[\mathfrak{D}_k] &amp;= \frac{2\lambda}{\gamma^2}\left( 1 + \frac{2\beta^2}{\gamma^2} \right) = 4
\end{align}
So now we can complete the computation:
$$
\mathbb{V}[d_N]=\sigma^{4N}\prod_{k=1}^N \mathbb{E}\left[ \mathfrak{D}_k^2 \right]
= \sigma^{4N}\prod_{k=1}^N (\mathbb{V}[\mathfrak{D}_k] + \mathbb{E}[\mathfrak{D}_k]^2) = 4^N\sigma^{4N} =: \varsigma^2_N
$$ 
Hopefully I didn't make any arithmetic mistakes.
Anyway, woah, so that is potentially a very large variance. Clearly it depends heavily on the value of $\sigma$. But anyway, ignoring the limit, we can bound our target using the <a href=""https://en.wikipedia.org/wiki/Chebyshev%27s_inequality"" rel=""nofollow noreferrer"">Chebyshev inequality</a>:
$$
P(|d_N-\mu_N|\geq \varsigma_N\kappa) \leq \frac{1}{\kappa^2}\;\;\;\implies\;\;\;
P(|d_N|\geq 2^N\sigma^{2N}\kappa) \leq \frac{1}{\kappa^2}$$
Maybe there is a better <a href=""https://en.wikipedia.org/wiki/Concentration_inequality"" rel=""nofollow noreferrer"">concentration inequality</a>. </p>

<p>But, if we denote $\sigma = \hat{\sigma}/\sqrt{2} $, then at least what this tells us is that: if $\hat{\sigma}&lt;1$, then the probability that $d_N$ is non-zero is essentially zero.</p>
"
"2378922","2378965","<p>Since $k$ is odd $m-1$ is not multiple of $3$. This is because it is a sum of multiples of $3$ and a $2^{k}=2\cdot 4^{(k-1)/2}=2\mod{3}$.</p>

<p>Now, $m^2-3n^2=1$. So,</p>

<p>$$(m-1)(m+1)=3n^2$$</p>

<p>Since $m-1$ is not divisible by $3$, the $3$ divides the factor $m+1$. </p>

<p>The only common divisor of $m-1$ and $m+1$ can be $2=(m+1)-(m-1)$. But $m$ is even. Therefore, $m-1$ and $m+1$ are relatively prime.</p>

<p>So, $m-1$ is a square.</p>
"
"2378930","2378990","<p>For $p = 1$, $F(t) = t$ works. Hence we assume $0 &lt; p &lt; 1$ in the following.</p>

<p>If $X$ has subsets of arbitrarily small positive measure, then we can find a sequence with $\int_X f_n = 1$ for all $n$, but $\int_X f_n^p \to 0$ and hence such an $F$ cannot exist. For if $\mu(A_n) &gt; \mu(A_{n+1}) \to 0$, let</p>

<p>$$f_n = \frac{n-1}{n\mu(A_n)} \chi_{A_n} + \frac{1}{n\mu(X\setminus A_n)} \chi_{X\setminus A_n}.$$</p>

<p>Then $\int_X f_n = 1$, and</p>

<p>$$\int_X f_n^p = \biggl(\frac{n-1}{n}\biggr)^p \cdot \mu(A_n)^{1-p} + \frac{\mu(X\setminus A_n)^{1-p}}{n^p} \leqslant \mu(A_n)^{1-p} + \frac{\mu(X)^{1-p}}{n^p} \to 0.$$</p>

<p>So we need a purely atomic measure space whose atoms have measure bounded below by a strictly positive constant. That means there are only finitely many atoms if $X$ shall have finite measure. And in that case, we can find such an $F$. Let $c$ be the smallest measure of an atom. Then $F(t) = \min \: \{t,c\}$ satisfies the inequality. If $f \leqslant 1$, then we have</p>

<p>$$\int_X f^p \geqslant \int_X f \geqslant F\biggl(\int_X f\biggr).$$</p>

<p>And if $f(x) \geqslant 1$ on some atom, then</p>

<p>$$\int_X f^p \geqslant c \geqslant F\biggl(\int_X f\biggr).$$</p>
"
"2378939","2378969","<p>For practical purposes, it means that the solution can be <em>estimated in terms of the data</em>. Consider the PDE
$$
u_t = Pu + F(x,t),
$$
where $P$ is some differential operator and $F$ is a forcing function, and where the problem is defined in some domain $\Omega$. Say that we additionally have the initial data
$$
u(x,0) = f(x),
$$
and that
$$
Lu = g(x,t),
$$
should be satisfied at the boundary $\Gamma$ of the domain $\Omega$. Here, $f$ and $g$ are known functions and $L$ is some boundary operator. Then, the problem is said to be (strongly) well posed if an estimate can be obtained along the lines of
$$
\| u(\cdot,t) \|^2 \leq K \left\{ \|f\|^2 + \int_0^t \|F\|^2 \text{d} \tau + \int_0^t \|g\|_\Gamma^2 \right\},
$$
where $K$ is a constant independent of $F$, $f$ and $g$.</p>

<p>Such an estimate tells us that the solution cannot be ""arbitrarily erratic"", e.g. it cannot run away to infinity in any finite time, unless the data already does this.</p>

<p>Sometimes other estimates than the one above are used, usually by removing one or both of the integral terms in the right-hand side. The type of estimate needed is largely problem dependent. The problem at hand will in general also determine the norms used in the estimate.</p>
"
"2378940","2378978","<p>If I get this right, we replicate procedure three times and want to know probability of obtaining precisely one ruby during the second draw.</p>

<p>We can avoid a ruby on the second draw with probability $$0.6\cdot 0.2 + 0.4\cdot 0.6 = 0.36$$
We can obtain a ruby on second draw with probability (following computation really unnecessary)$$0.6\cdot 0.8 + 0.4\cdot 0.4 = 0.64 $$
There are three ways this can happen so your solution must be correct.</p>
"
"2378953","2378955","<p>The statement is true for cardinality reasons. Any interval (not allowing one-point sets) contains a rational number, but there are only countably many rational numbers.</p>
"
"2378961","2378970","<p>Let $g(x)=f(a)+f'(x)(x-a)$. Then
$$f(x)-g(x)= f(x)-f(a)-f'(x)(x-a)= (x-a) \left(\frac{f(x)-f(a)}{x-a}-f'(x) \right)$$</p>

<p>If $L(x)$ is the tangent approximation, then 
$$f(x)-L(x)= (x-a) \left(\frac{f(x)-f(a)}{x-a}-f'(a) \right)$$</p>

<p>Note that as long as $f'(x)$ is continuous at $x=a$, everything works nicely, but as uniquesolution pointed, this might not work nicely when $f'$ is not continuous at $x=a$. In that case, your approximation is clearly worse than the tangent approximation.</p>

<p><strong>Note</strong> If you want to compare the two approximations, to decide which is better, the question simply becomes ""Does $\frac{f(x)-f(a)}{x-a}$ approximate better $f'(x)$ or $f'(a)$?""</p>

<p>Geometrically the question you ask is ""If $x$ is close to $a$, is the slope of the secant $(x,f(x))  - (a,f(a))$ closer to the tangent to the graph at $(x,f(x))$ or the tangent at $(a,f(a))$?""</p>

<p>I don't think that one should expect the answer to be independent of the choice of the function.</p>
"
"2378979","2378981","<p>If there are three integer solutions, they add to $0$ and so are $a$,
$b$ and $c=-a-b$. Then
$$-24=ab+ac+bc=ab-(a+b)^2=-a^2-ab-b^2.$$
You need to prove that $24$ is not represented over the integers
by the quadratic form $a^2+ab+b^2$. Maybe you could complete the square...</p>
"
"2378984","2379116","<p>Given the restrictions on $x$, $y$, and $z$ the given inequality is equivalent with
$${z\over y}&gt;{\log(1+x)\over\log{1\over 1-x}}\ .\tag{1}$$
Since ${1\over1-x}&gt;1+x$ when $0&lt;x&lt;1$ the right hand side of $(1)$ is $&lt;1$ when $0&lt;x&lt;1$. Therefore $z\geq y$ is sufficient, but <strong>not</strong> necessary for the given inequality to hold. Plotting the right hand side of $(1)$ as a function of $x$ one sees that, e.g., the condition
$${z\over y}\geq1-0.793 x$$
is also sufficient.</p>
"
"2378991","2379059","<p>Yes, the proof looks fine to me.</p>
"
"2378998","2379027","<p>As I pointed out in the comments, the main flaw of the solution is poor exposition, which makes it difficult to find the finer, intended error. A better version of this puzzle would be:</p>

<p>Let $\theta = \arcsin \frac{4}{5}$ so that $\sin \theta = \frac{4}{5}$. Then also $\sin( \pi - \theta ) = \frac{4}{5}$, thus $\pi - \theta = \arcsin \frac{4}{5}$. Matching it together, we get $\theta = \pi - \theta$ so $\theta = \frac{\pi}{2}$, therefore $\frac{4}{5} = \sin \frac{\pi}{2} = 1$.</p>

<p>The obvious fallacy is drawing the conclusion that $\pi - \theta = \arcsin \frac{4}{5}$ from the equation $\sin( \pi - \theta ) = \frac{4}{5}$, since $\arcsin$ is only a partial inverse of $\sin$. </p>

<hr>

<p>One could even argue I just made up the puzzle above myself, because there is so little similarity to the original puzzle in the structure of the proof. Well, I wouldn't deny. </p>

<p>But maybe the mess in the puzzle was intended to make it more obscure...</p>
"
"2379004","2379186","<p>Making my comment an answer, so this question no longer counts as unanswered. </p>

<p>Your parametrization is wrong, it should be: $$\vec{r}(t) = \Big(\dfrac{1}{\sqrt{2}}\tan t,\, \sec t\Big)$$</p>
"
"2379010","2379033","<p>The floor function doesn't have an inverse, since it's not bijective, as Dando18 pointed out, so you won't be able to invert it directly. At best, you'll be able to get an inequality for $I$, since you can use the fact that $\lfloor x \rfloor \le x$ - so $S \le\lfloor (2*B+I+E)*L/100 + 5\rfloor * N$. If any of your variables can be negative, then be careful with the direction of your inequality.</p>

<blockquote class=""spoiler"">
  <p> I get $I \ge (\frac{S}{N} - 5)*\frac{100}{L} - (2B+E)$, provided $L&gt;0$, and you already state $N \in \{0.9,1,1.1\}$. If $L&lt;0$, reverse the inequality. If $L=0$, you can't make $I$ the subject since the whole expression reduces to $S=\lfloor 5N \rfloor$.</p>
</blockquote>
"
"2379020","2379344","<p>You can write your expectation as a nested integral.  On the outside, expectation with respect to $c$. On the inside, expectation with respect to $a$.  You can re-express the number $|\langle a,c\rangle|^2$ as the matrix product $c' a a' c$, for which the expectation over $a$ works out to $c' c$.  You can (I hope) finish the job without trouble.</p>
"
"2379021","2379229","<p>There are no mistakes in 1 (nice job!).</p>

<p>Under the convention that $\binom{x}{y}=0$ if $x&lt;y$ define: $$f_y(r):=\sum_{x\in\mathbb Z}\binom{x}{y}r^x$$</p>

<p>Then we have:$$f_{y+1}(r)+f_y(r)=\sum_{x\in\mathbb Z}\binom{x}{y+1}r^x+\sum_{x\in\mathbb Z}\binom{x}{y}r^x=$$$$\sum_{x\in\mathbb Z}\binom{x+1}{y+1}r^x=r\sum_{x\in\mathbb Z}\binom{x}{y+1}r^x=rf_{y+1}(r)$$</p>

<p>So that: $$f_{y+1}(r)=\frac{r}{1-r}f_{y}(r)$$</p>

<p>Combined with $f_0(r)=\frac{1}{1-r}$ that leads to:$$f_y(r)=\frac{r^y}{(1-r)^{y+1}}$$</p>

<p>Writing $q$ for $1-p$ we then find:$$p_Y(y)=\left(\frac{p}{q}\right)^{y+1}f_y(q^2)$$
The working out of this I will leave up to you.</p>
"
"2379025","2379026","<p>You are correct for the compact set and for non compact set is not true for example $[0,1[$ is bounded, non compact and it does not contain his upper bound 1</p>
"
"2379028","2379040","<p>Using Weierstrass' test, from your previous estimate you see that the series is totally (hence uniformly) convergent on any bounded interval $[-a,a]$.
Since it is the sum of continous functions, the sum $f$ is then continuous on every interval $[-a,a]$, hence on all $\mathbb{R}$.</p>

<p>(To prove the continuity of $f$ on a given point $x_0\in\mathbb{R}$ it is enough to pick $a &gt; |x_0|$.)</p>
"
"2379042","2379062","<p>No, you haven't shown what was asked. You were asked to show that there exists $\lambda \in \mathbb{F}$ such that <strong>for all</strong> $v \in V$ we have $Tv = \lambda v$ (note the order of quantifiers). In particular, $\lambda$ shouldn't depend on $v$.</p>

<p>You have shown instead that for every $v \in V$ there exists $\beta \in \mathbb{F}$ (which possibly depends on $v$) such that $Tv = \beta w$. This is not what was asked. Note that your proof doesn't use the linearity of $T$ and without the linearity of $T$, the result is not true.</p>
"
"2379047","2379082","<p>Suppose that for some $c\in U$, 
$$
\|Df(c)\|=2+\varepsilon&gt;0.
$$
Then, there exists a unit vector $\xi\in\mathbb R^n$, (i.e., $\|\xi\|=1$),
such that
$$
\|Df(c)\xi\|=2+\varepsilon.
$$
However,
$$
Df(c)\xi=\left.\frac{d}{dt}\right|_{t=0}f(c+t\xi)=\lim_{t\to 0}\frac{f(c+t\xi)-f(c)}{t},
$$
and thus
$$
2+\varepsilon=\|Df(c)\xi\|=\lim_{t\to 0}\frac{1}{t}\|\,f(c+t\xi)-f(c)\|.
$$
Hence, for some $\delta&gt;0$, we have that 
$$
|t|&lt;\delta\quad\Longrightarrow\quad
\frac{1}{t}\|\,f(c+t\xi)-f(c)\|&gt;2+\frac{\varepsilon}{2}\quad\Longrightarrow\quad
\|\,f(c+t\xi)-f(c)\|&gt;\left(2+\frac{\varepsilon}{2}\right)\|t\xi\|.
$$</p>
"
"2379053","2379067","<p>The term <em>product</em> in the question probably means <em>cartesian product</em>. Since $\mathbb{Z}$ is the only infinite cyclic group up to isomorphism, the question reduces to:</p>

<blockquote>
  <p>Prove that $\mathbb{Z} \times \mathbb{Z}$ is not cyclic.</p>
</blockquote>

<p>Here is a roadmap:</p>

<ul>
<li><p>The image of a cyclic group under a homomorphism is cyclic</p></li>
<li><p>$C_2 \times C_2$ is a homomorphic image of $\mathbb{Z} \times \mathbb{Z}$</p></li>
<li><p>$C_2 \times C_2$ is not cyclic</p></li>
</ul>

<p>Here, $C_2$ is the cyclic group of order $2$.</p>
"
"2379060","2379073","<p>As $f$ is an odd function, therefore by definition $$f(-x)=-f(x)$$ for all $x.$ 
In particular, taking $x=0,$ we get $$f(-0)=-f(0)\\f(0)=0$$</p>

<p>Let the period of $f$ be $c$.</p>

<p>Then $$f(x+c)=f(x)$$ for all $x.$ In particular, $$f(0+c)=f(0)\\f(c)=0$$
Also, $$f(c+c)=f(c)\\f(2c)=0$$
Proceeding this way $$f(nc)=0$$ for all $n \in \mathbb Z.$</p>

<p>Thus, $f$ cuts the $x-$axis infinitely often.</p>
"
"2379072","2379420","<p>a) Looks good, although I would say that ""since all characteristic curves of the equation approach $(0,0)$, the value of $u$ on each of those curves is $u(0,0)$ by the continuity of $u$"", without talking about derivatives blowing up.</p>

<blockquote>
  <p>c) Show that there is a non-constant polynomial solution defined in some open set. </p>
</blockquote>

<p>This can't be true because if a polynomial solves that PDE in an open set, it solves it everywhere. Indeed, if $u$ is a polynomial, then so is $y\partial_x u + (-3y-2x)\partial_yu $. And if a polynomial vanishes on an open set, it vanishes identically. </p>

<p>Perhaps part c) should be ""Show that there is <strong>NO</strong> non-constant polynomial solution defined in an open set"" </p>
"
"2379089","2379097","<p>Your answer is correct, whoever wrote the answers made the mistake.</p>
"
"2379091","2379098","<p>Your argument is fine. </p>

<p>To prove the continuity of $F$, separately continuous would be enough if $f$ is bounded by an integrable function.</p>
"
"2379102","2379120","<p>Firstly, the operations of addition and multiplication are just binary operators, that is an operator say, $\#$, that works on any two elements out of a mathematical <a href=""https://en.wikipedia.org/wiki/Set_theory"" rel=""nofollow noreferrer"">set</a>, $S$. </p>

<p>We can say that those two elements $a,b \in S$, are in the set and can be operated on $a\; \# \;b$ to get another $c \in S$. This property, of taking a binary operation on two elements of a set to get another element of a set is the fundamental axiom of what we call a <a href=""https://en.wikipedia.org/wiki/Group_theory"" rel=""nofollow noreferrer""> group</a>. There are infact other properties that $\#$ must have to qualify for a group, notably there must be some $e \in S$ that $a\; \# \;e = e \; \# \; a = a$, and inverses that exist. However, you can look into that more on your own.</p>

<p>Back to your original question, addition and multiplication as you describe them, are simple two binary operations that have come to mean a whole lot to the world as a whole as they generally do a  good job at combining numbers we observe in every day life.</p>

<p>The property you describe as ""ease"" of multiplication I assume is in large part due to its distributivity, as both multiplication and addition are communities (the order of the elements being operated on does not matter) and associate (the order of successive calls of the operator does not matter). Which I should note, these properties, especially commutativity are rare and special to come by!</p>

<p>As multiplication is in fact a shorthand to express addition, distributivity can be ""proved"" as follows:</p>

<p>$$(a+b)*(c) = \sum^{c}_{1}{a+b} = \sum^{c}_{1}{a} + \sum^{c}_{1}{c} = a*c + b*c. $$</p>

<p>I also quite like this <a href=""http://www.quickanddirtytips.com/sites/default/files/styles/insert_large/public/images/2170/MD043-fig2.png?itok=03yGdCeI"" rel=""nofollow noreferrer""> picture</a></p>

<p>This property is in large part why it is so much easier to deal with values as you describe in physics, where simplification in large part is due to the grouping and solving of sub expressions that can be most readily extracted out using distributivity. </p>

<p>I also suggest you ponder over further opertators that are extensions atop multiplication themselves! Such as exponent, $2^3 = 2*2*2$, or even the awesome <a href=""https://en.wikipedia.org/wiki/Knuth%27s_up-arrow_notation"" rel=""nofollow noreferrer"">arrow notation</a>, $2 \uparrow \uparrow 3 = 2^{2^2} $. </p>

<p>It is a simple question, but one at the heart of <a href=""https://en.wikipedia.org/wiki/Abstract_algebra"" rel=""nofollow noreferrer"">abstract algebra</a> and <a href=""https://en.wikipedia.org/wiki/Number_theory"" rel=""nofollow noreferrer"">number theory</a>, two topics that would appear to interest you. Rings, groups, fields, and on! I hope this helps.</p>
"
"2379107","2379279","<p>The assertion is wrong (or maybe you misunderstood what they tried to do ...)</p>

<p>The negation of $\lnot \exists B. \forall n. P$ is of course just $ \exists B. \forall n. P$, and <em>not</em> $ \exists B. \lnot \forall n. P$</p>

<p>In general, we have:</p>

<p><strong>Quantifier Negation</strong></p>

<p>Where $\varphi$ is any formula:</p>

<p>$\neg \exists x \varphi \Leftrightarrow \forall x \neg \varphi$</p>

<p>$\neg \forall x \varphi \Leftrightarrow \exists x \neg \varphi$</p>

<p>...which makes intuitive sense: </p>

<p>There isn't something with property $\varphi$, if and only if everything lacks the property $\varphi$.  </p>

<p>And, not everything has property $\varphi$, if and only if there is something that lacks property $\varphi$ </p>

<p>You can also make sense of these equivalences like this:</p>

<p>An existential can be seen as kind of disjunction, that is, if $a,b,c,...$ denote the objects in your domain, then you can think of an existential like this:</p>

<p>$\exists x \: \varphi(x) \approx \varphi(a) \lor \varphi(b) \lor \varphi(c) \lor ...$ </p>

<p>Likewise, a universal is a kind of conjunction:</p>

<p>$\forall x \: \varphi(x) \approx \varphi(a) \land \varphi(b) \land \varphi(c) \land ...$ </p>

<p>So:</p>

<p>$$\neg \exists x \: \varphi(x) \approx $$</p>

<p>$$\neg (\varphi(a) \lor \varphi(b) \lor \varphi(c) \lor ...) \Leftrightarrow \text{ (DeMorgan)}$$</p>

<p>$$\neg \varphi(a) \land \neg \varphi(b) \land \neg \varphi(c) \land ...) \approx$$</p>

<p>$$\forall x \neg \varphi$$</p>

<p>and:</p>

<p>$$\neg \forall x \: \varphi(x) \approx $$</p>

<p>$$\neg (\varphi(a) \land \varphi(b) \land \varphi(c) \land ...) \Leftrightarrow \text{ (DeMorgan)}$$</p>

<p>$$\neg \varphi(a) \lor \neg \varphi(b) \lor \neg \varphi(c) \lor ...) \approx$$</p>

<p>$$\exists x \neg \varphi$$
(... which is why these equivalences are also known as the 'DeMorgan's Laws for quantifiers)</p>
"
"2379109","2379653","<p>If a group $G$ is cyclic, then the following assertion is clearly satisfied:
$$\forall x,y \in G \backslash \{1 \}, \ \exists n,m \in \mathbb{Z} \backslash \{0 \}, \ x^n=y^m.$$
Thus, in order to prove that the product of two infinite cyclic groups (ie., $\mathbb{Z} \times \mathbb{Z}$) is not cyclic, it is sufficient to notice that
$$n(1,0)=(n,0) \neq (0,m) = m(0,1)$$
for every $n,m \in \mathbb{Z} \backslash \{ 0 \}$. </p>
"
"2379110","2379158","<p>Here is a more elementary approach than just quoting Dirichlet.</p>

<p>Suppose $P_2\gt P_1$ and $P_2-P_1=d$. If $P_3=P_1+k$ we want $P_3+d=P_2+k$ not to be a prime. </p>

<p>So given $d$ we need to find a prime $P_3$ with $P_3+d$ not a prime.</p>

<p>Now we know that there are arbitrarily large gaps between primes e.g. $n!+2, \dots n!+n$ gives a gap of size at least $n-1$. (all these are composite by construction, so the prime before and the prime after will have a big gap)</p>

<p>We can choose $n\gt d+1$ here, so there will be a $P_3$ distance greater than $d$ from the next prime, and we can choose $k$ to pick this out.</p>
"
"2379111","2379299","<p>In finite dimensions, an absorbing convex set $A$ contains a ball centered at $0$, which can be proved by a compactness argument (involving a convergent subsequence of unit vectors). </p>

<p>By convexity, $rA = A+(r-1)A$  (Minkowski sum) for any $r&gt;1$. When $r$ is large enough, the set $(r-1)A$ contains a ball centered at $0$ with radius greater than $d(A,B)$. Hence $B\subset A+(r-1)A = rA$, which provides an affirmative answer to your question. </p>

<hr>

<p>In infinite dimensional spaces the result is false. Consider the space $c_{00}$ of sequences with finitely many nonzero elements. Equip it with the $\ell^2$ norm. Let $$A=\{x\in \ell^2:\|x\|_2\le 1\},\quad B=\{x\in \ell^2:\|x\|_1 \le 1\}$$
Then $A$ is bounded and $B\subset A$, so $d_H(A,B)$ is finite. Both sets are absorbing in $c_{00}$. Yet, there is no $r$ such that $A\subset rB$, because of vectors like
$$
x = \frac{1}{\sqrt{n}}(e_1+\dots+e_n)
$$
which satisfy $\|x\|_2 = 1$ and $\|x\|_1=\sqrt{n}$.</p>
"
"2379112","2379142","<p>The issue is when you took the derivative of $$\sqrt[\rho]\frac{\cosh \rho}{\cosh z}$$ Yor mistake is that $\frac{\partial}{\partial z}\frac{\cosh\rho}{\cosh z}\ne \sinh z$ but instead $$\frac{\partial}{\partial z}\frac{\cosh\rho}{\cosh z}=-\frac{\sinh z \cosh\rho}{\cosh^2 z}$$</p>
"
"2379129","2379237","<p>$d_{GH}(X, Y)$ is computed by looking at <em>all possible</em> ""ambient spaces"" $Z$. The phrasings one often sees, like</p>

<blockquote>
  <p>The infimum over all embeddings of $X, Y$ into a common space $Z$,</p>
</blockquote>

<p>can make it sound like there is a ""special"" $Z$. But rather, we're looking at all pairs of embeddings into all (possible) spaces.</p>

<p>EDIT: Specifically, here's a clearer way to define $d_{GH}(X, Y)$.</p>

<blockquote>
  <p>$d_{GH}(X, Y)$ is the infimum of the set of $\delta$ such that, for some metric space $Z$ and some embeddings $f, g: X, Y\rightarrow Z$, we have $d_{H}^Z(f(X), g(Y))=\delta$ (where ""$d_H^Z$"" denotes the usual Hausdorff distance in $Z$).</p>
</blockquote>

<p>In particular, we look at <em>all possible embeddings</em> of $X$ and $Y$ into <em>all possible spaces</em>. There is no single $Z$ we're looking at!</p>

<hr>

<p>Incidentally, it's not hard to prove - and fairly useful - that we can restrict attention to those $Z$s whose underlying set is $X\sqcup Y$ and whose metric restricted to $X$ (resp. $Y$) is just the original $X$-metric (resp. $Y$-metric).</p>

<p>Again, that is:</p>

<blockquote>
  <p><strong>Theorem</strong>. $d_{GH}(X, Y)$ is equal to the infimum of the set of $\epsilon$ such that, for some metric space $Z$ with underlying set $X\sqcup Y$ whose metric on the $X$ part (resp., $Y$ part) is the same as the $X$ metric (resp., $Y$ metric), we have $d_H^Z(X, Y)=\epsilon$.</p>
</blockquote>

<p>Note that this is made much clearer if we distinguish between the <em>set</em> $A$ and the <em>metric space</em> $\mathcal{A}=(A,d_A)$. Namely, we have:</p>

<blockquote>
  <p><strong>Theorem, rephrased</strong>. $d_{GH}((X, d_X),(Y, d_Y))$ is equal to the infimum of the set of $\epsilon$ such that, for some metric $d$ on $X\sqcup Y$ with $d\upharpoonright X=d_X$ and $d\upharpoonright Y=d_Y$, we have $\epsilon=d_H^{(X\sqcup Y, d)}(X, Y)$.</p>
</blockquote>

<p>Again, though, we're looking at <em>all possible metrics</em> on $X\sqcup Y$ which restrict to $d_X$ and $d_Y$ on $X$ and $Y$, respectively. </p>
"
"2379131","2379152","<p>This reasoning is not correct. If you want to use completeness, you would need that your sequence is Cauchy in $L^2$ (which it isn't). Furthermore, as you already mentioned yourself, the delta function is not a function (it is a distribution) and is therefore not in $L^2$.</p>
"
"2379136","2379140","<p>1 looks correct (you got the right idea, at least ... the wording and execution can be improved ... e.g. show that with 12 rolls you are still below $0.9$) </p>

<p>2 is correct ... </p>

<p>3 looks incorrect (you need to use a conditional probability here) ... </p>

<p>and 4 is certainly incorrect:</p>

<p>For 4: </p>

<p>$A$:storm</p>

<p>$B$: project completed</p>

<p>$$P(B) = P(B|A)\cdot P(A) + P(B|A^C)\cdot P(A^C)= 0.35\cdot 0.6+0.85\cdot0.4=0.55$$</p>

<p>For 3:  Think back to problem 1: what is the chance $P(\ge 1)$ of getting at least one 3 in 4 rolls?  Now calculate the chance $P(2)$ of getting exactly two 3's, and divide that by $p$ to get your answer, since:</p>

<p>$$P(2)= P(2 \cap \ge 1)= P(2|\ge 1)\cdot P(\ge 1)$$</p>

<p>and so:</p>

<p>$$P(2|\ge 1)=\frac{P(2)}{P(\ge 1)}$$</p>
"
"2379137","2379170","<p>For the last, $O(\cdot)$ question: you want to <a href=""https://en.wikipedia.org/wiki/Big_O_notation#Properties"" rel=""nofollow noreferrer"">read up on Landau and big-Oh notations.</a></p>

<hr>

<p>Your mistake starts at 5, but it's so far a misunderstanding of $O(\cdot)$ vs. $o(\cdot)$. Note that $\ln(1+u) = u+o(u)$ (<em>little</em> $o(\cdot)$, not ""big"" $O(\cdot)$: they have different meanings!).</p>

<p>Now, your <em>actual</em> mistake is at 6, when you replace $(1-t)^{2-t}$ by $1$. <em>You cannot do that</em>: the low-order terms of this quantity <strong>will</strong> matter, as we will see below. More generally, when confronted to something like that, I encourage you to rewrite it in the <em>exponential form</em>
$$
(1-t)^{2-t} = e^{(2-t)\ln(1-t)}
$$
and then apply the Taylor expansions of $\ln(1+u)$ and $e^u$ around $0$, in that order. (Doing so, and we will do so below, you would get $(1-t)^{2-t} = 1-2t + o(t)$, and the second term will be important: this basically changes a term $(1-t+o(t))$ to $(1-2t)(1-t+o(t)))= 1-3t+o(t).$</p>

<p>Let us focus on the first term, call it $A$, since the limit $e$ for the second was correct.
$$\begin{align}
A &amp;= \frac{(1+(-t))^{2-t}(\ln(1-t)+1)-1+t}{t} \\
&amp;= \frac{(1-t)^{2-t}(1-t+o(t))-1+t}{t}\\
&amp;= \frac{e^{(2-t)\ln(1-t)}(1-t+o(t))-1+t}{t}
\\
&amp;= \frac{e^{(2-t)(-t+o(t))}(1-t+o(t))-1+t}{t} \tag{Expansion for $\ln(1-u)$}\\
&amp;= \frac{e^{-2t+o(t)}(1-t+o(t))-1+t}{t} \\
&amp;= \frac{e^{-2t+o(t)}(1-t+o(t))-1+t}{t} \tag{as $-2t+t^2+o(t)=-2t+o(t)$}
\\
&amp;= \frac{(1-2t+o(t))(1-t+o(t))-1+t}{t} \tag{Expansion for $e^u$}\\
&amp;= \frac{1-3t+o(t)-1+t}{t} \tag{as $1-3t+2t^2+o(t)=1-3t+o(t)$}\\
&amp;= \frac{-2t+o(t)}{t}\\
&amp;= -2+o(1) \xrightarrow[t\to0]{} -2
\end{align}$$
giving the result you want.</p>
"
"2379139","2379155","<p>Here's a walkthrough:</p>

<ul>
<li>Show that $f(0) = 1$</li>
<li>Show that $f(nx) = f(x)^n$ for $n\in\mathbb N$</li>
<li>Show that $f(-x) = f(x)^{-1}$</li>
<li>Show that $f(q) = f(1)^q$ for $q\in\mathbb Q$</li>
<li>Use continuity to show that $f(x) = f(1)^x$ for $x\in\mathbb R$</li>
<li>Conclude</li>
</ul>
"
"2379143","2379149","<p>The last vote cast must go to either $A$ or $B$. The question only cares about the two relevant candidates, so we can assume that there are a total of $m+n$ votes cast, $n$ of which went to $A$ and $m$ of which went to $B$. Thus, if we only look at the last vote, since each ordering is equally likely, the probability that it is one of these $n$ that went to $A$ is $\frac{n}{n+m}$.</p>
"
"2379151","2379168","<p>Observe that $$y=\frac{x-1}{x-4}=1+\frac{3}{x-4}.$$ Therefore, $$x=4+\frac{3}{y-1}.$$ Can you figure out the range of the function? Can $y=1$?</p>
"
"2379172","2379265","<p>For 1): since 
$$X^t\left(   A+cI\right) X\geqslant -\sum_{i,j}\left\lvert a_{i,j}\right\rvert x_k^2 +\sum_{l=1}^ncx_l^2\geqslant      -\sum_{i,j}\left\lvert a_{i,j}\right\rvert x_k^2+c x_k^2 =\left(c-\sum_{i,j}\left\lvert a_{i,j}\right\rvert\right)   x_k^2 ,$$
we need $c-\sum_{i,j}\left\lvert a_{i,j}\right\rvert\gt 0$.</p>

<p>For 2): for any $i,j$, 
$$a_{i,j}x_ix_j\geqslant -\left\lvert  a_{i,j}x_ix_j\right\rvert=  -\left\lvert  a_{i,j}\right\rvert\left\lvert x_i\right\rvert\left\lvert x_j\right\rvert\geqslant  -\left\lvert  a_{i,j}\right\rvert\left\lvert x_k\right\rvert^2,          $$ 
where $k$ is such that $\left\lvert x_k\right\rvert =\max_{1\leqslant l\leqslant n}    \left\lvert x_l\right\rvert$ (in particular, if $X\neq 0$, this guarantees that $ x_k\neq 0$ hence that $A+cI$ is <em>positive</em> definite).    </p>
"
"2379175","2379179","<p>Note that $$(-12)^7 \equiv ((-12)^2)^3(-12) \equiv 1^3(-12) \equiv -12 \pmod {143}$$
since $12^2 = 144$.</p>
"
"2379182","2379218","<p>A <em>necessary</em> condition is that the number of island-bridge incidences (or bridge-ends) is even (as it is twice the total number of bridges). As $1^2+2^2+\ldots+n^2=\frac{n(n+1)(2n+1)}{6}$, this means that $n\equiv 0\pmod 4$ or $n\equiv 3\pmod 4$.
Also, it is necessary that $n^2\le 1^2+2^2+\ldots+(n-1)^2$ or else we're forced to have some bridge on island $n$ loop back to the same island.
This condition is equivalent to $n^2\le \frac{(n-1)n(2n-1)}6$, i.e.,  $6n\le (n-1)(2n-1)$, or: $n\ge 5$.</p>

<p>These conditions are also <em>sufficient:</em> Suppose that $n\ge 5$ and $n\equiv 0$ or $\equiv 3\pmod 4$.
Then we can assign $1^2+2^2+\ldots+n^2$ bridge-ends to the islands such that the $i$th island obtains $i^2$ bridge-ends.
Our task is to match bridge-ends in pairs to form bridges.
It is straightforward to first join island $i$ with $i+1$ in order to ensure that we have connectivity as required (this uses one bridge-end for $i=1$ and for $i=n$, and two bridge-ends for all other $i$). Among all pairings among the remaining bridge-ends, pick one that minimizes the number $\ell$ of loops (i.e., bridges running from an island to itself). Assume $\ell&gt;0$.
Let island $a$ have a loop between bridge-ends $u$ and $v$.
As the remaining at most $n^2-2$ bridge-ends on island $a$ cannot use up the at least $1^2+2^2+\ldots+(n-1)^2$ bridge-ends on the other islands, there exists a bridge between an end $x$ on an island $b\ne a$ and an end $y$ on island $c\ne a$ (whereas we do allow $b=c$).
Then we remove bridges $uv$ and $xy$ and replace them with $ux$ and $vy$. Note that this does not destroy connectivity (we can walk from $b$ to $c$ via $a$) and we end up with at most $\ell-1$ loops, contradicting minimality of $\ell$.
We conclude that either $\ell=0$, as desired.</p>
"
"2379183","2379231","<p>As you note, line $(1)$ is only true if $X - Y$ is independent of $Y$.  In the Brownian motion case, you're all good since you know $W_t - W_s$ is independent of $W_s$.  </p>

<p>To see that your first statement isn't true in general, note that if you take $Y \sim N(0,1)$ and $X = 2Y$ then $X - Y = Y$ so $E[g(X)|Y] = g(X)$ since $X$ is measurable with respect to $\sigma(Y)$ (where $\sigma(Y)$ is the smallest $\sigma$-algebra for which $Y$ is measurable.)</p>
"
"2379185","2379202","<p>Let $R$ be any ring. Then $R[[X]]$ denotes the ring of <a href=""https://en.wikipedia.org/wiki/Formal_ower_series"" rel=""nofollow noreferrer"">formal power series</a> over $R$.</p>

<p>Informally, one often simply <em>defines</em> it as the set
$$\Big\{\sum_{n=0}^âa_nX^n;~a_n â R~\text{for all}~nââ_0\Big\},$$
and then proceeds to say âIts elements are <em>formal expressions</em>â (without clarifying what formal expressions are) and âThis yields a ring when equipped with the suggestive addition and multiplication on it.â One then calls â$X$â a <em>formal variable</em> and leaves it undefined and uninterpreted.</p>

<p>To the formalist, this is of course extremely unsatisfactory. But fortunately, the wikipedia article above gives a formal definition of it <a href=""https://en.wikipedia.org/wiki/Formal_power_series#Ring_structure"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Following this definition, â$\sum_{n=0}^â a_nX^n$â simply becomes a suggestive notation for the sequence $(a_n)_{n â â_0}$. Again, this might be unsatisfactory. <em>However</em>, if you define $X$ to be the special sequence
$$X = (0,1,0,â¦),$$
this notation actually makes sense: One may interpret $R[[X]]$ as <em>topological</em> ring which is <em>complete</em> in a certain sense and then the formal series literally becomes
$$\sum_{n=0}^âa_nX^n = \lim_{Nââ} \sum_{n=0}^N a_nX^n.$$
If you are interested in this viewpoint, I suggest you first read wikipedial on <a href=""https://en.wikipedia.org/wiki/Topological_ring"" rel=""nofollow noreferrer"">topological rings</a>, specifically the <a href=""https://en.wikipedia.org/wiki/Topological_ring#Examples"" rel=""nofollow noreferrer"">examples</a> section, discussing the $I$-adic topology. You may read further about this in any book on commutative algebra such as AtiyahâMacdonaldâs book.</p>

<p>By the way, this trick also works for polynomial rings $R[X]$ (with which I presume you are at least somewhat/intuitively familiar), defining $R[X]$ as a set of sequences in $R$ and $X = (0,1,0,â¦)$: You just have to restrict yourself to sequences that are nonzero only at finitely many terms. But then, the polynomial $\sum_{n=0}^N a_nX^n$ too really <em>is</em> the sum of monomials $a_nX^n$ and $X^n$ really <em>is</em> the product of some special element $X = (0,1,0,â¦)$ by itself.</p>
"
"2379197","2379204","<p>For positives $a$ and $b$ By C-S we obtain:
$$\left(\frac{1}{a+b}+\frac{1}{a+2b}+\cdots+\frac{1}{a+nb}\right)^2$$</p>

<p>$$\leq n\left(\frac{1}{(a+b)^2}+\frac{1}{(a+2b)^2}+\cdots+\frac{1}{(a+nb)^2}\right)$$</p>

<p>$$&lt;n\left(\frac{1}{(a+\frac{b}{2})(a+\frac{3b}{2})}+\frac{1}{(a+\frac{3b}{2})(a+\frac{5b}{2})}+\cdots+\frac{1}{(a+\frac{(2n-1)b}{2})(a+\frac{(2n+1)b}{2})}\right)$$</p>

<p>$$=\frac{n}{b}\left(\frac{1}{a+\frac{b}{2}}-\frac{1}{a+\frac{3b}{2}}+\frac{1}{a+\frac{3b}{2}}-\frac{1}{a+\frac{5b}{2}}+\cdots+\frac{1}{a+\frac{(2n-1)b}{2}}-\frac{1}{a+\frac{(2n+1)b}{2}}\right)$$</p>

<p>$$=\frac{n}{b}\left(\frac{1}{a+\frac{b}{2}}-\frac{1}{a+\frac{(2n+1)b}{2}}\right)=\frac{n^2}{(a+\frac{b}{2})(a+\frac{(2n+1)b}{2})}&lt;\frac{n^2}{a(a+nb)}.$$
Done!</p>
"
"2379201","2379210","<p>The existence of $C$ comes from an equivalent condition for local compactness: in a locally compact space $X$, every point $x\in X$ has a local base $\mathcal{B}$ of compact neighborhoods. This implies that for a neighborhood $N$ of $x$, there is some compact neighborhood $C\subseteq N$ of $x$. As $X$ is Hausdorff, $C$ is closed. Then, the existence of an open $M\subseteq C$ comes from the <a href=""https://proofwiki.org/wiki/Definition:Neighborhood_(Topology)/Point"" rel=""nofollow noreferrer"">definition of neighborhood</a>: a neighborhood of $x$ is a subset of $X$ that contains an open set containing $x$.</p>
"
"2379211","2379215","<p>Here are equivalent formluations
$$\begin{eqnarray*}
 \frac{1}{a+b} &amp;=&amp; \frac{1}{a} + \frac{1}{b}\\
1 &amp;=&amp; \frac{a+b}{a} + \frac{a+b}{b}\\
1 &amp;=&amp; 2 + \frac{b}{a} + \frac{a}{b}\\
0  &amp;=&amp; 1 + \frac{1}{z} + z\\
0 &amp;=&amp; z^2 + z+ 1\\
0 &amp;=&amp; \left(z+\frac{1}{2}\right)^2 + \frac{3}{4}\\
z &amp;=&amp; -\frac{1}{2} \pm  \frac{i}{2} \sqrt 3
\end{eqnarray*}$$</p>
"
"2379217","2379300","<p>Words to equations:
$$\begin{cases}100m+100t+105b\geq 101\times 3\\5m+9t+4b\geq 6\times 3\\m+t+b=3\end{cases}$$
where $m$ is the pounds of Marsbars, $t$ for Twix, and $b$ for Bounty. We want to minimize $P=4m+7t+3b$, where $P$ is the total price. The above system yields $b\geq\frac{3}{5}$ and $t\geq\frac{9}{10}$ and $m\geq\frac{3}{2}$ by simple rearranging and substitution. Hence, the minimum solution occurs at $(m,t,b)=\left(\frac{3}{2},\frac{9}{10},\frac{3}{5}\right)$ with a price of $\$14.10$.</p>
"
"2379220","2379230","<p>Are your subcategories equivalent to elements? The notation is a bit unclear; working under this assumption, what you can do is add a subcategory to each category that represents ""not choosing"" that category. So we would have $A = \{a,b,0\}, B = \{c,d,e,0\}$, etc. For example, to represent choosing $\{Ab,Cg\}$, we would consider this equivalent to $\{Ab,B0,Cg,D0,E0\}$.</p>

<p>Then choosing any combination is just choosing exactly one element from each of your new category. This can be done through straightforward application of the multiplication rule; in this case, you would get $3 \cdot 4 \cdot 3 \cdot 3 \cdot 4 = 432$ valid choices.</p>
"
"2379223","2379245","<p>For a start,<br>
$$
\sum_{n=2}^{\infty} \frac{1}{(\log 1)^p+(\log 2)^p+\cdots +(\log n)^p}
\ge \sum_{n=2}^{\infty} \frac{1}{n(\log n)^p},
$$
which diverges if $p\le 1$ (well-known and not hard to show with the standard techniques).</p>

<p>Further,
$$
\sum_{n=2}^{\infty} \frac{1}{(\log 1)^p+(\log 2)^p+\cdots +(\log n)^p}
\le \sum_{n=2}^{\infty} \frac{1}{n/2 \cdot (\log (n/2))^p},
$$
which converges if $p&gt; 1.$ </p>
"
"2379233","2379249","<p>The <a href=""https://en.wikipedia.org/wiki/Bessel_function#Modified_Bessel_functions:_I.CE.B1_.2C_K.CE.B1"" rel=""nofollow noreferrer"">modified Bessel function of the first kind</a> has a power series expansion
$$ I_{\alpha}(x)=\sum_{k=0}^{\infty}\frac{1}{k!\Gamma(k+\alpha+1)}\Big(\frac{x}{2}\Big)^{2k+\alpha} $$</p>

<p>Taking $\alpha=0$ and using $\Gamma(k+1)=k!$, and then setting $x=4$, we get
$$ I_0(4)=\sum_{k=0}^{\infty}\frac{4^k}{(k!)^2} $$
which is your sum.</p>
"
"2379242","2379270","<p>The first  guaranteed value is $$ b = a^3  $$ which gives
$$ \frac{a^2 + b^2}{ab+1} = \frac{a^2 + a^6}{a^4 + 1} = a^2 $$</p>
"
"2379250","2382477","<p>This question can be answered easily by performing singular value decomposition on $J$.</p>

<p>Since $J$ has full column rank, any singular value decomposition of $J$ must be in the form of $U\pmatrix{S\\ 0}V^T$ for some positive diagonal matrix $S$. Therefore $J^+AJ=VS^{-1}BSV^T$, where $B$ is the leading principal $n\times n$ submatrix of $U^TAU$. Since $A$ is positive definite, so is $B$ (Sylvester's criterion). Hence $J^+AJ$, being similar to $B$, must have a real positive spectrum.</p>
"
"2379257","2379263","<p>To find a basis for the range, just apply the transformation to basis elements of your domain.</p>

<p>In this case, using the standard basis, we find that $$T\left(\begin{bmatrix}
        1 &amp; 0 \\
        0 &amp; 0 \\
        \end{bmatrix}\right) = \begin{bmatrix}
        1 &amp; -1 \\
        0 &amp; 0 \\
        \end{bmatrix}$$ and $$T\left(\begin{bmatrix}
        0 &amp; 0 \\
        1 &amp; 0 \\
        \end{bmatrix}\right) = \begin{bmatrix}
        0 &amp; 0 \\
        1 &amp; -1 \end{bmatrix}$$ 
Which give a basis (note that the other basis vectors give us multiples of these, so we are finished).</p>
"
"2379261","2379268","<p>Yes. For there's a projective transformation that takes some great circle to this line (you can find it by conjugating the map $z \mapsto z + \frac{1}{2}$ by stereographic projection to get a map from $S^2$ to itself (i.e., ""project to $\Bbb C$; add $\frac{1}{2}$; project back to $S^2$""). Call the inverse of this map from $S^2$ to $S^2$ by the name $F$. </p>

<p>Then let $g$ be the usual metric on $S^2$, and define a metric $G$ whose value at a point $P \in S^2$ is computed by 
$$
G_P(u, v) = g(F_{*}(P)(u), F_{*}(P)(u))
$$
where $u$ and $v$ are tangent vectors to $S^2$ at $P$, and $F_{*}$ is the derivative, so that $F_{*}(P)$ is a map from $T_PS^2 -&gt; T_{F(P)}S^2$.</p>

<p>The function $G$ is a Reimannian metric on $S^2$ with your line as a geodesic. </p>

<p>(By the way, this is just a slight variant of @AlfredYerger's answer). </p>
"
"2379262","2379307","<p>Note that $P(x) = ax^2+bx+a$ solves this problem, as \begin{align*} P(x+y+z)-P(x)-P(y)-P(z) &amp;= a(x+y+z)^2-a(x^2+y^2+z^2)-2a \\ &amp;= 2a(xy+yz+zx-1) \end{align*} Then, we can impose $P(0) = a = 1$ and $P(1) = 2a+b = 4$. The latter implies $b = 2$, so we have $P(x) = x^2+2x+1 = (x+1)^2$. Then we find $P(2017) = 2018^2$.</p>

<hr>

<p>To prove that this is the only solution, note that for $$P^*(x, y, z) := P(x+y+z)-P(x)-P(y)-P(z)$$ $P^*(x, y, z)$ must be in the ideal generated by $xy+yz+zx-1$ in $\mathbb{C}[x, y, z]$ as this polynomial is irreducible over $\mathbb{C}$. Therefore, $$P^*(x, y, z) = Q(x, y, z)(xy+yz+zx-1)$$ for some polynomial $Q$. Note that $P^*(x, y, z)$ and  $xy+yz+zx-1$ are both symmetric polynomials. In particular, $xy+yz+zx-1 = \frac{1}{2}p_1^2-\frac{1}{2}p_2-1$, where $p_k := x^k+y^k+z^k$. Now, note that for $P(x) = \sum_{k=0}^n a_kx^k$, $$P^*(x, y, z) = \sum_{k=0}^n a_k(p_1^k-p_k)$$ As $Q(x, y, z)$ is the quotient of symmetric polynomials, it must also be a symmetric polynomial. Thus, we write $R^*(p_1, p_2, \ldots, p_n)$ and $R(p_1, p_2, \ldots, p_n)$ to be the polynomial representations of $P^*$ and $Q$ respectively in terms of power sums (which are algebraically independent). After switching to $R$ and $R^*$, our equation in $P^*$ and $Q$ becomes $$R^*(p_1, \ldots, p_n) = \left(\frac{1}{2}p_1^2-\frac{1}{2}p_2-1\right)R(p_1, \ldots, p_n)$$ If $a_n\neq 0$ for some $n\geq 3$, then $R^*(p_1, \ldots, p_n)$ will have a nonzero term $-a_np_n$. However, this term cannot appear on the right-hand side, as if $R$ has a term $a_np_n$, then it would imply that $R^*$ also has terms $-\frac{a_n}{2}p_1^2p_n$ and $-\frac{a_n}{2}p_2p_n$ (which do not appear in the above sum). Thus, $R^*$ must equal $-2a_0+a_2(p_1^2-p_2)$, which implies that $\deg(R) = \deg(R^*)-2 = 0$. Then, $R\equiv c$ for some constant $c$, and $a_0 = a_2 = \frac{c}{2}$. Letting $a = \frac{c}{2}$ and $b = a_1$, we have $$P(x) = ax^2+bx+a$$ as above.</p>
"
"2379269","2379272","<p>Let $a\in H$ and $x\in G$ since $x^{-1}(xa)\in H$, we deduce that $xax^{-1}\in H$.</p>
"
"2379271","2379472","<blockquote>
  <p>Why do most probability graphs show a bell curve?</p>
</blockquote>

<p>As you suspect, there is a natural tendency for distributions to be bell-shaped.</p>

<p>There are some distributions that are not bell-shaped at all. For example, the outcome of a roll of one fair die is a <a href=""https://en.wikipedia.org/wiki/Discrete_uniform_distribution"" rel=""noreferrer"">discrete uniform distribution</a>:</p>

<p><a href=""https://commons.wikimedia.org/wiki/File:Uniform_discrete_pmf_svg.svg#/media/File:Uniform_discrete_pmf_svg.svg"" rel=""noreferrer""><img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/Uniform_discrete_pmf_svg.svg/1200px-Uniform_discrete_pmf_svg.svg.png"" alt=""Discrete uniform probability mass function for n = 5""></a><br>By <a href=""//commons.wikimedia.org/wiki/User:IkamusumeFan"" rel=""noreferrer"" title=""User:IkamusumeFan"">IkamusumeFan</a> - Own work
<a href=""//commons.wikimedia.org/wiki/File:LibreOffice_4.0_Draw_Icon.svg"" rel=""noreferrer"" title=""File:LibreOffice 4.0 Draw Icon.svg""></a>
This drawing was created with LibreOffice Draw, <a href=""http://creativecommons.org/licenses/by-sa/3.0"" rel=""noreferrer"" title=""Creative Commons Attribution-Share Alike 3.0"">CC BY-SA 3.0</a>, <a href=""https://commons.wikimedia.org/w/index.php?curid=27391798"" rel=""noreferrer"">Link</a></p>

<p>The roll of one die is a pretty simple process. What about the sum of two dice? <a href=""https://wizardofodds.com/gambling/dice/"" rel=""noreferrer"">The Wizard of Odds illustrates</a>:</p>

<p><a href=""https://i.stack.imgur.com/aHGLl.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/aHGLl.gif"" alt=""enter image description here""></a></p>

<p>Starting to look a little like a bell, right? What about the totals of three, or four dice? <a href=""http://mathworld.wolfram.com/Dice.html"" rel=""noreferrer"">Wolfram MathWorld provides a nice illustration</a>:</p>

<p><a href=""https://i.stack.imgur.com/6MZmc.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/6MZmc.gif"" alt=""enter image description here""></a></p>

<p>You can see where this is headed. Nature is full of complex processes. How tall are you? Well, it depends on genetics, nutrition, exercise, injuries, bone loss, and so many more things. The <a href=""https://en.wikipedia.org/wiki/Central_limit_theorem"" rel=""noreferrer"">central limit theorem</a> shows (see <a href=""https://math.stackexchange.com/questions/2379271/why-do-bell-curves-appear-everywhere/2379472#comment4906601_2379472"">symplectomorphic's comment</a> below) that when adding the sum of a large number of things together, the resulting distribution is not just any bell-looking curve, but specifically the <a href=""https://en.wikipedia.org/wiki/Normal_distribution"" rel=""noreferrer"">normal distribution</a>. Or for things with multiplicative combination, the <a href=""https://en.wikipedia.org/wiki/Log-normal_distribution"" rel=""noreferrer"">log-normal distribution</a>.</p>

<p>Why does this happen? <a href=""https://math.stackexchange.com/a/2379288/139015"">mathreadler's answer</a> hints it has to do with <a href=""https://en.wikipedia.org/wiki/Convolution"" rel=""noreferrer"">convolving</a> distributions. The probability density function of a single die is a rectangular function (technically discrete, but let's pretend it's continuous). The sum of two rolls together is then the convolution of two rectangular functions.</p>

<p><a href=""https://commons.wikimedia.org/wiki/File:Convolution_of_box_signal_with_itself2.gif#/media/File:Convolution_of_box_signal_with_itself2.gif"" rel=""noreferrer""><img src=""https://upload.wikimedia.org/wikipedia/commons/6/6a/Convolution_of_box_signal_with_itself2.gif"" alt=""Convolution of box signal with itself2.gif""></a><br>By <a href=""//commons.wikimedia.org/wiki/File:Convolution_of_box_signal_with_itself.gif"" rel=""noreferrer"" title=""File:Convolution of box signal with itself.gif"">Convolution_of_box_signal_with_itself.gif</a>: Brian Amberg
derivative work: <a href=""//commons.wikimedia.org/wiki/User:Tinos"" rel=""noreferrer"" title=""User:Tinos"">Tinos</a> (<a href=""//commons.wikimedia.org/wiki/User_talk:Tinos"" rel=""noreferrer"" title=""User talk:Tinos"">talk</a>) - <a href=""//commons.wikimedia.org/wiki/File:Convolution_of_box_signal_with_itself.gif"" rel=""noreferrer"" title=""File:Convolution of box signal with itself.gif"">Convolution_of_box_signal_with_itself.gif</a>, <a href=""http://creativecommons.org/licenses/by-sa/3.0"" rel=""noreferrer"" title=""Creative Commons Attribution-Share Alike 3.0"">CC BY-SA 3.0</a>, <a href=""https://commons.wikimedia.org/w/index.php?curid=11003835"" rel=""noreferrer"">Link</a></p>

<p>Notice how the result (the black triangle) looks like the case of two dice above. If then convolve this triangle with another rectangle, you get three dice. The more times you do this, the closer the result gets to a normal distribution.</p>

<p>The probability density function of the normal distribution is a Gaussian function, which have some elegant properties:</p>

<ul>
<li>A Gaussian convolved with a Gaussian is another Gaussian.</li>
<li>The product of two Gaussians is a Gaussian.</li>
<li>The Fourier transform of a Gaussian is a Gaussian.</li>
</ul>

<p>From this you might intuitively see as things converge towards normal distributions, they ""want"" to stay as normal distributions since their ""Gaussianess"" is preserved under many operations.</p>

<p>Of course not everything is so simple as a single die roll, nor as complex as the determination of the height of a human. So there are a large number of distributions that look like a bell, but on careful examination aren't the normal distribution. <a href=""https://en.wikipedia.org/wiki/Poisson_distribution"" rel=""noreferrer"">Some of them exist in nature</a>, and some find application as <a href=""https://en.wikipedia.org/wiki/Student%27s_t-distribution"" rel=""noreferrer"">mathematical tools for some purpose</a>. Looking through  <a href=""https://en.wikipedia.org/wiki/List_of_probability_distributions"" rel=""noreferrer"">Wikipedia's list of probability distributions</a> you can see bell-like shapes are quite common, even if they aren't exactly the normal distribution.</p>

<p>But if you combine these two things:</p>

<ul>
<li>The central limit theorem means the normal distribution is common, and</li>
<li>many distributions look like bells but aren't the normal distribution,</li>
</ul>

<p>you might conclude <em>most probability graphs show a bell curve</em>.</p>
"
"2379276","2379297","<p>Yes, in general the proof is correct, but you need to be a bit more specific regarding your choice of $p_1$, $p_2$ in the first part of the proof. Simply requiring that $p_1(1),p_1(2),p_2(1),p_2(2) \neq 0$ is not sufficient, because they could have opposite signs and the terms would cancel. You can however require $$p_1(1)=p_1(2)=p_2(1)=p_2(2)=1\,,$$ which would be sufficient. Similarly, for the $\sin p(0)$ term, consider polynomials with 
$$p_1(0) = p_2(0) = \frac \pi 2\,.$$
Otherwise your reasoning is correct.</p>
"
"2379309","2379592","<p>As Rob Arthan stated, you are correct that the grammars provided do not accurately and precisely capture the concrete syntax of the lambda calculus. That's because they are not intended to. A statement like $\Lambda ::= V \mid (\Lambda \Lambda) \mid (\lambda V.\Lambda)$ is intended to be a declaration of <em>abstract</em> syntax.  It is very much akin to a <code>data</code> declaration in a language like Haskell, ML, or Agda.  For example, in Haskell syntax it might look like: <code>data Exp = Var V | Abs Exp Exp | Lam V Exp</code>. Many tools that are closer to specification than programming such as Agda, Coq, or the $\mathbb{K}$ Framework allow much more syntactic flexibility for a comparable investment of effort. For example, in the <a href=""http://www.kframework.org/index.php/Main_Page"" rel=""nofollow noreferrer"">$\mathbb{K}$ Framework</a> the following declaration captures some of the common conventions for lambda notation and produces a working parser:</p>

<pre><code>syntax L ::= Id
           | L L          [left]
           | ""("" L "")""    [bracket]
           &gt; ""Î»"" Id ""."" L
</code></pre>

<p>This correctly parses ""Î»x.Î»y.x y y"" and further produces the exact same AST for ""Î»x.Î»y.((x y y))"". My point with this is most work indicates the abstract syntax as that's what a semantics operates over. Obviously, at the end of the day they have to use some concrete syntax even to write down the abstract syntax. The precise details are rarely spelled out in anything other than descriptions of machine implementations. The details are usually tedious, unimportant, and, as the above demonstrates, largely mechanically derivable given a bit of additional information beyond the context-free grammar. Nevertheless, for me ""formal"" = ""machine-checked""; everything else is informal no matter how detailed. In that vein, here's a complete executable formalization of beta reduction in $\mathbb{K}$:</p>

<pre><code>require ""substitution.k""
module LAM
    imports SUBSTITUTION
    syntax L ::= Id
               | L L          [left]
               | ""("" L "")""    [bracket]
               &gt; ""^"" Id ""."" L [binder]

    syntax KVariable ::= Id

    rule (^ X:Id . M:L) N:L =&gt; M[N / X] [anywhere]
endmodule
</code></pre>

<p>where I've replaced Î» with ^ because while the generated parser doesn't have an issue with the Unicode, the rewrite engine does. This correctly reduces ""^y.(^x.^y.x y)y"" to ""^ y . ^ _1 . y _1"" where the inner ""y"" has been alpha-renamed to ""_1"". Of course, all the real work of this definition has been hidden away in the predefined SUBSTITUTION module. At any rate, as the tooling advances, there's less and less reason not to formalize these sorts of things in my sense of ""formalize"".</p>
"
"2379315","2379409","<p>You can try to prove that the closed interval and the circle are not homeomorphic. (The analogous problem for higher dimensions requires some algebraic topology.)</p>

<p>With the same idea, prove that the line is not homeomorphic to the plane.</p>

<p>You can try to prove that any injective continuous map from the closed (and bounded) interval into the plane is a homeomorphism onto it's image (with the subspace topology). Give a counter example if you work with the open interval instead.</p>

<p>You can classify the homeomorphism types of 1 dimensional manifolds without boundary. (This one might be tedious to do in detail. The others are essentially one liners, though they require clever ideas.)</p>

<p>You can pick a small integer n (say, 2 or 3 or 4) and see what you can say about the different topologies on a set with n elements. How many homeomorphism types are there?</p>
"
"2379316","2379324","<p>There is still such a surjection: send $r$ to the countable ordinal it codes if it codes a countable ordinal, and to (say) $17$ otherwise.</p>

<hr>

<p>Specifically, we can assign (exercise) a relation $R_r$ on $\mathbb{N}\times\mathbb{N}$ to each real $r$, in such a way that every binary relation on $\mathbb{N}$ winds up being of the form $R_r$ for at least one $r$. Now say $r$ codes a countable ordinal $\alpha$ if $(\mathbb{N},R_r)$ is a well-ordering of type $\alpha$. </p>

<hr>

<p>A more computability-theoretic approach: to each real $r$, assign its relative Church-Kleene ordinal $\omega_1^{CK}(r)$ (this is the least ordinal with no copy computable from $r$). The set $\{\omega_1^{CK}(r): r\in\mathbb{R}\}$ is cofinal in $\omega_1$ (exercise - this is basically the previous paragraph!), and so we can ""collapse"" it to get a surjection $\mathbb{R}\rightarrow\omega_1$. By ""collapse,"" I mean the following: for a set $S$ of ordinals, map $x\in S$ to the ordertype of $(\{y\in S: y\in x\}, \in)$. The image of $S$ under this map is an ordinal (exercise), and if $S$ is a cofinal subset of $\omega_1$ the image is in fact $\omega_1$ (exercise).</p>

<p><em>Interestingly, we don't need the regularity of $\omega_1$ to do this second exercise - which is good, since $\omega_1$ need not be regular in ZF alone!</em></p>
"
"2379327","2379336","<p>$$\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}$$</p>

<p>Let's compute the Jacobian determinant of your functions.</p>

<p>First, we need all the partial derivatives:
\begin{align*}
\pd{f_1}{x} &amp; = y \\
\pd{f_1}{y} &amp; = x \\
\pd{f_2}{x} &amp; = x \\
\pd{f_2}{y} &amp; = -y
\end{align*}
Now, we arrange them into the jacobian matrix:
$$J = \begin{pmatrix}y &amp; x \\ x &amp; -y\end{pmatrix}$$
Now, we compute the 'area element', which is $|\det(J)|$:
$$|\det J| = |(-y^2-x^2)| = x^2+y^2$$</p>

<p>I suspect you left off the absolute value.  Remember the Jacobian determinant is the <em>magnitude</em> of the determinant of the jacobian.</p>
"
"2379339","2379431","<p>Here is literally what you have to do.</p>

<p>(1) Simplify by noting that the absolute of some of the terms are unity,</p>

<p>$$
\begin{align}
\left\lvert \frac{(a+ib)^2 e^{ia+b}-(a-ib)^2 e^{ia-b}}{4abi} \right\rvert ^2
&amp;=\left\lvert \frac{e^{ia}}{4abi}\right\rvert ^2~\left\lvert (a+ib)^2 e^{b}-(a-ib)^2 e^{-b}\right\rvert ^2\\
&amp;=\frac{1}{16a^2b^2}~\left\lvert (a+ib)^2 e^{b}-(a-ib)^2 e^{-b}\right\rvert ^2\\
\end{align}
$$</p>

<p>(2) expand and collect the real and imaginary parts</p>

<p>$$
\begin{align}
\left\lvert (a+ib)^2 e^{ia+b}-(a-ib)^2 e^{ia-b}\right\rvert ^2
&amp;=\left\lvert (a^2-b^2) (e^{b}-e^{-b})+2abi(e^{b}+e^{-b}) \right\rvert ^2\\
&amp;=\left\lvert (a^2-b^2) 2\sinh b+2abi 2\cosh b \right\rvert ^2\\
&amp;=\sqrt{((a^2-b^2) 2\sinh b)^2+(2ab 2\cosh b)^2}^2\\
&amp;=(a^2-b^2)^2 4\sinh^2 b+16a^2b^2\cosh^2 b
\end{align}
$$</p>

<p>So that finally,</p>

<p>$$\left\lvert \frac{(a+ib)^2 e^{ia+b}-(a-ib)^2 e^{ia-b}}{4abi} \right\rvert ^2=\frac{(a^2-b^2)^2 \sinh^2 b}{4a^2b^2}+\cosh^2 b$$</p>

<p>I have verified this solution numerically for arbitrary (random) values of $a$ and $b$.</p>
"
"2379340","2379392","<p>You might start by writing this as</p>

<p>$$ (18 z + 1)^2 + (18 x + 13)^2 - 2 (18 y + 9)^2  = 8 $$</p>

<p>and thus</p>

<p>$$ Z^2 + X^2 - 2 Y^2 = 8 $$
where $X = 18 x + 13 \equiv 13 \mod 18$, $Y = 18y+9 \equiv 9 \mod 18$, $Z = 18 z + 1 \equiv 1 \mod 18$.  Note that this does work mod $18$.  </p>

<p>For fixed $Z$, this is a Pell equation.  If $\pmatrix{X\cr Y}$ is a solution, then so is $\pmatrix{3 &amp; 4\cr 2 &amp; 3\cr}^k \pmatrix{X \cr Y}$ for any $k$ (with the same $Z$).  If $k$ is divisible by $12$, this will be the same mod $18$ as $\pmatrix{X\cr Y}$.  Thus we will get infinite families of solutions.</p>
"
"2379342","2379352","<p>Using the fact that $f(x) + x f'(x) = \frac{d}{dx}(x\, f(x))$ and integrating by parts, we have:
\begin{align}
\int e^x\left[f(x) + x f'(x)\right]\, dx
&amp;= \int e^x\, \frac{d}{dx}(x\, f(x))\, dx\\
&amp;= e^x\, x\, f(x) \;-\;\int e^x\, x\, f(x)\, dx
\end{align}
I don't believe this can be simplified further without some knowledge of $f(x)$.</p>
"
"2379345","2379349","<p>Hints:
Mean = $5+x/5$, and median depends on the value of $x$. Possible arrangements are
$x4579,4x579,45x79,457x9,4579x$.
Calculate the median and check out if the mean can equal the median or not.</p>
"
"2379353","2379394","<p>Using number theory, you can say: </p>

<p>$$f(n)=n^{4}\bmod {15}$$ going to $$A=\{10\},B=\{6\},C=\{0\},D=\{1\}$$</p>

<p>If you want something more ""continuous,"" you could take, for example, $$f(n)=2\cos (2\pi n/3)-\cos(2\pi n/5)$$</p>

<p>$$A=[1.65,2.85],B=[-2,-2], C=[1,1], D=[-1.31,0]$$</p>

<p>The best approach is to use trig functions, since $f$ is inherently periodic.</p>

<p>A discrete Fourier transform gives us an ""exact"" function:</p>

<p>$$f(n)=\frac{13}{15}+\frac{2}{5}\left(\cos 2\pi n/5+\cos 4\pi n/5\right)+\frac{4}{3}\cos(2\pi n/3)$$
Then we get $A=\{2\},B=\{1\},C=\{3\},D=\{0\}.$ You can expand those to make them intervals.</p>
"
"2379356","2379373","<p>As Alex notes, we can neglect the $x+$ part of the function, and simply show that $g(x,y)=\frac{x}{\sqrt{x^{2}+y^{2}}}$ is continuous (if you are uneasy about this, tell me and I'll say more).  </p>

<p>I think the sensible thing to do here is interpret $g$ geometrically. Away from the origin (which is precisely the condition you insist on), $g(x,y)$ may be geometrically interpreted as $\cos \theta$, with $\theta$ the angle between $(x,y)$ and the $x$-axis.   </p>

<p>Let $(u,v)$ be an arbitrary point with $0&lt;\sqrt{u^{2}+v^{2}}\le 1$, and let $\varepsilon&gt;0$. We want to find $\delta&gt;0$ such that, whenever $(x,y)$ is inside the $\delta$-radius circle at $(u,v)$, the difference $|\cos \theta_{x}-\cos \theta_{u}|$ is less than $\varepsilon$. By continuity of $\cos$, this is the same as making $|\theta_{x}-\theta_{u}|$ appropriately small (I'm being intentionally vague as you only asked for a hint; I hope this doesn't make it more confusing).  </p>

<p>That should be enough information to <em>draw a good picture</em>, and then try to spot the right $\delta$ based on that.</p>
"
"2379358","2379368","<p>Yes.</p>

<p>Assume without loss of generality that $S$ has a least element. (Otherwise pick an element and handle the part of $S$ above and below the chosen element separately -- for those below mutatis mutandis with the order swapped).</p>

<p>Now we can keep assigning the ""smallest element of $S$ not yet used"" to integers starting with $0$ and counting upwards. (After the first step, the set of integers not yet used is always bounded below by the original least element).</p>

<p>This can <em>either</em> end when we've used up all of the elements, <em>or</em> when we've assigned elements to all the natural numbers. But in the latter case, the set of elements that we <em>have</em> assigned yet cannot be upwards bounded in $S$ -- because then they would have a largest element, which they haven't -- so also in that case there cannot be any elements left.</p>
"
"2379365","2379372","<p>Suppose $x \in O$, where  $O$ is open, and we want to show that there is some $U$ open such that $$x \in U \subseteq \overline{U} \subseteq O$$</p>

<p>which is an equivalent formulation of being regular.
Suppose not, then let $(V_n)_n$ be a decreasing countable local base at $x$ (we can always take finite intersections of initial parts to force the decreasingness), then this failure of regularity says that $\overline{V_n}\setminus O \neq \emptyset$ for all $n$.</p>

<p>In a countably compact space we then conclude that (why?)</p>

<p>$$\exists y \in \bigcap_n (\overline{V_n}\setminus O)$$</p>

<p>Now use Hausdorffness to get a contradiction.</p>
"
"2379369","2379371","<p><strong>HINT</strong></p>

<p>Notice that
\begin{align*}
\frac{1}{\sqrt{n+1} + \sqrt{n}} = \frac{1}{\sqrt{n+1} + \sqrt{n}}\times\frac{\sqrt{n+1} - \sqrt{n}}{\sqrt{n+1} - \sqrt{n}} = \sqrt{n+1} - \sqrt{n} 
\end{align*}</p>
"
"2379370","2380044","<p>Since $a(s)$ is regular, a Frenet-apparatus can be constructed:</p>

<p>$$
\begin{align}
T'(s) &amp;=\quad ||\alpha'(s)|| \cdot \kappa(s) \cdot N(s) \\
N'(s) &amp;=     -||\alpha'(s)|| \cdot \kappa(s) \cdot T(s) \\
\end{align}
$$</p>

<p>with $R(s)=\kappa^{-1}(s)$.</p>

<p>The velocity of the evolute is given by</p>

<p>$$
\begin{align}
E'(s) &amp;= \alpha'(s) + R'(s) \cdot N(s) + R(s) \cdot N'(s) \\[1ex]
      &amp;= ||\alpha'(s)|| \cdot T(s) + R'(s) \cdot N(s) - R(s) \cdot ||\alpha'(s)|| \cdot \kappa(s) \cdot T(s) \\[1ex]
      &amp;= R'(s) \cdot N(s) \\[3ex]
||E'(s)|| &amp;= ||R'(s) \cdot N(s)|| \\[1ex]
          &amp;= R'(s)
\end{align}
$$</p>

<p>$a)$ Assume $R'(s) = 0$ for $s \in ] s_0, s_1 [$. Then R(s) = R is constant, which means $\alpha$ is a circle segment over $] s_0, s_1 [$. Consequently, $E(s)$ is a singleton: the center of the circle. Since a singleton is a closed set, this concludes the proof.</p>

<p>$b)$ Assume $R'(s) \ne 0$. This means $E(s)$ is also regular, and a Frenet-apparatus can be constructed for the evolute:</p>

<p>$$
\begin{align}
T_E(s)  &amp;= \frac{E'(s)}{||E'(s)||} = N(s) \\[1ex]
T_E'(s) &amp;= N'(s) \\[3ex]
N_E(s)  &amp;= J \cdot T_E(s) = -T(s) \\[1ex]
N_E'(s) &amp;= -T'(s) \\
\end{align}
$$</p>

<p>Consequently, the curvature of the evolute equals</p>

<p>$$
\begin{align}
\kappa_E(s) &amp;= \frac{T'_E(s) \cdot N_E(s)}{||E'(s)||} = -\frac{N'(s) \cdot T(s)}{R'(s)} \\[1ex]
            &amp;= \frac{||\alpha'(s)|| \cdot \kappa(s)}{R'(s)} \cdot (T(s) \cdot T(s)) \\[1ex]
            &amp;= \frac{||\alpha'(s)||}{R'(s) \cdot R(s)} \ne 0\\[1ex]
\end{align}
$$</p>

<p>Since the curvature of the evolute is not zero, the evolute itself cannot be a straight line, thus concluding the proof.</p>
"
"2379374","2379399","<p>Let $v(n)$ be the nim-value of the game for $n$ points. Then we have $v(0)=v(1)=0$, $v(2)=1$, and the following recursive relation for $n\geq 3$:
$$v(n)=\textrm{mex}\{v(k)+v(n-2-k):0\le k \le n-2\}$$
where $\textrm{mex}$ is the <em>minimal excluded element</em> function.</p>

<p>Here are the first several values:</p>

<pre><code>0 1 1 2 0 3 1 1 0 3 3 2 2 4 0 5 2 2 3 3 0 1 1 3 0 2 1 1 0 4 5 2 7 4
0 1 1 2 0 3 1 1 0 3 3 2 2 4 4 5 5 2 3 3 0 1 1 3 0 2 1 1 0 4 5 3 7 4
8 1 1 2 0 3 1 1 0 3 3 2 2 4 4 5 5 9 3 3 0 1 1 3 0 2 1 1 0 4 5 3 7 4
8 1 1 2 0 3 1 1 0 3 3 2 2 4 4 5 5 9 3 3 0 1 1 3 0 2 1 1 0 4 5 3 7 4
8 1 1 2 0 3 1 1 0 3 3 2 2 4 4 5 5 9 3 3 0 1 1 3 0 2 1 1 0 4 5 3 7 4
...
</code></pre>

<p>The pattern repeats at a period of $34$, and the losing positions are exactly those where $$n \equiv 5,9,21,25, \textrm{or } 29 \pmod{34},$$
as well as (curiously) $n = 1, 15$, and $35$.</p>

<hr>

<p><strong>Code</strong></p>

<p>Here is the Ruby code I wrote (please note that I'm not a programmer, so I'm sure there are much better ways to do this):</p>

<pre><code>$stored_v = [nil]   # store previously calculated values of v so that
                    #  we don't have to recursively calculate them every time

def mex(l)          # mex (minimal excluded element) of a list l
    ls = l.sort.uniq

    for i in 0..ls.size do
        if i != ls[i] then
            return i
        end
    end
    return ls.size
end

def v(n)            # nim-value of a game with n points
    if $stored_v[n] != nil then
		return $stored_v[n]
    end

    if n &lt;= 1 then
        $stored_v[n] = 0
		return 0
	elsif n &lt;= 3 then
		$stored_v[n] = 1
        return 1
    else
        l = []
        for k in 0...n-2 do
            l = l + [v(k) ^ v(n-2-k)]   # bitwise XOR (i.e., nim sum) of
                                        #  games on k and n-2-k points
        end
        $stored_v[n] = mex(l)
        return mex(l)
    end
end
</code></pre>
"
"2379383","2379404","<p>It's always possible.</p>

<p>You want to multiply top and bottom by $M$ to get that $denominator*M$ has not radical.</p>

<p>As you have figured out:  If the denominator is $a + b\sqrt{c}$ you mulitply by the conjugate to get $(a + b\sqrt{c})(a - b\sqrt{c}) = a^2 - b^2*c$.</p>

<p>This will also work with $(\sqrt a + \sqrt b)(\sqrt a - \sqrt b) = a - b$.</p>

<p>So it's the same idea for $a + \sqrt[k] b$.  The trick is to realize that $(a + \sqrt[k]b)(a^{k-1} - a^{k-2}\sqrt[k]b + a^{k-3}(\sqrt[k]b)^2-..... \pm a(\sqrt[k]b)^{k-2} \mp (\sqrt[k]b)^{k-1} = a^k \pm b$.</p>

<p>Example:  To deradicalize $5 + \sqrt[3]7$ multiply by $5^2 - 5*\sqrt[3]7 + (\sqrt[3]7)^2$ to get $(5 + \sqrt[3]7)(5^2 - 5*\sqrt[3]7 + (\sqrt[3]7)^2) = 5^3 + 5^2\sqrt[3]7 -5^2\sqrt[3]7 - 5*(\sqrt[3]7)^2 + 5*(\sqrt[3]7)^2 + (\sqrt[3]7)^3 = 125 + 7$.</p>

<p>So to deradicalize $(2 + \sqrt 2 + \sqrt[3] 2)$ just deradicalize it term by term.</p>

<p>First let's get rid of the $\sqrt[3]2$ term.  So we multiply top and bottom by $(2+\sqrt 2)^2 - (2 +\sqrt 2)*\sqrt[3]2 + (\sqrt[3]2)^2$ to get $(2 + \sqrt 2 + \sqrt[3] 2)*[(2+\sqrt 2)^2 - (2 +\sqrt 2)*\sqrt[3]2 + (\sqrt[3]2)^2] = (2 + \sqrt 2)^3 + 2= 8 + 12 \sqrt 2 + 12\sqrt 2 + 2\sqrt 2 + 2 = 10 + 26\sqrt 2$.  Then we multiply that by $10  - 26 \sqrt 2$ to get $(10 + 26\sqrt 2)(10 - 26\sqrt 2) = 100 - 2*26^2$.</p>

<p>So example:</p>

<p>\begin{align} &amp;\frac 1 {2 + \sqrt 2 + \sqrt[3] 2} \\&amp;= 
\frac {(2 + \sqrt 2)^2 - (2+\sqrt2)\sqrt[3]2 + \sqrt[3]2^2}{(2+\sqrt 2)^3 + 2}\\&amp;=
\frac {(4 + 4\sqrt 2 + 2) -2\sqrt[3] 2 - \sqrt 2\sqrt[3]2 + \sqrt[3]2^2}{10 + 26\sqrt 2}\\&amp;=
\frac {[(4 + 4\sqrt 2 + 2) -2\sqrt[3] 2 - \sqrt 2\sqrt[3]2 + \sqrt[3]2^2](10 - 26\sqrt{2})}{100 - 2*26^2} \end{align}</p>

<p>Okay... admittedly that is a bear... but it is doable.</p>
"
"2379405","2379407","<p>Showing that there is an upper bound that diverges to $+\infty$ doesn't actually prove anything.  What we want is a lower bound:</p>

<p>$$x&gt;1\implies\frac{1+\cos^2x}{\sqrt{1+x^2}}\ge\frac1{\sqrt{x^2+x^2}}=\frac{2^{-1/2}}x$$</p>

<p>And it seems you already know that $\int_0^\infty\frac1x~\mathrm dx$ diverges.</p>
"
"2379427","2379457","<p>For every $n$ we have $\{x_m:m&gt;n\} \subset  U_{r_{n+1}}(x_{n+1}).$ So for every $n$ we have $$y=\lim_{m\to \infty}x_m\in Cl(  U_{r_{n+1}}  (x_{n+1})) =K_{r_{n+1}}(x_{n+1})\subset D_n\cap U_{r_n}(x_n)\subset D_n\cap U_r(x).$$</p>

<p>If we tried to work only with $U_{n_{r+1}}(x_{n+1})\subset U_{r_n}(x_n)$ we might find ourselves in trouble.</p>

<p>For example, if $X=\mathbb R$ and $x=0 \not \in D_1$, we might have $U_{r_n}(x_n)=(0,2^{-n}a).$ Then $(x_n)_n$ converges to $0$, which is  not  a member of $\cap_nD_n.$</p>

<p>So we need  $Cl(U_{r_{n+1}}(x_{n+1}))\subset U_{r_n}(x_n) .$ </p>

<p>Footnote: The usual notation for $U_r(x)$ is $B(x,r).$ And $K_r(x)=\overline {B(x,r)}=Cl(B(x,r)).$   </p>
"
"2379432","2379433","<p>$$2 &lt; \log_4 a &lt; \log_2 7$$</p>

<p>$$4^2 &lt; a &lt; 4^{\log_2 7}$$</p>

<p>$$4^2 &lt; a &lt; 2^{2\log_2 7}$$</p>

<p>$$4^2 &lt; a &lt; 2 ^ {\log_2 7^2}$$</p>

<p>$$4^2 &lt; a &lt; 7^2$$</p>
"
"2379434","2379437","<p>$\forall x \in \bigcap_{\delta &gt; 0}A_{\delta}$, then $\forall \delta &gt; 0$, $\exists a \in A$ s.t. $d(x,a) &lt; \delta$, thus $x \in \bar A$.</p>

<p>$\forall x \in \bar A$: if $x \in A$ we are done. Else, $x$ is a limit point of $A$, and thus $\forall \delta &gt; 0$, $\exists a \in A$, s.t. $d(x,a) &lt; \delta$, which means $x \in \bigcap_{\delta &gt; 0}A_\delta$</p>

<p>So we are done, </p>
"
"2379435","2379436","<p>Hint : The equation is equivalent to $$nx-ny=kx-ky$$ Now, factor out $n$ and $k$</p>
"
"2379439","2379478","<p>The very important topic of Conservation Laws makes great use of the Jacobian matrix. A set of conservation laws in one dimension takes the form of partial differential equations
$$\partial_t\mathbf{u}+\partial_x\mathbf{f}(\mathbf{u})=0$$
where $\mathbf{u}$ is a vector containing the conserved variables. In the canonical example of compressible fluid flow these are mass density, momentum density and energy density. $\mathbf{f}(\mathbf{u})$ is another vector that contains, loosely, the rates at which these quantities flow. This is often written as $$\mathbf{u}_t+\mathbf{A}\mathbf{u}_x=0$$
where $\mathbf{A}$ is the Jacobian matric relating changes in $\mathbf{f}$ to changes in $\mathbf{u}$.</p>

<p>The eigenvalues of $\mathbf{A}$ are the speeds with which various waves propagate with or through the flow. The right eigenvectors are the patterns of disturbance carried by a wave, and the left eigenvectors yield quantities that are constant along a wave path.</p>

<p>EDIT </p>

<p>Another very important application is to computational geometry.
Consider a mapping in $R^3$ where an initial point with position vector ${\mathbf x}$ is taken to a point with position vector $\mathbf{X}$. The Jacobian $\mathbf{J}=\frac{\partial {\mathbf X}}{\partial{\mathbf x}}$ is called the distortion gradient tensor. Assume the mapping is smooth enough that small parallelopipeds are taken to small parallelopipeds.
Then , starting with a control volume oriented with the $\mathbf{x}$ cordinates</p>

<ol>
<li><p>The rows of $\mathbf{J}$ are the vectors that represent the new edges.</p></li>
<li><p>The rows of the cofactor matrix of $\mathbf{J}$ are the normals to the new faces.</p></li>
<li><p>The determinant of $\bf J$ is the new volume, assuming the (now nonplanar) faces are closed by bilinear surfaces.</p></li>
</ol>

<p>This used in computing the behavior of materials undergoing large distortion.</p>
"
"2379440","2379496","<p>Let $r^2 = \sum x_i^2$.  Then your map $g$ sends $(x_1,..., x_n)$ to $\frac{1}{\sqrt{1-r^2}}(x_1,..., x_n)$.</p>

<p>Now, the question is basically: given the (non-linear) system of equations $\frac{1}{\sqrt{1-r^2}} x_i = y_i$, can we solve for $x_i$?</p>

<p>Notice first that if we can somehow compute $r^2$ from this system solely in terms of the $y_i$ variables, then we can easily solve for $x_i$:  $x_i = y_i \sqrt{1-r^2}$.</p>

<p>So lets do that.  Square each of the equations, getting $\frac{x_i^2}{1-r^2} = y_i^2,$ and then add them all together.  This gives $\frac{r^2}{1-r^2} = \sum y_i^2$.  Clearing denominators gives $r^2 = \sum y_i^2 - r^2 \sum y_i^2$.  Adding $r^2\sum y_i^2$ to both sides, factoring, and then dividing yields $$r^2 = \frac{\sum y_i^2}{1+\sum y_i^2}$$ and therefore, that $$x_i = y_i \sqrt{1-\frac{\sum y_i^2}{1+\sum y_i^2}}.$$</p>

<p>In other words, I am claiming that $g^{-1}$ maps $(y_1,...,y_n)$ to $\sqrt{1-\frac{\sum y_i^2}{1+\sum y_i^2}}(y_1,...., y_n)$.</p>

<p>Let's check it.  We see that \begin{align*} (g^{-1}\circ g)(x_1,..., x_n) &amp;= g^{-1}\left( \frac{x_1}{\sqrt{1-r^2}}, ..., \frac{x_n}{\sqrt{1-r^2}}\right)\\ &amp;= \sqrt{1-\frac{ \frac{1}{1-r^2}\sum x_i^2}{1+ \frac{1}{1-r^2} \sum x_i^2}}\left(\frac{x_1}{\sqrt{1-r^2}}, ..., \frac{x_n}{\sqrt{1-r^2}}\right).\end{align*}</p>

<p>Focussing on that horrid coefficient and recalling that $r^2 = \sum x_i^2$, we see that it simplifies to $$\sqrt{1 - \frac{\frac{r^2}{1-r^2}}{1+ \frac{r^2}{1-r^2}}} = \sqrt{1 - \frac{\frac{r^2}{1-r^2}}{\frac{1}{1-r^2}}} = \sqrt{1-r^2}.$$</p>

<p>Thus, $(g^{-1}\circ g)(x_1,..., x_n) = (x_1,..., x_n)$ as claimed.  You could check that $g\circ g^{-1}$ is the identity as well (but I leave that to you!), but note that you don't need to - From the chain rule, $Jac(g^{-1})Jac(g) = Id$, so your matrix must be invertible.</p>
"
"2379448","2379454","<p>Suppose the set has $m$ ones and $n$ zeroes and let $N = m + n$</p>

<p>Then the number of even triples is $$\binom{m}{2} + \binom{n}{3}$$</p>

<p>For a given $N$ this takes at most $N$ different values (vary $m$ from $0$ to $n$), but $\binom{N}{3}$ is $\theta(N^3)$.</p>

<p>Hence the answer is no, there are only finitely many $N$ for which your statement is true, and those $N$ necessarily satisfy $N \ge \binom{N}{3}$.</p>
"
"2379450","2380667","<p>This is a rather famous example (assuming your first fundamental forms came from the two surfaces I'm thinking of). See <a href=""https://math.stackexchange.com/questions/1973957/non-isometric-surfaces-with-equal-curvature"">this posting</a> and my answer (with comments).</p>

<p>(In general, you should try to search with keywords before you post!)</p>
"
"2379453","2379529","<p>$\newcommand{\bbx}[1]{\,\bbox[15px,border:1px groove navy]{\displaystyle{#1}}\,}
 \newcommand{\braces}[1]{\left\lbrace\,{#1}\,\right\rbrace}
 \newcommand{\bracks}[1]{\left\lbrack\,{#1}\,\right\rbrack}
 \newcommand{\dd}{\mathrm{d}}
 \newcommand{\ds}[1]{\displaystyle{#1}}
 \newcommand{\expo}[1]{\,\mathrm{e}^{#1}\,}
 \newcommand{\ic}{\mathrm{i}}
 \newcommand{\mc}[1]{\mathcal{#1}}
 \newcommand{\mrm}[1]{\mathrm{#1}}
 \newcommand{\pars}[1]{\left(\,{#1}\,\right)}
 \newcommand{\partiald}[3][]{\frac{\partial^{#1} #2}{\partial #3^{#1}}}
 \newcommand{\root}[2][]{\,\sqrt[#1]{\,{#2}\,}\,}
 \newcommand{\totald}[3][]{\frac{\mathrm{d}^{#1} #2}{\mathrm{d} #3^{#1}}}
 \newcommand{\verts}[1]{\left\vert\,{#1}\,\right\vert}$</p>

<blockquote>
  <p>$\ds{{1 \over 2\pi}\int_{0}^{2\pi}
\ln\pars{\verts{r\expo{\ic\theta} - a}}\,\dd\theta = \max\braces{\ln\pars{r},\ln\pars{\verts{a}}}:\ {\large ?}.
\qquad a \in \mathbb{C}.}$</p>
</blockquote>

<p>Lets $\ds{r = \verts{r}\expo{\ic\phi_{\large r}}}$ and
$\ds{a = \verts{a}\expo{\ic\phi_{\large a}}}$ where
$\ds{\phi_{r}, \phi_{a} \in \left[0,2\pi\right)}$. Note that
\begin{align}
&amp;\bbox[15px,#ffe]{\ds{{1 \over 2\pi}\int_{0}^{2\pi}
\ln\pars{\verts{r\expo{\ic\theta} - a}}\,\dd\theta}}  =
{1 \over 2\pi}\int_{0}^{2\pi}
\ln\pars{\verts{\verts{r}\expo{\ic\pars{\phi_{\large r} + \theta}} - \verts{a}\expo{\ic\phi}}}\,\dd\theta
\\[5mm] &amp; =
{1 \over 2\pi}\int_{0}^{2\pi}
\ln\pars{\verts{\verts{r}
\expo{\ic\pars{\theta + \phi_{\large r}- \phi_{\large a}}} - \verts{a}}}\,\dd\theta =
{1 \over 2\pi}\,\Re\int_{0}^{2\pi}
\ln\pars{\verts{r}\expo{\ic\pars{\theta + \phi_{\large r}- \phi_{\large a}}} - \verts{a}}\,\dd\theta
\\[5mm] = &amp;\
{1 \over 2\pi}\,\Re\oint_{\verts{z}\ =\ \verts{r}}\ln\pars{z - \verts{a}}
\,{\dd z \over \ic z} =
{1 \over 2\pi}\,\Im\oint_{\verts{z}\ =\ \verts{r}}
{\ln\pars{z - \verts{a}} \over z}\,\dd z
\end{align}
I'll consider the branch-cut
$$
\ln\pars{z - \verts{a}} =
\ln\pars{\verts{\vphantom{\Large A}z - \verts{a}}} +
\mrm{arg}\pars{z - \verts{a}}\ic.\qquad
-\pi &lt; \mrm{arg}\pars{z - \verts{a}} &lt; \pi\,,\quad z \not= \verts{a}
$$
<hr>
<strong>$\ds{\Large\verts{a} &lt; \verts{r}:\ {\large ?}.\quad}$</strong>
$\ds{\large Note\ that\ r \not= 0}$.
\begin{align}
&amp;{1 \over 2\pi}\int_{0}^{2\pi}
\ln\pars{\verts{r\expo{\ic\theta} - a}}\,\dd\theta
\\[5mm] &amp; \stackrel{\mrm{as}\ \epsilon\ \to\ 0^{+}}{\sim}
{1 \over 2\pi}\,\Im\left[%
-\int_{-\verts{r}}^{\verts{a}}
{\ln\pars{\verts{a} - x} + \ic\pi\over x + \ic\epsilon}\,\dd x -
\int_{\pi}^{-\pi}{\ln\pars{\epsilon} + \ic\theta \over \verts{a}}
\,\epsilon\expo{\ic\theta}\ic\,\dd\theta\right.
\\[2mm] &amp;\ \left.\phantom{\stackrel{\mrm{as}\ \epsilon\ \to\ 0^{+}}{\sim}
{1 \over 2\pi}\,\Im\left[\,\right.}
-\int_{\verts{a}}^{-\verts{r}}{\ln\pars{\verts{a} - x} - \ic\pi \over x - \ic\epsilon}\,\dd x\right]
\\[5mm] \stackrel{\mrm{as}\ \epsilon\ \to\ 0^{+}}{\large \to}\,\,\, &amp;
{1 \over 2\pi}\,\Im\left[%
-\,\mrm{P.V.}\int_{-\verts{r}}^{\verts{a}}
{\ln\pars{\verts{a} - x} + \ic\pi \over x}\,\dd x +
\ic\pi\bracks{-\verts{r} &lt; 0 &lt; \verts{a}}\bracks{\ln\pars{\verts{a}} + \ic\pi}\right. +
\\[2mm] &amp; \phantom{{1 \over 2\pi}\left[-\,\,\,\right.}
\left.\mrm{P.V.}\int_{-\verts{r}}^{\verts{a}}
{\ln\pars{\verts{a} - x} - \ic\pi \over x}\,\dd x +
\ic\pi\bracks{-\verts{r} &lt; 0 &lt; \verts{a}}\bracks{\ln\pars{\verts{a}} - \ic\pi}
\right]
\\[5mm] = &amp;\
-\,\mrm{P.V.}\int_{-\verts{r}}^{\verts{a}}{\dd x \over x} +
\bracks{a \not= 0}\ln\pars{\verts{a}}
\\[5mm] = &amp;\
-\ \underbrace{\mrm{P.V.}\int_{-\verts{r}}^{\verts{r}}{\dd x \over x}}
_{\ds{=\ 0}}\ -\
\int_{\verts{r}}^{\verts{a}}{\dd x \over x} +
\bracks{ar \not= 0}\ln\pars{\verts{a}}
\\[5mm] &amp; =
 -\bracks{a \not = 0}\ln\pars{\verts{a} \over \verts{r}} +
\bracks{a \not = 0}\ln\pars{\verts{a}} = \bracks{a \not= 0}\ln\pars{\verts{r}}
\end{align}
<hr>
<strong>$\ds{\Large\verts{a} &gt; \verts{r}:\ {\large ?}.\quad}$</strong>
$\ds{\large Note\ that\ a \not= 0}$.
\begin{align}
&amp;{1 \over 2\pi}\int_{0}^{2\pi}
\ln\pars{\verts{r\expo{\ic\theta} - a}}\,\dd\theta =
\bracks{r \not= 0}{1 \over 2\pi}\,\Im\bracks{2\pi\ic\ln\pars{-a}} =
\bracks{r \not= 0}\ln\pars{\verts{a}}
\end{align}
<hr>
Then,
$$
\bbox[#ffe,15px,border:1px dotted navy]{\ds{{1 \over 2\pi}\int_{0}^{2\pi}
\ln\pars{\verts{r\expo{\ic\theta} - a}}\,\dd\theta = \max\braces{\ln\pars{r},\ln\pars{\verts{a}}}\,,\qquad ar \not= 0}}
$$</p>
"
"2379458","2379459","<p>In base 2, it is not finite or recurring, so it is irrational.</p>
"
"2379460","2381188","<p>The answer of @EpsilonNeighborhoodWatch is not quite correct.</p>

<p>First, it makes an impression that prime $p$ is selected independently for each pair of $x,y$, while in fact prime $p$ should be the same for all elements $1,2,\dots,m$.</p>

<p>Second, the $n$-th power residue class is defined as follows. Two non-zero residues $x$ and $y$ modulo $p$ belong to the same $n$-th power residue class iff $x/y\equiv z^n\pmod{p}$ for some $z$ (in other words, $x/y$ is an $n$-th power residue modulo $p$).</p>

<p>Equivalently, the $n$-th power residue class can be defined via a primitive root $r$ modulo $p$ as follows. Let $x\equiv r^k\pmod{p}$ (i.e., $k$ is the discrete log of $x$ base $r$ modulo $p$). Then the $n$-th power residue class of $x$ is uniquely determined by the value $k\bmod\gcd(p-1,n)$.
In particular, there exist exactly $\gcd(p-1,n)$ different $n$-th power residue classes modulo $p$. To maximize the number of classes for a given $n$, one needs a prime $p$ such that $n\mid p-1$, giving the total of $n$ $n$-th power residue classes modulo $p$.</p>
"
"2379461","2379476","<p>Let $n$ be the number of vertices of $G$. Take then an $a$-coloring of $G$ and a $b$-coloring of $\overline{G}$. Try to produce from that a coloring of the union of $G$ and $\overline{G}$ (and note that it is a complete graph on $n$ vertices).</p>
"
"2379462","2379471","<p>Rewrite as</p>

<p>$$1 - y = 1 + (-1)\cdot y = 1 + y \cdot (-1) = y\cdot(-1) + 1$$</p>

<p>Now equate the coefficients to get</p>

<p>$$ x_0 = -1, x_1 = 1$$</p>
"
"2379463","2379467","<p>Here is an algorithm which is not brute force:</p>

<p>Given a number $N$ (to which we need to find the closest power)</p>

<p>Consider numbers of the form $$A^{B}$$</p>

<p>For $B \in \{1, 2, \dots, 1+ \lfloor\log_2 N\rfloor]\}$</p>

<p>Fix $B$, then do a binary search for $A$ to figure out which is the closest.</p>

<p>This will be an $O(\log^2 N)$ operations algorithm.</p>
"
"2379466","2379523","<p>$1)$ All possible two member subsets: combination of $10$ digits taken $2$ at a time.</p>

<p>$2)$ All possible two member subsets satisfying $|k-m|=2: \{0,2\}, \{1,3\}, \cdots, \{7,9\}.$</p>

<p>$3)$ The probability is to divide $2)$ by $1)$ to get $8/45$.</p>

<p>Note: $\{0,2\}=\{2,0\}$. If insisted on distinct $k$ and $m$, then $1)$ permutation, $2)$ twice more, $3)$ the same answer.</p>
"
"2379469","2383379","<p>The Matrix Cookbook is only a list of formulas without proof. It absolutely does not learn to understand the theory - and the practice - of derivation. Better, you can have a look at the solutions given in this website. Roll up your sleeves, my friend.</p>

<p>Let $g(U)=1/2tr(U^TU),f(W)=1/2tr((XW-T)^T(XW-T))$.</p>

<p>If $K$ has same dimensions as $U$, then $Dg_U(K)=1/2(tr(K^TU)+tr(U^TK))=tr(K^TU)$.</p>

<p>We deduce that, if $H$ has same dimensions as $W$, then $Df_W(H)=tr((XH)^T(XW-T))=tr(H^TX^T(XW-T))$.</p>

<p>If, for every $H$, $Df_W(H)=0$, then $X^T(XW-T)=0$, that implies </p>

<p>$W=(X^TX)^{-1}X^TT$ (if $X^TX$ is invertible).</p>
"
"2379481","2379545","<p>These problems are usually quite annoying without galois theory, and usually trivial with galois theory, so I'll assume you're unfamiliar with it.  </p>

<p>We know that $|\mathbb{Q}(\sqrt7) : \mathbb{Q}| = 2$ </p>

<p>Now we want to show  $|\mathbb{Q}(\sqrt{7+\sqrt7}) :  \mathbb{Q}(\sqrt7)| = 2.$
And similarly  $|\mathbb{Q}(\sqrt{7-\sqrt7}) :  \mathbb{Q}(\sqrt7)| = 2.$</p>

<p>We then can conclude  $|\mathbb{Q}(\sqrt{7+\sqrt7}) :  \mathbb{Q}| =  |\mathbb{Q}(\sqrt{7-\sqrt7}) :  \mathbb{Q}| = 4$</p>

<p><strong>Proof</strong>: </p>

<p>By the work you've done you should easily see that $p(x) = x^2 +  (-7 +\sqrt7)$ has $\pm \sqrt{7+\sqrt7}$ as roots.  So we let's hope $p(x)$ is irreducible over  $\mathbb{Q}(\sqrt7) $.</p>

<p>Suppose it were reducible, meaning $p(x) = (x-a)(x-b) = x^2 -x(a+b) +ab$ with $a,b \in \mathbb{Q}(\sqrt7)$.</p>

<p>We find that $a+b = 0$ since there is no $x$ coefficient in $x^2 +  (-7 +\sqrt7)$, so $a = -b.$</p>

<p>Now we have $ab = -a^2 =  -7 +\sqrt7$ and since $a \in \mathbb{Q(\sqrt7)}, \ \ a = c +d\sqrt7$ with $ \ c,d \in \mathbb{Q} $ </p>

<p>So $(c+d\sqrt7)^2 = c^2 +7d^2 + 2cd\sqrt7 = 7 - \sqrt7$ which implies</p>

<p>$c^2 + 7d^2 - 7 = -2cd\sqrt7 - \sqrt7 = -\sqrt7(2cd -1)$ and we have that </p>

<p>$$\frac{c^2 +7d^2 -7}{2cd -1} = -\sqrt7$$ 
Now if $2cd - 1 \neq 0$ then the left hand side is in $\mathbb{Q}$ and the right hand side isn't, so that's a contradiction and therefore $x^2 + (\sqrt7 - 7)$ is irreducible over $\mathbb{Q(\sqrt7)}$</p>

<p>Suppose $2cd - 1 = 0$.  Then $c = \frac{1}{2d}$ and our earlier equation reads as $$(\frac{1}{2d} + d\sqrt7)^2 = \frac{1}{4d^2} + \sqrt7 +7d^2 = 7 - \sqrt7$$ so $$\frac{1}{4d^2} +7d^2 - 7 = -2\sqrt7 $$</p>

<p>Again the L.H.S is in $\mathbb{Q}$ and the right hand side isn't, so we've shown that   $x^2 + (\sqrt7 - 7)$ is irreducible over $\mathbb{Q(\sqrt7)}$.</p>

<p>Very similar steps can be taken to show that $(x - (\sqrt{7 - \sqrt7})(x -
 (-\sqrt{7-\sqrt7})$ is irreducible $\mathbb{Q(\sqrt7)}$</p>

<p>So now we have that 
 $|\mathbb{Q}(\sqrt{7+\sqrt7}) :  \mathbb{Q}(\sqrt7)| = |\mathbb{Q}(\sqrt{7-\sqrt7}) :  \mathbb{Q}(\sqrt7)| = 2$ </p>

<p>Now if we show that $\mathbb{Q}(\sqrt{7+\sqrt7}) / \mathbb{Q(\sqrt7)} \neq  \mathbb{Q}(\sqrt{7-\sqrt7}) / \mathbb{Q(\sqrt7)}$ we can conclude that $|\mathbb{E}:\mathbb{Q}| = 8$</p>

<p>This can be done without too much trouble by showing that $\sqrt{7 - \sqrt7} \notin \mathbb{Q}(\sqrt{7+\sqrt7}) / \mathbb{Q\sqrt7)} $</p>

<p>The rest of the details should be filled in without too much trouble.</p>
"
"2379493","2379547","<blockquote>
  <p>To elaborate more, let's say that in a certain population it is likely that males are 10 percent more likely show up to the event than females. Then can I say that proportion of males who show up is 55%? </p>
</blockquote>

<p>No.</p>

<p>Let $x$ be the proportion of males expected to show up at the event and $1-x$ the proportion of females. &nbsp; Interpreting ""$10\%$ more likely"" to mean $x=1.1(1-x)$. &nbsp; Then we conclude $x=11/21$. </p>
"
"2379503","2379512","<p>$$(1-x)\frac{dy}{dx}-2y=0$$</p>

<p>We assume that a power series solution exists for this differential equation</p>

<p>$$y=\sum_{n=0}^{\infty}c_{n}x^{n}$$</p>

<p>Differentiating we got,</p>

<p>$$\frac{dy}{dx}=\sum_{n=1}^{\infty}nc_{n}x^{n-1}$$</p>

<p>Now substituting it into the differential equation,</p>

<p>$$(1-x)\sum_{n=1}^{\infty}nc_{n}x^{n-1}-2\sum_{n=0}^{\infty}c_{n}x^{n}=0$$</p>

<p>$$\sum_{n=1}^{\infty}nc_{n}x^{n-1}-\sum_{n=1}^{\infty}nc_{n}x^{n}-2\sum_{n=0}^{\infty}c_{n}x^{n}=0$$</p>

<p>Making index shift,</p>

<p>$$\sum_{n=0}^{\infty}(n+1)c_{n+1}x^{n}-\sum_{n=1}^{\infty}nc_{n}x^{n}-2\sum_{n=0}^{\infty}c_{n}x^{n}=0$$</p>

<p>Taking out a few terms,</p>

<p>$$c_1-2c_0+\sum_{n=1}^{\infty}(n+1)c_{n+1}x^{n}-\sum_{n=1}^{\infty}nc_{n}x^{n}-2\sum_{n=1}^{\infty}c_{n}x^{n}=0$$</p>

<p>Assuming $c_0\neq 0$</p>

<p>$$c_0=\frac{c_1}{2}$$</p>

<p>The recurrence formula is then,</p>

<p>$$c_{n+1}=\frac{(n+2)c_n}{n+1}, n\geq 1$$</p>

<p>We can obtain our first few terms,</p>

<p>$$c_{2}=\frac{3c_1}{2}$$</p>

<p>I hope that you can move on.</p>
"
"2379504","2379581","<p>In <a href=""https://math.stackexchange.com/questions/401734"">this posting</a>, several users proved that</p>

<p>$$ \lim_{x\to-\infty} \sum_{k=1}^{\infty} \frac{x^k}{k^k} = -1. $$</p>

<p>A slight generalization is also possible: In <a href=""https://math.stackexchange.com/questions/403478/what-is-the-answer-to-this-limit/403578#403578"">my previous answer</a>, I proved that</p>

<p>$$ \lim_{x\to-\infty} \sum_{k=1}^{\infty} \frac{x^k}{k^{k-m}} = \begin{cases} -1, &amp; m = 0 \\ 0, &amp; m = 1, 2, \cdots \end{cases} $$</p>

<hr>

<p>On the other hand, I am not sure if any non-trivial special values for this power series is known. Although I am quite skeptical, one might benefit from the following integral representation</p>

<p>$$ \sum_{k=1}^{\infty} \frac{x^k}{k^k} = \int_{0}^{1} x \cdot u^{-xu} \, du $$</p>

<p>which is a generalization of the <a href=""https://en.wikipedia.org/wiki/Sophomore%27s_dream"" rel=""nofollow noreferrer""><em>Sophomore's dream</em></a>.</p>
"
"2379505","2379509","<p>Notice that $p \in K^c$ is any given point. Finite intersection of open sets of $V_{q_n}$ is still open.</p>

<p>Since each $V_{q_n}$ contains $p$, and $V_{q_n} \cap W_{q_n} = \emptyset$ for all $n$, we have $V \cap K = \emptyset$, i.e. $p \in V \subset K^c$. Thus, it proves that each given $p \in K^c$ is an interior point of $K^c$. So $K^c$ is open.</p>
"
"2379510","2379542","<p>\begin{align}a&amp;=\sum_{n=0}^{\infty}m\{x:\ |f|\geq n\}\\
&amp;=\sum_{n=0}^{\infty}\left(\sum_{k=n}^{\infty}m\{x:\ k\leq|f|&lt;k+1\}\right)\\
&amp;=\sum_{n=0}^{\infty}(n+1)m\{x:\ n\leq|f|&lt;n+1\}\\
&amp;\geq\sum_{n=0}^{\infty}\int_{\{x:\ n\leq|f|&lt;n+1\}}|f|\\
&amp;=\int_{}|f|
\end{align}</p>

<p>As you can see, we didn't need the $m(X)$ in the bound. But it is likely that in your problem the definition of $a$ was the sum starting at $n=1$. The term $m\{x:\ |f|\geq0\}$ is the $m(X)$. </p>
"
"2379516","2379568","<p>Got some time, so I can detail what I said in the comment.</p>

<p>Let $M=A^TA$, where $A$ might be complex.</p>

<p>Then </p>

<p>\begin{align}
-x^TMx+r^Tx&amp;=-(Ax)^T(Ax)+r^Tx-\frac{1}{4}r^TA^{-1}(A^{-1})^{T}r+\frac{1}{4}r^TA^{-1}(A^{-1})^{T}r\\
&amp;=-(Ax-(A^{-1})^Tr)^T(Ax-(A^{-1})^Tr)+\frac{1}{4}r^TA^{-1}(A^{-1})^{T}r\\
&amp;=-(Ax-(A^{-1})^Tr)^T(Ax-(A^{-1})^Tr)+\frac{1}{4}r^TM^{-1}r
\end{align}</p>

<p>Therefore, making the change of variable $y=Ax-(A^{-1})^Tr$ in the integral we get that it is equal to</p>

<p>$\det(A^{-1})e^{\frac{1}{4}r^TM^{-1}r}\int e^{-y^Ty}=\frac{1}{\sqrt{\det(M)}}e^{\frac{1}{4}r^TM^{-1}r}\sqrt{\pi^N}$</p>
"
"2379518","2379600","<p>Your idea to replace $\sin(nx)$ by $e^{inx}$ is good. Now you can write $\sum_{n=0}^\infty n x^n e^{inx} = \sum_{n=0}^\infty n (x e^{ix})^n$ and let $\alpha := x e^{ix}$. Our goal is to compute
$$\sum_{n=0}^\infty n \alpha^n \text{.}$$</p>

<h3>First way</h3>

<p>Remark that the sum is
$$\begin{align*}
\alpha^1 + \alpha^2 + \alpha^3 + \cdots\\
\phantom{\alpha^1} + \alpha^2 + \alpha^3 + \cdots\\
\phantom{\alpha^1 + \alpha^2} + \alpha^3 + \cdots\\
\phantom{\alpha^1 + \alpha^2 + \alpha^3} + \cdots\\
\end{align*}$$</p>

<p>You probably know a formula for each line.</p>

<p>Alternatively, you can see that the sum is $(1 + \alpha + \alpha^2 + \cdots)(\alpha^1 + \alpha^2 + \alpha^3 + \cdots)$. Yet an other way to present that is to multiply the sum by $1-\alpha$ (âdiscrete differentiationâ): we get $\sum_{n=1}^\infty \alpha^n$ (multiplying again by $1-\alpha$ gives $\alpha$). If you continue on that idea, you see Pascal's triangle appearing and it gives you a way to compute sums of the form $\sum_{n=0}^\infty P(n) \alpha^n$ where $P$ is a polynomial.</p>

<h3>Second way</h3>

<p>We know that $\sum_{n=0}^\infty \alpha^n = \frac{1}{1-\alpha}$. Now take the derivative of each side of the equality.</p>
"
"2379519","2381856","<p>Let $K/F$ be a finite Galois extension, and let $G = \textrm{Gal}(K/F)$.  Let $\alpha \in K$, and let $f$ be the minimal polynomial of $\alpha$ over $F$.  Let $\Phi$ be the set of roots of $f$.  Since $K/F$ is normal, $\Phi$ is contained in $K$.  Since $K/F$ is separable, the roots of $f$ are distinct.  Thus</p>

<p>$$f(x) = \prod\limits_{\beta \in \Phi} (x - \beta)$$</p>

<p>If I understand your question, what you want to know is why the set $\Phi$ is equal to the set $G.\alpha = \{ \sigma(\alpha) : \sigma \in G \}$.</p>

<p>First, $G.\alpha$ is contained in $\Phi$.  This is easy to see, because if $\sigma \in G$, then $\sigma$ fixes the elements of $F$, hence the coefficients of $f$.  </p>

<p>The converse inclusion is more difficult.  You can prove it using the two equivalent definitions of what it means for a field extension to be normal:</p>

<p><strong>Proposition</strong>: Let $E$ be an algebraic extension of a field $F$.  The following are equivalent:</p>

<p>(i): If $a \in E$, then all the roots of the minimal polynomial of $a$ over $F$ are also in $E$.</p>

<p>(ii): If $\Omega$ is a field containing $E$, and $\phi: E \rightarrow \Omega$ is a homomorphism which fixes $F$ (pointwise), then $\phi$ is actually just an automorphism of $E$.</p>

<p>Now let $\beta$ be another root of $f$. We want to show that $\beta = \sigma(\alpha)$ for some $\sigma \in G$.  There is a unique isomorphism of fields $\phi: F(\alpha) \rightarrow F(\beta)$ which fixes $F$ pointwise and sends $\alpha$ to $\beta$.  </p>

<p>Fix an algebraic closure $\overline{F}$ of $F$ containing $E$.  The isomorphism extension theorem (<a href=""https://en.wikipedia.org/wiki/Isomorphism_extension_theorem"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Isomorphism_extension_theorem</a>) tells you that $\phi$ extends (in general, nonuniquely!) to homomorphism $\tilde{\phi}$ of $K$ into $\overline{F}$.  This can be proved using Zorn's lemma.  </p>

<p>But since $K/F$ is a normal extension, $\tilde{\phi}$ is just an automorphism of $K$ fixing $F$.  In other words, $\tilde{\phi} \in G$.  And $\tilde{\phi}(\alpha) = \phi(\alpha) = \beta$, as required.</p>
"
"2379520","2379530","<p>Note that $\frac{2}{\pi}x\leq \sin(x)\leq x$ for all $x\in [0, \pi/2]$, so we have $$\frac{(2/\pi)x}{x^p} = \frac{2}{\pi}x^{1-p}\leq \frac{\sin(x)}{x^p}\leq \frac{x}{x^p} = x^{1-p}$$ on this interval. Therefore, $$\frac{2}{\pi}\int_0^{\pi/2} x^{1-p}\,\mathrm{d}x\leq \int_0^{\pi/2} \frac{\sin(x)}{x^p}\,\mathrm{d}x\leq \int_0^{\pi/2} x^{1-p}\,\mathrm{d}x$$ For $p &lt; 2$, this implies $$\frac{(\pi/2)^{1-p}}{2-p}\leq \int_0^{\pi/2} \frac{\sin(x)}{x^p}\,\mathrm{d}x\leq \frac{(\pi/2)^{2-p}}{2-p}$$ and thus the integral is finite. For $p\geq 2$, $\int_0^{\pi/2} x^{1-p}\,\mathrm{d}x$ diverges, and therefore so does $\int_0^{\pi/2} \frac{\sin(x)}{x^p}\,\mathrm{d}x$.</p>
"
"2379525","2379531","<p>$$|-1|e=-1$$</p>

<p>Hence $e=-1$.</p>

<p>$$|1|e=1$$</p>

<p>Hence $e=1$.</p>

<p>Hence we have a contradiction.</p>
"
"2379537","2379641","<p>For each $x \in \mathbb{R}$, its irrationality measure $\mu(x)$ is defined as</p>

<p>$$ \mu(x) = \inf \left\{ c \in \mathbb{R} : \left| x - \frac{a}{b} \right| \leq  \frac{1}{|b|^c} \text{ for at most finitely many } (a, b) \in \mathbb{Z}\times\mathbb{Z}^{*}\right\}. $$</p>

<p>Let us collect some basic properties of $\mu$:</p>

<ol>
<li><p>It is clear that $\mu(x) = 1$ if $x$ is rational. On the other hand, if $x$ is irrational, then by <a href=""https://en.wikipedia.org/wiki/Dirichlet%27s_approximation_theorem"" rel=""nofollow noreferrer""><em>Dirichlet's approximation theorem</em></a> we have $\mu(x) \geq 2$.</p></li>
<li><p>We have $\mu(x) = \mu(1/x)$. Indeed, it suffices to prove this for irrational $x$. To this end, notice that if $0 &lt; c &lt; \mu(x)$, then there exists $(a_j, b_j) \in \mathbb{Z}^* \times \mathbb{Z}^*$ such that $|b_j| \to \infty$ and that $|x - (a_j/b_j)| \leq |b_j|^{-c}$. In particular, $a_j/b_j \to x$. Then it follows that</p>

<p>$$ \left| \frac{1}{x} - \frac{b_j}{a_j}\right|
= \frac{|b_j/a_j|}{|x|} \left| x - \frac{a_j}{b_j} \right|
\leq \frac{\text{const}}{|b_j|^c}
\leq \frac{\text{const}}{|a_j|^c}
\leq \frac{1}{|a_j|^{c-\epsilon}} $$</p>

<p>if $\epsilon &gt; 0$ and $j$ is sufficiently large. So we have $c-\epsilon \leq \mu(1/x)$ and this is enough to conclude the claim.</p></li>
<li><p>It is well-known that $\mu(\pi) &lt; \infty$.</p></li>
</ol>

<p>Using these properties, we find that $\mu(1/\pi) &lt; \infty$. So we can pick $c &gt; \mu(1/\pi)$. Then</p>

<p>$$ |\cos n| = \left|\sin\pi\left(\frac{n}{\pi} - \frac{1}{2} - a \right) \right| $$</p>

<p>for any $a \in \mathbb{Z}$. Pick $a$ such that $\left| \frac{n}{\pi} - \frac{1}{2} - a \right| \leq \frac{1}{2}$. Then using the inequality $|\sin(\pi x)| \geq 2|x|$ for $|x| \leq \frac{1}{2}$, for large $n$ we have</p>

<p>$$ |\cos n| \geq |2n|\left|\frac{1}{\pi} - \frac{2a+1}{2n} \right| \geq |2n|\cdot\frac{1}{|2n|^{c}} = \frac{1}{|2n|^{c-1}}. $$</p>

<p>This shows that $|\tan n| \leq |2n|^{c-1}$ for large $n$ and hence the series converges absolutely by comparison test.</p>
"
"2379556","2379580","<p>The derived subgroup of $U(n)$ is $SU(n)$, so any homomorphism into $S^1$ factors through the projection $U(n)\to U(n)/SU(n)$, whose codomain is isomorphic to $S^1$. Your question then amounts to asking what are the continuous homomorphisms $S^1\to S^1$. These are parametrized by the integers. In particular, your guess is correct.</p>
"
"2379578","2379586","<p>I presume $R'$ is also an $A$-algebra. Since $P$ is a polynomial ring over
$A$, the projection map $P\to R$ factors through $R'\to R$ via an algebra map
$\phi:P\to R'$ (just pick suitable images for $x_1,\ldots,x_n$). All one has to do is to show that $\phi(I^2)={0}$ in $R'$, for then one gets a map
$P/I^2\to R'$, etc. Let $f:R'\to R$ denote the map from the second exact sequence. If $a$, $b\in I$ then $f(\phi(a))=0$ and $f(\phi(b))=0$.
Then $\phi(a)$, $\phi(b)\in J$ so $\phi(ab)=0$. This does it.</p>
"
"2379599","2379606","<p>If $G$ is any group acting on an algebraic variety $X$, we get an action of $G$ on $K[X]$ (ie a map $G\times K[X]\to K[X]$) as you describe. However, this doesn't say anything about whether the action respects the algebraic structure of $G$ (if it has one). To make a geometric analogy, your proposed construction is like having an action of a Lie group on a manifold such that $gm$ is a continuous function of $m$ but maybe not a continuous function of $g$.</p>

<p>A ring homomorphism as in (1) is like a group action that is continuous in both variables. You can recover your maps $\psi_x^*$ if you like. For each closed point $g\in G$ we have a map $e_g:K[G]\to K$ (namely evaluation at $g$), and composing with $\psi^*$ gives a map
$$
  (e_g\otimes1)\psi^*:K[X]\to K[X].
$$
This is your $\psi_{g^{-1}}^*$ (the inverse is just to keep it a left action).</p>
"
"2379612","2379694","<p>After ajotatxe's answer, and replacing the $6$'s by $a$ to make the problem more general, decompose the base $x$ as $x=a \cos(\theta)+a+a \cos(\theta)$ and the total area will be given by $$A=\frac 12 a^2\sin(\theta)\cos(\theta)+a^2 \sin(\theta)+\frac 12 a^2\sin(\theta)\cos(\theta)=a^2\sin(\theta)\left(1+ \cos(\theta)\right)$$ So, differentiating with respect to $\theta$
$$A'=-\sin ^2(\theta)+\cos ^2(\theta)+\cos (\theta)=\cos (\theta)+\cos (2\theta)$$ $$A''=-\sin (\theta)-2 \sin (2 \theta)$$ The first derivative cancels when $\theta=\frac \pi 3$ and the second derivative test shows that this is a maximum.</p>

<p>Back to $x$, we then get $x=2a$ and a maximum area $A=\frac{3 \sqrt{3}}{4}a^2$</p>
"
"2379613","2379766","<p>There are $m^N= 2^{100}$ cases in total.<br>
There are $1231762993555129375668495000350$ desirable cases.</p>

<p>To count each the desirable cases exactly once you can do this for all $i\in[k,N]$:</p>

<ol>
<li>Choose a common color for the balls with position $i-k+1$ to $i$. Count a factor $m$</li>
<li>Choose any color for the balls with position $i+1$ to $N$. Count a factor $m^{N-i}$</li>
<li>Partition (with ordering) the balls with position $1$ to $i-k$ into groups of size $1$ to $k-1$. E.g. for $i=9,k=5$ the valide group sizes are
$\quad\{\{4\},\{3,1\},\{1,3\},\{2,2\},\{2,1,1\},\{1,2,1\},\{1,1,2\},\{1,1,1,1\}\}$<br>
For each partition the last group must have a color different from that of step 1. The second last group must have a color different from the last group etc. Hence a given partition yields $(m-1)^q$ cases, where $q$ is a group count. In total we get a factor $S=\sum_q(m-1)^q$. For the example ($i=9,k=5$) we have $q=1,2,2,2,3,3,3,4$</li>
</ol>

<p>The sum depends on variables $k$, $i$ and constant $m$. We have</p>

<p>$S(1, i) = \max(2-i, 0)
\\S(k, i)=\begin{cases} 
      S(k-1,i-1)&amp; k\leq i\leq 2 k-2 \\
      (m-1)\sum_{j=1}^{k-1}S(k,i-j)&amp; \text{else} 
   \end{cases}$</p>

<p>To get the result multiply the factors and sum over $i$:
$$\sum_{i=k}^N S(k,i)m^{N-i+1}$$</p>
"
"2379622","2379628","<p>There are non-constant continuous functions with an uncountable number of zeroes.</p>

<p>My first thought was to use the <a href=""https://en.wikipedia.org/wiki/Cantor_set"" rel=""nofollow noreferrer"">Cantor set</a> $C$, and a search for prior art led to this existing example already on this site:</p>

<p><a href=""https://math.stackexchange.com/questions/1445252/non-constant-continuous-function-having-uncountably-many-zeros"">Non-constant continuous function having uncountably many zeros?</a></p>

<p>The function is:</p>

<p>$$f:[0,1]\to\mathbb{R}, f(x) = \inf_{c \in C}\{ |x - c|\}$$</p>

<p>It can be shown that $f(x) = 0 \quad\forall x \in C$ and that $f$ is continuous on $[0,1$].</p>
"
"2379645","2379671","<p>The image of a function is the set of values that it meets. For example, if $g(n)=n^2$ and the domain of $g$ is $\Bbb Z$, then the image of $g$ is the set of perfect squares.</p>

<p>So: if $n$ is even, how is $n+3$? Is there any number of this kind that can't be $n+3$? If $n$ is odd, how is $2(n-1)$? Perhaps you are thinking ""even"", but can <em>any</em> even number be expressed as $2(n-1)$ for odd $n$? For example: $6$? $10$? $14$?</p>
"
"2379647","2379679","<p>If you sample a probability distribution $n$ times, you get a different probability distribution called the sample distribution, which is given by picking a random sample. The theoretical moments of the sample distribution are the sample moments. In various senses, the sample distribution converges to the original distribution as $n \to \infty$, and the same is true of the moments. </p>

<p>The two are not the same. For example, the original distribution might be a (fair) coin flip. If you sample a coin flip $n$ times - that is, if you flip $n$ independent coins - the sample distribution is the distribution of heads and tails you see, which need not be an even 50/50 split, and in fact can't be if $n$ is odd. </p>
"
"2379659","2379705","<p>A simple proof is to use</p>

<p>$$|a - b| \ge |a| - |b|$$</p>

<p>$$|L|/2 \gt |f'(x) - L| = |L - f'(x)| \ge |L| - |f'(x)|$$</p>

<p>Thus</p>

<p>$$|f'(x)| + |L|/2 \gt |L| \implies |f'(x) \gt |L| - |L|/2 = |L|/2$$</p>
"
"2379665","2379701","<p>If $\{\theta,\phi\}\subset\left(0,\frac{\pi}{2}\right)$ and $n&gt;0$ then by AM-GM we obtain:</p>

<p>$$\tan^2(\theta-\phi)=\left(\frac{\tan\theta-\tan\phi}{1+\tan\theta\tan\phi}\right)^2=\left(\frac{(n-1)\tan\phi}{1+n\tan^2\phi}\right)^2=$$
$$=\frac{(n-1)^2}{(\cot\phi+n\tan\phi)^2}\leq\frac{(n-1)^2}{(2\sqrt{\cot\phi\cdot n\tan\phi})^2}=\frac{(n-1)^2}{4n}.$$
The equality occurs for $\cot\phi=n\tan\phi$, which says that $\frac{(n-1)^2}{4n}$ is the answer.</p>

<p>Otherwise, the maximum does not exist, of course, because we can get $\cot\phi+n\tan\phi\rightarrow0$. </p>
"
"2379673","2379860","<p>You let $y$ go from $0$ to $1$.  When $y$ is at some value, say $y=1/2$, then the range for $x$ is not $-1$ to $0$.  The left bound is the line $y=1+x$ and when $y=1/2,$ the left limit for $x$ is $-1/2$.  And it's different for every value of $y$.  So the $x$ limits can't be $-1$ to $0$.  They have to depend on $y$.  </p>

<p>In general, as $y$ goes from $a$ to $b$, then $x$ goes from ""left-function of $y$"" to ""right function of $y$"".   (Or as $x$ goes from $a$ to $b$, then $y$ goes from ""bottom function of $x$"" to ""top function of $x$."")  A region that can't be expresses in one of these two ways needs to be cut into regions which can be.</p>
"
"2379677","2380339","<p>If I read your post correctly (but beware that I checked none of the numerical values involved), you are successively solving two different problems. </p>

<p>In both cases, one is given a sequence $(X_n)_{n\geqslant1}$ i.i.d. standard normal and one considers its running maximum defined for every $n\geqslant1$ as $M_n=\max\{X_k\mid1\leqslant k\leqslant n\}$.</p>

<p><strong>Approach ""741"":</strong> Let $\theta_3=E(T_3)$ where $T_3=\inf\{n\geqslant1\mid X_n\geqslant3\}$, then $\theta_3=P(X_1\geqslant3)^{-1}$ and you say that $\theta_3\approx741$.</p>

<p><strong>Approach ""444"":</strong> Let $\mu_3=\inf\{n\geqslant1\mid E(M_n)\geqslant3\}$, then you say that $\mu_3\approx444$.</p>

<p>Since $T_3$ is also $T_3=\inf\{n\geqslant1\mid M_n\geqslant3\}$, one is considering either
$$E(\inf\{n\geqslant1\mid M_n\geqslant3\})$$ or $$\inf\{n\geqslant1\mid E(M_n)\geqslant3\}$$ which need not coincide.</p>
"
"2379681","2379709","<p>Dirichlet's test claims that for two <em>continuous</em> functions $f,g\in[a,\infty]$ where $f,g\geq 0$, if a certain $M$ exists such that $\left|\int_a^bf(x)dx\right|\leq M$ for every $a\leq b$, and $g(x)$ is monotonically decreasing, and $\lim_{X\to\infty}g(x)=0$, then $\int_a^\infty fg$ is convergent.</p>

<p>So let's check this here, with $f(x)=\sin 2x$ and $g(x)=\frac{\log x}{1+x}$. The function $g(x)$ decreases as soon as $x\geq e$, and we have $\lim_{x\to\infty}g(x)=0$. Moreover, for all $b\geq 1$ 
$$\left|\int_ 1^b\sin 2x\,dx\right|=\frac{1}{2}|\cos 2-\cos 2b|\leq 1$$
It follows from Dirichlet's test that the integral $\int_1^{\infty}\frac{\log x}{1+x}\sin 2x\,dx$ converges. </p>

<p>So it seems your textbook has it wrong.</p>
"
"2379683","2379688","<p>Let $\|f\|_\infty = \max_{x \in [0,1]} |f(x)| \in \mathbb{R}$, by Weierstrass' theorem. Then, for any $x&lt;y$,
$$
\left| g(x) - g(y) \right| \leq \int_x^y |f(t)| \, dt \leq \|f\|_\infty |x-y|.
$$</p>
"
"2379684","2379717","<p><a href=""https://math.stackexchange.com/questions/128/how-do-you-prove-that-pn-xi-for-xi-irrational-and-p-a-polynomial-is-un"">Weyl's equidistribution theorem</a> implies that the fractional part of $M\pi$ is uniformly distributed in $(0,1)$.</p>

<p>Thus if $M\pi = n + f$ with $f \to 0$, then $f \gt \sin f= |\sin(M\pi -f)| = |\sin(n)|$</p>

<p>This shows that $|\sin(n)|$ can get arbitrarily close, infinitely often to any value in $(0, \sin 1)$.</p>
"
"2379699","2379713","<p>Your proof for $2$ and $3$ is wrong.</p>

<hr>

<p>For symmetry:</p>

<ul>
<li>Yes, from $(x,y)\sim (a,b)$, you can conclude that $(x,y)=(\lambda a, \lambda b)$ for some $\lambda$.</li>
</ul>

<p><strong>however</strong>, what you then did was assume that $(a,b)\sim (x,y)$, when in fact, that's what you want to <strong>prove</strong>.</p>

<p>So, I suggest you try this again. And just to recap:</p>

<p><strong><em>What you know</em></strong>:</p>

<blockquote>
  <p>You know that $(x,y)\sim (a,b)$, i.e. you know that there exists some $\lambda&gt;0$ such that $(a,b) =(\lambda x, \lambda y)$</p>
</blockquote>

<p><strong><em>What you need to prove</em></strong>:</p>

<blockquote>
  <p>You need to prove that $(a,b)\sim (x,y)$, that means you need to prove that there exists some $\lambda &gt; 0$ such that $(x,y)=(\lambda a, \lambda b)$.</p>
</blockquote>

<hr>

<p>For transitivity:</p>

<p>You do not know that $(x,y)\sim (c,d)$ when $\lambda=1$. For example, if $(x,y)=(1,1)$, and $(a,b)=(2,2)$, and $(c,d)=(4,4)$, then $(1,1)\sim (4,4)$ even though $\lambda$ in that case is not equal to $1$.</p>

<p>Again, try this proof again, and again, here is a recap:</p>

<p>So, I suggest you try this again. And just to recap:</p>

<p><strong><em>What you know</em></strong>:</p>

<blockquote>
  <p>You know that $(x,y)\sim (a,b)$ and you know that $(a,b)\sim (c,d)$. In other words, you know that there exists some $\lambda_1&gt;0$ such that $(a,b) =(\lambda_1 x, \lambda_1 y)$. And you know that there exists some $\lambda_2&gt;0$ such that $(c,d)=(\lambda_2 a, \lambda_2 b)$.</p>
</blockquote>

<p><strong><em>What you need to prove</em></strong>:</p>

<blockquote>
  <p>You need to prove that $(x,y)\sim (c,d)$, that means you need to prove that there exists some $\lambda &gt; 0$ such that $(c, d)=(\lambda x, \lambda y)$.</p>
</blockquote>
"
"2379710","2379719","<p>Your proof is almost complete - to nail it down, you may argue as follows:
Since $\lim_{x\to\infty}f'(x)=0$, for each $\varepsilon&gt;0$ there exists some
$M&gt;0$ such that for all $x&gt;M$ you have $|f'(x)|&lt;\varepsilon$. In particular, if $x&gt;M$, then for $x&lt;c&lt;x+1$ you have:
$$|f(x+1)-f(x)|=|f'(c)|&lt;\varepsilon$$
so by definition, $\lim_{x\to\infty}(f(x+1)-f(x))=0$</p>
"
"2379711","2379736","<p>If $y(t)$ is derivable, then by chain rule:
$$\int{\frac{dy(t)}{y(t)}}=\int{\frac{y'(t)}{y(t)}dt}=\int{\frac{d\,\ln(y(t))}{dt}dt}=\ln{(|y(t)|)}+C$$</p>
"
"2379712","2379772","<p>Not necessarily.  Consider, in $\mathbb{Z}/4\mathbb{Z}$, the subring $R=\{\bar 0, \bar 2\}$.  Then it has characteristic $2$, but as a ring, it does not contain $\mathbb{F}_2$, since it has only two elements but no identity element.</p>
"
"2379716","2379726","<p>Hint: $r(|\cos^3\theta|+2|\sin^3\theta)|) \le r(1+2)=3r=3 \sqrt{x^2+y^2} \le 3(|x|+|y|)$</p>
"
"2379718","2379779","<p>No, it's not possible.</p>

<p>Since $[0,1]$ is compact, we find a finite set of neighborhoods $U_1, \ldots, U_m$ covering $[0,1]$.</p>

<p>Assume that $d(\{n:x_n \in U_k\})$ exists and equals zero for every $k$. </p>

<p>For any $\epsilon &gt; 0$ we have an $n$ such that for every $k$ we have
$$
\frac{|\{i\leq n: x_i \in U_k|}{n} &lt; \epsilon.
$$</p>

<p>It follows
$$ 1 \leq \sum_{k=1}^m \frac{|\{i\leq n: x_i \in U_k|}{n} &lt; m\epsilon $$</p>

<p>where the first inequality is due to the $U_k$ covering $[0,1]$.</p>

<p>by picking $\epsilon$ small enough, in particular less than $\tfrac{1}{m}$ we have a contradiction.</p>
"
"2379746","2379762","<p>When you compute the union $A\cup B$, then this can be written as </p>

<p>$$\bigcup \{A,B\},$$</p>

<p>i.e. compute the union of all sets contained in the set $\{A,B\}$. But what if you choose to compute</p>

<p>$$\bigcup \varnothing\quad?$$</p>

<p>This is <em>by definition</em> $\varnothing$ itself. So when you unify no sets you get the empty set. Mostly one also uses</p>

<p>$$\bigcap\varnothing=X,$$</p>

<p>which gives the whole space $X$. But this depends on the space you are in. It makes no sense in general set theory because there is no universe of all sets.</p>

<p>You can think of it this way: When <em>unifying</em>, you start with an empty set and then you include more sets. When you <em>intersect</em>, you start with the whole space and then you remove some parts. So it is clear that when you do not include any set then you get $\varnothing$ for the union, and when you do not cut any parts you get $X$ for the intersection.</p>
"
"2379747","2380316","<p>Subtracted the first equation from the second:
$$\left(x^2 z+x y^2+y z^2\right)-\left(x^2 y+x z^2+y^2 z\right)=2$$
Simplify and factor:
$$-x^2 y+x^2 z+x y^2-x z^2-y^2 z+y z^2=(x-z)(-x y+x z+y^2-y z)= (x - z)(y - x) (y - z)$$
Because $x$, $y$ and $z$ are integers, $x-z$, $x-y$ and $z-y$ must be integers too, and the only integers possible are $\pm 1$ and $\pm 2$, so $x$, $y$ and $z$ must also be consecutive integers.</p>

<p>Because the equations are symmetrical in interchange of $x$, $y$, and $z$, without loss of generality, we can choose $x$ to be the largest.</p>

<p>This gives us $y-x=-2,\;x-z=1,\;y-z=-1$ that is $y = x-2 , \;z =x -1 $</p>

<p>Plug these results in one of the two equations $x^2 y+x z^2+y^2 z=2186$ to get
$$x^2 (x-2)+(x-1)^2 x+(x-1) (x-2)^2-2186=0$$
Expand and simplify</p>

<p>$$x^3-3 x^2+3 x-730=0$$</p>

<p>$$(x-1)^3 - 729 = 0$$</p>

<p>Since $729=9^3$, the solution is $x=10$.</p>

<p>Substituting in the other equations gives $y=8$ and $z=9$.</p>

<p>$x^2+y^2+z^2=10^2+8^2+9^2=245$</p>
"
"2379753","2379969","<p>You can chain any binary relations.
For example,
$$
a\leq b=c&lt;d\geq e\approx f
$$
means that $a\leq b$, $b=c$, $c&lt;d$, $d\geq e$, and $e\approx f$.
Each binary relation only describes the relation between the two numbers (or what have you) around it.
It does not mean that $a\leq f$ or $a\approx f$ or anything like that.</p>

<p>From the above chain you can conclude, for example, that $a&lt;d$, but you can't compare $c$ and $f$ without more information.
What you can conclude about $a$ and $f$ depends on all the symbols in between.</p>

<p>For another example,
$$
g=h\approx i=j=k\approx l
$$
does indeed imply that $g\approx l$.</p>

<p>I would argue that two approximations is still usually fine.
The relation ""$\approx$"" is not rigorously defined, and both the question and this answer are outside the realm of fully rigorous mathematics.
It is defined heuristically (we kind of know what it means), and this heuristic meaning has properties.
One of the problematic properties is transitivity.
If stretched too far, it becomes ridiculous: $1.00\approx1.01\approx1.02\approx\cdots\approx1.99\approx2.00$ and so $1\approx2$, and by induction $n\approx m$ for any two integers $n,m$.
However, I don't see an issue in practical calculations as in the question, and I would confidently conclude $g\approx l$ in my second example.
The approximation gets potentially worse when repeated, but just a couple of steps is no big issue in this context.</p>

<p>It is important to tell your students explicitly and repeatedly that
$$
129.87\cdot 6\approx 130 \cdot 6 = 780
$$
means two statements in one: $129.87\cdot 6\approx 130 \cdot 6$ and $130 \cdot 6 = 780$.
My experience suggests that not all students will understand this unless (even if?) told.
It follows from these two statements that $129.87\cdot 6\approx780$, but this is not stated <em>directly</em>.</p>

<p>I would suggest your first option.
But the second option isn't really wrong either; both $130 \cdot 6 = 780$ and $130 \cdot 6 \approx 780$ are true.
But it can be confusing to use ""$\approx$"" when the two things are actually equal.</p>
"
"2379764","2379798","<p>You may know that (sometimes this is used as definition of $e$) 
$$\lim_{n\to\infty}\left(1+\frac1n\right)^n=e $$
Taking $k$th powers, $k\in\Bbb N$, we obtain
$$e^k=\lim_{n\to\infty}\left(1+\frac1{n}\right)^{nk}=\lim_{n\to\infty}\left(1+\frac k{nk}\right)^{nk}.$$
The latter limit is the limit of a subsequence of $\lim_{n\to\infty}\left(1+\frac k{n}\right)^{n}$, hence this also converges to $e^k$, once we know it converges at all. In fact, the same method shows that more generally
$$\lim_{n\to\infty}\left(1+\frac {ak}n\right)^n =\left(\lim_{n\to\infty}\left(1+\frac {a}n\right)^n\right)^k$$
for $k\in\Bbb N$ and arbitrary $a$ (provided both limits exist).
As a consequence, $$\lim_{n\to\infty}\left(1+\frac {a}n\right)^n=e^a\qquad \text{for all }a\in\Bbb Q_{\ge0}.$$
Finally, using $(1-\frac1n)^n(1+\frac1n)^n=(1-\frac1{n^2})^n$, you can show that the same also hods for $a=-1$ and hence also for all $a\in\Bbb Q$.</p>
"
"2379765","2380182","<p>Easier to see this after changing variables, $u_i=x_i+y_i, v_i=x_i-y_i$, Then the involution is $u_i\mapsto u_i, v_i\mapsto -v_i$. Then, it is easy to see that the invariants are generated by $u_i, v_iv_j, 1\leq i,j\leq n$ ($i=j$ included).</p>

<p>For the corresponding filed extension, we have $L=k(u_i, v_iv_j)\subset k(u_i,v_j)$. Take one of the $v_i$s, say $v_1$. Then we have both $v_1v_j$ and $v_1^2$ in $L$ and so, $v_j/v_1\in L$. Note that $k(u_i,v_j)=k(u_i, v_1,v_2/v_1,\ldots, v_n/v_1)$ Thus, we see that the fixed field contains all the generators but $v_1$. So, the extension is of the form $k(u_i,v_1^2,v_2/v_1,\ldots, v_n/v_1)=L\subset L(v_1)$. Since $v_1^2\in L$ we are done.</p>
"
"2379788","2379797","<p>We have that $f=u+iv$ is holomorphic and have to show that $g:=v-iu$ is holomorphic.</p>

<p>But this is easy, since $g=-if$.</p>
"
"2379789","2379815","<p>No. Take for example
$$A=\begin{pmatrix}0&amp;\sqrt2\\-\frac1{\sqrt2}&amp;0\end{pmatrix},\ \ B=\begin{pmatrix}0&amp;-\sqrt2\\\frac1{\sqrt2}&amp;0\end{pmatrix}\in\mathrm{SL}_2(\mathbb R)$$</p>

<p>Then since $A$ and $B$ have the same eigenvalues, they are conjugate in $\mathrm{GL}_2(\mathbb C)$, and in fact $\mathrm{SL}_2(\mathbb C)$. But they are not conjugate in $\mathrm{SL}_2(\mathbb R)$: it's straightforward to check that any matrix $P\in\mathrm{GL}_2(\mathbb R)$ with $PA = BP$ must be of the form
$$\begin{pmatrix}a&amp;2c\\c&amp;-a\end{pmatrix}$$
which can't possibly be in $\mathrm{SL}_2(\mathbb R)$. A similar example should work for other quadratic extensions $F/E$.</p>

<p>The obstruction comes from Galois cohomology. I'm not so fluent in this, but here is the key point. Assume that $E/F$ is Galois. If $G$ is an algebraic group and $M\in G(F)$ (maybe satisfying some conditions?), there is a bijection between the $G(F)$ conjugacy classes of $M$ which become conjugate in $G(E)$ and
$$\ker \big(H^1(\mathrm{Gal}(E/F), Z_G(M)(E)))\to H^1(\mathrm{Gal}(E/F), G(E)\big),$$
where $Z_G(M)(E)$ are the elements of $G(E)$ which commute with $M$.</p>

<p><a href=""https://www-fourier.ujf-grenoble.fr/~berhuy/fichiers/NTUcourse.pdf"" rel=""nofollow noreferrer"">These notes</a> contain an introduction to Galois cohomology, with the motivating example being exactly your problem.</p>
"
"2379790","2379848","<p>Denote by $\mathbb{M}^3(c)$ a simply-connected Riemannian $3$-space with constant sectional curvature $c=-1,0,1$. That is, $\mathbb{M}^3(c)$ denotes the hyperbolic space $\mathbb{H}^3$ if $c=-1$, the Euclidean space $\mathbb{R}^3$ when $c=0$, or the sphere $\mathbb{S}^3$ if $c=1$.</p>

<p>Now,  consider an  oriented Riemannian surface $\Sigma$ with induced metric $I=\langle,\rangle$ and $f:\Sigma\to\mathbb{M}^3(c)$ an isometric immersion. Let
$N$ be a unit normal along $\Sigma$ which is compatible with the orientation, and
$II$ its associated second fundamental form. That is, $II(X,Y)=\langle-\nabla_X N,Y\rangle$, where $X,Y$ are smooth vector fields in $\Sigma$ and $\nabla$ the Levi-Civita connection of the ambient space.</p>

<p>If $k_1$, $k_2$ are the principal curvatures of the immersion, namely, the eigenvalues of the shape operator $S$, given by $SX=-\nabla_X N$, then we shall denote by
$$K=k_1k_2,\quad H={1\over 2}(k_1+k_2)$$
the extrinsic curvature and the mean curvature of the immersion, respectively. These two quantities are <em>extrinsic</em> and depend on how $S$ is immersed into $\mathbb{M}^3(c)$.</p>

<p>On the other hand, we denote by $K(I)$ the curvature of the metric $I$, namely, the Gauss curvature, which is an <em>intrinsic</em> quantity. For the previous immersion
$f:\Sigma\to\mathbb{M}^3(c)$ the following relations must be satisfied
$$K(I)=K+c,\quad\text{(Gauss equation)}$$
$$\nabla_X SY-\nabla_Y SX-S[X,Y] = 0,\quad\text{(Mainardi-Codazzi equation)},$$
where $\nabla$ is the Levi-Civita connection of the metric $I$.</p>

<p>You can see with these examples, the space forms, that the Gauss curvature is the product of the eigenvalues of the shape operator when your ambient space is $\mathbb{R}^3$, but there is a term (the sectional curvature $c$) that modifies it in general.</p>
"
"2379793","2379799","<p>Step 1. Prove that each $f_n$ is Lipschitz continuous with the same constant $L=1$.</p>

<p>This is an easy consequence of the mean value theorem: given $x &lt; y$, there exists $c\in (x,y)$ such that
$$
|f_n(x) - f_n(y)| = |f_n'(c)| \cdot |x-y| \leq |x-y|.
$$</p>

<p>Step 2. For every fixed $x,y\in\mathbb{R}$, by step 1 you have that
$$
|f_n(x) - f_n(y)| \leq |x-y|
\qquad \forall n\in\mathbb{N}.
$$
Now it is enough to pass to the limit as $n\to +\infty$, obtaining
$$
|g(x) - g(y)| \leq |x-y|.
$$
(To be precise, you should use the triangular inequality:
$$
\begin{split}
|g(x) - g(y)| &amp; \leq |g(x) - f_n(x)| + |g(y)-f_n(y)| + |f_n(x) - f_n(y)|
\\ &amp; \leq |g(x) - f_n(x)| + |g(y)-f_n(y)| + |x-y|
\end{split}
$$
and then pass to the limit as $n\to +\infty$.)</p>
"
"2379794","2379819","<p>Let $d=\gcd(a+b,a-b)$. This means that $d\mid (a+b)$ and $d\mid(a-b)$ and $d$ is the largest such integer. So $d$ must divide any linear combination of $(a+b)$ and $(a-b)$. Now we know that $\gcd(a,b)=1$. So we will make use of this fact. So we take the given linear combinations OR in fact anything else that will work. Then by those linear combinations we see that $d\mid2a$ and $d\mid2b$ because $d$ divides any linear combination of $(a+b)$ and $(a-b)$. Since $\gcd(a,b)=1$, we have integers $r,s$ such that $ar+bs=1$, whence $(2a)r+(2b)s=2$. But since $d\mid 2a$ and $d\mid 2b$ we have $d\mid ((2a)r+(2b)s)$. So $d\mid 2$ and you have $d=1$ or $2$.</p>
"
"2379801","2379816","<p>It is true. Assume $f:[1,\infty)\to \mathbb R$ is monotonically decreasing and prove
$$
\int_1^\infty f(x)~dx \text{ convergent }\Rightarrow \lim_{x\to \infty} f(x)= 0
$$
by contraposition.<p>
Let be $\lim f(x)\neq 0$. If $f(x)\to -\infty$ then $\int_1^\infty f(x)~dx=-\infty$ is obvious. Otherwise let be $c:=\lim_{x\to\infty} f(x)\in\mathbb R\setminus\{0\}$. <p>
First, $c&gt;0$. Then we use $f(x)\geq c$ and conclude
$$
\int_1^\infty f(x)~dx=\lim_{R\to\infty}\int_1^Rf(x)~dx\geq \lim_{R\to\infty}\int_1^Rc~dx=\lim_{R\to\infty}(R-1)c=\infty.
$$
<p>
Next, $c&lt;0$. Since $f(x)\to c$ we get $X&gt;0$ such that $f(x)\leq \frac12c$ for all $x\geq X$ and conclude
\begin{align}
\int_1^\infty f(x)~dx&amp;=\int_1^Xf(x)~dx+\int_X^\infty f(x)~dx=\int_1^X f(x)~dx+\lim_{R\to\infty}\int_X^Rf(x)~dx\\
&amp;\geq \int_1^Xf(x)~dx+\lim_{R\to\infty}\int_1^R\frac12c~dx\\
&amp;=\int_1^Xf(x)~dx+\lim_{R\to\infty}\frac12(R-X)c=\infty.
\end{align}</p>
"
"2379823","2387195","<p>Appropriately, we have</p>

<p>$$S(z+2)=\sum_{n=1}^\infty\frac{z^n}n=-\operatorname{Log}(1-z)$$</p>

<p>Indeed, note for $|z-2|&lt;1$ and $z+2=x+iy$, we have</p>

<p>$$\arg(x+iy)=\operatorname{atan2}(y,x)\in(-\pi/2,\pi/2)$$</p>

<p>which is not an entire branch.  That is, we have</p>

<p>$$S(z+2)=-\ln|1-z|-i\arg(z)$$</p>

<p>For any choice of $\arg(z)\in(-\pi/2,\pi/2)$.  The principle branch works as well.</p>
"
"2379828","2380489","<p>It's sort of hard to figure out what your difficulty is.  There are two steps here.  One is to take the binomial theorem and plug in $1$ for $x$ and $1$ for $y$ and $2m+1$ for $n$.  That gives you your equality.  Second, the inequality follows because every term in the expansion is positive.  Since the right side is missing most of the terms, it has to be less than the left side.</p>
"
"2379832","2379953","<p>Such self-referential phrases ($p$ is about $p$) do not fit in the framework of Boolean algebra/propositional logic, they are not acceptable propositions. </p>

<p>A proposition must have a well-defined truth value, either $\text{true}$ or $\text{false}$, and $\text{true}\lor\lnot\text{true}$, $\text{false}\lor\lnot\text{false}$ do hold.</p>
"
"2379836","2380596","<p>There is a paper ""Defining and computing persistent Z-homology in the general case"" by Romero et al., arxiv:1403.7086.</p>
"
"2379837","2380078","<p>To solve it in the <strong>particular way specified</strong>, reverse the problem.</p>

<p>Imagine that the balls are identical,<br>
and that that they magically acquire the color of $6$ distinctly colored boxes in which they are put,<br>
none of which can hold more than $2$ balls, which translates to solving over non-negative integers,
 $x_1+x_2+.....+ x_6 = 6,\;\; 0\le x_i\le2$</p>

<p>To take care of the upper limit, we exclude inadmissible combos by putting $3$ in one or more boxes and apply inclusion-exclusion, thus</p>

<p>$\binom{11}5 - \binom61\binom{8}5 + \binom62\binom{5}5  = 141$</p>
"
"2379841","2380521","<p>The $\ell_1$ and $\ell_\infty$ norms are duals of each order in the sense that $\|w\|_1 = \max_{\|x\|_\infty \le 1}w^Tx$. Thus for any $\epsilon &gt; 0$, by an trivial change of variable, you have
$$ \max_{\|x\|_\infty \le \epsilon}w^Tx = \epsilon \|w\|_1.
$$
Taking $x = \epsilon\operatorname{sign}(w)$, clearly attains this maximum, since $\langle \epsilon\operatorname{sign}(w),w\rangle = \epsilon \sum_{i}|w_i| := \epsilon\|w\|_1$. Conclude.</p>
"
"2379843","2379847","<p>Let $x_0$ be an element of $[1,2]$. Then</p>

<p>$$[1,2] = \{x\in [1,2]: d(x, x_0) &lt; 5\} = B_{[1,2]}(x_0, 5)$$</p>

<p>so $[1,2]$ is an open ball around $x_0$ with a radius of $5$ (this follows from the definition of open balls).</p>

<hr>

<p>The thing is that an open ball in a metric space <strong>depends</strong> on the metric space. So, $[1,\frac32)$ is an <strong>open</strong> ball around $1$ in the metric space $[1,2]$ (and the standard metric). The very same set is <strong>not</strong> open in the metric space $\mathbb R$, but that's irrelevant.</p>

<hr>

<p>To prove that $X$ is open whenever $(X, d)$ is an metric space, you have to prove that for every $x_0\in X$, there exists some open ball that is contained in $X$ and $x_0$ is it's center. But <strong>any</strong> open ball would do, so you can just take $B(x_0, 1)$ and be done with it.</p>
"
"2379858","2380190","<p>If you want to prove a statement of the form $\forall x (k &lt; x \rightarrow P(x))$ by induction, then the second method is really the only method you can use:</p>

<p>Since induction proves a statement of the form $\forall x \ \varphi(x)$, your $\varphi(x)$ will have to be all of $k &lt; x \rightarrow P(x)$ ... your first method really cannot be used, since you are not trying to prove $\forall x \ P(x)$ (or: as applied tyo your specific example: you are not trying to prove $\forall x \ 1 &lt; x + x$ ... since that is just not true!)</p>

<p>So, this means that the base case is:</p>

<p>$k &lt; 0 \rightarrow P(0)$</p>

<p>... which, as you point out, is easy to prove, since $k &lt; 0$ will be false, and hence $k &lt; 0 \rightarrow P(0)$ will be true.</p>

<p>For the step, you take some arbitrary $x$, and assume (inductive hypothesis) $k &lt; x \rightarrow P(x)$, and you now want to show $k &lt; s(x) \rightarrow P(s(x))$  (the expression $s(x)$ is what PA typically uses instead of $x + 1$)</p>

<p>Now, as again you say, this will be a bit tricky, since once you assume $k &lt; s(x)$ to set up the conditional proof, it of course does not follow that $k &lt; x$, since possibly $x = k$, and so it seems you can't use the inductive hypothesis. </p>

<p>However, the solution is easy:  If $k &lt; s(x)$, then it follows (this is easily proven in PA) that $k = x \lor k &lt; x$. So, do a proof by cases: </p>

<p>If $k = x$, then show the specific case that $P(s(k))$, from which $P(s(x))$ immediately follows</p>

<p>If $k &lt; x$, then you can use the inductive hypothesis, so get $(P(x)$, and from that, presumably, you can now infer $P(s(x))$.</p>

<p>Hence, you get $P(s(x))$ in both cases, and that completes the conditional proof to get $k &lt; s(x) \rightarrow P(s(x))$</p>

<p>Below is a formal proof (I assumed some elementary truths as premises, but again, these are all easily provable in PA ... note how line 5 is the one specific case $P(s(k))$):</p>

<p><a href=""https://i.stack.imgur.com/4VYGL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4VYGL.png"" alt=""enter image description here""></a></p>
"
"2379863","2379944","<p>Put $u = \sqrt{\dfrac{B}{A}}$, then we have:</p>

<p>$$1-x = u x^{1+\frac{\epsilon}{2}}= u x\exp\left[\frac{\epsilon}{2}\log(x)\right] = u x \left[1 + \frac{\epsilon}{2}\log(x) + \frac{\epsilon^2}{8}\log^2(x)+\cdots\right]$$</p>

<p>We then substitute the formal expansion:</p>

<p>$$x = x_0 + \epsilon x_1 + \epsilon^2 x_2 +\cdots$$</p>

<p>and expand the equation in powers of $\epsilon$ and equate equal powers of $\epsilon$. You then find the unperturbed solution:</p>

<p>$$x_0 = \frac{1}{1+u}$$</p>

<p>The coefficient of $\epsilon$ of the equation yields:</p>

<p>$$(1+u)x_1 + \frac{u}{2} x_0 \log(x_0) = 0$$</p>

<p>therefore:</p>

<p>$$x_1 = \frac{u\log(1+u)}{2(1+u)^2}$$</p>

<p>Extracting the coefficient of $\epsilon^2$ of the equation yields:</p>

<p>$$(1+u)x_2 + \frac{u}{2} x_1 \left[1+\log(x_0)\right] +\frac{u}{8}x_0\log^2(x_0)=0  $$</p>

<p>Here we've used the expansion:</p>

<p>$$\log(x) = \log(x_0 + \epsilon x_1 + \cdots) = \log(x_0) + \log(1+ \epsilon \frac{x_1}{x_0}+\cdots) = \log(x_0) + \epsilon \frac{x_1}{x_0}+\cdots$$</p>

<p>So, this way we get an expression for $x_2$ in terms of $u$ and it's easy to proceed in this way to find the higher order terms. A problem you may encounter when proceeding to higher and higher orders is that the perturbation series may not converge for the desired value of $\epsilon$. You can then resort to resummation methods.</p>
"
"2379865","2379879","<p><strong>Hint</strong>. One may recall the <a href=""https://math.stackexchange.com/questions/157372/infinite-product-of-sine-function"">Weierstrass product of the sine function</a>
$$
\frac{\sin x}{x}=\prod\limits_{n=1}^\infty\left(1-\frac{x^2}{\pi^2 n^2}\right),
\qquad x\in(0,\pi),
$$ then one may write, for appropriate values of $x$,
$$
\prod _{n=1}^{\infty } \frac{1}{2} \sqrt{\frac{4 (\pi  n-x) (\pi  n+x)}{\pi ^2 n^2-4 x^2}}=\sqrt{\frac{\prod _{n=1}^{\infty }\left(1-\frac{x^2}{\pi^2 n^2}\right)}{\prod _{n=1}^{\infty }\left(1-\frac{4x^2}{\pi^2 n^2}\right)}}=\sqrt{\frac{\frac{\sin x}{x}}{\frac{\sin (2x)}{2x}}}=\frac1{\sqrt{\cos x}}.
$$</p>
"
"2379870","2379883","<p>Considering that $\overline{z}^2=\overline{z^2}$, then
$$
0=z^2+\overline{z^2}=2 \mathrm{Re}(z^2) \Longleftrightarrow \mathrm{Re}(z^2)=0 \Longleftrightarrow |\mathrm{Re}(z)|=|\mathrm{Im}(z)|.
$$</p>
"
"2379872","2379936","<p>Part 1. Let's calculate the expression $A_0B_0+A_0B_1+A_1B_0-A_1B_1$:</p>

<p>$$A_0B_0+A_0B_1+A_1B_0-A_1B_1=A_0(B_0+B_1)+A_1(B_0-B_1)$$</p>

<p>Now we have 2 cases: either $B_0=B_1$ or $B_0=-B_1$</p>

<p>In the 1st case we get $2A_0B_0$, in the 2nd case we get $2A_1B_0$. In both cases</p>

<p>$$A_0B_0+A_0B_1+A_1B_0-A_1B_1=\pm2$$</p>

<p>and it follows from here that</p>

<p>$$-2\leqslant E[A_0B_0+A_0B_1+A_1B_0-A_1B_1]\leqslant 2$$</p>
"
"2379873","2379885","<ul>
<li>$(-\infty, 0)$ is a validly defined set.</li>
<li>$(0, \infty)$ is a validly defined set.</li>
<li>if $A$ and $B$ are sets, then $A\cup B$ is a validly defined set.</li>
</ul>

<p>In fact, $$(-\infty, 0)\cup (0, \infty) = \{x| x&lt;0\}\cup \{x| x&gt;0\} = \{x| x&lt;0\lor x&gt;0\} = \{x| x\neq 0\} = \mathbb R\setminus \{0\}$$</p>

<p>All expressions above denote the same set and are equally valid (although the first and last are most common because they are easiest to recognize)</p>
"
"2379880","2379896","<p>""We consider those sets $X \subseteq A$ such that $\exists k \in \mathbb{Z}.\forall x \in X. F(x) = k$.""</p>
"
"2379889","2379894","<p>You're almost there:
$$\cos ^2x ( \cosh ^2 y + \sinh ^2 y) + \sin ^2 x (\cosh^2 y + \sinh^2 y)=$$
$$(\cos ^2x + \sin ^2 x) (\cosh^2 y + \sinh^2 y)=$$
$$(\cosh^2 y + \sinh^2 y)=$$
$$2\sinh^2 y + 1\geq1$$</p>

<p>The last step uses the hyperbolic identity $$ \cosh ^2 y - \sinh ^2 y = 1$$ (note the sign difference with the more familiar goniometric one).</p>
"
"2379895","2379937","<p>Your contour integration should have been, conveniently parameterised:</p>

<p>$$a_{-1} = \lim_{\epsilon \to 0}\frac{1}{2\pi i}\oint_{|z-\ln{2}|\leq\epsilon}{f(z)\,dz} =\lim_{\epsilon \to 0}\frac{1}{2\pi i}\int_{0}^{2\pi}{f(\epsilon\exp{(i\theta)}+\ln{2})\,\epsilon i\exp{(i\theta)}\,d\theta} \tag{*}$$</p>

<p>If now, one puts your $f$ into $(*)$ it turns to:
 $$a_{-1} = \lim_{\epsilon \to 0}\frac{1}{2\pi i}\int_{0}^{4\pi}{\frac{\epsilon i\exp{(i\theta)}}{2-2\exp{(\epsilon\exp{(i\theta}))}}\,d\theta}\tag{**}$$
We know that:
$$1-\exp{(\epsilon\exp{(i\theta)})}=-\epsilon\exp{(i\theta)}+\mathcal{O}(\epsilon^2)$$
Pluging this expansion into $(**)$ one has:
$$ a_{-1} = -\lim_{\epsilon \to 0}\frac{1}{4\pi}\int_{0}^{2\pi}{\frac{\epsilon \exp{(i\theta)}}{\epsilon \exp{(i\theta)}+\mathcal{O}(\epsilon^2)}\,d\theta}=-\frac{1}{2}$$</p>
"
"2379904","2379951","<p>The following representation might be helpful.</p>

<blockquote>
  <p>We obtain
  \begin{align*}
\color{blue}{\sum_{i=1}^n\sum_{{j=1}\atop{j\ne i}}^nx_ix_j}
&amp;=\sum_{1\leq i&lt;j\leq n}x_ix_j+\sum_{1\leq j&lt;i\leq n}x_ix_j\\
&amp;=\sum_{1\leq j&lt;i\leq n}x_jx_i+\sum_{1\leq j&lt;i\leq n}x_ix_j\\
&amp;=2\sum_{1\leq j&lt;i\leq n}x_ix_j\\
&amp;\color{blue}{=2\sum_{i=2}^n\sum_{j=1}^{i-1}x_ix_j}\\
\end{align*}</p>
</blockquote>

<p><em>Hint:</em> In the expression $\sum_{i=1}^n\sum_{j&lt;i}x_ix_j$ is the inner sum with entry  $i=1$ an <em>empty sum</em> equal to zero.</p>
"
"2379910","2379926","<p>You're on the right track.  I would do two additional things.</p>

<p>1)  When you take natural logarithm of complex numbers an additive constant of $i2k\pi$ comes in, as the logarithm is multivalued.  Thus</p>

<p>$iz=\ln (2\pm \sqrt 3)+i2k\pi$</p>

<p>2)  Multiply by $-i$ to isolate $z$.  Note that the numbers $\ln (2\pm \sqrt 3)$ are negatives of each other:</p>

<p>$z=-i \ln (2\pm \sqrt 3)+2k\pi=i\ln (2\mp \sqrt 3)+2k\pi$</p>
"
"2379915","2379999","<p>It is the other way around. If there are more columns that rows, say $m$ and $n$ with $m&gt;n$ then the columns of the matrix $v_{1},...,v_{m}\in\mathbb{R}^{n}$ must be linearly dependent. </p>

<p>This is called the exchange lemma, that if $v_{1},...,v_{m}\in V$ and the dimension of $V$ is less than $m$ then $v_{1},...,v_{m}$ is linearly dependent. </p>

<p>The proof sketch is that we assume $v_{1},...,v_{m}$ is linearly independent and so we can take $v_{1},...,v_{n}$ as a basis where $n&lt;m$ is the dimension of the space. Then $v_{n+1}$ can be expressed as a linear combination of $v_{1},...,v_{n}$ which is a contradiction. </p>
"
"2379923","2380414","<p>Suppose $C$ is closed in $X$, this means that $U= X\setminus C$ is open in $X$.
This means that for all $i$, $X_i \cap U$ is open in $X_i$ or $X_i \setminus (X_i \cap U) = (C \cap X_i)$ is closed in $X_i$</p>

<p>So the closed sets of $X$ have an analogous description as the open sets of $X$. As the $X_i$ are closed, this is convenient: in all spaces $X$: if $A$ is closed in $B$ and $B$ is closed in $X$, then $A$ is closed in $X$.</p>

<p>$C \subseteq X_i$ is closed in $X_i$, then $C$ is closed in $X_{i+1}$ by the above fact and $X_i$ being closed in $X_{i+1}$ for all $i$. By induction we show that $C$ is closed in $X_j$ for all $j &gt; i$. And by definition $C \cap X_m$ is closed in $X_m$ for all $m &lt; i$ as well. It follows that $C$ is closed in $X$. And the reverse holds by the first paragraph. So the closed sets in $X_i$ are just the closed sets in $X$ intersected with $X_i$. This shows a). </p>

<p>In your proof, for $U_i \in \tau_i$ you claim that $U_i$ is open in all $X_j$; this does not always hold (e.g. $(0,1]$ is open in $[0,1]$ but not in $[0,2]$ nor $[0,n]$). This is the reason I went for closed sets, as there we do have ""closed in closed is closed"".</p>

<p>As to b): If $O \subseteq Y$ is open $f^{-1}[O] \cap X_i = f|_{X_i}^{-1}[O]$ is open in $X_i$ as $f|_{X_i}: X_i \to Y$ is continuous. So $f^{-1}[O]$ is open in $X$.  </p>

<p>In your proof $U_i$ need not be open in $X$ (being relatively open in a closed subset, e.g. $(0,1]$ is open in $[0,1]$ but not in the reals).</p>

<p>c) looks OK, though the hint is I think not needed;  choose $f_0:X_0 \to [0,1]$ such that $f_0[A \cap X_0] = \{0\}$ and $f_0[B \cap X_0] = \{1\}$. Then given $f_i: X_i \to [0,1]$ continuous with $f_i[A \cap X_i] = \{0\}$ and $f_i[A \cap X_i] = \{1\}$, and such that $f_i|_{X_{i-1}} = f_{i-1}$ for $i \ge 1$, then define $g : (A \cap X_{i+1}) \cup (B \cap X_{i+1}) \cup X_i$ by setting it to $0$ on the first set, $1$ on the second and $f_i$ on the third. This is a consistent definition, and so the pasting lemma for finitely many closed sets (as all these sets are closed in $X_{i+1}$) says that $g$ is continuous on its (closed) domain, and then its Tietze-extension $f_{i+1}: X_{i+1} \to [0,1]$ is as required for $i+1$. This concludes the recursive construction of the $f_i$. Then $f(x) = f_i(x)$ for $x \in X_i$ is well-defined (as every $f_i$ restricted to a lower-indexed $X_j$ is $f_j$ again, so all is consistent.) and continuous by b) and clearly $f[A]  =\{0\}$ and $f[B] = \{1\}$. Then finish as in your proof. </p>

<p>As to your proof: why is $A \cup B \cup X_i$ normal, so that Tietze applies? $A, B$ possibly live in all $X_i$? All we know is that $X_i$ is normal.. You just seem to copy the hint without critique or argument. </p>
"
"2379934","2379948","<p>The second one is true because $\lim_{n\to(-\infty)}10^n=0$, so we get 
$$\lim_{n\to(-\infty)}\bigg(\frac{2\times10^n-1}{2\times10^n-3}\bigg)^{10^n-1}=\bigg(\frac{-1}{-3}\bigg)^{-1}=3.$$
For the first one, note that
$$\lim_{m\to\infty}\bigg(\frac{2m+1}{2m-1}\bigg)^m=\lim_{m\to\infty}\bigg(1+\frac{1}{m-1/2}\bigg)^{m-1/2}\bigg(1+\frac{1}{m-1/2}\bigg)^{1/2}.$$
The second bracket goes to $1$ and the first to $e$. Since your sequence is just a subsequence of this one, it has the same limit.</p>
"
"2379947","2379961","<p>Suppose that it is not true that there is a constant $\theta&lt;1$ such that
$|\langle x_n|y_n\rangle|\leq\theta &lt;1 $ for all $n$ sufficiently large. Therfore, for every positive integer $k$, the number $\theta=1-1/k$ does not satisfy
$|\langle x_n|y_n\rangle|\leq 1-1/k $ for all $n$ sufficiently large, which means that it is not true that there exists an $N$ such that for all $n&gt;N$ this inequality is valid. Therefore, there exists some $n_k$ such that 
$1\geq |\langle x_{n_k}|y_{n_k}\rangle|&gt; 1-1/k $ where the l.h.s inequality from from the fact that $x_n,y_n$ are unit vectors, and in fact we can always choose $n_{k+1}&gt;n_k$ (why?). It follows that
$$\lim_{k\to\infty}|\langle x_{n_k}|y_{n_k}\rangle|=1$$
as was declared.</p>
"
"2379959","2380589","<p>The rules can be applied in any order. The first three rules are mutually exclusive, while the last rule corresponds corresponds to evaluating the <code>let</code>s from outside in. Obviously, the last rule trivially overlaps with all the others via the first context, <code>[ ]</code>, but this is unimportant. Using the second context <code>let x = [ ] in t</code> does overlap non-trivially with the third rule and corresponds to the decision between un-nesting a <code>let</code> and then evaluating outside in, or evaluating a nested <code>let</code> outside in and un-nesting it later. It's relatively easy to see that this ambiguity makes no difference. The last context <code>let x = v in e</code> looks like it might overlap with the first rule, but there are no reduction rules for <code>x y</code>, so whenever the first rule applies the fourth rule does not (except via the trivial context).</p>

<p>$(\lambda f.f x x x)$ is not a term in this calculus. The term that would correspond to this is $(\lambda f.\mathsf{let}\ f_1 = f x\ \mathsf{in\ let}\ f_2 = f_1 x\ \mathsf{in}\ f_2 x)$. (Or you could consider various ways of nesting the <code>let</code>s; it will just be undone by the second evaluation rule.) This is what it means to be in <a href=""https://en.wikipedia.org/wiki/A-normal_form"" rel=""nofollow noreferrer"">ANF (administrative normal form or A-normal form)</a> which is mentioned by only very briefly explained: ""That is, every intermediate value is abstracted out in a let binding."" However, it doesn't need to be explained because the syntax for terms enforces this.</p>

<p>When the syntax description states ""$x$, $y$, $z$ <strong>Variable</strong>"" it means the meta-variables $x$, $y$, and $z$ (and variations like $x'$ or $y_1$) represent variables in the syntax. Similarly, $v$ and variations always represent a value, and $s$, $t$, and $u$ always represent a term. This means that the rule <code>let x = y in t -&gt; [x:=y]t</code> only applies if $y$ is a variable. That rule does not allow, say, <code>let x = y z in t -&gt; [x:=(y z)]t</code>. This is the formalization of the comment by the authors that ""[r]eduction uses only variable/variable renamings instead of full substitution."" Similarly, ANF is enforced because in the syntax of terms the only applications allowed are variables applied to variables (i.e. $x\, y$). $(x\, y)\, z$ is not a term. If such expression were intended to be allowed, the syntax would have had something like $s\, t$.</p>

<p>So, it should be mostly clear by now, but the answer to your last question is that there are no ""implicit"" rules. Arbitrary lambda terms are not terms in this syntax, but they can be encoded into terms via translation to ANF which simply involves <code>let</code> binding all subterms so that all applications are variables applied to variables. This translation can easily be formalized: $$\begin{align}
\mathcal{A}(x) &amp; = x \\
\mathcal{A}(\lambda x.M) &amp; = \lambda x.\mathcal{A}(M) \\
\mathcal{A}(M N) &amp; = \mathsf{let}\ x_1 = \mathcal{A}(M)\ \mathsf{in\ let}\ x_2 = \mathcal{A}(N)\ \mathsf{in}\ x_1\, x_2 \\
&amp; \text{(where }x_1\text{ and }x_2\text{ are fresh)}
\end{align}$$</p>
"
"2379983","2379988","<p>You are right. In general, when the order of the numerator and the denominator are equal, as it is in your example, it is useful to check the behaviour of the function along lines of the form $y=kx$ for various values of $k$. For example, if you take $y=2x$ you get for $x\neq 0$,
$$f(x,2x)=\frac{2x^2(x^2-4x^2)}{x^4+16x^4}=-\frac{6}{17}$$
but if you take $y=x$ you get $f(x,x)=0$, so the limit $\lim_{(x,y)\to (0,0)}f(x,y)$ does not exist, as it has different limits along different lines approaching zero.</p>
"
"2379989","2379998","<p>It is better to veiw this in a group theoretic sense. </p>

<p>Then it is easy to see that $-1=\varepsilon ^ {\dfrac{q-1}{2}}$. </p>

<hr>

<hr>

<p>If $q-1 \overset{4}{\equiv} 0  \ , $ then $a=\varepsilon ^ {\dfrac{q-1}{4}}$ satisfies the relation $a^2=-1$.</p>

<hr>

<hr>

<p>Now consider the case $q-1 \overset{4}{\equiv} 2  \ , $  suppose on contrary that there is $a$ such that $a^2=-1$, 
then $ord(a)=4$ and we can conclude that $4 \mid q-1$, which is a contradiction.</p>

<p>Also in this case we can easilly chack that $b=\varepsilon ^ {\dfrac{q-3}{4}}$ satisfies the relation $-1=b^2 \varepsilon$. </p>

<p>[Because we have $\dfrac{q-1}{2}=2*\dfrac{q-3}{4}+1$. ] </p>

<hr>

<hr>

<hr>

<hr>

<hr>

<hr>

<p><strong>It is better to veiw this in a group theoretic sense.</strong></p>

<p>Let $G$ to be a finite cyclic multiplicative group with the element $-1$; 
i.e. an element of order $2$, 
so by lagrange theorem the order of the group is even. 
Let's denote the generator of $G$ by $\varepsilon$. 
Also let's call the order of $G$ to be $N$. It is easy to see that $-1=\varepsilon ^ {\dfrac{N}{2}}$.</p>

<hr>

<hr>

<p>If $N \overset{4}{\equiv} 0  \ , $ then $a=\varepsilon ^ {\dfrac{N}{4}}$ satisfies the relation $a^2=-1$.</p>

<hr>

<hr>

<p>Now consider the case $N \overset{4}{\equiv} 2  \ , $  suppose on contrary that there is $a$ such that $a^2=-1$, 
then $ord(a)=4$ and we can conclude that $4 \mid N$, which is a contradiction.</p>

<p>Also in this case we can easilly chack that $b=\varepsilon ^ {\dfrac{N-2}{4}}$ satisfies the relation $-1=b^2 \varepsilon$. </p>

<p>[Because we have $\dfrac{N}{2}=2*\dfrac{N-2}{4}+1$. ] </p>
"
"2379991","2379997","<p>The identity $\sqrt[n]{x} = x^{1/n}$ is only valid for <em>nonnegative</em> $x$, exactly because otherwise you run into problems like the one you have here.</p>

<p>In fact, even <em>definining</em> $x^a$ with $a\notin \Bbb Z$ is tricky for negative bases, so comparing it to some root of $x$, which may or may not exist, remains a dubious prospect.</p>
"
"2379992","2380553","<p>Let $\tau_h\colon\mathbb L^p\to\mathbb L^p$ be defined by $\tau_h\left(f\right)(x)=f\left(x+h\right)$. We have to show that $\left\lVert \tau_h(f)-f\right\rVert_p$ goes as $h$ goes to zero, which is done <a href=""https://math.stackexchange.com/questions/842937/show-that-lim-r-to-0-t-rf%E2%88%92f-l-p-0"">here</a>.               </p>
"
"2380004","2380011","<p>The discriminant of $$2x^2+(k+2)x$$ is equal to $$(k+2)^2$$ which is always positive. This is why the equation $$2x^2+(k+2)x = 0$$ always has at least one solution (i.e., $x=0$), and if $k\neq -2$, then it has two distinct solutions (the other one being $-\frac{2}{k+2}$)</p>
"
"2380010","2383829","<p>I don't think equation (1) is equivalent to (2). What they do in that paper is introducing equation (2), which has an additional unknown, the function $\psi$. The advantage in doing so is that equation (2) maybe simpler to solve or simpler to study numerically. The reason why they consider the parameter $c$ in the hyperbolic choice (3) is that when $c\to\infty$ equation (2) converges to equation (1), and so solutions of (3) should converge to solutions of (1).
This is what I think they mean with the sentence ""We try to choose $\mathcal{D}$ and the initial and boundary conditions for $\psi$ in such a way that a numerical approximation to (4), (5) is a good approximation to the original system (1c), (2)"". 
If you look at the reference [35] on Proposition 2 in the last page, they consider a corrector $\mathcal{D}$ of the form $\varepsilon \partial_t p$ and then prove that as $\varepsilon\to 0$ solutions of the $\varepsilon$ problem converge to solutions of the original problem.</p>
"
"2380030","2380074","<p>Let $S$ be the set of all primes dividing $n$. Then we can expand</p>

<p>$$\prod_{p \mid n} \biggl( 1 + \frac{1}{p-1}\biggr) = \prod_{p \in S}\biggl(1 + \frac{1}{p-1}\biggr) = \sum_{A \subset S}\Biggl( \prod_{p \in A} \frac{1}{p-1}\Biggr).\tag{1}$$</p>

<p>For $A \subset S$, we have $\prod_{p \in A} (p-1) = \phi\bigl(\prod_{p \in A} p\bigr)$ and thus</p>

<p>$$\sum_{A \subset S} \Biggl(\prod_{p\in A} \frac{1}{p-1}\Biggr) = \sum_{A \subset S} \frac{1}{\varphi\bigl(\prod_{p \in A} p\bigr)}.\tag{2}$$</p>

<p>The products of distinct prime divisors of $n$ are just the squarefree divisors of $n$, and the squarefree divisors of $n$ are in a natural bijection to the subsets of $S$ (namely, $A$ corresponds to $\prod_{p \in A} p$), thus we can write the right hand side of $(2)$ as</p>

<p>$$\sum_{\substack{d \mid n \\ d \text{ squarefree}}} \frac{1}{\phi(d)}.\tag{3}$$</p>

<p>Furthermore, $\mu^2(d)$ is $1$ for squarefree $d$ and $0$ for $d$ divisible by the square of a prime, so we can use $\mu^2(d)$ to sieve out the non-squarefree divisors and write $(3)$ as</p>

<p>$$\sum_{d \mid n} \frac{\mu^2(d)}{\phi(d)}.$$</p>
"
"2380033","2380157","<p>The result will follow from: If $g_n$ is bounded in $L^2([0,1])$ and $g_n\to 0$ a.e. on $[0,1],$ then $g_n\to 0$ in $L^1.$ Hint for the proof of this: Egorov's theorem.</p>

<p>Note that this result fails on $[0,\infty)$ as the sequence $g_n = \chi_{[n,n+1]}$ shows. So finite measure is important</p>
"
"2380034","2381238","<p>Thinking again about this question I am not anymore sure if there is a direct argument via projections. For the argument in your reference question I was thinking that infiniteness of a projection is invariant under Murray von Neumann equivalence. If I will find an argument involving this this idea, I will post it here.</p>

<p>However, using scaling elements (V.2.2.8) one can prove this result quite fast. </p>

<p>By V.2.3.6 for a simple C*-algebra containing an infinite projection is equivalent to containing a scaling element. However, if $B \otimes \mathbb K$ contains a scaling element, then so does $B \otimes M_n$ for some $n$.</p>
"
"2380035","2380058","<p>Consider the following function
$$ f: S^1 \times [-1,1] \to \mathbb R^3\,,\quad
f(\theta,z) = (z \cos \theta, z \sin \theta, z)\,,$$
that maps a cylinder into $\mathbb R^3$. If needed, we can extend it to a map from the torus into $\mathbb R^3$.</p>

<p>I would claim that it is not possible to approximate $f$ by an immersion in the $C^1$-norm, because $f$ is orientation-reversing for $z &lt; 0$ and orientation-preserving for $z &gt; 0$, while an immersion could be only one of those.</p>
"
"2380050","2380154","<p>Since (writing $d$ for $d(n)$)</p>

<p>$$d\cdot 10^{d-1} \leqslant k &lt; d\cdot 10^d,$$</p>

<p>you have</p>

<p>$$d-1 + \log_{10} d \leqslant \log_{10} k &lt; d + \log_{10} d.$$</p>

<p>That gives a fairly small range of possible values for $d$ starting with $d \approx \log_{10} k$ (the range is $\approx \log_{10} \log_{10} k$). Once $d$ is determined, one has</p>

<p>$$n = \frac{k}{d}.$$</p>
"
"2380054","2380073","<p>Notice that $V(\alpha)$ is quadratic in $\alpha$ - you have an $\alpha^{2}$ term, a $(1-\alpha)^{2}=\alpha^{2}-2\alpha+1$ term, and a $\alpha(1-\alpha)=\alpha-\alpha^{2}$ term. So you can complete the square, then set the squared term to zero. However, this will only give you a minimum if the squared term's coefficient is positive.</p>

<p>You can also solve it via calculus, like DMcMor did. In general calculus is the way to go, but for quadratics completing the square is a nice alternative.</p>

<p>Answer in spoilers:</p>

<blockquote class=""spoiler"">
  <p> $V(\alpha) = (x_{1}+x_{2}-2x_{3})\alpha^{2} - (2x_{2}-2x_{3})\alpha + x_{2} = a(\alpha - b)^{2} + c$ where $a = x_{1}+x_{2}-2x_{3}$, $b = (x_{2}-x_{3})/a$ and $c = x_{2} - ab^{2} = (x_{1}x_{2} - x_{3}^{2})/a$. Only if $a&gt;0$ will you actually get a minimum, in which case you have $\alpha = b$ and $V(\alpha_{min})=c$.</p>
</blockquote>

<p>EDIT:</p>

<p>Since OP has stated $\alpha$ is restricted to the interval $[0,1]$ (correct me if the interval is not closed), it might be the case that the minimum obtained by either this method or calculus will lie outside that interval. In this case, one would pick the value that is as close as possible to that minimum - say you got $1.5$, you would choose $\alpha=1$. If you got $-0.5$, you'd pick $\alpha=0$. You can do this because a quadratic is monotonic either side of its turning point. In fact, if you don't find any turning points in the allowed interval, the function must be monotonic if it is continuous, so the minimum in that interval must be one of the two endpoints (and it's usually simple to determine which).</p>
"
"2380056","2380061","<p>The <strong>sum</strong> of two subspaces of a vector space $V_1,V_2\subseteq V$ (denoted as $V_1+V_2$ is defined as</p>

<p>$$V_1 + V_2 = \{v_1 + v_2| v_1\in V_1\land v_2\in V_2\}$$</p>

<p>and it is a vector subspace of $V$.</p>

<hr>

<p>The <strong>direct sum</strong> of two vector spaces $V_1, V_2$, (denoted as $V_1\oplus V_2$) is a vector space $V_1\times V_2$ with operations defined as you wrote.</p>

<hr>

<p>The <strong>connection</strong> between a direct and ordinary sum is that if $V_1, V_2\subset V$ and $V_1$ and $V_2$ are linearly independent, then $V_1+V_2$ is <strong>isomorphic</strong> to $V_1\oplus V_2$.</p>
"
"2380076","2380236","<p>Is $\quad d\theta = (-3Y dX + 2 X dY)/( 2g_2 X+3g_3)\quad$ nice enough?</p>
"
"2380077","2380090","<p>You could use the <a href=""http://mathworld.wolfram.com/LawofCosines.html"" rel=""nofollow noreferrer"">Law of Cosines</a> (which you might think of as a super-powered version of the Pythagorean Theorem, which allows you to work with triangles that are not right triangles).  It states that in any triangle with angles $a,b,c$ opposite sides of length $A,B,C$ (respectively), we have
$$ A^2 = B^2 + C^2 - 2AB \cos(a). $$
In your problem, we know the three sides, and so we get
$$ L^2 = r^2 + (r+L_0)^2 - 2r(r+L_0) \cos(\theta)
\implies \cos(\theta) = \frac{L^2 - r^2 - (r+L_0)^2}{2r(r+L_0)}.$$
Therefore, up to a choice of quadrant,
$$ \theta = \arccos\left( \frac{L^2 - r^2 - (r+L_0)^2}{2r(r+L_0)} \right). $$</p>
"
"2380081","2380224","<p>As has been discussed in the comments, the key observation is that for any $\alpha&lt;\omega^{\beta_1}$, $\alpha+\omega^{\beta_1}=\omega^{\beta_1}$.  It follows that \begin{align*}
\beta_0\cdot 2 &amp;= \omega^{\beta_1}\cdot m_1+(\omega^{\beta_2}\cdot m_2+\dots+\omega^{\beta_k}\cdot m_k+\omega^{\beta_1})+\omega^{\beta_1}\cdot(m_1-1)+\omega^{\beta_2}\cdot m_2+\dots+\omega^{\beta_k}\cdot m_k \\
&amp;= \omega^{\beta_1}\cdot m_1+\omega^{\beta_1}+\omega^{\beta_1}\cdot(m_1-1)+\omega^{\beta_2}\cdot m_2+\dots+\omega^{\beta_k}\cdot m_k \\
&amp;= \omega^{\beta_1}\cdot m_1\cdot 2+\omega^{\beta_2}\cdot m_2+\dots+\omega^{\beta_k}\cdot m_k.
\end{align*}</p>

<p>In the same way, we can prove by induction on $\alpha$ that $$\beta_0\cdot\alpha=\omega^{\beta_1}\cdot m_1\cdot\alpha+\omega^{\beta_2}\cdot m_2+\dots+\omega^{\beta_k}\cdot m_k$$ if $\alpha$ is a successor and $$\beta_0\cdot\alpha=\omega^{\beta_1}\cdot m_1\cdot\alpha$$ if $\alpha$ is a limit.  In particular, taking $\alpha=\omega^{\gamma_0}$, we find $$\beta_0\cdot\omega^{\gamma_0}=\omega^{\beta_1}\cdot m_1\cdot \omega^{\gamma_0}=\omega^{\beta_1}\cdot\omega^{\gamma_0}$$ (the latter equality is because $m_1\cdot \omega^{\gamma_0}=\omega^{\gamma_0}$).</p>
"
"2380082","2380120","<p>Notice that for sufficiently large $n\in \mathbb{N}$, we have that $$1= \lvert\lvert{u_{n}\rvert\rvert}^{2} = \lvert {\alpha_{n}\rvert}^{2} + \lvert{\beta_{n}\rvert}^{2} + 2 \Re\left(\alpha_{n}\bar{\beta_{n}}\langle x_{n}, \, y_{n} \rangle\right) \geq \lvert {\alpha_{n}\rvert}^{2} + \lvert{\beta_{n}\rvert}^{2} - 2\lvert\alpha_{n}\rvert \lvert \beta_{n} \rvert \theta = $$ $$\left(\lvert \alpha_{n} \rvert - \lvert \beta_{n} \rvert\right)^{2} + 2 \lvert\alpha_{n}\rvert \lvert \beta_{n} \rvert\left(1-\theta\right) \geq 2 \lvert\alpha_{n}\rvert \lvert \beta_{n} \rvert\left(1-\theta\right)\geq 0$$</p>

<p>Thus shows that the $\alpha_{n}$'s and the $\beta_{n}$'s must be bounded. </p>

<p>Inspired by DominikS, we have that 
$$ 1 \geq \lvert {\alpha_{n}\rvert}^{2} + \lvert{\beta_{n}\rvert}^{2} - 2\lvert\alpha_{n}\rvert \lvert \beta_{n} \rvert \theta = (1-\theta^{2})\lvert{\alpha_{n}\rvert}^{2} + \left( \lvert \beta_{n} \rvert - \theta \lvert \alpha_{n} \rvert \right)^{2} \geq (1-\theta^{2})\lvert{\alpha_{n}\rvert}^{2}$$</p>

<p>By symmetry you can adapt the same argument to the $\beta_{n}$'s.</p>
"
"2380083","2380123","<p>according to your variables $u=x^2+y^2$ and $v=x$ you have a simplified equation:
$$y\frac{\partial f}{\partial v}=0$$
Since this equation must always hold (for any arbitrary value of $y$) the derivative of $f$ wrt. $v$ must be zero, and therefore $f$ must be a function of $u$ only:
$$f=h(u) = h(x^2+y^2)$$</p>

<p>Another approach is:</p>

<p>You can put your equation for $f(x,y)$:
$$ y\frac{\partial f(x,y)}{\partial x}-x\frac{\partial f(x,y)}{\partial y}=0\tag{*}$$
 as the total derivative of $f$ wrt. some parameter $t$ supposing:
$$x=x(t) \qquad y=y(t)$$
 equated to $0$. This is:
$$\frac{df(x(t),y(t))}{dt} = \frac{\partial f}{\partial x}\frac{dx}{dt}+\frac{\partial f}{\partial y}\frac{dy}{dt}=0\tag{**}$$</p>

<p>You can immediately see that equations $(*)$ and $(**)$ have a lot in common, and they are identical iff:
$$\frac{dx}{dt} = y \qquad \frac{dy}{dt}=-x$$ 
The latter equations can be multiplied by $x$ and $y$ respectively and adding them toguether gives:
$$ x\frac{dx}{dt}+y\frac{dy}{dt}=\frac{d}{dt}\left(x^2+y^2\right) = 0$$ </p>

<p>Therefore the taryectories described by curves on $xoy$ plane such as $x^2+y^2$ have a constant $f$ throughout the motion. Therefore, by equation $(**)$ the function $f$ must be a function of $x^2+y^2$, namely $f=h(x^2+y^2)$ since</p>

<p>$$\frac{df(x,y)}{dt} = h'(x,y)\frac{d}{dt}\left(x^2+y^2\right)=0$$</p>
"
"2380084","2380097","<p>I am not sure that I understand you when you write âdo we need to checkâ. Assuming that $\operatorname{rank}(A)=n$, then $\operatorname{rank}(A|b)=n$ if and only if the system $Ax=b$ has a solution. Does this answer your question?</p>
"
"2380098","2380174","<p>Yes you can. Let assume we have a sequence of random real variables $(X_i)_{i \in \mathbb{N}}$ which converge on low to a random variable $Z$ and let assume we have a sequence $(a_i)_{i \in \mathbb{N}}$ which converge to a real value $a$. By Slutsky Lemma the sequence $(X_i-a_i)$ converge on low to $Z - a$ and so $P(0\le X_i - a_i)$ converge to $P(0 \le Z - a)$.</p>
"
"2380101","2380153","<p>The pattern is more obvious if you write in base $2$:</p>

<p>$$\begin{align}f(10011_2)&amp;=101_2\\
f(10100_2)&amp;=1010_2\\
f(10101_2)&amp;=10101_2\\
f(10110_2)&amp;=1010_2\\
f(10111_2)&amp;=101_2\\
...
\end{align}$$</p>

<p>Show, in general, the value of $f(n)$ is the binary digits of $n$, removing repeated digits.</p>

<p>So, since $2011= 11111011011_2$, you get $f(n)=10101_2$, and you need to find all $n&lt;2011$ which can be written by taking $10101$ and expanding any digit by repeating digits.</p>

<p>These can be written as $2^{a}-2^{b}+2^{c}-2^d+2^e-1$ where $a&gt;b&gt;c&gt;d&gt;e&gt; 0$.</p>

<p>For example, $2011=2^{11}-2^{6}+2^{5}-2^{3}+2^{2}-1$.</p>

<p>There are $\binom{10}{5}=252$ cases $a\leq 10$. The cases where $a=11$ is harder - you need $b\geq 6$, and then have to continue counting cases.</p>

<p>The full count is, I think:</p>

<p>$$\binom{11}{5}-\binom{6}{4}+\binom{5}{3}-\binom{3}{2}+\binom{2}{1}-1$$</p>

<p>Essentially, $\binom{11}{5}-\binom{6}{4}$ counts the cases where $a\leq 11$, but removing the cases where $a=11$ and $b\leq 6$. Then $\binom{5}{3}-\binom{3}{2}$ count the cases where $a=11,b=6$, excepting the cases where $c=5,d\leq 3$. And $\binom{2}{1}-1$ counts the cases where $a=11,b=6,c=5,d=3.$ and the resulting number is not $n$.</p>

<p>Note that the top parts of the binomials are $11,6,5,3,2$, which are the exponents of $2011=2^{11}-2^6+2^5-2^3+2^2-1$. That might give you an idea for the general formula, given an $m$ for the count of $n&lt; m$ with $f(n)=f(m)$.</p>

<p>Every natural number $m\neq 0$ can be written uniquely as $m=\sum_{i=0}^{2k-1} (-1)^{i+1}2^{a_i}$, where $k\geq 1$ and $0\leq a_0&lt;a_1&lt;\dots&lt;a_{2k-1}$. If $m$ is odd, then $a_0=0$, and the number of $n&lt;m$ such that $f(n)=f(m)$ is:</p>

<p>$$\sum_{i=0}^{2k-1}(-1)^{i+1}\binom{a_i}{i}$$</p>

<p>So, for example, if $m=17=2^{5}-2^{4}+2^1-2^0$, you get $$\binom{5}{3}-\binom{4}{2}+\binom{1}{1}-\binom{0}{0}=10-6+1-1=4.$$</p>

<p>And the $n&lt;m$ with $f(n)=f(m)$ are $101_2,1001_2,1011_2,1101_2$.</p>

<p>If $m$ is even, then $a_0&gt;0$ and you get:</p>

<p>$$\sum_{i=0}^{2k-1}(-1)^{i+1}\binom{a_i}{i+1}$$</p>

<p>For example, if $m=26=11010_2=2^5-2^3+2^2-2^1$ then:</p>

<p>$$\binom{5}{4}-\binom{3}{3}+\binom{2}{2}-\binom{1}{1}=4$$</p>

<p>And there are $1010_2,10100_2,10110_2,10110_2$.</p>
"
"2380103","2380119","<p>If both $A$ and $B$ are equipped with the discrete topology, the the product topology on $A\times B$ is again the discrete topology. This is so because if $a\in A$ and $b\in B$, then $\{(a,b)\}=\{a\}\times\{b\}$ and therefore $\{(a,b)\}$ is the cartesian product of two open sets. So, $\{(a,b)\}$ is an open set. Since all singletons are open sets, the topology is the discrete topology.</p>
"
"2380118","2380124","<p>Let $a $ be an arbitrary real.</p>

<p>for $x\ne a $,
$$|f (x)-f (a)|\le M|x-a|^\alpha $$
and</p>

<p>$$\frac {|f (x)-f (a)|}{|x-a|}\le M |x-a|^\beta $$
with $$\beta =\alpha-1&gt;0$$</p>

<p>thus if $x $ goes to $a ,$</p>

<p>$f (x)-f (a) $ goes to zero and</p>

<p>$\frac {f (x)-f (a)}{x-a} $ goes to zero.</p>

<p>this means that $f $ is continuous at $a $ and differentiable at $a $ with $$f'(a)=0$$</p>

<p>You can finish.</p>
"
"2380129","2380184","<p>The set $\mathcal{P}_1$ is not closed. For $x&gt;0$ define $\mu_x=(1-p)\delta_0+p\delta_x$ where $p=c/x^4$. Then $\mu_x\in\mathcal{P}_1$ but 
$\mu_x\to\delta_0$ as $x\to\infty$ and $\delta_0\notin \mathcal{P}_1$. </p>
"
"2380131","2380168","<p>First, enforcing the substitution $\theta=\arctan(x)$ reveals </p>

<p>$$\int_0^{\pi/2}\frac{1}{\left(\cos^3(\theta)+\sin^3(\theta)\right)^{2/3}}\,d\theta=\int_0^\infty \frac{1}{\left(1+x^3\right)^{2/3}}\,dx$$</p>

<p>Next, we let $y=x^3$ to obtain</p>

<p>$$\begin{align}
\int_0^\infty \frac{1}{\left(1+x^3\right)^{2/3}}\,dx&amp;=\frac13\int_0^\infty \frac{y^{-2/3}}{(1+y)^{2/3}}\,dy\\\\
&amp;=\frac13 B\left(1/3,1/3\right)\tag 1\\\\
&amp;=\frac13 \frac{\Gamma^2(1/3)}{\Gamma(2/3)}\tag 2
\end{align}$$</p>

<p>where $B(x,y)$ is the Beta function and $\Gamma(x)$ is the Gamma function.  In going from $(1)$ to $(2)$ we used the relationship $B(x,y)=\frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}$.</p>
"
"2380133","2380149","<p>First off, I think you mean $\mathbb{P}(\Omega) \cup \{\emptyset\}$ in your last line, as $\mathbb{P}(\Omega) \cup \emptyset = \mathbb{P}(\Omega)$.  But that is just a minor nitpick.</p>

<p>With regard to your question, recall that $\sigma(\mathbb{C})$ is the smallest $\sigma$-field that contains $\mathbb{C}$ (note that $\sigma(\mathbb{C}) \supseteq \mathbb{C}$, and not the other way around, as you wrote).  Thus it has to contain all of the sets of $\mathbb{C}$, and just enough extra sets to ensure that it is closed under complements and unions, and includes all of $\Omega$.</p>

<p>This is a relatively small example, so you should be able to brute force it.  Starting with $\mathbb{C}$, you need to throw in other sets so that you get the properties that you want (closure under unions, complements, etc).  So you might proceed as follows:</p>

<ol>
<li>You know that $\mathbb{C}$ has to be in $\sigma(\mathbb{C})$, so start with that.  So far, we have $\{ \{2,4\}, \{6\} \}$.</li>
<li>You require $\Omega \in \sigma(\mathbb{C})$, so throw that in: $\{ \{2,4\}, \{6\}, \Omega \}$.</li>
<li>Now, $\sigma(\mathbb{C})$ has to be closed under complements, so include the complements of the three sets that are in there:  $\{ \{2,4\}, \{1,3,5,6\}, \{6\}, \{1,2,3,4,5\}, \Omega, \emptyset \}$.</li>
<li>Next, we need to make sure that we have closure under unions, which means that we need to throw in unions of any sets.  So... $\{2,4\} \cup \{1,3,5,6\}$ is already in the collection; $\{2,4\} \cup \{6\} = \{2,4,6\}$, and $\{1,3,5\} \cup \{6\} = \{1,3,5,6\}$ is already in the collection.  No other pairwise unions give us anything new, nor do unions including more sets.  So now we have the collection $\{ \{2,4\}, \{1,3,5,6\}, \{6\}, \{1,2,3,4,5\}, \Omega, \emptyset, \{2,4,6\} \}$.</li>
<li>We introduced a new set, so we ned to include its complement.  Throwing that in, we get $\{ \{2,4\}, \{1,3,5,6\}, \{6\}, \{1,2,3,4,5\}, \Omega, \emptyset, \{2,4,6\}, \{1,3,5\} \}$.</li>
<li>Again, check all unions.  In this case, taking the union of the new set $\{1,3,5\}$ with any other set in our collection gives us nothing new.  Therefore the collection in step 5 is closed under unions.  Since step 5 ensured that it was closed under complements as well, and step 2 included the entire space, this collection gets the job done.</li>
</ol>

<p>Therefore
$$ \sigma(\mathbb{C})
= \{ \emptyset, \{6\}, \{2,4\}, \{1,3,5\}, \{2,4,5\}, \{1,3,5,6\}, \{1,2,3,4,5\}, \Omega \} $$
Note that this is not the powerset of $\Omega$, as (for example) none of the singleton sets other than $\{6\}$ are included.</p>
"
"2380137","2380514","<p>It would be helpful to sketch a proof for the case $N \ge 3$ first. </p>

<ul>
<li>trace map $W^{1,2}(\Omega) \to W^{1/2,2}(\partial \Omega)$ is bounded,</li>
<li>inclusion $W^{1/2,2}(\partial \Omega) \to L^1(\partial \Omega)$ is compact, </li>
<li>inclusion $W^{1/2,2}(\partial \Omega) \to L^{q^*}(\partial \Omega)$ with $q^* = \frac{2(N-1)}{N-2}$ is bounded. </li>
</ul>

<p>By interpolation, the inclusion $W^{1/2,2}(\partial \Omega) \to L^{q^*}(\partial \Omega)$ is compact for all $q \in [1,q^*)$, and the claim follows. </p>

<p>The only modification needed in the case $N=2$ is that this time $W^{1/2,2}(\partial \Omega) \to L^{q}(\partial \Omega)$ is bounded for all $q&lt;q^*=\infty$, but not for $q=q^*$. This is because the Sobolev embedding doesn't work if $\textrm{order of derivatives} \times \textrm{exponent} = \textrm{dimension}$. The claim follows in the same fashion. </p>

<p>If you need a reference, you can find these embeddings in <a href=""https://arxiv.org/abs/1104.4345"" rel=""nofollow noreferrer"">Hitchhiker's guide to the fractional Sobolev spaces</a>. </p>
"
"2380138","2380145","<p><strong>Hint:</strong> If we set</p>

<p>$$a_n := \frac{1}{n} \sum_{k=0}^{n-1} f(T^k x)$$</p>

<p>then 
$$\frac{1}{n} f(T^n x) = \frac{n+1}{n} a_{n+1} - a_{n} = (a_{n+1}-a_{n}) + \left(\frac{n+1}{n}-1 \right) a_{n+1}$$</p>

<p>Now use the convergence of the sequence $a_n$ to show that the right-hand side converges to $0$ as $n \to \infty$.</p>
"
"2380147","2380163","<p>$\sin{y}=3\sin{x}$ and $\cos{y}=2\cos{x}$.</p>

<p>Thus,
$$1=\sin^2y+\cos^2y=9\sin^2x+4\cos^2x=4+5\sin^2x,$$
which is impossible.</p>

<p>If $\sin{y}=\frac{1}{3}\sin{x}$ we obtain:
$$1=\sin^2y+\cos^2y=\frac{1}{9}\sin^2x+4\cos^2x=\frac{1}{9}+\frac{35}{9}\sin^2x.$$
Now, you can get $\sin^2x$, $\sin^2y$ and the rest for you. </p>
"
"2380151","2381206","<p>Regarding Question 1: ""assigns no mass to the origin"" means $\nu(\{0\})=0$. Similarly, ""$\nu$ assigns finite mass to a certain interval $I$"" means $\nu(I)&lt;\infty$. So the assumptions on $\nu$ can be summarized as follows: $\nu$ has to satisfy $\nu(\{0\})=0$ and $\nu(\mathbb{R} \backslash [-1,1])&lt;\infty$ but $\nu([-1,1])$ might be infinite.</p>

<p>Regarding Question 2: The key point is that $M$ might assign mass to the origin, i.e. $M(\{0\})$ does not need to be $0$. Since</p>

<p>$$\lim_{x \to 0} \frac{e^{itx}-1-itx/(1+x^2)}{x^2} = \frac{1}{2} (itx)^2 = - \frac{1}{2} t^2$$</p>

<p>we have</p>

<p>$$\int_{\mathbb{R}} \left( e^{itx}-1-\frac{itx}{1+x^2} \right) \frac{dM(x)}{x^2} = - \frac{1}{2} t^2 M(\{0\}) + \int_{\mathbb{R} \backslash \{0\}} \left( e^{itx}-1-\frac{itx}{1+x^2} \right) \frac{dM(x)}{x^2}.$$</p>

<p>If we set $\sigma^2 := M(\{0\})$ and $d\nu(dx) := 1_{\mathbb{R} \backslash \{0\}}(x) dM(x)/x^2$, then we get the standard representation for $\phi$:</p>

<p>$$\phi(t) = \exp \left(ibt - \frac{1}{2} \sigma^2 t^2 + \int_{\mathbb{R}} \left( e^{itx}-1-\frac{itx}{1+x^2} \right) d\nu(x) \right).$$</p>

<p>(You can easily verify that $\nu$ has all the properties I explained in the first part of my answer.)</p>
"
"2380152","2380156","<p>When you write
$$ e^x = \sum_{k=0}^{\infty} \frac{x^k}{k!}, $$
the value of $x$ remains constant with respect to the summation.  Unfortunately, $k^2$ is not constant with respect to the summation, so you can't set $x=k^2$.</p>

<p>Try this:
$$ \sum_{k=1}^{\infty} \frac{k^2}{k!}
 = \sum_{k=1}^{\infty} \frac{k}{(k-1)!}
 = \sum_{k=0}^{\infty} \frac{k+1}{k!}
 = \sum_{k=0}^{\infty} \left[ \frac{k}{k!} + \frac{1}{k!}\right].
$$</p>
"
"2380161","2380288","<p>Actually $X$ is not well defined because it is not stated what value it takes if all $n$ experiments succeed.</p>

<p>Let me preassume then that in that case $X$ takes value $n$</p>

<p>So if indeed $X$ takes value $n$ then $K=0$ so that $P(K=0\mid X=n)=1$ and consequently $P(K=k\mid X=n)=0$ for $k\neq0$.</p>

<p>If $X$ takes value $n-1$ then $K=1$ so that $P(K=1\mid X=n-1)=1$ and consequently $P(K=k\mid X=n-1)=0$ for $k\neq1$.</p>

<p>If $X$ takes some value $x\in\{0,1,\dots,n-2\}$ then the $x+1$-th experiment is the first one that fails. After that $n-x-1$ experiments will follow so that $K-1$ has binomial distribution with parameters $n-x-1$ and $1-p$. </p>

<p>That leads to $P(K=k\mid X=x)=P(K-1=k-1\mid X=x)=\binom{n-x-1}{k-1}p^{n-x-1-k}(1-p)^k$.</p>

<p>This all shows that your approach $P(K=k,X=x)=P(K=k\mid X=x)P(X=x)$ might bear fruit.</p>
"
"2380169","2380840","<p>$\newcommand{\bbx}[1]{\,\bbox[15px,border:1px groove navy]{\displaystyle{#1}}\,}
 \newcommand{\braces}[1]{\left\lbrace\,{#1}\,\right\rbrace}
 \newcommand{\bracks}[1]{\left\lbrack\,{#1}\,\right\rbrack}
 \newcommand{\dd}{\mathrm{d}}
 \newcommand{\ds}[1]{\displaystyle{#1}}
 \newcommand{\expo}[1]{\,\mathrm{e}^{#1}\,}
 \newcommand{\ic}{\mathrm{i}}
 \newcommand{\mc}[1]{\mathcal{#1}}
 \newcommand{\mrm}[1]{\mathrm{#1}}
 \newcommand{\pars}[1]{\left(\,{#1}\,\right)}
 \newcommand{\partiald}[3][]{\frac{\partial^{#1} #2}{\partial #3^{#1}}}
 \newcommand{\root}[2][]{\,\sqrt[#1]{\,{#2}\,}\,}
 \newcommand{\totald}[3][]{\frac{\mathrm{d}^{#1} #2}{\mathrm{d} #3^{#1}}}
 \newcommand{\verts}[1]{\left\vert\,{#1}\,\right\vert}$
\begin{align}
&amp;\int_{0}^{\infty}{\sin\pars{x} \over x}\,\expo{\ic tx}\,\dd x =
\int_{-\infty}^{\infty}\bracks{x &gt; 0}{\sin\pars{x} \over x}\,\expo{\ic tx}
\,\dd x
\\[5mm] = &amp;\
\int_{-\infty}^{\infty}\bracks{\int_{-\infty}^{\infty}{\expo{\ic kx} \over
k - \ic 0^{+}}\,{\dd k \over 2\pi\ic}}
{\sin\pars{x} \over x}\,\expo{\ic tx}\,\dd x =
\int_{-\infty}^{\infty}{1 \over k - \ic 0^{+}}\int_{-\infty}^{\infty}
{\sin\pars{x} \over x}\,\expo{\ic\pars{k + t}x}\,\dd x\,{\dd k \over 2\pi\ic}
\\[5mm] = &amp;\
\int_{-\infty}^{\infty}{1 \over k - \ic 0^{+}}\int_{-\infty}^{\infty}
\pars{{1 \over 2}\int_{-1}^{1}\expo{-\ic qx}\,\dd q}
\,\expo{\ic\pars{k + t}x}\,\dd x\,{\dd k \over 2\pi\ic}
\\[5mm] = &amp;\
{1 \over 2}\int_{-\infty}^{\infty}{1 \over k - \ic 0^{+}}\int_{-1}^{1}
\int_{-\infty}^{\infty}
\expo{\ic\pars{k + t - q}x}\,\dd x\,\dd q\,{\dd k \over 2\pi\ic}
=
{1 \over 2}\int_{-\infty}^{\infty}{1 \over k - \ic 0^{+}}\int_{-1}^{1}
2\pi\,\delta\pars{k + t - q}\,\dd q\,{\dd k \over 2\pi\ic}
\\[5mm] = &amp;\
{1 \over 2\ic}\int_{-\infty}^{\infty}
{\bracks{-1 &lt; k + t &lt; 1} \over k - \ic 0^{+}}\,\dd k =
{1 \over 2\ic}\int_{-1 - t}^{1 - t}{\dd k \over k - \ic 0^{+}}
\\[5mm] = &amp;\
{1 \over 2\ic}\,\mrm{P.V.}\int_{-1 - t}^{1 - t}{\dd k \over k} +
{1 \over 2\ic}\,\ic\pi\bracks{-1 - t &lt; 0 &lt; 1 - t}
\\[5mm] &amp; =
-\,{\ic \over 2}\left\{%
\bracks{1 - t &lt; 0}\ln\pars{t - 1 \over t + 1} +
\bracks{-1 - t &lt; 0}\bracks{1 - t &gt; 0}\ln\pars{1 - t \over 1 + t}\right.
\\ &amp;\ 
\left.\phantom{-\,{\ic \over 2}\left\{\right.} +
\bracks{-1 - t &gt; 0}\ln\pars{1 - t \over - 1 - t}\right\} +
{\pi \over 2}\,\bracks{-1 &lt; t &lt; 1}
\\[5mm] &amp; =
-\,{\ic \over 2}\left\{%
\bracks{t &gt; 1}\ln\pars{t - 1 \over t + 1} +
\bracks{\verts{t} &lt; 1}\ln\pars{1 - t \over 1 + t}\right.
\\ &amp;\ 
\left.\phantom{-\,{\ic \over 2}\left\{\right.} +
\bracks{t &lt; - 1}\ln\pars{1 - t \over - 1 - t}\right\} +
{\pi \over 2}\,\bracks{-1 &lt; t &lt; 1}
\\[5mm] = &amp;\
\bbx{\bracks{\vphantom{\Large A}\verts{t} &lt; 1}{\pi \over 2} +
{1 \over 2}\bracks{\vphantom{\Large A}\verts{t} \not= 1}
\ln\pars{\verts{t + 1 \over t - 1}}\,\ic}
\end{align}
<a href=""https://i.stack.imgur.com/6cs0q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6cs0q.png"" alt=""enter image description here""></a></p>
"
"2380172","2380178","<p>The roots of a polynomial don't determine its coefficients. For example, $X+1$ and $0.5X+0.5$ have the same roots.</p>

<p>Nevertheless, the set of the numbers that are roots of some polynomial in $\Bbb Z[X]$ is important, and it has a name: the set of algebraic numbers. So we could say that a <em>necessary</em> condition on the roots of $P$ for $P\in\Bbb Z[X]$ is that the roots are algebraic numbers. But this gives no criterion or information. It's only a name. There are wide areas of algebra dedicated to the algebraic numbers, though.</p>
"
"2380183","2380448","<p>By C-S 
$$\sum_{cyc}bc\sum_{cyc}\frac{1}{a^2+2}\geq\left(\sum_{cyc}\sqrt{\frac{bc}{a^2+2}}\right)^2=\left(\sum_{cyc}\sqrt{\frac{1}{a^3+2a}}\right)^2.$$
Let $a=e^x$, $b=e^y$ and $c=e^z$.</p>

<p>Hence, $x+y+z=0$ and it's enough to prove that $\sum\limits_{cyc}f(x)\geq\sqrt3,$ where 
$$f(x)=\frac{1}{\sqrt{e^{3x}+2e^x}}.$$
But $$f''(x)=\frac{9e^{4x}-4e^{2x}+4}{4\sqrt{e^x(e^{2x}+2)^5}}&gt;0$$ and by Jensen
$$\sum\limits_{cyc}f(x)\geq3f\left(\frac{x+y+z}{3}\right)=3f(0)=\sqrt3$$
and we are done!</p>
"
"2380188","2381135","<p>Let me elaborate a bit more. </p>

<h2>Diagonal self-adjoint operators</h2>

<p>Let $\mathsf{H}$ be a (separable) Hilbert space with an orthonormal basis $(e_n)$. Take a non-empty compact set $K \subset \mathbb{R}$ and a sequence $(\lambda_n)$ of distinct points from $K$ such that the closure of the set $\{ \lambda_n \colon n \in \mathbb{N}\}$ is $K$. 
We construct a self-adjoint operator on $\mathsf{H}$ with spectrum $\sigma(A)=K$ by setting
$$Ax = \sum_{n \in \mathbb{N}}\lambda_n \left&lt;x, e_n\right&gt;e_n$$ 
for all $x \in \mathsf{H}$.</p>

<p>The function $E \colon \mathcal{B}(K) \rightarrow B(\mathsf{H})$, where $\mathcal{B}(K)$ stands for the set of all bounded Borel function on $K$ and $B(\mathsf{H})$ is the space of linear bounded operators on $\mathsf{H}$, defined by 
$$E(B)x = \sum_{n \in \mathbb{N}} \textbf{1}_{B}(\lambda_n) \left&lt;x,e_n\right&gt;e_n$$
is the resolution of identity (spectral measure, <a href=""http://planetmath.org/spectralmeasure"" rel=""nofollow noreferrer"">link</a>) for $A$ and it is often denoted by $\mathbf{1}_B(A):=E(B)$ (this notation is consistent with the application of functional calculus, discussed further in the answer) for all $B \in \mathcal{B}(K)$. Furthermore the following holds
$$(\star) \ \ \qquad  Ax = \int_{\sigma(A)} \lambda \  E(\mathrm{d}x)x = \sum_{n \in \mathbb{N}}\lambda_n \left&lt;x, e_n\right&gt;e_n,$$
where $\int_{\sigma(A)} \lambda \  E(\mathrm{d}x)$ is an integral with respect to the spectral measure $E$ of $A$ (I'm not going to define this integral here, check this <a href=""http://planetmath.org/spectralmeasure"" rel=""nofollow noreferrer"">link</a>, $(\star)$ supposed to give the intuition). </p>

<h2>Notation</h2>

<p>In Physics people often use the Dirac notation, that is, since the above $A$ can be written by $\sum_{n \in \mathbb{N}} \lambda_n \left|e_n\right&gt;\left&lt;e_n\right|$ or even $\sum_{i}  \lambda_i  \left|i\right&gt; \left&lt;i\right|$ (if we use the notation $(\left|i\right&gt;)$ to denote the orthonormal basis) then the following notation can be often seen $$A = \int \lambda \ \mathrm{d}\left|i\right&gt;\left&lt;i\right|$$
for any self-adjoint operators (not-necessarily diagonal). </p>

<h2>General case</h2>

<p>Now take an arbitrary self-adjoint operator on $\mathsf{H}$ (not-necessarily diagonal). The spectral theorem states that
$$A = \int_{\sigma(A)} \lambda \ E(\mathrm{d}\lambda),$$
where $E$ is the corresponding resolution of identity (spectral measure) for $A$. </p>

<p>In particular, one may want to consider the Borel functional calculus. Given a bounded borel function $f \in \mathcal{B}(\sigma(A))$ we have that 
$$ f(A) = \int_{\sigma(A)} f(\lambda) \ E(\mathrm{d} \lambda).$$ </p>

<h2>Name</h2>

<p>The term identity in the name probably comes from the fact that
$$\mathbf{1}_{\sigma(A)}(A) = I_{\mathsf{H}},$$
and I think that resolution corresponds to the spectral form of the operator.</p>

<h2>Spectral theorem - different version</h2>

<p>There is also a multiplication operator version of the spectral theorem which says that any self-adjoint operator is unitarily equivalent to the operator of multiplication on some space $L^2(\mu)$ for some measure $\mu$. Recall that a multiplication operator with symbol $f\in L^{\infty}(\mu)$ is $M_f \in B(L^2(\mu))$ given by $(M_f g)(x) = f(x)g(x)$. One may want to check <a href=""http://www.math.wsu.edu/faculty/watkins/Math502/pdfiles/spectral.pdf"" rel=""nofollow noreferrer"">this link for further information</a>. </p>

<p>In particular, the spectral measure for multiplication operator $M_f$ is given by $E(B) = \mathbf{1}_{f^{-1}(B)}$ for $B \in \mathcal{B}(\sigma(M_f))$.</p>
"
"2380189","2380242","<p>I want to explain the idea by Youem a bit more in detail.</p>

<p>If $(x_n)_n$ is bounded, there exists a convergent subsequence. Let $(x_{n_k})_k$ be such a sequence converging to some $x'$. Now $f(x_{n_k})$ converges to $f(x')$ (by continuity) and to $f(x)$ (by assumption). Using injectivity, it follows that $(x_{n_k})_k$ converges to $x$. </p>

<p>We now know that every convergent subsequence of $(x_n)_n$ has limit $x$. We still have to prove that $(x_n)_n$ is convergent. So let us assume that $(x_n)_n$ were not convergent. Then there exists a subsequence $(x_{n_k})_k$ such that $|{x_{n_k}} - x| &gt; \varepsilon$ for some $\varepsilon &gt; 0$ and all $k$. Since $(x_{n_k})_k$ is bounded, this sequence has a convergent subsequence and by what we have proven so far, it has to converge to $x$. However, we also have 
$|{x_{n_k}} - x| &gt; \varepsilon$ for all $k$ which gives the desired contradiction.</p>

<p>Note that the target of $f$ did not really matter. It could be $\Bbb R$, $\Bbb C$ or any other topological space in which limits of sequences are unique. The domain of $f$ can be $\Bbb R$ or $\Bbb C$ or any other metric space in which bounded sequences have convergent subsequences. Surely, this is not the most general type of space, but I just wanted to note that this is not a special property of $\Bbb R$ or anything like this. Anyway the proof above works for 1. as well as for 3.</p>
"
"2380193","2380302","<p>Quicker kill, with initial condition $u(0) = 0$: the Laplace transform.</p>

<p>Transforming both sides yields</p>

<p>$$ U(s) = \frac{1}{s^2} - \frac{U(s)}{s^2} $$</p>

<p>(I rewrote the integral as $-\int_0 ^x u(t)(x-t) \ dt $ to properly use the convolution formula.)</p>

<p>Now, rearrange to get</p>

<p>$$ U(s) \left( 1 + \frac{1}{s^2} \right) = \frac{1}{s^2} $$</p>

<p>and some division gives</p>

<p>$$ U(s) = \frac{1}{s^2 + 1} $$</p>

<p>giving the desired solution $u(x) = \sin x$. </p>
"
"2380194","2380228","<p>For integer $n \ge 1$ and $k = 1$, the statement is true and the fraction is $1$.</p>

<p>Assume that, for some $k = h$, the fraction is an integer:</p>

<p>$$\frac{10^{hn}-1}{10^n-1}\in\mathbb Z$$</p>

<p>For $k=h+1$,</p>

<p>$$\frac{10^{(h+1)n}-1}{10^n-1} = \frac{10^{(h+1)n} - 10^{hn} + 10^{hn} -1}{10^n-1}  = 10^{hn} + \frac{10^{hn}-1}{10^n-1} \in \mathbb Z$$</p>

<p>By induction, the fraction is an integer for $k,n \ge 1$.</p>
"
"2380196","2380219","<p>A right (or left) primitive ring is also prime, and hence semiprime.</p>

<p>To expand a bit, let's talk about four classes of rings:</p>

<p><a href=""https://i.stack.imgur.com/nVD25.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nVD25.png"" alt=""enter image description here""></a></p>
"
"2380197","2380220","<p>For question $1$, I assume you meant $4^7$ rather than $4^4$?  (for there are $7$ people that can each order one of $4$ pizzas).</p>

<p>Anyway, that doesn't work as an answer to question $1$, because it would be $4^7$ only if the order of the people matters, but it doesn't.  For example, if Bob, Alice, Carrie, and Dave, Edgar, Felice, and Greg order (in the order as listed) pizzas $A,B,C,B,C,C,D$ respectively, then that it is in effect the same pizza order as when they would have ordered $B,C,B,A,C,D,C$:  it is still $1$ pizza $A$, $2$ pizza's $B$, $3$  pizza's $C$, and 1 pizza $D$.</p>

<p>For part $2$, you need to consider the chance of each of the possible orders being made and ordered.  Note that we are assuming that the 7 friends chose their pizzas randomly as well (not very realistic, but hey ...).  Anyway, as an example:</p>

<p>The pizza order $7$ pizza's $A$ has a chance of $1$ in $4^7$ of being ordered and has the same chance of being made. So, a match for that particular order is $1$ in $4^{14}$</p>

<p>But the pizza order I gave as an example earlier, $1$ pizza $A$, $2$ pizza's $B$, $3$  pizza's $C$, and 1 pizza $D$ has a much larger chance of being ordered and made: 7 different people could have ordered pizza $A$, and from the remaining $6$, there are ${6 \choose 2} = 15$ pairs of people that can order pizza $B$, etc.  So the chance of this order being ordered and made is:</p>

<p>$1$ in $( {7 \choose 1}\cdot {6 \choose 2} \cdot  {4 \choose 3} \cdot {1 \choose 1})^2$</p>

<p>So this is what you need to do for all possible orders.  As @S.Ong said, it's tedious.  (though mathematicians more clever than I am will probably know some better method that is far quicker than this ...)</p>
"
"2380211","2380229","<p>Why not expand that all out?</p>

<p>$$E_{n+1}=E_n+2\lambda\sqrt{E_n}+\lambda^2=(\sqrt{E_n}+\lambda)^2$$</p>

<p>Square root both sides,</p>

<p>$$\sqrt{E_{n+1}}=\sqrt{E_n}+\lambda$$</p>

<p>This is an arithmetic sequence with</p>

<p>$$\sqrt{E_n}=\sqrt{E_0}+n\lambda$$</p>
"
"2380212","2380216","<p>A big hint: define $O_n=\{ x \in X : \exists \delta &gt; 0 \: \forall y,z \in B_\delta(x) \: d_Y(f(y),f(z))&lt;1/n \}$. Show that $O_n$ is open and that $A=\bigcap_{n=1}^\infty O_n$.</p>
"
"2380233","2380273","<p>At the informal level, when we talk about a scalar, it is a quantity with magnitude but no direction, and when we talk about a vector, it is a quantity with magnitude and direction, so we never compare a scalar with a vector. Here we imagine a vector as a directed line segment as they are represented in say high school textbooks.</p>

<p>At the formal level, the set of all real numbers $\mathbb R$ is a one-dimensional real vector space, and its elements are vectors. Obviously, these real numbers are also scalars. Here the scalar and the vector are indeed the same mathematical object.</p>
"
"2380238","2380257","<p>That's impossible!</p>

<p>For $ab * cd = abcd$, $cd$ would need to be at least $100$</p>
"
"2380261","2380268","<p>Firs of all note that $(0,0)$ is not a point of the graph of the function. So fix $x_0\in\mathbb{R}.$ The tangent line at $(x_0,y_0)$ is given by $$y-e^{x_0/2}=\frac 12 e^{x_0/2}(x-x_0).$$ (Note that $f'(x)=\frac12 e^{x/2}$.) If this line contains the point $(0,0)$ then we have $$e^{x_0/2}=\frac 12 e^{x_0/2}x_0.$$ Since $e^{x_0/2}\ne 0$ we get that $x_0=2.$ Thus the tangent line is</p>

<p>$$y-e=\frac{e}{2}(x-2)$$ That is $$y=\frac{e}{2}x.$$</p>
"
"2380274","2389926","<p>I didn't figure out how to prove it rigurously but I have a strong suspicion it isn't possible. I tried to find the solution for the simplest case, only one period with an investiment at the beginning and a single payoff at the end. In this case the problem would be</p>

<p>Find $\alpha$ such that
$$ -I+ \frac{P}{1+b_{1}+\alpha} = 0 $$
where $I$=investment, $P$=payoff, $b_{1}$=index return, and $\alpha$=excess return.</p>

<p>There's a simple solution by setting $r = b_{1} + \alpha$, but this is not valid because $r$ is dependent on $b_{1}$. I can't see a way to transform it as you want.</p>

<p>You can simplify the problem by using continuous compounding because the problem is reduced to</p>

<p>Find $\alpha$ such that
$$ -I+ Pe^{-(b_{1}+\alpha)} = -I+ Pe^{-b_{1}}e^{-\alpha} = 0 $$
There you have it factorized as you wanted, the only problem is XIRR doesn't calculte IRR's with continuous compounding, but you can use the equivalent rate of return $(b_{i}^{'} = e^{b_{i}}-1 \enspace \forall i)$ using the cashflows $P_{i}e^{-b_{i}} \enspace \forall i$.</p>

<p>This won't give you the exact result as the original problem, because the original problem is the degenerate version of the problem</p>

<p>Find $\alpha$ such that
$$ -I+ P\left(\frac{1}{1+b_{1}}\right)\left(\frac{1}{1+\alpha}\right) = 0 $$
$$ -I+ P\left(\frac{1}{1+b_{1}+\alpha+\color{red}{b_{1}\alpha}}\right) = 0 $$
which is the equivalent problem of the continuous case. The good news is that if the numbers are small, the cross term will be even smaller and it would serve as a pretty good approximation. If you want to be safe better use an iterative root finding algorithm.</p>
"
"2380281","2394100","<p>There is no permit in $\mathbb{R}$ ,but It has a full mean ,when numeber is $\mathbb{C}$
$$(-1)^{\frac{2}{6}}\neq +1 ,when \in\mathbb{R}$$</p>
"
"2380282","2380375","<p>Counterexample.</p>

<p>Let $\Omega=\mathbb R$ be equipped with Borel $\sigma$-algebra and a probability measure that gives value $0$ to singletons. </p>

<p>For every $n$ we define $X_n(x):\Omega\to\mathbb R$ by prescription $\omega\mapsto 1_{\{x\}}(\omega)$ and $X(x):\Omega\to\mathbb R$ by prescription $\omega\mapsto0$.</p>

<p>The $X_n$ are a.s. constant hence independent, and have equal probability with:$$\{\lim X_n(x)=X(x)\}=\mathbb R-\{x\}$$</p>

<p>So that: $$P(\{\lim X_n(x)=X(x)\})=P(\mathbb R-\{x\})=1$$</p>

<p>However: $$\bigcap_{x\in\mathbb R}\{\lim X_n(x)=0\}=\bigcap_{x\in\mathbb R}(\mathbb R-\{x\})=\varnothing$$</p>

<p>So that: $$P(\{\forall x\in\mathbb R[\lim X_n(x)=X(x)]\})=P(\varnothing)=0$$</p>
"
"2380285","2380292","<p>Often, if you want to show that one of multiple conclusions is true, you can assume that all but one are false and show that the last one has to be true.</p>

<p>Here, assume that $R \not\equiv \varnothing$ and $R \not\equiv \{\varepsilon\}$. We show that $L(R)$ must be infinite.</p>

<p>We know that $L(R)$ must contain a nonempty string, call it $w$. Then we also know that $w^k$ satisfies $R^k$. But we have that $R \equiv R^k$ for any $k \in \mathbb{Z}^+$ (by induction, $R^{k+1} \equiv RR^k \equiv RR \equiv R$). Thus, $w^k \in L(R)$ for any $k \in \mathbb{Z}^+$. This shows that our language contains an infinite number of strings, and we are done.</p>

<p>Note that it's not true that $L(R)$ being infinite implies that it is closed under Kleene-$*$. For example, consider $R = ab*$. This contains an infinite number of strings, but is in fact not closed under Kleene-$*$, since every string in the language must contain exactly one $a$. In this case, we are close, since $R \equiv R^k$ for any $k \in \mathbb{Z}^+$, but we don't neccessarily know if this is true for $k = 0$, which is required for closure under Kleene-$*$.</p>
"
"2380306","2380327","<p>Let $B_+=\bigl\{a^2\,|\,a\in A\cap[0,+\infty)\bigr\}$ and let $B_-=\bigl\{a^2\,|\,a\in A\cap(-\infty,0]\bigr\}$. Since $B=B_+\cup B_-$, if you prove that $B_+$ and $B_-$ are both measurable, then $B$ is measurable.</p>

<p>Now, note that if $s(x)=\sqrt x$, then $B_+=s^{-1}\bigl(A\cap[0,+\infty)\bigr)$. Since $s$ is continuous and $A\cap[0,+\infty)$ is measurable, $B_+$ is measurable. A similar argument shows that $B_-$ is measurable.</p>
"
"2380307","2380439","<p>The main result at play is this:</p>

<blockquote>
  <p>Let $X$ be a locally convex space, and let $M$ be a proper subspace of $X$.  If $x$ is not in the closure of $M$, then there is some $f\in X^*$ such that $ f(M)=\{0\}$ while $f(x)\neq0$.  </p>
</blockquote>

<p>Indeed, if we apply the Hahn-Banach separation theorem to the compact set $\{x\}$ and the closed convex set $cl(M)$, we obtain some $f\in X^*$ such that $f(\{x\})$ and $f(cl(M))$ are disjoint.  But then $f(cl(M))$ is a proper subspace of $\mathbb R$ (or $\mathbb C$), and thus $f(cl(M))=\{0\}$.  Since $f(cl(M))$ and $f(\{x\})$ are disjoint, we know $f(x)\neq0$.</p>

<p>Now we apply this result to your problem.  If $cl(R(A))\neq Y$, there is some non-zero $y\in Y$ such that $y\notin cl(R(A))$.  Now, the mentioned results guarantees the existence of some $y^*\in Y^*$ such that $y^*(y)\neq0$ but $y^*(cl(R(A)))=\{0\}$.  Thus $\langle y^*,Av\rangle=0$ for all $v\in V$.</p>
"
"2380311","2380430","<p>The domain is a required part defining a function (albeit often implicit) so the domain is whatever you say it is.  </p>

<p>Perhaps the the question should be what is the most all encompassing possible domain.  That depends on what the definition of $\sqrt{x}$ is.</p>

<p>If $\sqrt{x}$ is defined as: $c$ so that $c^2 = x$, then the largest possible domain is $\{0\}$ .  Is is because: $c$ so that $c^2 = x$ is only well, and uniquely defined if $x = 0$.  For any other value of $x$ there will be two possible values of $c$ and the ""function"" is not well-defined.</p>

<p>If $\sqrt{x}$ is defined as: $c$ so that $c^2 = x$, and $c$ is the non-negative square root.  Then the assumption is that $x$ has two square roots of which one is positive and the other is negative.  This implies that $x$ is a non-negative real number so the domain is $(0,\infty)$.</p>

<p>If $\sqrt{x}$ is: $c$ so that $c^2 = x$ and $x \in \mathbb R$ and $c \in (0, \infty)$ if $x \ge 0$ and $c = ai|a \in (0, \infty)$ if $x &lt; 0$. Then the domain is $\mathbb R$.  </p>

<p>But that is because we <em>SAID</em> $x \in \mathbb R$.</p>

<p>If we define $\sqrt{x}$ as: $c $ so that $c^2 = x$ and $x \in \mathbb C$ and $c \in \mathbb C$ so that $c = a + bi;$ and $a \ge 0$.  Then the domain is $\mathbb C$.  But again, that is because we <em>said</em> it was.</p>

<p>==== 2nd answer =====</p>

<p>The question as asked has two problems in how it is asked and what assumptions are being made in the asking of the question.</p>

<p>1) A function must have the domain either explicitly or implicitly stated as part of the definition of a function.</p>

<p>A function is defined as $f: X \rightarrow Y$ where the function is a mapping between pairs of values of $X$ and values of $Y$ where each and every value of $X$ (the domain) is mapped, <em>exactly once</em>, to multiples of $Y$.  (Not every value of $Y$ needs to be mapped to, and a value of $Y$ may be mapped to several times but every value of $X$ must be mapped from and can only be mapped from once).</p>

<p>By definition, the domain $X$ is a specific part of $f$ and so to say ""what is the domain of $f$"" is simply a matter of well, what did you <em>define</em> the domain to be?""</p>

<p>However the domain may be implicit and not explicitly stated.  If $f(n) = \frac n2 - 7$ and its understood we are mapping natural numbers to natural numbers, It's not possible for <em>any</em> natural number to be input.  An odd number will not be mapped to any natural number.  Any number $14$ or less won't be mapped into a natural number either.  So implicitly the domain is $\{n \in \mathbb N: 2\mid n; n &gt; 14\}$.</p>

<p>Here we aren't told anything about what the input and output of $f(x) = \sqrt{x}$ are  supposed to be.  Are the supposed to be natural numbers?  Integers?  Rationals?  Reals?  Complex?  A  vector space? The space of functions?  Bananas?</p>

<p>2) $\sqrt{x} = $ ""the square root of $x$"" is  ill-defined.  i) Depending upon what the output is supposed to be the input/domain is restricted.  If the output must be natural numbers the input must be a perfect square.  If the output must be real then input must be real non-negative.  ii) Given a domain of input there may be multiple options for the output.  e.g. The square root of 36 could be 6 or -6.  The square root of $-25$ could be 5i or $-5i$.  The square root of $i$ could be $\frac {\sqrt 2}2 + i \frac {\sqrt 2}{2}$ or $ - \frac {\sqrt 2}2 - i\frac {\sqrt 2}{2}$. etc.</p>

<p>And the thing is if we define with square root to ""pick"" we are implicitly indicating what the domain is.  If we say $\sqrt x$ is ""the positive one"" that implies $\sqrt x \in \mathbb R$ which implies $x \in (0, \infty)$. On the other hand is we claim the domain is $\mathbb R$ i.e. we so $x$ can be any real number. then for $x &lt; 0$ we have two possible square roots:  $ai$ and bi$ where $a^2 = |x|$ and $a = -b$. We could say pick the one where $a \ge 0$.  And if we want to say $x \in \mathbb C$ we would have to define a way of picking which of the two square roots we want.</p>

<p>In all of these though, choosing the domain, as well as the vaule of the range, is <em>not</em> a matter of answering a question.  It's a matter of making a choice in the definition of our function. </p>

<p>And we could choose anything we like.</p>
"
"2380314","2380322","<p>If $srs^{-1} = r$, then multiplying both sides on the right by $s$ you get that $sr = rs$, i.e., that $s$ and $r$ commute with each other.</p>

<p>Now, show that in ANY group, if you have two elements $a$ and $b$ that commute with each other, then the order of $ab$ divides the LCM of the orders of $a$ and $b$ <s>and is divisible by the GCD of the orders of $a$ and $b$.</s> and is divisible by the (LCM / GCD).</p>

<p>What does that mean if the orders of $a$ and $b$ are relatively prime?</p>

<p>Edit: It seems I can't get my basic facts straight; fixed the fact above.</p>
"
"2380318","2380382","<p>If the axis of the cylinder is the line through $\vec{x}_0 = (x_0,y_0,z_0)$ and in the direction of $\vec{v} = (a,b,c)$ with radius $R$, assuming $|\vec{v}| = 1$, we can write the equation of the cylinder as the set of all points $\vec{x} = (x,y,z)$ satisfying:</p>

<p>$$|\vec{x} - \vec{x}_0|^{2} = R^{2} + [(\vec{x} - \vec{x}_0)\bullet \vec{v}]^{2}$$</p>
"
"2380319","2380332","<p>Your idea is correct, but your proof looks backwards. I suggest the following modifications. Observe how now the proof looks like a tale of deduction rather than one of back-engineering, if you may.</p>

<blockquote>
  <p>Suppose that $a,b \in \mathbb{Z}$. Then, by definition of congruence
  modulo, <strong>we must show that</strong> $5|(a+b)^5-(a^5+b^5)$. <strong>This means</strong>
  that there exists an integer $k$ such that $$(a+b)^5-(a^5+b^5)=5k$$ 
  <strong>Let us show this holds. To see this, note that</strong>, $$(a+b)^5 = a^5+5a^4b+10a^3b^2+10a^2b^3+5b^4a+b^5 = a^5+b^5+5k$$ </p>
  
  <p><strong>where $a^4b+2a^3b^2+2a^2b^3+b^4=k$ is obtained by collecting terms.</strong> This shows that <strong>the difference $(a+b)^5 -a^5-b^5$</strong> is divisible by 5, hence,
  for all integer $a,b,\in\mathbb{Z}$, $(a+b)^5 \equiv a^5+b^5 \mod 5$.</p>
</blockquote>
"
"2380334","2380359","<p>If $f(x) = e^{x^2}$ then the mean value theorem tell us </p>

<p>$$ g(y) = \frac{e^{(x+y)^2} - e^{x^2}}{y} = \frac{e^{(x+y)^2} - e^{x^2}}{(x+y) -x } = f_{y}'(c) $$ where the notation implies that for different values of $y$, you get different values of $f'$.  But for $x \in [0,2]$ we have that </p>

<p>$f_{y}'(c) = g(y) &lt; \max_{y \in [0,2]}\{f'(y)\} =  f'(2)$ so we may apply DCT and get </p>

<p>$$ \lim_{y \to 0}\int_{0}^{2} \frac{e^{(x+y)^2} - e^{x^2}}{y} dx = \int_{0}^{2} \lim_{y \to 0} \frac{e^{(x+y)^2} - e^{x^2}}{y} dx = \int_{0}^{2} 2xe^{x^2} dx $$</p>
"
"2380335","2385525","<p>This problem was solved in</p>

<p>Edmonds, Allan L.; Ewing, John H.; Kulkarni, Ravi S.
Regular tessellations of surfaces and (p,q,2)-triangle groups. 
Ann. of Math. (2) 116 (1982), no. 1, 113â132. </p>

<p>From the MathReview of the paper: </p>

<blockquote>
  <p>The authors consider regular tessellations of type $(p,q)$ on a surface; a generalization of the Platonic solids. The parameter $p$ is the number of edges on each face while $q$ is the valence of each vertex. The main theorem states that as long as obvious necessary conditions, based on Euler's formula, are satisfied then a regular tessellation of type $(p,q)$ exists on a surface $M$. Furthermore, the edges can be geodesics and a classification of regular tessellations of type $(p,q)$ on $M$ is given based on $\pi_1(M)$. The main technique used to prove the theorem is construction of appropriate branched coverings over surfaces with irregular patterns.</p>
</blockquote>

<p>The main result of their paper is Theorem 1:</p>

<p>Let $M$ be a closed surface and let $p, q, V, E, F$ be positive
 integers such that
 $$
V-E+F=\chi(M)$$
 and 
$$
pF=2E=qV$$</p>

<p>Then: There exists a $(p, q)$-tessellation on $M$ consisting of $F$ $p$-sided faces, $E$ edges and $V$ vertices each of valence $q$; except when $M$ is the projective plane, $(p, q) = (3,3)$, $V = F = 2$, and $E = 3$.</p>

<p>Note that the above conditions are also necessary for the existence of a $(p,q)$-tessellation. </p>

<p>An important remark: For this theorem to hold, some of the polygonal faces $\sigma$ of the tessellation are allowed to be ""singular'' in the sense that some of the edges of $\sigma$ are glued to each other on the surface $M$. A simple example of a singular face is the tessellation of the 2-torus which has unique 2-dimensional rectangular face, whose edges are identified in the standard manner. In other words, the CW complex underlying the tessellation, need not be regular. </p>
"
"2380340","2380341","<p>Hint: $f(\text{connected set}) = \text{connected set}$</p>
"
"2380351","2380355","<p>If $x \to 0$ then $ax \to 0$ for any constant $a$. Now try a substitution of variables $y=ax/2$. </p>
"
"2380365","2380398","<p>If my life depended on it, I would probably compute the seventh roots of $8$ and $9$ in base $2$ with the root algorithm used in written computation. Omitting details (the algorithms is actually a dichotomic search), the crucial step in the algorithm is to compute </p>

<p>$$(2x+1)^7,$$ where $x$ is an approximation. You can do this using the Binomial expansion (and noting that multiplying by a power of $2$ is a mere shift). The coefficients are $1,7,21,35,35,21,7,1$ (decimal).</p>

<p>But I guess that the computation via $y^7=y\,y^2(y^2)^2$ will be faster/simpler.</p>

<p>Next you convert to base $10$ and multiply by $112$. A first step is to determine the number of significant bits required. Count two of three hours of work.</p>

<hr>

<p>Alternatively, the seventh roots can be evaluated by Newton-Raphson, but this will take the evaluation of sixths powers and divisions.</p>

<hr>

<p>Another interesting alternative is the secant method, starting from the inequalities</p>

<p>$$1.3^7=6.2748517&lt;8&lt;9&lt;1.4^7=10.5413504.$$</p>

<p>As the curve is pretty flat, the next estimates are obtained by linear interpolation. Optionally, when a new approximation is obtained, it can be truncated to a smaller number of decimals to ease the evaluation of the seventh power.</p>

<p>For example, the next approximation of $\sqrt[7]8$ is $1.340434755\cdots$, and you can use $1.34$ instead.</p>

<hr>

<p>If you have a fast way to compute the square roots, by some magic, you can use the fixed-point iteration</p>

<p>$$x\leftarrow \sqrt{\sqrt{\sqrt{yx}}},$$ the solution of which is $x=y^{1/7}$.</p>
"
"2380368","2380464","<p><strong>Succinct definition of boundary starting from open sets:</strong></p>

<p>Let $X$ be a set and its topology is just a collection $T\subset P(X)$ of some of its subsets that are going to be called open. The collection is required to have some properties which you can check in Wikipedia's article for topology.</p>

<p>The set $X$ with the collection $T$ is called a topological space.</p>

<p><strong>Examples:</strong></p>

<ol>
<li>$X=\mathbb{R}$ and $T$ consist of the sets that can be obtained by making arbitrary unions of intervals of the form $(a,b)$.</li>
<li>$X=\mathbb{N}$ and $T$ consists of all possible subsets of $X$.</li>
<li>$X=\{3,4,5\}$ and $T$ consists of all possible subsets of $X$.</li>
</ol>

<p><strong>Interior:</strong> Given a topological space $X$ with topology $T$ and given a subset $A\subset X$, the <em>interior</em> of $A$ is the union of all elements of $T$ that are subsets of $A$.</p>

<p><strong>Example:</strong> </p>

<ol start=""4"">
<li>The interior of $[a,b)$ in the topological space (1) is $(a,b)$.</li>
<li>The interior of $\{3,4,5\}$ in the topological space (2) is $\{3,4,5\}$.</li>
</ol>

<p><strong>Closure:</strong> Given a topological space $X$ with topology $T$ and given a subset $A\subset X$, the <em>closure</em> of $A$ is the complement of the union of all elements of $T$ that are completely outside of $A$.</p>

<p><strong>Example:</strong></p>

<ol start=""6"">
<li>The closure of the example (4) is $[a,b]$</li>
<li>The closure of the example (5) is $\{3,4,5\}$</li>
</ol>

<p><strong>Boundary:</strong> Given a topological space $X$ with topology $T$ and given a subset $A\subset X$, the <em>boundary</em> of $A$ is the difference of its closure minus its interior.</p>

<p><strong>Example:</strong></p>

<ol start=""8"">
<li>The boundary of example (4) is $\{a,b\}$</li>
<li>The boundary of example (5) is empty</li>
</ol>

<hr>

<p><strong>The case of boundary of intervals in $\mathbb{R}$</strong></p>

<p>Given an finite interval that includes or not its extreme points ($A=(a,b)$, $[a,b)$, $(a,b]$, or $[a,b]$) it is always the case that its boundary is the set of its extreme points. </p>

<p>Observe that the interior is the interval excluding its extreme points $(a,b)$. This is an element of the topology (see example 1) and we cannot fit any more intervals without extreme points inside $A$.</p>

<p>The closure is always $[a,b]$. Observe that in the complement of $A$ we can fit the intervals $(b,+\infty)$ and $(-\infty,a)$. Any other is inside those two. The complement of $(-\infty,a)\cup(b,+\infty)$ is $[a,b]$, and that is the closure.</p>

<p>Finally the boundary is closure $[a,b]$ minus interior $(a,b)$. That is $\{a,b\}$.</p>

<hr>

<p><strong>Topology from metric</strong></p>

<p>Given a set $X$ and a metric $d$ on it. One can define a topology $T$ to consist of arbitrary unions of balls $B(a,r)=\{x\in X:\ d(a,x)&lt;r\}$.</p>

<p><strong>Examples:</strong></p>

<ol start=""10"">
<li>The usual topology in $\mathbb{R}$ is of this form. Observe that the intervals $(a,b)$ are just the balls $B(\frac{a+b}{2},\frac{b-a}{2})$.</li>
</ol>

<hr>

<p><strong>Impossibility of a topology on $\mathbb{N}$ such that the boundary of finite intervals consists of its extreme points</strong></p>

<p>Assume we want to put a topology on $\mathbb{N}$ such that for every finite interval (finitely many consecutive integers) the boundary is its extreme points.</p>

<p>If the boundary of $A=\{a,a+1,a+2,...,a+n\}$ ought to be $\{a,a+n\}$. Then its interior would have to be $\{a+1,a+2,...,a+n-1\}$. Since $a$ and $n$ were arbitrary, that means (taking $n=1$) that all sets of the form $\{a\}$ are open (elements of the topology). By a property required for topologies, which you are supposed to read, this means that all subsets are open.</p>

<p>But if all subsets are open, then $A$ is open. Therefore, the interior of $A$ would have to be $A$ and its complement would have to be $A$ as well. This causes that the boundary is empty.</p>

<p>Thereofore, there is no such topology as we wanted. In particular, there is no such topology defined by a metric.</p>
"
"2380379","2380468","<p>Because $f(z)$ is non-zero in $D$ (which is simple-connected), the log of $f(z)$ is well defined and analytic in $D$. Define $g(z):=\ln(f(z)) = \ln(|f(z)|)+i\phi(z)$. We apply Cauchy's theorem to $g(z)$ to get $g(0)=\ln(|f(0)|)+i\phi(0)$:</p>

<p>$$ \begin{align}
g(0) = \ln(|f(0)|)+i\phi(0) &amp;= \frac{1}{2\pi i}\int_{\partial D}\left(\frac{\ln(|f(z)|)}{z}+i\frac{\phi(z)}{z}\right)\,dz \\
&amp;=\int_0^1t(1-t)\,dt+i\int_0^1\phi(e^{2\pi it})dt.
\end{align}$$</p>

<p>The real part of this equation gives:</p>

<p>$$ \ln(|f(0)|) =   \int_0^1t(1-t)\,dt = \frac{1}{6}.$$</p>

<p>So $|f(0)|=e^{1/6}$</p>
"
"2380386","2380418","<p>For any set $X$, $C^0(X)$ is simply the set of all real-valued (or complex-valued) continuous functions on the set $X$.  It is also often denoted $C(X)$.</p>

<p>Generally, $C^k(X)$ denotes the set of functions on $X$ for which <strong>at least</strong> the first $k$ (partial) derivatives exist and are continuous.  So with $k=0$, there are no differentiability requirements, and $C^0$ contains all the continuous functions, differentiable or not.</p>
"
"2380391","2393597","<p>Hint : Multiply and divide by $e^{40x}$ and then take $e^{40x}$ of denominator inside the whole square bracket. Then the derivative of entity in denominator will be present in numerator</p>
"
"2380409","2380431","<p>Say that $a \equiv_n b$ if $a - b = kn$ for some nonzero integer $k$.  Then $\equiv_n$ is the usual equivalence relation modulo $n$ for $n\in\mathbb{Z}$.  On the other hand, $\equiv_0$ can be made sense of, whereas ""division by zero"" doesn't really make sense.</p>

<p>Specifically, $a \equiv_0 b$ if $a-b = k0$ for some nonzero integer $k$.  But this means that $a-b = 0$, which implies that $a=b$.  Hence $\overline{a} = \{a\}$ for each $a\in\mathbb{Z}$.  This implies that $\mathbb{Z}_0 = \{\overline{a} : a\in\mathbb{Z}\} \cong \mathbb{Z}$, where the isomorphism is the obvious choice.</p>
"
"2380412","2380581","<p>Maybe I am missing something but $\int_0^1 (f(x) \log (x))\frac{dx}{\log (x)} = 1$ for any function $f$ that satisfies $\int_0^1 f(x)\;dx = 1$ and there are very many polynomial examples. So is $g(x) := f(x) \log(x)$ a simple function, and if not, why not?</p>
"
"2380417","2380421","<p>This doesn't seem to be any different from how we normally talk about equivalence classes:  the author defines a binary relation over a set $S$, where any two objects from the set are related 'if they provide the same answer to all queries'. This relation is obviously reflexive, symmetric, and transitive, and hence is an equivalence relation. And the equivalence class $C$ is still defined relative to any object $o$ in the set, in that it is the set of all objects from that set standing in that equivalence relation to the object $o$.  So this is all still relative to elements of the set.</p>
"
"2380426","2390482","<p>As <a href=""https://en.wikipedia.org/wiki/Asymptote"" rel=""nofollow noreferrer"">wikipedia</a> says, asymptotes are often considered only for real curves, and in this answer I will treat $C$ and $D$ as such. Your curves are of a particularly simple type, which makes finding the asymptotes easier.</p>

<p>$C$ is the graph of the rational function $Y=\frac{X^4}{X^3+1}=X-\frac{X}{X^3+1}=X-\frac{X}{(X+1)(X^2-X+1)}$ so $Y=X$ is an oblique asymptote, also $X+1=0$ is a vertical asymptote.</p>

<p>$D$ is the graph of the rational function $Y=\frac{X^3}{X^2+1}=X-\frac{X}{X^2+1}$ so $Y=X$ is an oblique asymptote.</p>

<p>Parabolic branches for graphs of rational functions are when the long division yields a higher degree polynomial than degree 1. Then the graph of the quotient polynomial, like $X$ above, is what the graph of the rational function tends to. If you want to find parabolic branches for more general algebraic curves, the link at wikipedia contains some information.</p>
"
"2380428","2380452","<p>For a differentiable function, the global Lipschitz constant is given by $\sup|f'(x)|$. In this case,
$$\cos(x^2)' = -2x\sin(x^2).$$
But this expression is is unbounded as $|x|\to\infty$, take the sequence $(x_n)_{n\in\mathbb N}$ with $x_n = \sqrt{2\pi n + \frac \pi 2}$:
$$\left|-2x_n\sin(x_n^2)\right| = \sqrt{2\pi n +\frac\pi 2}\cdot 1 \ge \sqrt n.$$
Hence the first derivative becomes arbitrarily large, and so does the lower bound for any Lipschitz constant. Since we are not able to find a Lipschitz constant, $f$ is not globally Lipschitz.</p>
"
"2380436","2380547","<p>Multiple loop. If we have a partial product, one of the ""sofar"" numbers, that is already larger than the target $n,$ we can only do worse by multiplying by another one of the primes (by increasing that exponent). All the primes are below 20, so it suffices to consider partial products up to $20n.$</p>

<p>BETTER: on reflection, we may take the tighter bound $2n.$ We know that we may get an answer no worse than that using just a power of $2.$ </p>

<p>==========================================</p>

<pre><code>  n = 2 * 3 * 5 * 7 * 11 * 13;
  n *= 17 * 19 * 23 * 29;
  n *= 31 * 37 * 41 * 43 * 47;
  n *= 53 * 59 * 61 * 67 *71;
----------------------------------------
  target  n = 557940830126698960967415390

950551829131524428824142413  =   11^12 13^13   :   0,0,0,0,12,13
804313086188212978235812811  =   11^13 13^12   :   0,0,0,0,13,12
680572611390026366199533917  =   11^14 13^11   :   0,0,0,0,14,11
575869132714637694476528699  =   11^15 13^10   :   0,0,0,0,15,10
570389064953586206121040247  =   7^2 11^23 13   :   0,0,0,2,23,1
564735647321977610130979327  =   7^3 11^4 13^18   :   0,0,0,3,4,18
559361527685078641337745931  =   7^5 11^12 13^9   :   0,0,0,5,12,9
558040721244263815738365185  =   5 7^4 11^10 13^11   :   0,0,1,4,10,11
558024051986000542605257595  =   3^5 5 7^9 11^9 13^6   :   0,5,1,9,9,6
558007383225665396344586265  =   3^10 5 7^14 11^8 13   :   0,10,1,14,8,1
557975702410928445496978125  =   3^29 5^5 7^2 11 13^6   :   0,29,5,2,1,6
557959035094845525129909375  =   3^34 5^5 7^7 13   :   0,34,5,7,0,1
557947432080954688960687500  =   2^2 3^14 5^6 7^4 11^5 13^6   :   2,14,6,4,5,6
557941472774676767041705440  =   2^5 3^4 5 7^23 11^2 13   :   5,4,1,23,2,1
557941063680000000000000000  =   2^31 3^5 5^16 7^2 11 13   :   31,5,16,2,1,1

557940830126698960967415390  is target n 
</code></pre>

<p>====================================</p>

<p>==============================================================</p>

<pre><code>  n = 1024 * 1024;
  n *= 1024;

  n += 153;
----------------------------------------------------
  target  n = 1073741977

10604499373  =   13^9   :   0,0,0,0,0,9
8973037931  =   11 13^8   :   0,0,0,0,1,8
7592570557  =   11^2 13^7   :   0,0,0,0,2,7
6424482779  =   11^3 13^6   :   0,0,0,0,3,6
5436100813  =   11^4 13^5   :   0,0,0,0,4,5
4599777611  =   11^5 13^4   :   0,0,0,0,5,4
3892119517  =   11^6 13^3   :   0,0,0,0,6,3
3293331899  =   11^7 13^2   :   0,0,0,0,7,2
2786665453  =   11^8 13   :   0,0,0,0,8,1
2357947691  =   11^9   :   0,0,0,0,9,0
2095756663  =   7 11^6 13^2   :   0,0,0,1,6,2
1773332561  =   7 11^7 13   :   0,0,0,1,7,1
1500512167  =   7 11^8   :   0,0,0,1,8,0
1333663331  =   7^2 11^5 13^2   :   0,0,0,2,5,2
1128484357  =   7^2 11^6 13   :   0,0,0,2,6,1
1096135733  =   7^7 11^3   :   0,0,0,7,3,0
1093547455  =   5 7^6 11 13^2   :   0,0,1,6,1,2
1091179089  =   3^2 7^2 11^4 13^2   :   0,2,0,2,4,2
1088602515  =   3^2 5 7 11^2 13^4   :   0,2,1,1,2,4
1086032025  =   3^2 5^2 13^6   :   0,2,2,0,0,6
1084620537  =   3^5 7^4 11 13^2   :   0,5,0,4,1,2
1083647565  =   3^9 5 7 11^2 13   :   0,9,1,1,2,1
1081088775  =   3^9 5^2 13^3   :   0,9,2,0,0,3
1076168025  =   3^16 5^2   :   0,16,2,0,0,0
1074218750  =   2 5^11 11   :   1,0,11,0,1,0
1073765616  =   2^4 3 7^5 11^3   :   4,1,0,5,3,0

1073741977  is target n 
</code></pre>

<p>============================================================== </p>

<pre><code>int main()
{

  mpz_class n;
  n = 2 * 3 * 5 * 7 * 11 * 13;
  n *= 17 * 19 * 23 * 29;
  n *= 31 * 37 * 41 * 43 * 47;

  cout &lt;&lt; endl &lt;&lt; ""  target  n = ""  &lt;&lt; n &lt;&lt; endl &lt;&lt; endl;

  mpz_class best = n * n;

  mpz_class sofar2 = 1;

  for(int a = 0; sofar2 &lt;= n * 2; ++a) 
  {
    mpz_class sofar3 = sofar2;
    for(int b =0;  sofar3 &lt;= n * 2; ++b)
    {
       mpz_class sofar5 = sofar3;
    for(int c = 0; sofar5 &lt;= n * 2; ++c) 
    {
      mpz_class sofar7 = sofar5;
      for(int d = 0; sofar7 &lt;= n * 2; ++d) 
      {
        mpz_class sofar11 = sofar7;
        for(int e = 0; sofar11 &lt;= n * 2; ++e) 
        {
          mpz_class sofar13 = sofar11;
          for(int f = 0; sofar13 &lt;= n * 2; ++f) 
          {
                if ( sofar13 &lt; best  &amp;&amp; sofar13 &gt;= n)
                {
                  best = sofar13;
                  cout &lt;&lt; best &lt;&lt; ""  =  "" &lt;&lt; mp_Factored(best) &lt;&lt; ""   :   "" &lt;&lt;  a &lt;&lt; "",""  &lt;&lt; b &lt;&lt; "",""  &lt;&lt; c &lt;&lt; "",""  &lt;&lt; d &lt;&lt; "",""  &lt;&lt; e &lt;&lt; "",""  &lt;&lt; f &lt;&lt;    endl;
                }  // if

               sofar13 *= 13;
          } // 13
          sofar11 *= 11;
        } // 11
        sofar7 *= 7;
      } // 7
      sofar5 *= 5;
    } // 5
       sofar3 *= 3;
    } // 3
    sofar2 *= 2;
  } // 2

    cout &lt;&lt; endl   &lt;&lt; n  &lt;&lt;  ""  is target n  "" &lt;&lt; endl &lt;&lt; endl;

  return 0;
}
</code></pre>

<p>==============================================================</p>

<pre><code>     n = 1024 * 1024;
      n *= 1024;
        n *= 1024;
      n -= 1;
    --------------------
  target  n = 1099511627775

1792160394037  =   13^11   :   0,0,0,0,0,11
1516443410339  =   11 13^10   :   0,0,0,0,1,10
1283144424133  =   11^2 13^9   :   0,0,0,0,2,9
1270933805449  =   7^2 11^10   :   0,0,0,2,10,0
1129612841357  =   7^3 11^7 13^2   :   0,0,0,3,7,2
1126945514695  =   5 7^2 11^5 13^4   :   0,0,1,2,5,4
1124284486325  =   5^2 7 11^3 13^6   :   0,0,2,1,3,6
1121629741375  =   5^3 11 13^8   :   0,0,3,0,1,8
1108332850625  =   5^4 7 11^7 13   :   0,0,4,1,7,1
1105715771875  =   5^5 11^5 13^3   :   0,0,5,0,5,3
1102876442745  =   3 5 7^3 11^8   :   0,1,1,3,8,0
1100272248075  =   3 5^2 7^2 11^6 13^2   :   0,1,2,2,6,2
1099805224518  =   2 3^6 7^4 11 13^4   :   1,6,0,4,1,4
1099598500000  =   2^5 5^6 7 11 13^4   :   5,0,6,1,1,4
1099535990784  =   2^14 3 7^5 11^3   :   14,1,0,5,3,0
1099511627776  =   2^40   :   40,0,0,0,0,0

1099511627775  is target n 
</code></pre>

<p>===========================================================</p>
"
"2380437","2380477","<p>Not sure if thereâs a neat direct way. But I learnt that the trick is to add another more conceptual equivalent statement.</p>

<blockquote>
  <p>Let $K / F$ be a finite extension of fields and let $L$ be an algebraically closed field containing $K$. So we assume $F â K â L$. The following are equivalent:</p>
  
  <ol>
  <li>$K$ is a splitting field for some polynomial $f â F[X]$.</li>
  <li>Every field embedding $Ï \colon K â L$ fixing $F$ restricts to $K â K$.</li>
  <li>Every irreducible polynomial $p â F[X]$ with some root $Î± â K$ splits completely in $K$.</li>
  </ol>
</blockquote>

<p><em>Proof</em>. (1) â (2): Let $f â F[X]$ be a polynomial for which $K$ is a splitting field, say of degree $n$ and monic (without loss of generality). Then $f$ has $n$ roots $Î±_1, â¦, Î±_n â K$ (possibly counted with multiplicites) and $f = (X - Î±_1)Â·â¦Â·(X - Î±_n)$ in $K$. Then $K = F(Î±_1, â¦, Î±_n)$, as the latter field is a subextension in which $f$ splits and $K$ is by definition the smallest such extension.</p>

<p>Let $Ï \colon K â L$ be any field embedding fixing $F$. Then $Ï$ map zeroes of $f$ to zeros of $f^Ï = f$, that is: $f(\{Î±_1,â¦,Î±_n\}) = \{Î±_1,â¦,Î±_n\}$. As $Ï$ fixes $F$ and $K = F(Î±_1,â¦,Î±_n)$, this implies $Ï(K) â K$.</p>

<p>(2) â (3): Let $p â F[X]$ be any irreducible polynomial with some root $Î± â K$. We assume itâs nonzero and monic. Then $p$ splits in $L$ because $L$ is algebraically closed, say $p = (X - Î±_1)Â·â¦Â·(X - Î±_n)$ for some $Î±_1, â¦, Î±_n â L$. Have a look at $E = F(Î±)$. Then we have $n$ maps $Ï_1, â¦, Ï_n$ all $E â L$ with $Ï_i(Î±) = Î±_i$ for $i = 1, â¦, n$. (For this, we need $p$ to be irreducible!) We can extend those to maps $K â L$, which then have to restricct to maps $K â K$ by assumption. As $Î±_1, â¦, Î±_n$ are in the image of $Ï_1, â¦, Ï_n$, we therefore have $Î±_1, â¦, Î±_n â K$ and so $p = (X - Î±_1)Â·â¦(X - Î±_n)$ in $K[X]$.</p>

<p>(3) â (1): As a finite extension, there are some $Î²_1, â¦, Î²_n â K$ with $K = F(Î²_1, â¦, Î²_n)$ â for example, take an $F$-basis of $K$. Let $p_1, â¦, p_n â F[X]$ be the minimal polynomials of $Î²_1, â¦, Î²_n$. Being irreducible, all of them split in $K[X]$, so $f = p_1Â·â¦Â·p_n â F[X]$ does a well and $K$ then is the splitting field of $f$.</p>

<p>In $L$, the polynomial $p$ has to split: Let $Î±_1, â¦, Î±_n$ be the $n$ zeroes of $p$ in $L$. Then we have $n$ different field embeddings $Ï_i \colon F(Î±) â L$ with $Ï_i(Î±) = Î±_i$ for $i = 1, â¦, n$.</p>
"
"2380438","2380466","<p>You can take both $S_1$ and $S_2$ to be the space $H(\mathbb D)$ of functions $f$ analytic in the open unit disk $\mathbb D = \{z: |z| &lt; 1\}$.  Note that the Maclaurin series of such a function converges absolutely to the function in the disk, and uniformly on compact subsets of it; conversely if the series converges absolutely at points arbitrarily close to $1$, the radius of convergence is $1$.  Moreover, $|z^{n(n+1)}| \le |z^n|$ so 
$T(f) \in H(\mathbb D)$.</p>

<p>EDIT: I don't see how you get your integral</p>

<p>$$ \int_0^1 (I-T)f(x) \dfrac{dx}{\log x} = 1 $$</p>

<p>I get $$\int_0^1 \frac{x^k - x^{k(k+1)}}{\log x}\; dx = \log(k+1) - \log((k(k+1)+1)  $$</p>

<p>and the partial sums $$\sum_{k=1}^N \frac{\mu(k)}{k} (\log(k+1)-\log(k(k+1)+1))$$ </p>

<p>for $100 \le N \le 10000$ are all between $0.535$ and $0.868$.
Maple's numerical evaluation of the series is $0.717481715309322$.
The <a href=""https://isc.carma.newcastle.edu.au"" rel=""nofollow noreferrer"">Inverse Symbolic Calculator</a> and <a href=""http://oeis.org/search?q=.717481715309322&amp;language=english&amp;go=Search"" rel=""nofollow noreferrer"">OEIS</a> find nothing for this.</p>
"
"2380447","2380454","<p>Here $\subset$ must be interpreted as $\subseteq$ (i.e. equality is allowed), otherwise the statement is false.</p>

<p>$\text{ch}(x_1, \ldots, x_k) \subset \text{ch}(x_1, \ldots, x_n)$ is obvious (any convex combination of $x_1, \ldots, x_k$ is a convex combination of $x_1, \ldots, x_n$: just give coefficient $0$ to $x_{k+1}, \ldots, x_n$). 
 If $A \subset B$ then $\text{int}(A) \subset \text{int}(B)$</p>
"
"2380450","2380802","<p>A lot of reasoning is always omitted in it, so we have to always fill the gap of proofs of Rudin.</p>

<p>For the proof, you have to consider 2 cases.</p>

<p>Given a set $E$ measurable and $\epsilon&gt;0$, </p>

<ol>
<li>$\mu(E)&lt;\infty$</li>
</ol>

<p>In this case, by (a), we can choose a closed set $F$ with the measure $\mu(E-F)&lt;\epsilon$. 
Then since $F$ is $\sigma$-compact and of finite measure, we can choose a compact set $C\subset F$ such that $\mu(F-C)&lt;\epsilon$ </p>

<p>(Or, you may consider a increasing sequence of compact sets converging to $F$. Note that finite union of compact sets is compact.) </p>

<p>Thus we have $\mu(E-C)&lt;2\epsilon$ or $\mu(E)&lt;\mu(C)+2\epsilon$. I think the finiteness condition should be used here.</p>

<ol start=""2"">
<li>$\mu(E)=\infty$</li>
</ol>

<p>I'd like to leave this case as an exercise since it is almost the same proof as above, except that we cannot use $\epsilon$ argument. Instead, you can choose a closed subset $F$ of infinite measure and a increasing sequence of compact sets in $F$ whose measure converges to $\infty$, which anyway implies that $\mu(E)=sup\{\mu(C): C \text{ is compact and }C\subset E \}$</p>
"
"2380455","2380462","<p>You can do $((10)^*1 + (01)^*)00(1+0)^*11(1+0)^*$. The first part should take care of any $01$ or $10$ substrings at the head.</p>
"
"2380467","2380476","<p>The isomorphism tells you that $\Gamma$ is generated by a single element $a$. W.l.o.g. $0&lt;a$ (otherwise take $-a$ as generator). Then $na\mapsto n$ gives you the desired isomorphism preserving the ordering.</p>
"
"2380480","2380481","<p>It is the $L_{p}$ norm that is used here. To see why, just note that every norm $|| \cdot ||$ can induce a metric by $(x,y) \mapsto || x - y ||$.</p>
"
"2380485","2380493","<p>One of the best books I know for these topics is <a href=""http://www.springer.com/us/book/9780387973296"" rel=""nofollow noreferrer"">A Classical Introduction to Modern Number Theory</a> by Ireland and Rosen. In particular, see Chapter $5$ on quadratic reciprocity, and for its generalisations in Chapter 9, on cubic and biquadratic reciprocity.</p>
"
"2380490","2380548","<p>Let $J(R)=rR$; then $J(R)^n=r^nR$. If $x\in M_0$, then, for every $n\ge0$, $x=r^nx_n$, for some $x_n\in R$.</p>

<p>In particular, for every $n&gt;1$, $rx_1=r^nx_n$, which implies, $R$ being a domain,
$$
x_1=r^{n-1}x_n
$$
whence $x_1\in\bigcap_{n\ge0}J(R)^n=M_0$.</p>
"
"2380492","2380525","<p>$\newcommand{\bbx}[1]{\,\bbox[15px,border:1px groove navy]{\displaystyle{#1}}\,}
 \newcommand{\braces}[1]{\left\lbrace\,{#1}\,\right\rbrace}
 \newcommand{\bracks}[1]{\left\lbrack\,{#1}\,\right\rbrack}
 \newcommand{\dd}{\mathrm{d}}
 \newcommand{\ds}[1]{\displaystyle{#1}}
 \newcommand{\expo}[1]{\,\mathrm{e}^{#1}\,}
 \newcommand{\ic}{\mathrm{i}}
 \newcommand{\mc}[1]{\mathcal{#1}}
 \newcommand{\mrm}[1]{\mathrm{#1}}
 \newcommand{\pars}[1]{\left(\,{#1}\,\right)}
 \newcommand{\partiald}[3][]{\frac{\partial^{#1} #2}{\partial #3^{#1}}}
 \newcommand{\root}[2][]{\,\sqrt[#1]{\,{#2}\,}\,}
 \newcommand{\totald}[3][]{\frac{\mathrm{d}^{#1} #2}{\mathrm{d} #3^{#1}}}
 \newcommand{\verts}[1]{\left\vert\,{#1}\,\right\vert}$</p>

<blockquote>
  <p>Lets $\ds{\mrm{f}\pars{z} \equiv
{1 + 1/z \over \expo{z} - 1} + {1 \over \pars{\expo{z} - 1}^{2}}.\qquad
0 &lt; \Gamma \ll 1}$.</p>
</blockquote>

<p>\begin{align}
\int_{\Gamma}^{\infty}\mrm{f}\pars{z}\,\dd z &amp; =
\int_{\Gamma}^{1}\mrm{f}\pars{z}\,\dd z +
\int_{1}^{\infty}\mrm{f}\pars{z}\,\dd z
\\[5mm] &amp; =
\braces{\int_{\Gamma}^{1}\pars{{2 \over z^{2}} - {1 \over 2z}}\,\dd z +
\int_{\Gamma}^{1}\bracks{\mrm{f}\pars{z} - {2 \over z^{2}} + {1 \over 2z}}
\,\dd z} +
\int_{1}^{\infty}\mrm{f}\pars{z}\,\dd z
\\[5mm] &amp; =
-2 + {2 \over \Gamma} + {1 \over 2}\,\ln\pars{\Gamma} -
\int_{0}^{\Gamma}\bracks{\mrm{f}\pars{z} - {2 \over z^{2}} + {1 \over 2z}}
\,\dd z\label{1}\tag{1}
\\ &amp; +\
\underbrace{\int_{0}^{1}\bracks{\mrm{f}\pars{z} - {2 \over z^{2}} +
{1 \over 2z}} + \int_{1}^{\infty}\mrm{f}\pars{z}\,\dd z}
_{\ds{\equiv\ \alpha + 2}: \mbox{a constant}}
\end{align}</p>

<blockquote>
  <p>The integral in \eqref{1} can be estimated by expanding
  $\ds{\mrm{f}\pars{z} - {2 \over z^{2}} + {1 \over 2z}}$ in power of $\ds{z}$.</p>
</blockquote>

<p>For instance,
$$
\int_{0}^{\Gamma}\bracks{\mrm{f}\pars{z} - {2 \over z^{2}} + {1 \over 2z}}
\,\dd z \sim
\int_{0}^{\Gamma}{z^{2} \over 360}\,\dd z = {\Gamma^{3} \over 1080}
\qquad\mbox{as}\quad \Gamma \to 0^{+}
$$
such that
$$
\bbx{\int_{\Gamma}^{\infty}\mrm{f}\pars{z}\,\dd z \sim
{1 \over 2}\,\ln\pars{\Gamma} + {2 \over \Gamma} + \alpha + 
{\Gamma^{3} \over 1080}\qquad\mbox{as}\quad \Gamma \to 0^{+}}
$$</p>

<blockquote>
  <p>Numerically, $\ds{\alpha \approx -1.1304}$. </p>
</blockquote>
"
"2380494","2380577","<p>Let's change it to ""how sure we can be that the mouse learned <em>something.</em>""
That is, we test $H_0: p = .5$ [ignorant mouse performing at random] vs.
$H_a: p &gt; .5$ [trained mouse is correct more often than not]. With $n = 100$
and $X = 60$ correct performances, the mouse's proportion correct is
$\hat p = X/n = 60/100 = 0.6.$</p>

<p>With $n = 100$ trials, it is safe to use a normal approximation, with
test statistic $Z = \frac{\hat p - .5}{\sqrt{\hat p(1-\hat p)/n}},$
and rejecting $H_0$ at the 5% level of significance if $Z &gt; 1.645.$</p>

<p>In your case, $Z = \frac{.1}{\sqrt{.24/100}} = 2.04124 &gt; 1.645,$ so we conclude
that the mouse has done significantly (if not spectacularly) better than random. </p>

<p>Here is Minitab 17 output for this procedure (slightly edited for relevance).
The P-value &lt; 0.05 indicates significance at the 5% level.</p>

<pre><code>Test of p = 0.5 vs p &gt; 0.5

Sample   X    N  Sample p  Z-Value  P-Value
1       60  100  0.600000     2.00    0.023

Using the normal approximation.
</code></pre>

<p>The figure below shows the PDF of $\mathsf{Binom}(n=100, p=.5)$ along
with the density curve of the 'best-fitting' normal curve. The P-value
of the test is the probability to the right of the dotted red line.</p>

<p><a href=""https://i.stack.imgur.com/gKFXm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gKFXm.png"" alt=""enter image description here""></a></p>

<p><em>Note:</em> An 'exact test' (not using a normal approximation) would get the P-value 0.028 by summing the heights of the binomial bars at and above 60. (Computation uses R statistical software.)</p>

<pre><code>1 - pbinom(59.5, 100, .5)
## 0.02844397
</code></pre>
"
"2380495","2380513","<p>If I read this right, this can be visualized as follows: (1) Draw a circle of radius r on a piece of paper; (2) Cut out the circle; (3) Cut the circle in half along a diameter; (4) Cut one of the half-circles in half along a radius; (5) Take one of the quarter-circles and roll it up, joining the two radius-edges.  (6) You now have a cone - calculate its volume.</p>
"
"2380497","2380515","<p>There is a ""proof without words"" picture at <a href=""https://en.wikipedia.org/wiki/Young%27s_inequality_for_products#Standard_version_for_increasing_functions"" rel=""nofollow noreferrer"">the Wikipedia page for ""Young's inequality""</a>.</p>
"
"2380499","2381486","<p>Part (b) of this problem was not correct. I figured out how to solve my own problem by doing the following: </p>

<p>Consider $g(z) = -\lambda + z$,</p>

<p>$$
|f(z) + g(z)| = | \lambda - z - e^{-z} + -\lambda + z | = |-e^{-z}| 
$$</p>

<p>Then</p>

<p>$$
|f(z)| + |g(z)| = |\lambda - z - e^{-z}| + |-e^{-z}| 
$$</p>

<p>Thus,</p>

<p>$$
|f(z) + g(z)| = |-e^{-z}| &lt; |f(z)| + |g(z)| = |f(z)| + |e^{-z}|
$$</p>

<p>Hence, by Rouche's Theorem</p>

<p>$$
Z_f + P_f = Z_g + P_g
$$</p>

<p>But $P_f = P_g = 0$. Since $g$ has only one zero in $|z - \lambda| &lt; 1$, then $f(z)$ has only one zero also.$\square$</p>
"
"2380506","2380588","<p>Revised 8 August.
Since $|z+x|^m$ is convex for all $z\in\mathbb R$ and $m\ge 1$, if  $m\ge 1$ then $f$ is convex.  </p>

<p>In general, if $m&gt;0$ then $f(x)$ is convex near $x=0$.  This follows from the formula $$f(x) = 2^{m/2}\frac{\Gamma\big(\frac{1+m}2\big)}{\sqrt{\pi}} M(-m/2, 1/2;  -x^2/2),$$  where $M$ is the confluent hypergeometric function, a.k.a. ${{}_1}F_1$.  The Taylor expansion of $M(-m/2,1/2; -x^2/2) $ starts out  $1+\frac m 2 x^2 +\ldots$ from which $f''(0)&gt;0$ follows.  Thus $f$ is concave for no value of $m&gt;0$.</p>

<p>Added 16 August, revised 18 August.  I believe that for $0&lt;m&lt;1$, there exists an $x_m&gt;0$ such that $f$ is convex on $[0, x_m]$ and concave on $[x_m,\infty)$. But I don't see a proof yet.</p>

<p>This would imply that, in the case $m&lt;1$,  the upper concave envelope $f^*$ of $f$ interpolates linearly between $f(0)$ and $f(x_m)$ on the range $[0,x_m]$ and is equal to $f$ on $[x_m,\infty)$, which in turn has implications for the OP's motivating optimization problem.</p>
"
"2380510","2380517","<p>The key is the multiplication principle: If a task can be done in $m$ ways and another in $n$ ways, the number of ways we can do both is $mn$.</p>

<p>The idea is that you're choosing a digit from $0$ to $9$ and constructing a six digit PIN. We can start with the case of one digit, then two, then I'll leave it to you to extend to six.</p>

<p>There are only ten one-digit PINs: $0, 1, 2, 3, 4, 5, 6, 7, 8, 9$. Ten digits.</p>

<p>What about two? We could list them out, but let's be smart: this is the same as the concatenation of two one-digit PINs. We have ten choices for the first digit and ten for the second. How many total two-digit PINs are there? By the multiplication prinicple, this is $10 \cdot 10 = 100$. There are a hundred possible 2-digit PINs.</p>

<p>Can you take it from here?</p>
"
"2380527","2380531","<p><strong>Hint</strong> If $0&lt;x &lt;y$ then $g(x) \leq g(y)$ and</p>

<p>$$g(y)-g(x) \leq 2(y-x)$$</p>

<p>Use this to show that $g$ is continuous.</p>
"
"2380534","2380557","<p>Note that this is can be modeled by a <a href=""https://en.wikipedia.org/wiki/Hypergeometric_distribution"" rel=""nofollow noreferrer"">hypergeometric</a> distribution for $a-1$ successes out of the first $c-1$ guesses, along with a $\frac{1}{b-c+1}$ chance of getting the last number on the next guess. Plugging in to the formula should give us $\frac{\binom{a}{a-1}\binom{b-a}{c-a}}{\binom{b}{c-1}} \times \frac{1}{b-c+1}$.</p>

<p>Alternatively, this can be seen as the probability of winning in at most $c$ turns but not winning in at most $c-1$ turns. There are $\binom{a}{a}\binom{b-a}{c-a}$ ways to win in $c$ turns, out of a total of $\binom{b}{c}$ possible turns, so this gives us
$$\frac{\binom{a}{a}\binom{b-a}{c-a}}{\binom{b}{c}} - \frac{\binom{a}{a}\binom{b-a}{c-a-1}}{\binom{b}{c-1}}$$
We check to make sure that this matches the above answer:
\begin{align*}
&amp;= \frac{\frac{(b-a)!}{(c-a)!(b-c)!}}{\frac{b!}{c!(b-c)!}} - \frac{\frac{(b-a)!}{(c-a-1)!(b-c+1)!}}{\frac{b!}{(c-1)!(b-c+1)!}}\\
&amp;=\frac{(b-a)!c!}{(c-a)!b!} - \frac{(b-a)!(c-1)!}{(c-a-1)!b!}\\
&amp;=\frac{(b-a)!c!- (c-a)(b-a)!(c-1)!}{(c-a)!b!}\\
&amp;=\frac{(b-a)!(c - (c-a))(c-1)!}{(c-a)!b!}\\
&amp;=\frac{a(b-a)!(c-1)!}{(c-a)!b!}\times\frac{\frac{1}{(b-c)!}}{\frac{1}{(b-c)!}}\\
&amp;=\frac{a\frac{(b-a)!}{(c-a)!(b-c)!}}{\frac{b!}{(c-1)!(b-c)!}}\\
&amp;=\frac{a\binom{b-a}{c-a}}{\frac{b!}{(c-1)!(b-c+1)!}}\times\frac{1}{b-c+1}\\
&amp;=\frac{a\binom{b-a}{c-a}}{\binom{b}{c-1}}\times\frac{1}{b-c+1}\\
\end{align*}
as desired.</p>
"
"2380549","2380790","<p>Making the problem more general, let us consider $$T=P\left [1+\sum_{i=1}^{n}a^i \right ]$$ and remember the geometric series
$$\sum_{i=1}^{n}a^i =\frac{a \left(a^n-1\right)}{a-1}$$ Now, rearrange to isolate the term $a^n$ and get $$a^n=\frac{(a-1) T+P}{a P}$$ Now take logarithms of both sides to get $$n=\frac{\log\left(\frac{(a-1) T+P}{a P} \right) }{\log(a)}$$ which is the exact solution.</p>

<p>Now, if $a=1+\epsilon$ where $\epsilon\ll 1$ and, for conveniency, setting $T=\lambda P$ we have $$n=\frac{\log \left(\frac{1+\lambda  \epsilon }{1+\epsilon}\right)}{\log (1+\epsilon
   )}$$ Using Taylor expansion around $\epsilon=0$, this would give $$n=(\lambda -1)-\frac{1}{2} \lambda\left(\lambda -1\right) \epsilon +\frac{1}{12}
   \left(4 \lambda ^3-3 \lambda ^2-\lambda \right) \epsilon ^2+O\left(\epsilon
   ^3\right)$$ For illustration purposes, let us consider $\lambda=10$ (that is to say $T=10P$) and $\epsilon=0.02$; the above approximation would give $n=8.22300$ while the exact value should be $n=8.20694$</p>

<p><strong>Edit</strong></p>

<p>If you want to avoid logarithms and Taylor series, you could use instead PadÃ© approximants. The simplest would give $$n=\frac{(\lambda -1) ((1+\lambda)  \epsilon  +6)}{(4 \lambda+1)  \epsilon  +6}$$ which, for the worked example, would give $n=\frac{2799}{341}\approx 8.20821$.</p>
"
"2380558","2380569","<p>Essentially you're dividing your circle into $4$ annuli of width $1$ each.</p>

<p>That's all well and good, but then you seem to be saying that the area of each annulus is $1$ times its length measured <em>along the outside</em>.</p>

<p>But actually the area of a ""fat curve"" of constant width, is the width of the curve times the length of the curve's <em>mid-line</em>. So you should be adding up the lengths of circles with radius $\frac12$, $1\frac12$, $2\frac12$, and $3\frac12$ instead -- giving you a sum of $8$, which does equal $\frac12 r^2$ in this case.</p>
"
"2380561","2380579","<p>This seems to be a matter of perspective. Adjust the ranges on either axis as needed. Below are two plots using a custom range and the default in Mathematica (which you appear to be using to generate the given plot):</p>

<p><a href=""https://i.stack.imgur.com/msHbd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/msHbd.png"" alt=""enter image description here""></a></p>

<p>The logarithm is plotted in blue, and the partial sum (10th in the images above) in orange. Note that the 10th partial sum may not be totally convincing, so feel free to tack on as many as you need to convince yourself. Also note that the logarithm is much more difficult to sort out from the plot on the right, thanks to the default plot range. On the left, the vertical axis ranges from -3 to 3. Tick marks are added at $x=\pm3$ for emphasis. (Code within spoilers)</p>

<blockquote class=""spoiler"">
  <p> <code>Plot[{Log[1 + x/3], -Sum[(-x/3)^n/n, {n, 1, 10}]}, {x, -20, 20}</code></p>
</blockquote>

<p>.</p>

<blockquote class=""spoiler"">
  <p> <code>Plot[{Log[1 + x/3], -Sum[(-x/3)^n/n, {n, 1, 10}]}, {x, -10, 10}, PlotRange -&gt; {-3, 3}, Ticks -&gt; {{-3, 3}, None}]</code></p>
</blockquote>

<p>Why at $x=\pm3$? Recall that</p>

<p>$$\ln(1+x)=-\sum_{n\ge1}\frac{(-x)^n}n$$</p>

<p>By the ratio test, one can show that the series converges for</p>

<p>$$\lim_{n\to\infty}\left|\frac{\frac{(-x)^{n+1}}{n+1}}{\frac{(-x)^n}{n}}\right|=|x|\lim_{n\to\infty}\frac n{n+1}=|x|&lt;1$$</p>

<p>Replacing $x$ with $\dfrac x3$, one can establish that</p>

<p>$$\ln\left(1+\frac x3\right)=-\sum_{n\ge1}\frac{\left(-\frac x3\right)^n}n$$</p>

<p>converges for $\left|\dfrac x3\right|&lt;1$, or $|x|&lt;3$, and the image seems to corroborate this at first glance.</p>
"
"2380570","2380599","<p>I presume this is in the plane, with no three points collinear (otherwise with three collinear points you could say there are infinitely many intersections), 
and you're talking about the intersections (other than the points themselves) of the line segments joining the points. </p>

<p>Consider first the case $n=4$.  If one of the points is inside the convex hull of the other three, there are no intersections.  Otherwise, there is one.</p>

<p>Now for the general case with $n$ vertices.  If the points are in ""general position"", no points are the intersection of more than two lines, and there is a one-to-one map from the intersections to unordered $4$-tuples of points, taking each intersection to the endpoints of the lines that intersect there.  If none of the points is in the convex hull of the others (e.g. if the points are all on a circle), this map is also onto.  Thus the maximum number of intersections is $n \choose 4$.</p>
"
"2380582","2380611","<p>Take $ R = \mathbb{Z}$. Any finite $\mathbb{Z}$-module is isomorphic to $\mathbb{Z}_{d_1} \times \cdots \times \mathbb{Z}_{d_n}$, where $d_1 \ |\  d_2 \ | \cdots \ |\ d_n$ are positive integers. If $n \geq 2$, then we can take</p>

<p>$$M_1 = \mathbb{Z}_{d_1} \times \{0\} \times \cdots \times \{0\}$$ and  $$M_2 = \{0\} \times \mathbb{Z}_{d_2} \times \{0\} \times\cdots \times \{0\}$$ (or any two factors of the above factorization) Indeed, $Ann(M_i) = d_i \mathbb{Z}$, and $Ann(M_1 \cap M_2) = \mathbb{Z}$. Then</p>

<p>$$  Ann(M_1) + Ann(M_2) = d_1 \mathbb{Z} \subsetneq \mathbb{Z} = Ann(M_1 \cap M_2)  $$</p>
"
"2380583","2380842","<p>Assuming that $p^m=q^n$:</p>

<p>Certainly then $p\mid q^n$ and $q\mid p^m$</p>

<p>$p$ prime $\implies p\mid q$</p>

<p>$q$ prime $\implies q\mid p$</p>

<p>Giving $p=q$ and this contradicts the specification that $p$ and $q$ are distinct primes.</p>
"
"2380593","2380600","<p><strong>hint</strong></p>

<p>Let $g (x)=f (x)-x $</p>

<p>then
$$g (0)=g (1)=g (2)=0$$</p>

<p>Apply Rolle's Theorem at $[0,1] $ and $[1,2] $ to get $c_1$ and $c_2$.
then apply it again to the derivative $g'$ at $[c_1,c_2] $.</p>

<p>observe that $$g''(x)=f''(x). $$</p>
"
"2380605","2380606","<p>Ruling out the solution $(0,0,0)$, we have $xyz \neq 0$.
WLOG $x \geq y \geq z$, then $n(x+y+z) \leq 3n x$ and so we must have $yz \leq 3n$, in particular $\max(y,z) \leq 3n$. So there are finitely many possible $(y,z)$ and finally $x$ is determined by $(y,z)$.</p>
"
"2380609","2380632","<p>Any polynomial can be written as $p(x) = \displaystyle\sum_{i=0}^n a_ix^i$, where the $a_i$ are the respective coefficients, and $n$ is the degree of the polynomial if $a_n \neq 0$.</p>

<p>Now, if $f$ is as given in the question,
$$
\int_0^1 f(x)p(x) = \int_0^1 f(x) \left(\displaystyle\sum_{i=0}^n a_ix^i\right) = \int_{0}^1 \sum_{i=0}^n a_i f(x)x^i = \sum_{i=0}^n a_i\left(\int_0^1 f(x)x^i\right) = 0
$$</p>

<p>Which is simply written as : the integral is linear, so you can inspect the behaviour of monomials to understand the behaviour of polynomials under the integral.</p>

<p>As for the question of finite intervals, and intervals with negative end points, the theorem still holds true, because the Weierstrass theorem still holds true for these intervals. Depending on the proof you have seen for the Weierstrass theorem, you will be able to adapt the proof for the general case. For example, you can scale the Bernstein polynomials appropriately for a general interval, when it is given for $[0,1]$. The rest of this proof is as you have written.</p>
"
"2380616","2381644","<p>Performance variables, i.e. the variables you actually want to control, in contrast to $y$ which are the measured signals.</p>
"
"2380618","2380622","<p>Remember that the entries on the left are <em>coefficients</em> of variables. So from your final reduction, we would end up with the RREF:
$$\left[\begin{array}{ccc|c}
1&amp;0&amp;0&amp;0\\
0&amp;0&amp;1&amp;0\\
0&amp;0&amp;0&amp;0\\
\end{array}\right]$$</p>

<p>The first line tells us that $1x + 0y + 0z = 0$, so $x = 0$. The second line tells us that $0x + 0y + 1z = 0$, i.e. $z=0$. But we have $y$ as a free variable, so our solution is $(0,y,0)$.</p>

<p>Note that we can tell that $y$ is free by observing the original matrix - there is no coefficient in its column, so its value doesn't affect our final solution.</p>
"
"2380624","2380661","<p>A rational expression involving a set of numbers (or variables, or other things you can add and multiply) is an expression using those numbers and the operations addition, subtraction, multiplication and division. </p>

<p>Since integral powers are just repeated multiplications, they are allowed in rational expressions. </p>

<p>So
$$
\frac{3x^2}{x-1}
$$
is a rational expression in $x$.</p>

<p>Sometimes an expression that does not look like a rational expression has a (more or less) equivalent form that is. So
$$
\sqrt[3]{(x  - 3)^3}
$$
isn't a rational expression, but it's equal to the rational expression $x-3$.  </p>

<p>I think that may explain the ""reducible"" and ""irreducible"" examples in the definition, but I don't like that distinction.</p>

<p>With square roots (as in the example you cite) you have to be careful about positive square roots when necessary - but that's a whole other kind of question.
The fact that $|x|$ is either $x$ or $-x$ does <em>not</em> make $|x|$ a rational expression, since you need the full logic ""if $x \ge 0$ then $|x| = x$ else ..."" to calculate it, and the ""if"" part isn't a rational expression.</p>
"
"2380627","2380637","<p>Let $\,x \gt 1\,$ be the expression in question, then:</p>

<p>$$\require{cancel}
2x^8=2207 + \sqrt{2207^2-4} \\
(2x^8-2207)^2=2207^2-4 \\
4x^{16}-4\cdot 2207 x^8+\cancel{2207^2}=\cancel{2207^2}-4 \\
x^{16}-2207 x^8 + 1 = 0 \\
x^8 + \frac{1}{x^8}=2207 \\
\left(x^4+\frac{1}{x^4}\right)^2 = 2209 \\
x^4+\frac{1}{x^4} = 47 \\
\left(x^2+\frac{1}{x^2}\right)^2 = 49 \\
x^2+\frac{1}{x^2} = 7 \\
\left(x+\frac{1}{x}\right)^2 = 9 \\
x+\frac{1}{x} = 3 \\
x^2-3x+1=0
$$</p>
"
"2380628","2380656","<p>In first-order logic you can quantify over individuals (that is, elements of the universe of a structure).</p>

<p>In second-order logic you can <em>additionally</em> quantify over predicates and functions that take individuals as arguments. For example you can write things like $\forall x \forall y \exists P : P(x) \land \neg P(y)$. The $\exists P$ is not allowed in first-order logic.</p>

<p>In <em>third</em>-order logic you can also quantify over meta-predicates (and meta-functions) that take ordinary predicates/functions as arguments.</p>

<p>And so forth.</p>

<p>Generally, <em>higher-order logic</em> allows you to take this to any depth, and you get an entire type theory to play with.</p>

<p>ZFC and PA are both first-order theories. (In ZFC, of course, you can quantify over <em>sets</em>, which looks a lot like quantifying over predicates -- but with the caveat that only some of the possible predicates correspond to sets).</p>
"
"2380638","2383641","<p>Because for all $x&gt;0$ we have $0&lt;\arctan x&lt;x$ we conclude by a simple induction argument that the sequence $(x_n)_{n\ge0}$ is positive decreasing, so it must converge to some limit $\ell$ that satisfy $\ell=\arctan \ell$, that is $\ell=0$. Thus,
$$\lim_{n\to\infty}x_n=0\tag{1}$$
It follows that
$$\lim_{n\to\infty}\frac{x_{n+1}}{x_n}=\lim_{n\to\infty}
\frac{\arctan{x_n}}{x_n}=1\tag{2}$$
Now,
$$\eqalign{\frac{1}{x_n^2}-\frac{1}{x_{n+1}^2}&amp;=\frac{x_n^2}{x_{n+1}^2}\cdot\frac{(\arctan x_n)^2-x_n^2}{x_n^4}\cr
&amp;=\frac{x_n^2}{x_{n+1}^2}\cdot\frac{(x_n-x_n^3/3+O(x_n^5))^2-x_n^2}{x_n^4}\cr
&amp;=\frac{x_n^2}{x_{n+1}^2}\cdot\left(-\frac{2}{3}+O(x_n^2)\right)
}$$
It follows that
$$\lim_{n\to\infty}\left(\frac{1}{x_{n+1}^2}-\frac{1}{x_{n}^2}\right)=\frac{2}{3}\tag{3}$$
Using  <a href=""https://en.m.wikipedia.org/wiki/Stolz%E2%80%93Ces%C3%A0ro_theorem"" rel=""noreferrer"">Stolz-CesÃ ro</a> theorem we conclude that
$$\lim_{n\to\infty}\frac{1}{nx_n^2}=\frac{2}{3}\tag{4}$$
Or
$$\lim_{n\to\infty}\sqrt{n}x_n=\sqrt{\frac{3}{2}}$$
Which is the desired conclusion.</p>
"
"2380643","2381905","<ol>
<li><p>Your proof of convergence in $L^2$ is correct. </p></li>
<li><p>Let us prove  the pointwise convergenge in the open interval $(-2,2)$.</p></li>
</ol>

<p>Consider, for $-2 &lt; x &lt; 2$, </p>

<p>$$ F(x) = \frac{4}{\pi}\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n}e^{ \frac{n\pi x}{2}i}$$ </p>

<p>If $x\in (-2,2)$, then we have that $\sum_{n=1}^{\infty}(-1)^{n+1}e^{ \frac{n\pi x}{2}i}$ converges. In fact,
$$\sum_{n=1}^{\infty}(-1)^{n+1}e^{ \frac{n\pi x}{2}i}=   
 \left (\frac{e^{ \frac{\pi x}{2}i}}{1+e^{ \frac{\pi x}{2}i}} \right )$$</p>

<p>So, there is $M&gt;0$ such that, for all $N\geqslant 1$,</p>

<p>$$\left | \sum_{n=1}^{N}(-1)^{n+1}e^{ \frac{n\pi x}{2}i}\right| \leqslant M $$</p>

<p>And since, $\{\frac{1}{n}\}_{n\geqslant 1}$ is a non-increasing sequence of real number such that, as $n \to \infty$, $ \frac{1}{n} \to 0$, we can apply Dirichlet's test to conclude that, if $x\in (-2,2)$, then $F(x)$ converges. </p>

<p>So we have prove that, for all $x\in (-2,2)$, </p>

<p>$$ F(x) = \frac{4}{\pi}\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n}e^{ \frac{n\pi x}{2}i}$$ </p>

<p>converges pointwisely. </p>

<p>Now, note that </p>

<p>$$f(x) = \frac{4}{\pi}\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n}\sin \frac{n\pi x}{2}$$</p>

<p>is the imaginary part of $F(x)$. So we have that, for all $x\in (-2,2)$, </p>

<p>$$f(x) = \frac{4}{\pi}\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n}\sin \frac{n\pi x}{2}$$</p>

<p>converges pointwisely. </p>

<ol start=""3"">
<li>Uniform convergence. Note that </li>
</ol>

<p>$$f(x) = \frac{4}{\pi}\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n}\sin
    \frac{n\pi x}{2}$$ 
does NOT converge uniformly on $(-2,2)$. </p>

<p>To see it, consider </p>

<p>$$f_N(x) = \frac{4}{\pi}\sum_{n=1}^{N}\frac{(-1)^{n+1}}{n}\sin
    \frac{n\pi x}{2}$$ </p>

<p>Note that for all $N \geqslant 1$, $f_N$ is continuous on $[-2,2]$. Note also that $f$ (defined by $f(x)=x$ is also continuous on $[-2,2]$.</p>

<p>From item 2, we know that, for all $x\in (-2,2)$, $f_N(x)$ converges to $f(x)$. However, for $x=-2$, for all $N\geqslant 1$, $f_N(-2)=0$ and $f(-2)=-2$. In a similar way, for $x=2$, for all $N\geqslant 1$, $f_N(2)=0$ and $f(2)=2$.</p>

<p>So $f_N$ does not converge uniformly to $f$ on $(-2,2)$. </p>

<p>In fact, suppose $f_N$  converges uniformly to $f$ on $(-2,2)$. Then there is $N_0 \in \mathbb{N}$ such that for any $N&gt;N_0$, and any $x \in (-2,2)$,</p>

<p>$$|f_N(x)-f(x)| &lt;1/2$$</p>

<p>In particular, for any $x \in (-2,2)$,</p>

<p>$$|f_{N_0+1}(x)-f(x)| &lt;1/2  \tag{1}$$</p>

<p>But, since $f_{N_0+1}$ is continuous on $[-2,2]$, there is $\delta_1&gt;0$ such that, for all  $x \in (-2,-2+\delta_1)$</p>

<p>$$|f_{N_0+1}(-2)-f_{N_0+1}(x)| &lt;1/2 \tag{2} $$</p>

<p>Since $f$ is continuous on $[-2,2]$, there is $\delta_2&gt;0$ such that, for all  $x \in (-2,-2+\delta_2)$</p>

<p>$$|f(x)-f(-2)| &lt;1/2 \tag {3}$$</p>

<p>Take $\delta = \min\{\delta_1, \delta_2\}$. Combining $(1)$, $(2)$, $(3)$, we have, for all  $x \in (-2,-2+\delta)$</p>

<p>\begin{align*} 2 = &amp;|f_{N_0+1}(-2) - f(-2)| \leqslant \\ &amp; \leqslant  |f_{N_0+1}(-2)- f_{N_0+1}(x)|+ |f_{N_0+1}(x)-f(x)| + |f(x)-f(-2)|&lt; \\ &amp;&lt; (1/2)+ (1/2)+(1/2) =3/2
\end{align*}
Contradiction. So we have proved that $f_N$ does not converge uniformly to $f$ on $(-2,2)$. </p>

<p>It means the series $$\frac{4}{\pi}\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n}\sin \frac{n\pi x}{2}$$ does not converge uniformly to $f$ on $(-2,2)$. </p>
"
"2380644","2380655","<p>The continuity at $x=3$, gives</p>

<p>$$18-9b=a $$</p>

<p>the differentiability at $x=3$, yields to</p>

<p>$$\lim_{3^-}\frac {6x-18}{x-3}=\lim_{3^+}\frac {bx^2-9b}{x-3} $$</p>

<p>or
$$6=6b $$
thus
$$b=1 \;\;,\; \;a=9$$</p>
"
"2380646","2380649","<p>the fact that $|f_n(x)-f_n(y)|\leq |x-y|^{\alpha}$ implies that $f_n$ is equicontinuous (thus continuous).</p>

<p>To see this, consider $c&gt;0$, $|x-y|^{\alpha}=exp(\alpha ln(|x-y|)&lt;c$ is equivalent to $ln(|x-y|)&lt;{{ln(c)}\over\alpha}$ and $|x-y|&lt;exp({{ln(c)}\over\alpha})$, so if you take $|x-y|&lt;exp({ln({c)}\over\alpha})$, you obtain that $|f_n(x)-f_n(y)|\leq|x-y|^{\alpha}&lt;c$, this shows that the sequence $f_n$ is equicontinuous and $f_n$ henceforth continuous for every $n$.</p>
"
"2380658","2380676","<p>To add to @Peyton:</p>

<p>Regard $H$ as all $L$ tuples, whose $i$th component is chosen from the real vector space $H_i$. Then for $h \in H$
$$\Vert h \Vert = \Vert (h_1, \ldots, h_L)\Vert = \sqrt{\Vert h_1 \Vert^2 + \ldots \Vert h_L \Vert^2}$$
where the norm of each $h_i$ is from its respective space. Though I don't know the whole context, I think it is safe to assume that the norms on each $H_i$ is also the Euclidean norm
$$\Vert h_i \Vert = \sqrt {\sum_j h_{i, j}^2 }$$
where $h_{i,j}$ is the $j$-coordinate of $h_i$. That way</p>

<p>$$\Vert h \Vert = \sqrt {\sum_{i,j} h_{i, j}^2 }$$</p>

<p>and the entire space is just the Euclidean space of dimension $\sum_i \dim H_i$. If you give the composing subspaces a fancy norm, you can get norms on the direct sum. But it would be a weird object, given your paper's title.</p>
"
"2380663","2380665","<p>If you have to choose $2$ our of $4$ female teachers, then you have to choose $0$ out of $4$ males, namely, $\binom{4}{0}$ males. And then multiply it by the total number of possible $2$ out of $3$ females, namely $\binom{3}{2}$, i.e., 
$$
\frac{\binom{4}{0} \binom{3}{2}}{\binom{7}{2}}
$$</p>
"
"2380669","2380675","<p>In integration theory when you want to use a u-subsitution you have to use a function which is a $C^1$-diffeomorphism. Otherwise you could end up with funny things.</p>

<p><strong>Edit</strong> Actually a diffeomorphism is not necessary as pointed out by @zhw. To apply the substitution $x=\psi(u)$ from $u \in [c,d]$ to $x \in [a,b]$ you ""only"" need to have $\psi$ to be $C^1$ and $\psi([c,d])$ included in the domain of $f$, with $\psi(c)=a$ and $\psi(d)=b$.</p>

<p>For example if you want to compute $\int_{-1}^1 x^2\mathrm{d}x$, using a u-substitution $u=x^2$ will change the bounds to $1$ and $1$ and give 0, which is not true. This is because the function $x \in [-1,1] \mapsto x^2$ is not a bijection.</p>

<p>In your case your function $x \in [a, b] \mapsto x(x-a-b)$ has the following  graph for $a=-1$ and $b=1$. You clearly see that this function is not 1-1. <a href=""https://i.stack.imgur.com/M61GR.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/M61GR.png"" alt=""enter image description here""></a></p>

<p>The fact that your substitution is not 1-1 is clearly due to the square. Let $\phi: x \in [a,b] \mapsto x(x-a-b)$. The function $\phi$ reaches its minimum in $c=(a+b)/2$ and is bijective on $[a, c]$ and on $[c, b]$. If you split your integral into 2 integrals on these intervals you can use your substitution.</p>

<p>You would get $\int_a^b f(x) \mathrm{d}x=\int_a^c f(x)\mathrm{d}x + \int_b^c f(x)\mathrm{d}x$</p>

<p>Now use $u=\phi(x)=x(x-a-b)$.
You have therefore $x=\dfrac{a+b-\sqrt{(a+b)^2+4u}}{2}=\psi_1(u)$ on $[a,c]$ and $x=\dfrac{a+b+\sqrt{(a+b)^2+4u}}{2}=\psi_2(u)$ on $[c,b]$</p>

<p>This would give $$\int_a^b f(x) \mathrm{d}x=\int_{-ab}^{-(a+b)^2/4}f(\psi_1(u))\psi_1'(u)\mathrm{d}u + \int_{(a+b)^2/4}^{-ab}f(\psi_2(u))\psi_2'(u)\mathrm{d}u$$</p>

<p>And you see that you cannot ""merge"" these two integrals because their content is now different.</p>

<p>Of course this u-substitution does not seem to help a lot to compute the integral, particularly because $f$ is general. But if $f$ has a particular form, maybe this could help.</p>
"
"2380670","2380721","<p>For $\ell&gt;2$ a rectangle $a(\ell)\times b(\ell)$,</p>

<p>\begin{align}
a(\ell)&amp;=\frac{\ell}2\,(\ell-\sqrt{\ell^2-4})
,\\
b(\ell)&amp;=\frac{\ell}2\,(\ell+\sqrt{\ell^2-4})
,\\
S&amp;=a\,b = \ell^2
,\\
L&amp;=2\,(a+b)=2\,\ell^2
.
\end{align}</p>
"
"2380679","2380690","<p>Seeking a contradiction, suppose $\{a_1,\dotsc,a_n,a_{n+1}\}$ is linearly dependent, so the equation
$$
\lambda_1\cdot a_1+\dotsb+\lambda_n\cdot a_n+\lambda_{n+1}\cdot a_{n+1}=\mathbf 0\tag{1}
$$
has a solution where $\lambda_k\neq0$ for some $k$. Note that $\lambda_{n+1}=0$ implies that $\lambda_i=0$ for each $i$ since $\{a_1,\dotsc,a_n\}$ is linearly independent. Thus $a_{n+1}\neq0$. Put $\mu_i=-\lambda_i/\lambda_{n+1}$. Then dividing (1) by $\lambda_{n+1}$ and solving for $a_{n+1}$ gives
$$
\mu_1\cdot a_1+\dotsb+\mu_n\cdot a_n=a_{n+1}
$$
a contradiction since $a_{n+1}\not\in\DeclareMathOperator{Span}{Span}\{a_1,\dotsc,a_n\}$. Hence $\{a_1,\dotsc,a_n,a_{n+1}\}$ is linearly independent.</p>
"
"2380696","2380701","<p>No. The variable of integration is a loop variable and it goes out of scope before the endpoints get evaluated.</p>
"
"2380698","2381341","<p>Let $f$ be a complex-valued function. We denote $g=\text{Re} f$ and $h=\text{Im} f$. As $|f|\leq g^++g^-+h^++h^-$ then 
$$\int_A|f|\leq \int_Ag^++\int_Ag^-+\int_Ah^++\int_Ah^-\leq4\max\left(\int_Ag^+,\int_Ag^-,\int_Ah^+,\int_Ah^-\right).$$
If we suppose that $\max\left(\int_Ag^+,\int_Ag^-,\int_Ah^+,\int_Ah^-\right)=\int_Ag^+$ then
$$\int_A|f|\leq4\int_Ag^+=\int_Ag1_{\{g\geq0\}}=4\int_{A\cap\{g\geq0\}}g=4\left|\int_Bg\right|\leq4\left|\int_Bf\right|$$
with $B=A\cap\{g\geq0\}\subset A$ and belongs to $\Sigma$.</p>
"
"2380702","2380719","<p>Rewrite it as $x^2 +\dfrac{1}{x^2} - a\left(x+\dfrac{1}{x}\right) + 11 = 0$, and put $y = x+\dfrac{1}{x}\implies y^2- ay+9=0$, and this equation has $2$ positive distinct roots. Thus $\triangle &gt; 0 \implies a^2-36 &gt; 0\implies  a &gt; 6 = m$. Thus $y = \dfrac{a \pm \sqrt{a^2-36}}{2}&gt; 2\implies a - \sqrt{a^2-36} &gt; 4\implies (a-4)^2 &gt; a^2-36 \implies 8a &lt; 52 \implies a &lt; \dfrac{13}{2} = M \implies m+2M = 6 + 2\cdot \dfrac{13}{2} = 19$.</p>
"
"2380708","2381290","<p>Suppose the two digits of a solution $x$ are $a$ and $b$, giving $x=10a +b$. Since $b&lt;10$, $10a&gt;ab$ and so $x&gt;ab$. Therefore any solution must obey $10a+b - ab = 12$.</p>

<p>This factorizes as $(10-b)(a-1)+10 =12$, giving $(10-b)(a-1) = 2$. Since $a&gt;0$, both of the factors are positive and must be $1$ and $2$ in some order. </p>

<p>$$
\left . 
\begin{align}
(a-1) &amp;=2 \\
(10-b) &amp;=1 
\end{align}
\right\} (a,b) = (3,9)
$$
$$
\left . 
\begin{align}
(a-1) &amp;=1 \\
(10-b) &amp;=2 
\end{align}
\right\} (a,b) = (2,8)
$$</p>

<p>Given this solution framework, it's an interesting exercise to substitute other values in place of the $12$ and find the cases which have four solutions. </p>
"
"2380710","2381925","<p>No, Your solution dose not work. Indeed,</p>

<blockquote>
  <p>$$\sin z_1=sin z_2\Leftrightarrow z_1-z_2=2k\pi ~\text{or }~z_1+z_2=2k\pi +\pi(k\in \mathbb{Z}).$$</p>
</blockquote>

<p>$\newcommand{\dj}{\Leftrightarrow } \newcommand{\yb}[1]{\frac{#1}{2}}$
Proof. \begin{align*}
&amp; \quad ~\sin z_1=\sin z_2\\
&amp; \dj e^{iz_1}-e^{-iz_1}=e^{iz_2}-e^{-iz_2}\\
&amp; \dj (e^{i\yb{z_1-z_2}}-e^{-i\yb{z_1-z_2}})(e^{i\yb{z_1+z_2}}+e^{-i\yb{z_1+z_2}})=0\\
&amp; \dj e^{i\yb{z_1-z_2}}=e^{-i\yb{z_1-z_2}} ~\text{or}~ e^{i\yb{z_1+z_2}}=-e^{-i\yb{z_1+z_2}}\\
&amp; \dj e^{i(z_1-z_2)}=1 ~\text{or}~ e^{i(z_1+z_2)}=-1\\
&amp; \dj z_1-z_2=2k\pi ~\text{or}~ z_1+z_2=2k\pi +\pi(k\in \mathbb{Z}).
\end{align*}</p>

<p>Then injectivity can be easily obtained by this.  </p>
"
"2380712","2380870","<p>Denote $S(n) = \sum_{k=1}^n \frac{X_n}{n}$. </p>

<p>Write 
$$
\mathrm{E}\left[\sum_{n=2}^\infty n^3\big(S(n^4) - S((n-1)^4)\big)^2\right] = \sum_{n=2}^\infty n^3\mathrm{E}\left[\big(S(n^4) - S((n-1)^4)\big)^2\right] \\
= \sum_{n=2}^\infty  n^3\sum_{k=(n-1)^4 +1}^{n^4} \mathrm{E}\left[\frac{X_k^2}{k^2}\right]\le  \sum_{n=2}^\infty n^3\cdot \frac{4n^3}{(n-1)^8}&lt;\infty.
$$
Therefore, 
$$
\sum_{n=2}^\infty n^3\big(S(n^4) - S((n-1)^4)\big)^2&lt;\infty
$$
almost surely, in particular, 
$$
n^{3/2}\big|S(n^4) - S((n-1)^4)\big|\to 0, \ n\to\infty,
$$
almost surely. 
Therefore, 
$$
\sum_{n=2}^\infty \big|S(n^4) - S((n-1)^4)\big|&lt;\infty
$$
almost surely. 
On the other hand,
$$
\max_{k \in ((n-1)^4,n^4]} |S(k) - S((n-1)^4)|\le \frac{4n^3}{(n-1)^4}\to 0,n\to\infty.
$$
It follows from the last two facts (an exercise for you) that the sequence of partial sums is Cauchy almost surely. Consequently, it converges almost surely. </p>

<p>This argument works for any uncorrelated centered variables $X_n$ bounded by the same constant.</p>

<hr>

<p>As a side remark, I consider the linearity of expectation the most powerful tool in the probability theory. It is a miracle how many things you can prove with it. For example, the above argument uses no other probabilistic tools, just linearity of expectation; everything else is some routine calculus.</p>
"
"2380713","2381824","<p>Let $T$ be the true value and $E_{0.1}$ and $E_{0.05}$ denote the errors with step sizes $\Delta t = 0.1$ and $\Delta t = 0.05$ respectively. Since the method is fourth order accurate we expect the ratio of the errors to scale as $2^4$ i.e.
$$
\frac{E_{0.1}}{E_{0.05}} = \frac{T - 24.1}{T - 23.5} = 2^4 = 16.
$$
Solving for $T$ gives
$$
T = 23.46,
$$
from which we find that
$$
E_{0.1} = 24.1 - 23.46 = 0.64
$$
(and $E_{0.05} = 0.04$), so the answer is (d).</p>
"
"2380723","2380749","<p>Interpreting the math as a computer program, which is often called <a href=""https://en.wikipedia.org/wiki/Pseudocode"" rel=""nofollow noreferrer"">pseudocode</a>, we have:</p>

<p>$\quad a \leftarrow 10$</p>

<p>$\quad i \leftarrow 20$</p>

<p>$\quad a \leftarrow a+b \quad \textbf{a=10+20=30}$</p>

<p>$\quad b \leftarrow a+b \quad \textbf{b=30+20=50}$</p>

<p>I've added comments in bold, hoping that this makes things more clear.  So at the end of this code, we have $a=30$ and $b=50$.  For the next code, I've added a loop:</p>

<p>$\quad a \leftarrow 0$</p>

<p>$\quad i \leftarrow 1$</p>

<p>$\quad$ For $j=1$ to $5$ Do</p>

<p>$\quad \quad a \leftarrow a+0 \quad \textbf{a=a}$</p>

<p>$\quad \quad i \leftarrow a+i \quad \textbf{i=1+0=1}$</p>

<p>$\quad$ EndFor</p>

<p>We can observe that $a$ does not change in the loop, since we always add $0$ to $a$, which equals $a$.  So we can basically ignore $a$, which I hope is clear.</p>

<p>Then, having this intuition, we see that $i$ will not change in the loop, since we always add $0$ to it.  So no matter how many times we loop, $i$ will always be equal to $1$.</p>
"
"2380726","2381911","<p>I will work out a hypothetical example.  You should be able to adapt this to your needs.  </p>

<p>This cryptanalysis assumes the plaintext was in natural and grammatical written English.  (As you're working in $\mathbb{Z}_{26}$ I believe this is safe to assume.)  </p>

<p>The hard part of breaking Hill is that you must make two very good guesses about your original plaintext.  These guesses are more sensitive than the ""most common character"" kind of guess you make on, say, an affine cipher.  So, it is easier to guess wrongly, in which case you must guess again.</p>

<p>Let's suppose that the (unknown) invertible key matrix is
$$
K = \begin{pmatrix} a &amp; b \\ c &amp; d\end{pmatrix}.
$$
Your first assignment is to find the first <strong>and second</strong> most common digraphs (or bigrams) in your intercepted ciphertext.  For you this will be hard because your ciphertext is too short:  ""su"" is the most common at 4 occurrences, but then there is (unfortunately for you) a 5-way tie for the second-most common bigram, each occurring 3 times.  </p>

<p>For my hypothetical example, let's suppose that everything is more clear cut, and that the most common bigram is <strong>fv</strong>  and the second-most common is <strong>qt</strong>.  This means, respectively, that the column vectors $(5,21)^t$ and $(16,19)^t$ are the first- and second-most common cipher-image vectors.  </p>

<p>Here's where we make an assumption on our plaintext.  The best probabilistic guess is that <strong>th</strong> and <strong>he</strong> are the first- and second-most common bigrams in the plaintext.  This isn't hard to believe, but is often wrong in practice, especially for smaller plaintexts.  Since <strong>th</strong> is $(19,7)^t$ and <strong>he</strong> is $(7,4)^t$ this gives us a pair of equations:
$$
K(19,7)^t = (5,21)^t\\
K(7,4)^t = (16,19)^t.
$$</p>

<p>However, we can be slicker and take these two matrix-by-column=column equations and stack the columns next to each other to give a <strong>single</strong> matrix-by-matrix=matrix equation.  In my example we get
$$
\begin{pmatrix} a &amp; b \\ c &amp; d\end{pmatrix} \begin{pmatrix} 19 &amp; 7 \\ 7 &amp; 4\end{pmatrix} = \begin{pmatrix} 5 &amp; 16 \\ 21 &amp; 19\end{pmatrix}.
$$</p>

<p>Now we can easily solve for $K = \begin{pmatrix} a &amp; b \\ c &amp; d\end{pmatrix}$ since $\begin{pmatrix} 19 &amp; 7 \\ 7 &amp; 4\end{pmatrix}$ is invertible (this is a <strong>very</strong> happy coincidence of the English language).  </p>

<p>In fact, this latter matrix has determinant 1, so it is even easier to invert.  Doing so, its inverse is
$$
\begin{pmatrix} 4 &amp; 19\\ 19 &amp; 19\end{pmatrix}.
$$</p>

<p>Now we can right-multiply (be careful!) in our equation above and directly solve for $K$:
$$
K = \begin{pmatrix} 5 &amp; 16 \\ 21 &amp; 19\end{pmatrix} \begin{pmatrix} 4 &amp; 19\\ 19 &amp; 19\end{pmatrix}= \begin{pmatrix} 12 &amp; 9 \\ 3 &amp; 6\end{pmatrix}.
$$</p>

<p>This is your absolute best key-guess.  Try to decrypt your ciphertext under this key and see if you get decent plaintext.  If not, you must make new guesses and restart the whole process again. Good luck!</p>
"
"2380736","2380763","<p>Every location in the matrix is unique, and there are $m\times n$ possible elements to be put in one position, after which there are $m \times n -1$ possible elements to be put in the next position ... etc.</p>

<p>So, it is simply $(m\times n)!$</p>
"
"2380737","2380771","<p>Considering the function $$f(x) = e^{-x}\sin(x)$$ there are a few things we can notice first $$f(-\pi)=0 \qquad f(0)=0 \qquad f(\pi)=0$$ On the other side, and this would help $$f(-x)=-\frac 1 {f(x)}$$ Now, considering $$f'(x) = e^{-x}(\cos(x)-\sin(x))$$ there is a maximum at $x=\frac \pi 4$ and a minimum at  $x=-\frac {3\pi} 4$.</p>

<p>Let us try to evaluate $$f(-\frac {3\pi} 4)=-\frac{e^{3 \pi /4}}{\sqrt{2}}$$ We know that $e=2.718$; let us use $3$ instead. $\frac {3\pi} 4 \approx 2.36$; let us use $2$; this makes $e^{3 \pi /4}\approx 3^2=9$ and being lazy, use $\sqrt{2} \approx 1.5$. Combining all of that would give $$f(-\frac {3\pi} 4)\approx -6\implies f(\frac {\pi} 4)\approx \frac 16$$ Now, try to draw a smooth curve going through these five points.</p>
"
"2380744","2380757","<p>First of all, you <em>can</em> quantify over classes in NBG: in NBG, unlike ZFC, variables can refer to classes, and you are free to quantify over these variables.  So there is no difficulty at all formulating something like recursion over $Ord$ in NBG.</p>

<p>In ZFC, you are correct that since you can only refer to classes by writing down specific formulas, you can only prove that a class with certain properties ""exists"" by exhibiting a formula for it.  As for uniqueness, it doesn't actually present any challenge you haven't already described how to handle.  Just like you handled ""for all $G:V\to V$"" by a theorem schema, you can handle the universal quantifier in expressing uniqueness by a theorem schema.  That is, for each pair of formulas $F$ and $F'$, you have a separate theorem saying that if $F$ and $F'$ both satisfy the condition then $F$ and $F'$ define the same class.</p>

<p>In certain specific examples, there may be ways to indirectly make quantified statements over classes with just a single sentence in ZFC.  For instance, consider the statement ""there exists a class which is a well-ordering of $V$"".  A priori, this cannot be expressed in ZFC without specifying a specific formula which you are using to well-order $V$.  However, you can prove a theorem that says there exists a class which well-orders $V$ iff there exists a set $A$ such that $V=HOD[A]$.  More precisely, assuming $V=HOD[A]$, you can write down a specific class which well-orders $V$, and you can prove a theorem schema saying that for any class $F$, if $F$ well-orders $V$, then there exists a set $A$ such that $V=HOD[A]$.  So for practical purposes, you can use the statement ""there exists a set $A$ such that $V=HOD[A]$"" (which is a perfectly good sentence in the language of ZFC) as a proxy for the statement ""there exists a class which is a well-ordering of $V$"".</p>
"
"2380745","2380806","<p>Consider $G=\text{SL}(2,F)$ for an arbitrary field $F$. For an element to have order $m$ (coprime to the characteristic of $k$ if this is prime)
its eigenvalues are $\zeta$ and $\zeta^{-1}$ where $\zeta$ is a primitive
$m$-th root of unity, that is $\zeta$ is a zero of the $m$-th cyclotomic
polynomial $\Phi_m$. As long as $m\ge3$, the trace $t=\zeta+\zeta^{-1}$
must therefore be a root of the ""real cyclotomic"" polynomial $\Psi_m$.
This isn't an established name or notation, but is the minimal polynomial
for $2\cos(2\pi/m)$ and satisfies
$$\Psi_m(X+X^{-1})=X^{-\phi(m)/2}\Phi_m(X)$$
where $\phi$ is Euler's totient. For example
$$\Psi_5(X+X^{-1})=X^2+X+1+X^{-1}+X^{-2}$$
and so
$$\Psi_5(Y)=Y^2+Y-1.$$</p>
"
"2380746","2380929","<p>Using a little theory, if the second difference is constant, the sequence is quadratic. If the constant is 1, the coefficient of the $n^2$-term is $1/2$. If $a_{19}=a_{92}=0$, then the quadratic has the factors $n-19$ and $n-92$. Now just put it all together. </p>
"
"2380758","2382260","<p>$y$ doesn't even need to have a $\sqrt{}$ in it at all. For any $n\in\mathbb N$ there is a solution $y=2n, a=2n^2+1, b=2n^2-1.$ This is because</p>

<p>$$a^2=(2n^2+1)^2=4n^4+4n^2+1$$
$$b^2+2y^2=(2n^2-1)^2+2(2n)^2=4n^4-4n^2+1+8n^2=a^2$$</p>
"
"2380768","2380784","<p>You're confusing two different notations. There is a long way to describe a permutation and a short way. The long way list which element maps to which:</p>

<p>$$\sigma = \begin{pmatrix}
1 &amp; 2 &amp; 3 &amp; 4 \\
1 &amp; 4 &amp; 2 &amp; 3 \\
\end{pmatrix}$$
Here we have $1$ as a fixed element, while $\sigma(2)=4, \sigma(3)=2$, and $\sigma(4)=3$.</p>

<p>The short way is to write out cycles; the same permutation can be written
$$\sigma = (1)(2 4 3)$$
showing that $1$ is fixed, while we have $2 \to 4 \to 3 \to 2$.</p>

<p>In your answer for $\alpha^2$ and $\alpha^2\beta$, you've written out the bottom row of the long notation, which doesn't make sense when read as cycle notation.</p>
"
"2380769","2380774","<p>No. &nbsp; That's got the right idea, but not the right application.</p>

<p>You are given the probabilities for failure on any particular day are $p_1,p_2$ for each pump. &nbsp; These shall be assumed to have independence on any given day.</p>

<p>So the probability for failure of either pump on any particular day is: $p_1+p_2-p_1p_2$, by your inclusion/exclusion formula.</p>

<p>Now you just want to find the probability for $i-1$ consecutive days with no failure followed by the first failure on day $i$.</p>

<blockquote class=""spoiler"">
  <p> $$(1-p_1-p_2+p_1p_2)^{i-1}\cdot(p_1+p_2-p_1p_2)$$</p>
</blockquote>
"
"2380793","2380837","<p>What they are telling is that $$C=k v^3+128$$ and that $$k 12^3=27 \implies k=\frac{27}{12^3}=\frac{27}{1728}=\frac{1}{64}$$ This is the cost per hour. But, in one hour, the ship run $v$ miles. So the cost per mile is just $$\frac C v=\frac{\frac{v^3}{64}+128} v=\frac{v^2}{64}+\frac {128}v$$ and this is what you want to minimize.</p>
"
"2380808","2384892","<p>The problem breaks down naturally into two parts:</p>

<ol>
<li>Put the marbles into the $k$ bags that contain them.</li>
<li>Nest the $k$ bags into other bags.</li>
</ol>

<p>Fixing a particular value of $k$, the first task can be accomplished in $S(n,k)$ ways, where $S(n,k)$ is the Stirling number of the second kind.</p>

<p>For the second task, treat the table on which the ""outer"" bags sit as a ""super-bag"". Then the bagging including the ""super-bag"" is a rooted, labelled tree on $k+1$ nodes, of which there are $(k+1)^{k-1}$ possibilities.</p>

<p>Summing these values for $k \in \{1 \ldots n\}$, we find the number of baggings of $n$ objects is $$\displaystyle \sum_{k=1}^n S(n,k) (k+1)^{k-1}$$</p>

<p>The first couple of terms in the sequence are 1, 4, 26, 243, 2992, 45906. Running up a few extra terms and checking in OEIS, this sequence is indexed as <a href=""http://oeis.org/A052880"" rel=""nofollow noreferrer"">A052880</a>. </p>

<p>Addendum: To address the issue brought up by @Jens, here is an enumeration for $n=3$</p>

<ol>
<li>Three outer bags - 1 way: (A)(B)(C)</li>
<li>Two outer bags: - 9 ways: (AB)(C), (AC)(B), and (A)(BC); (A(B))(C), (B(A))(C), (A(C))(B), (C(A))(B), (A)(B(C)), (A)(C(B))</li>
<li>One outer bag, three balls in outer bag - 1 way: (ABC)</li>
<li>One outer bag, two balls in outer bag - 3 ways: (AB(C)), (AC(B)), (BC(A))</li>
<li>One outer bag, one ball in outer bag - 12 ways: there are three ways to pick the ball in the outer bag (say A). Then there are four ways to arrange B &amp; C: (A(BC)), (A(B)(C)), (A(B(C))), (A(C(B))).</li>
</ol>

<p>Adding yields 26 possible combinations.</p>
"
"2380815","2380958","<p>This is immediate from the definition of a coproduct.  By definition, for any collection of maps $f_i:A_i\to A$, there is a <em>unique</em> map $f:\coprod A_j\to A$ such that $fs_i=f_i$ for all $i$, where $s_i:A_i\to \coprod A_j$ is the inclusion.  The uniqueness part of this means that if two maps $\coprod A_j\to A$ have the same compositions with $A_i\to\coprod A_j$ for each $i$, then the two maps are equal.</p>
"
"2380821","2380826","<p>We have $$\sqrt{x}-\sqrt{y}=\frac{y+z}{2}-\frac{z+x}{2}$$ or
$$(\sqrt{x}-\sqrt{y})(2+\sqrt{x}+\sqrt{y})=0,$$
which gives $x=y$.</p>

<p>Similarly we have $x=z$.</p>

<p>Thus, $x=y=z$, $x=\sqrt{x}$ and we get the answer:
$$\{(0,0,0),(1,1,1)\}$$</p>
"
"2380822","2389457","<p>Let's just carry your first case to its logical conclusion.
Let $\hat{X}\in\mathbb{R}^{|D|\times(m+1)}$, $\hat{W}\in\mathbb{R}^{(1+m)\times n}$, ${X}\in\mathbb{R}^{|D|\times m}$, ${W}\in\mathbb{R}^{m\times n}$, $\vec{1}\in\mathbb{R}^{|D|\times 1}$ be the vector of ones, $W_i$ be the $i$th column of $W$, and $b\in\mathbb{R}^{1\times n}$. Then your case 1 looks like:
$$
y = \hat{X}\hat{W} = \left[\vec{1}\; X\right]\begin{bmatrix}b\\ W\end{bmatrix}=
\left[b_1\vec{1}+XW_1,\ldots,b_n\vec{1}+XW_n\right]=\vec{1}b+XW=B+XW
$$ where $B=\vec{1}b=\vec{1}\otimes b\in\mathbb{R}^{|D|\times n}$.
(For your example, $|D|=3,m=3,n=2$).</p>

<p>So the equivalent bias to your ""first case"" has the bias $B$ as a rank 1 <em>matrix</em>, not a vector.</p>

<hr>

<p>Personally, I'd write it like this. For a single artificial neuron (with weight vector $w=\langle w_1,\ldots,w_n\rangle$), let $\sigma:\mathbb{R}\rightarrow\mathbb{R}$ be the sigmoid function (or e.g. tanh; doesn't matter), input $x_i$, and scalar bias $b$.</p>

<p>Then the output of the node on input $i$ is:
$$
y_i = \sigma(w\cdot x_i + b) = \sigma\left(b+\sum_{k=1}^n w_kx_{ik} \right)
=\sigma\left(w_01+w_1x_{i1}+\ldots+w_nx_{in} \right)=\sigma(\tilde{w}\cdot\tilde{x}_i)
$$
where we renamed $b=w_0$, and let $\tilde{w}=[w_0\;\, w]$, $\tilde{x}_i=[1\;\,x_i]$ by concatenation.</p>
"
"2380827","2380831","<p>Aside from the mistakes it is fine. </p>

<ul>
<li><p>Notice $(nx)\cdot(ny) = n(nxy)$, not $n(xy)$.</p></li>
<li><p>""We have that $nx$ and $ny$."" (are what?).</p></li>
</ul>

<p>Other than that you might want to include a justification for why you can say $(x+y)\in \Bbb Z$ and $(nxy)\in\Bbb Z$. &nbsp; What property of $\Bbb Z$ allows this?</p>

<hr>

<p>PS: You only stated that $n&gt;0$.  Is it supposed to be assumed to be integer?  Because if it is not, you may not have multiplicative closure.</p>
"
"2380829","2381755","<p>I'm afraid there is no canonical choice. </p>

<p>Let $F$ be the set of $C^1$-smooth functions $f: \mathbb{R}\to\mathbb{R}$ for which the seminorm 
$$\|f\|_F = \sup_{x\in\mathbb{R}} |x^2f'(x)|$$ 
is finite. Then $E = \{f\in F:\|f\|=0\}$ is one-dimensional, the space of constant functions. </p>

<p>However, different choices of complements of $E$ yield inequivalent norms on $F$. For example, we could take $H=\{f\in F: f(1)=0 \}$, leading to the norm 
$$\|f\|_1 =  \|F\|_F + |f(1)|$$
Or take  $H=\{f\in F: f(-1)=0 \}$, leading to the norm 
$$\|f\|_{2} =  \|F\|_F + |f(-1)|$$
These two are not comparable, and do not yield the same topology on $F$. Indeed, the sequence of functions 
$$
f_n(x) = \begin{cases} 0,\quad &amp;x\le 0, \\  n^2x^2,\quad &amp;0\le x\le 1/n \\ 
2-n^2(x-2/n)^2,\quad &amp; 1/n \le x \le 2/n \\   2  \quad &amp; x\ge 2/n
\end{cases}
$$
converges to $0$ in the second norm  but not the first. To check this, notice that 
$$
f_n'(x) = \begin{cases} 0,\quad &amp;x\le 0, \\  2n^2x,\quad &amp;0\le x\le 1/n \\ 
 -2n^2(x-2/n),\quad &amp; 1/n \le x \le 2/n \\   0  \quad &amp; x\ge 2/n
\end{cases}
$$
hence $\|f_n\|_F\to 0$. Since $f_n(-1)=0$, we get $\|f_n\|_2 \to 0$. But $f_n(1) = 2$ for large $n$, so $\|f_n\|_1\to 2$.</p>

<hr>

<p>The underlying problem is that to form a norm (or locally convex topology) one essentially needs some linear functionals that are not constant on $E$, i.e., not continuous with respect to $F$-seminorm. And there is no canonical choice of discontinuous linear functionals on a normed space... as Tolstoy wrote, every discontinuous linear functional is discontinuous in its own way.</p>
"
"2380843","2380860","<p>I suggest trying the substitution $u=\frac yx$ and $v=x^2+4y^2$. Solving for $x$ and $y$ yields $x=\sqrt{\frac{v}{1+4u^2}}$ and $y=u\sqrt{\frac{v}{1+4u^2}}$. With a little effort, $\frac{\partial(x,y)}{\partial(u,v)}=-\frac{1}{8u^2+2}$ and this problem reduces to integrating $f(u,v)=\frac{u}{8u^2+2}$ over the rectangular subset of the $(u,v)$ plane $[0,1]\times[0,4].$ You get $\frac{\ln(5)}{4}.$</p>
"
"2380844","2380852","<p>$\sin x$ is increasing on the interval, starting at $0$ and ending at $1/\sqrt{2}$. $\cos x$ is decreasing on the interval, starting at $1$ and ending at $1/\sqrt{2}$, so $\cos x$ is bigger. Since $\cos x \leq 1, \cos^2x\leq\cos x$, so $\cos x$ is still bigger. Since $\cos 2x$ is decreasing from $1$ to $0$ on the same interval, it must be decreasing faster, so $\cos x$ is bigger than that. Lastly, since $\sin x \leq 1, \sin x \cos x \leq \cos x$, so yet again, $\cos x$ is bigger. Since $\cos x$ is strictly greater than all of the other functions on the given interval (endpoints notwithstanding), it's integral must be greater as well.</p>
"
"2380845","2380868","<p>With your notation, $\frac{s(t+h)-s(t)}{h}$ is the average rate of change of the $x$ coordinate of the particle in the interval of time $(t,t+h)$, which is the same as the average of the $x$ component of the (instantaneous) velocity of the particle in that time interval. If $h\to 0$ this quantity becomes the $x$ component of the instantaneous velocity of the particle at time $t$, usually denoted by $v_x(t)$. This is just one of the three Cartesian components of the instantaneous velocity, so in general it is not the same as the instantaneous velocity. In one dimensional motion along the $x$ axis, you could identify the instantaneous velocity $\vec v$ with its only component $v_x$, although technically there are no the same, one is a vector the other is a component: $\vec v =v_x\hat i$. For three dimensional motion you will need also the $y$ and $z$ components of the velocity to get the instantaneous velocity.</p>

<p>Most people will use $x(t)$ for the position of the particle in the $x$ axis, and some will use $s(t)$ for the total distance traveled by the particle since $t=0$ along its trajectory (not necessarily the $x$ axis). In that case $\frac{s(t+h)-s(t)}{h}$ is the average rate at which distance is being traveled, and its limit as $h\to0$ the instantaneous speed $v(t)$ of the particle (do not confuse it with the instantaneous velocity $\vec v(t)$ of the particle, which is a vector, $v=|\vec v|$). Note that in general $v_x$ is not the same as $v$ even in one dimensional motion, in which case $v=|v_x|$. The component $v_x$ can be negative, but $v$ is always non-negative.</p>
"
"2380846","2380850","<p>You can you Jensen for $f(x)=x^2+\frac{1}{x^2}$ because $f''(x)=2+\frac{6}{x^4}&gt;0$.</p>

<p>Hence, $$\sum_{cyc}\left(a^2+\frac{1}{a^2}\right)\geq3\left(\left(\frac{a+b+c}{3}\right)^2+\frac{1}{\left(\frac{a+b+c}{3}\right)^2}\right)=\frac{82}{3}.$$
Thus, $$\sum_{cyc}\left(a+\frac{1}{a}\right)^2\geq\frac{82}{3}+6=33\frac{1}{3}.$$</p>

<p>Also you can use C-S and Holder:
$$\sum_{cyc}a^2+\sum_{cyc}\frac{1}{a^2}+6=\frac{1}{3}\sum_{cyc}1^2\sum_{cyc}a^2+\left(\sum_{cyc}a\right)^2\sum_{cyc}\frac{1}{a^2}+6\geq$$
$$\geq\frac{1}{3}(a+b+c)^2+(1+1+1)^3+6=33\frac{1}{3}.$$</p>

<p>Also, we can make the following thing.</p>

<p>We need to prove that
$$\sum_{cyc}\left(a+\frac{1}{a}\right)^2\geq33\frac{1}{3}$$ or
$$\sum_{cyc}\left(\left(a+\frac{1}{a}\right)^2-\frac{100}{9}\right)\geq0$$ or
$$\sum_{cyc}\frac{(3a-1)(a-3)(3a^2+10a+3)}{a^2}\geq0$$ or
$$\sum_{cyc}\left(\frac{(3a-1)(a-3)(3a^2+10a+3)}{a^2}+160(3a-1)\right)\geq0$$ or
$$\sum_{cyc}\frac{(3a-1)^2(a^2+54a+9)}{a^2}\geq0.$$
Done!</p>
"
"2380875","2380891","<p><a href=""https://i.stack.imgur.com/45xhL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/45xhL.jpg"" alt=""enter image description here""></a></p>

<p>so here i attach the graph of $y = \sin(x) \cos(x) - 3\cos(x)$ in the interval $[-4\pi,4\pi]$. There are 17 roots (intersection of $ y = 0 $ with $y = \sin(x) \cos(x) - 3\cos(x)$ ) in the interval  $[-4\pi,4\pi]$.</p>

<p>As you have said, you have roots of the form $n\pi$ (which are $-4\pi \ldots 4\pi)$. Those are 9 roots. </p>

<p>The other 8 are in the form of $\cos(x) = \frac{3}{4}$ also as you said. Also as @Mundron Schmidt suggests, the solutions of $\cos(x) = \frac{3}{4}$ are in the form of 
\begin{equation}
x = \pm \arccos(\frac{3}{4}) + 2n \pi \qquad n \in \mathbb{Z}
  \end{equation}
which is the simplest form. And those are the 8 other roots in $[-4\pi,4\pi]$</p>
"
"2380877","2381005","<p>To counter the ineffectiveness of the other answer, here is the approach I've hinted at in my comments:
$$\begin{align}
1^2 + 2^2 + 3^2 + \cdots + (n-1)^2 + n^2 &amp;= \frac{n(n+1)(2n+1)}{6}\\
1^2 + 2^2 + 3^2 + \cdots + (n-1)^2 &amp;= \frac{n(n+1)(2n+1)}{6} - n^2\\
0^2 + 1^2 + 2^2 + 3^2 + \cdots + (n-1)^2 &amp;= \frac{n(n+1)(2n+1)}{6} - n^2
\end{align}
$$
Now it's just a matter of simple algebra to check whether $$\frac{n(n+1)(2n+1)}{6} - n^2$$ is actually equal to $$\frac{(n-1)((n-1)+1)(2(n-1)+1)}{6}$$</p>
"
"2380880","2381094","<p>The answer is $D$.</p>

<p>Express it as:
$$2y_1+3y_2+5y_3+4y_4=2(\underbrace{y_1+y_2}_{\le 1})+(\underbrace{y_2+y_3}_{\le 1})+4(\underbrace{y_3+y_4}_{\le 1})\le7.$$
Equality occurs when $(y_1,y_2,y_3,y_4)=(1,0,1,0); (\frac12,\frac12,\frac12,\frac12);(0,1,0,1); etc.$</p>
"
"2380883","2380901","<p>Let $x$ be the length of the side along the road and let $y$ be the length of the sides perpendicular to the road. Then you know that $xy=2\,800$, which means that $y=\frac{2\,800}x$. He will pay $x\times5\$+x\times2\$+y\times2\$+y\times2\$$ for the fence; in other words, the cost (in dollars) will be $7x+4y$. But since $y=\frac{2\,800}x$, this means that the farmer will pay $7x+\frac{11\,200}x$ dollars. Let $c(x)=7x+\frac{11\,200}x$. Then $c'(x)=7-\frac{11\,200}{x^2}$ and the only positive $x$ such that $c'(x)=0$ is $x=40$. Furthermore, $c'(x)&gt;0$ if $c&gt;40$ and $c'(x)&lt;0$ if $x&lt;40$. Therefore, the minimum of $c$ is $c(40)=560$ and the farmer will have to pay $560\$$.</p>
"
"2380884","2381054","<p>Hints:</p>

<ul>
<li>Note that $||Ax-b||^2$ is always bounded below by zero for every choice of $x$.</li>
<li>Note that $\min_{x}||Ax-b||^2$ is a convex problem. Thus KKT conditions are necessary and sufficient. In simple words, differentiating this w.r.to $x$ and setting that to zero should be a necessary and sufficient condition for the solution to satisfy (strictly speaking, there are addition conditions).</li>
<li>Use above two, can you solve (a)?</li>
<li>Can you comment on the rank of $A^TA$ given the rank of $A$. Given that and the facts above can you solve (b)?</li>
<li>Can you do the same for the optimization problem $$\min_{x}||Ax-b||^2+\epsilon||x||_2^2$$</li>
<li>Can you convince yourself that $(A^TA+\epsilon I)$ is invertible? (Further Hint: Use SVD of $A=U\Sigma U^H$)</li>
<li>Now do you see the relative between (c) and the optimization problem stated above. When $\epsilon \to 0$, can you comment on the optimization problem above?</li>
<li>After all is done, can you look upon Tikhonov Regularization?</li>
</ul>
"
"2380885","2383498","<p>Taking the numbered statements one by one ...</p>

<blockquote>
  <ol>
  <li>It means that there exists a sentence $\psi$ that doesn't have a truth value(i.e., neither true nor false.)</li>
  </ol>
</blockquote>

<p>No. The situation is really quite different from your example where $\Gamma=\{P,Q\}$ and we want to know the truth-value of $R$.  In that case, we could consider interpretations relative to $\Gamma$ that simply do not consider $R$, and you could in some sense say that $R$ does not have a truth-value (some will disagree and say that as soon as you refer to a proposition $R$ it must have a truth-value, which is why I say 'in some sense').  </p>

<p>In the case of Godel and arithmetic, however, $\Gamma$ is a first-order logic theory of arithmetic, meaning that it contains sentences described by the language of arithmetic, which includes symbols $\mathbf{0}$, $\mathbf{s}$, $\mathbf{+}$, and $\mathbf{\cdot}$. The sentence $\phi$ (called the 'Godel sentence) is expressed in this language as well.  As such, any interpretation for $\Gamma$ is an interpretation of the the language of arithmetic, and thus the Godel sentence $\phi$ will always have a truth-value.</p>

<blockquote>
  <ol start=""2"">
  <li>It means that there exists a sentence $\psi$ having a truth value, but we don't know.</li>
  </ol>
</blockquote>

<p>No.  The Godel sentence $\phi$ such that $\Gamma\not\vdash \phi$ and $\Gamma\not\vdash\neg\phi$ for any consistent formal arithmetical theory $\Gamma$, when interpreted by the standard interpretation $N$ (which has as domain $\mathbb{N}$, and which maps $\mathbf{0}$ to $0$, $\mathbf{s}$ to $s$ (successor function), $\mathbf{+}$ to $+$, and $\mathbf{\cdot}$ to $\cdot$)), ends up saying ""I ($\phi$) cannot be derived from $\Gamma$"".  And since $\Gamma \not \vdash \phi$, it indeed cannot be derived from $\Gamma$, and thus it is true under the standard interpretation. </p>

<p>And when mathematicians talk about sentences being true, they of course use the 'standard interpretation' of their language.  In other words, $\phi$ is as much 'true' as '1+1=2'.  .... which in mathematics we simply consider true, period. So, $\phi$ is true, period.</p>

<blockquote>
  <ol start=""3"">
  <li>It means that there exists a true sentence $\psi$, but $\Gamma\not\vdash \psi$ and $\Gamma\not\vdash\neg\psi$. (ps: this saying is so blurred, how does ""true"" mean by him? Under what model? Under all model?)</li>
  </ol>
</blockquote>

<p>Yes. ""true"" under the 'standard interpretation', i.e. 'true' by normal mathematical standards, i.e. true in as much any other mathematical theorem is considered true.</p>

<blockquote>
  <ol start=""4"">
  <li>There exists sentence $\psi$ such that $\Gamma\not\vdash \psi$ and $\Gamma\not\vdash\neg\psi$, and there exists some model $\mathfrak{A}$ such that $\mathfrak{A}\vDash\Gamma$ and $V_{\mathfrak{A}}(\psi)=T$, and at the same time there also exist some model $\mathfrak{B}$ such that $\mathfrak{B}\vDash\Gamma$ and $V_{\mathfrak{B}}(\psi)=F$.</li>
  </ol>
</blockquote>

<p>Yes ...  but this is a weaker statement than 3.  We don't just mean 'some' model; we mean the 'standard model', i.e. we mean 'true' by normal mathematical standards. Again, it is as true as any other mathematical theorem.  So yes, there does exist a model in which it is true (namely the 'standard model'), and there is also a model (a 'non-standard model' or 'non-standard interpretation') in which it is false.  And we know the latter, since because:</p>

<blockquote>
  <ol start=""5"">
  <li>There exists sentence $\psi$ such that $\Gamma\not\vdash \psi$ and $\Gamma\not\vdash\neg\psi$, and for any model $\mathfrak{A}$ that $\mathfrak{A}\vDash\Gamma$, then $V_{\mathfrak{A}}(\psi)=T$.</li>
  </ol>
</blockquote>

<p>Is definitely false!  If $\phi$ is true under any interpretation, then $\vDash \phi$, and since first-order logic itself is complete (This is Godel Completeness Theorem (for logic),  not to be confused with his Incompleteness Theorem (for arithmetic)), $\phi$ can be derived from nothing (i.e. $\vdash \phi$) and so certainly from $\Gamma$ (i.e. $\Gamma \vdash \phi$.). So this is why in 4. we know there must be at least one model that sets $\phi$ to False.</p>

<p>To sum up: out of these 5, only 3 and 4 are true, and 3 is the best way to think about it: ""Given any consistent set of axioms about arithmetic, there is always some arithmetical truth that cannot be derived from those axioms.""  Or even shorter: ""arithmetic is not axiomatizable"", or shorter yet: ""arithmetic is incomplete""</p>

<p>P.s. It should really be: ""Any <em>recursive</em> and consistent set of axioms for arithmetic is incomplete"". For simplicity sake you can think of 'recursive' as 'finitely expressible' ... for if you don't put in that constraint, you can simply choose all arithmetical truths as your 'axiom set', and that axioms set is of course both consistent and complete. </p>
"
"2380897","2380932","<p>That particular point in the direction of $r$ can be expressed as $x=kr$. </p>

<p>Hence $$r^T(kr) = c$$
$$k(r^Tr)=c$$</p>

<p>$$k = \frac{c}{\|r\|^2}$$</p>

<p>Hence the point of interest is $x= \frac{c}{\|r\|^2}r$.</p>
"
"2380903","2380921","<p>Consider $$P(Y \leq y) = P(\sqrt{X} \leq y)$$
Since $X \sim U(0,1)$ this is the same as, $$P(X \leq y^2)$$
This is the key. Originally $F_X(x) = \int_{0}^{x} 1 \ dx$ was equal to $x$ because every successive interval $\delta x$ was equally likely, i.e $X$ assumed values between $[x_i, x_i+\delta x]$ and $[x_i + \delta x, x_i + 2\delta x]$ with equal likelihood $(\delta x)$. And hence $F_x(x+\delta x) = F_X(x) + \delta x = x + \delta x$.</p>

<p>But this isn't the case with $Y$. Note $(y+\delta y)^{2} \approx y^2 + 2y\delta y$. As you can see the width of the successive intervals $X \in\ [y^2, y^2 + 2y\delta y]$ widens with increasing value of $y$ as fast as $2y$ and since the probability that $X$ lies in this interval depends on the width of the interval hence $F_Y(y)$ scales as $2y$.</p>
"
"2380907","2380950","<p>Let $f,g$ be as in the diagram in the question and let $f$ be an epimorphism and $g$ a monomorphism (equivalent to $f$ has trivial cokernel and $g$ has trivial kernel).
Let us consider the case when $h \colon C \to D$ is an epimorphism, which holds in your more particular case. Then the composition  $A \overset{f}{\to} C \overset{h}{\to} D$  is an epimorphism, and thus the composition  $A \to B \overset{g}{\to} D$  would have to be an epimorphism. Then $g$ would have to be an epimorphism, and hence an isomorphism. 
So in this case there is such a map  $A \to B$  if and only if $g$ is an isomorphism. </p>

<p>If the map  $h \colon C \to D$  is arbitrary I am not much help. 
You can say such a map $A \to B$ exists if and only if the composition $\mathrm{Cok}(g) \circ h \circ f = 0$, but that is really just saying the same thing with different words, using that $g$, being a monomorphism, is the kernel of its cokernel.</p>
"
"2380909","2380935","<p>The trick is to switch notations: $F^Ã$ is an abelian group $(F^Ã,Â·,1)$. But we actually donât need to know that it is the multiplicative group of some field. In linear algebra, we are instead used to â$+$â and â$0$â, so writing â$Â·$â and â$1$â will only confuse us about what is linear and what not.</p>

<p>So letâs take instead a more general abelian group $H = (H,+,0)$. Then we have a map
$$\operatorname{Mat}_{nÃn}(â¤) â \operatorname{End}(H^n),~A â¦ m_A$$
given by setting for $A = (a_{ij})_{ij}^{nÃn}$,
$$m_A \colon H^n â H^n,~(x_1,â¦,x_n)  â¦ ({a_{11}}x + â¦ + a_{1n} x_n,â¦,a_{n1}x_1 + â¦ + a_{nn}x_n).$$
If you write $(x_1, â¦,x_n)$ as a column $x = \Big[\begin{smallmatrix} x_1 \\ \vdots \\ x_n \end{smallmatrix}\Big]$, you probably recognize this as the matrix product $m_A \colon x â¦ Ax$. You probably also already <em>know</em> that $m$ then defines a monoid homomorphism, that is: $m_{AB} = m_A â m_B$ for $A, B â \operatorname{Mat}_{nÃn}(â¤)$</p>

<p>In particular, the identity matrix $E_n$ is sent to the identity $m_{E_n} = \mathrm{id}_H$ and if $A$ is invertible with inverse $B$, so is $m_A$ with inverse $m_B$. Thus, $m$ restricts to a <em>group homomorphism</em>
$$m \colon \operatorname{GL}_n (â¤) â \operatorname{Aut}(H^n),$$
the image of an invertible matrix being an invertible endomorphism of $H^n$.</p>

<p>Setting again back $H = F^Ã$, the map $m_A$ becomes for $A = (a_{ij})_{ij}^{n Ã n}$,
$$m_A \colon (F^Ã)^n â (F^Ã)^n,~(x_1,â¦,x_n) â¦ (x_1^{a_{11}}Â·â¦Â·x_n^{a_{1n}},â¦,x_1^{a_{n1}}Â·â¦Â·x_n^{a_{nn}})$$
which is â up to a permutation of the entries â the map you are intersted in.</p>
"
"2380914","2380940","<p>Here's an idea you can try.  Let $v(x,t)$ solve $\partial_tv-\Delta v=f(v)$ for $v(x,0)=C$, and $w=v-u$.  Then $w$ solves $\partial_tw-\Delta w=f(v)-f(u)$.  By the fundamental theorem of calculus,
$$
f(v)-f(u)=(v-u)\int_0^1 f'((1-s)u+sv)ds=:w\,F,
$$
where $F(x,t)=\int_0^1f'((1-s)u(x,t)+sv(x,t))ds$.  So $w$ solves 
$$
\partial_tw-\Delta w=wF.
$$
But $wF=0$ when $w=0$, and $w(x,0)\ge 0$, so the supersolution argument you linked to seems to apply.  Note that $v(x,t)\le Ce^{Mt}$, which is straightforward to show, since $v(x,t)=V(t)$ solves $V'(t)=f(V(t))$.</p>
"
"2380926","2380931","<p>The answer to your main question, i.e.</p>

<blockquote>
  <p>When we can say a group can not be abelian if we only know the order of the group?</p>
</blockquote>

<p>is <strong>never</strong>. For every $n$, there exists an Abelian group of order $n$, that group is $\mathbb Z_n$.</p>
"
"2380934","2380952","<p>The expression you are considering is the definition of differential for a function obscured by the notation the author is (forced) to use. You can see it in one variable:</p>

<p>$$f(x+dx)=f(x)+f'(x)dx +[\text{terms of second and greater degree}]$$</p>

<p>so is, $f(x+dx)\approx f(x)+f'(x)dx$ or $f(x+dx)-f(x)\approx f'(x)dx$</p>

<p>Now, simply ""translate"" it considering that partial derivatives are the same as as derivatives for a single variable maintaining constant the variables not involved in the derivative:</p>

<p>$A_1h_2h_3|_{x^1x^2x^3}$ is some function of the three variables, say $g(x^1,x^2,x^3)$, then, we, following more strictly than the author the notation he chose, can write: $\left.\dfrac{\partial(A_1h_2h_3)}{\partial x^1}\right|_{(x^1,x^2,x^3)}=\dfrac{\partial g(x^1,x^2,x^3)}{\partial x^1}$, so is, the derivative wrt $x^1$ evaluated at the point $(x^1,x^2,x^3)$</p>

<p>You surely can complete the ""translation"" (remember that $x^2$ and $x^3$ act as simple constants here).</p>
"
"2380942","2380947","<p>Let $f_c:= 1_{\{x\leq \frac{1}{c}\}}$ and let $x \in \mathbb R$.</p>

<p>Case 1: $x \le 0$. Then $f_c(x)=1$ for all $c&gt;0$, hence</p>

<p>$\lim_{c \to \infty}f_c(x)=1.$</p>

<p>Case 2: $x&gt;0$. Then there is $c_0&gt;0$ such that $x&gt;\frac{1}{c_0}$ we then have: $x&gt;\frac{1}{c}$ for all $c&gt;c_0$ and hence $f_c(x)=0$ for all $c&gt;c_0$.</p>

<p>This gives</p>

<p>$\lim_{c \to \infty}f_c(x)=0$.</p>
"
"2380944","2382046","<p>Let me convert my comment to a full answer: </p>

<p><em>Unless one is (and you are not!) planning to write a PhD thesis in General Topology, Munkres is (more than) enough</em>.  </p>

<p>Depending on what you are planning to study later, you might encounter an issue requiring a bit more General Topology (e.g. proper maps and proper group actions, which you will find in Bourbaki), but you learn this on ""need to know"" basis (just pick up a General Topology book and look it up when necessary). Instead, my suggestion is to start reading Guillemin and Pollack, and Hatcher (or Massey). </p>

<p>In addition, you would want to (or, rather, have to) learn more functional analysis (say, Stein and Shakarchi) and PDEs (say, Evans) which will be handy if you are planning to go into modern differential topology (which most likely will require you dealing with nonlinear PDEs, believe it or not), and, in case of algebraic topology, - basic category theory (at least be comfortable with the language), Lie theory (at least to know the basic correspondence between Lie groups and Lie algebras), see suggestions <a href=""https://math.stackexchange.com/questions/194419/whats-a-good-place-to-learn-lie-groups"">here</a>. Yes, General Topology is fun and there are many neat old theorems that you will learn by studying it in more detail, but you have to prioratize: Life is short and your time in graduate school is even shorter. </p>
"
"2380956","2380973","<p>Note that 
$$
|g^{-1} | \ \neq \frac{1}{|g|}.
$$
Generally, if $Y = g(X)$, and $g$ is monotone increasing function then 
$$
F_Y(y) = P(Y\le y) = P(g(X)\le y)=P(X\le g^{-1}(y)) = F_X(g^{-1}(y)),
$$
hence using the chain rule you can get the density, 
$$
f_Y(y)  =F'_Y(y)= \frac{d}{dy}F_X(g^{-1}(y)) = f_X(g^{-1}(y))\frac{d}{dy}g^{-1}(y)). 
$$</p>
"
"2380959","2381004","<p>There are no real solutions other than $(0,0)$. (Just draw the curves $y=ax^3$ and $x=-ay^3$ and see where they intersect.)</p>
"
"2380960","2380980","<p>I think you mean the following.</p>

<blockquote>
  <p>For all natural $n\geq2$ prove that:
  $$n^n&gt;1\cdot3\cdot5\cdot...\cdot(2n-1).$$</p>
</blockquote>

<p>If so, we can prove it by AM-GM:</p>

<p>$$n^2=1+3+5+...+2n-1&gt;n\sqrt[n]{1\cdot3\cdot5\cdot...\cdot(2n-1)}$$ and we are done!</p>
"
"2380962","2381214","<p>Some observations: the standard notation for the $k$-th prime is $p_{k}$ or $q_{k}$. The symbol $\pi\left(k\right)$ usually represents the counting function of primes up to $k$, i.e., $$\pi\left(k\right)=\sum_{p\leq k}1,\,p\mathrm{\,is\,a\,prime\,number}.$$ Assuming that with the symbol $\pi\left(k\right)$ you intend $p_{k}$, you may observe that $$\sum_{k\geq n}\frac{1}{p_{k}p_{k+1}}\leq\sum_{k\geq n}\frac{p_{k+1}-p_{k}}{p_{k}p_{k+1}}=\sum_{k\geq n}\left(\frac{1}{p_{k}}-\frac{1}{p_{k+1}}\right)=\frac{1}{p_{n}}$$ and since <a href=""https://en.wikipedia.org/wiki/Prime-counting_function#Inequalities"" rel=""nofollow noreferrer"">we know</a> that $$p_{n}&gt;n\log\left(n\log\left(n\right)\right)-n,\,n\geq2$$ we get $$S&lt;\sum_{k\leq n-1}\frac{1}{p_{k}p_{k+1}}+\frac{1}{n\log\left(n\log\left(n\right)\right)-n}$$ so taking, for example, $n=11$, we obtain $$S&lt;\sum_{k\leq10}\frac{1}{p_{k}p_{k+1}}+\frac{1}{11\log\left(11\log\left(11\right)\right)-11}\approx \color{red}{0.334}.$$</p>
"
"2380972","2380990","<p>Consider that for $x\in \mathbb{R}$, \begin{align*} g''(x) &amp;= \lim_{h\to 0} \frac{g(x+h)+g(x-h)-2g(x)}{h^2} \\ &amp;= \lim_{h\to 0} \lim_{n\to \infty} \frac{f_n(x+h)+f_n(x-h)-2f_n(x)}{h^2} \end{align*} As $f_n''\geq 0$, we have that $f_n$ is convex for all $n$. Therefore, $$f_n(tx_1+(1-t)x_2)\leq tf_n(x_1)+(1-t)f_n(x_2)$$ for all $x_1, x_2\in \mathbb{R}$ and $t\in [0, 1]$. If we let $x_1 = x-h$, $x_2 = x+h$, and $t = \frac{1}{2}$, we therefore get $$f_n(x)\leq \frac{1}{2}(f_n(x+h)+f_n(x-h))$$ This implies that $$\frac{f_n(x+h)+f_n(x-h)-2f_n(x)}{h^2}\geq 0$$ for all $x\in \mathbb{R}$, $h &gt; 0$, and $n\in \mathbb{N}$. Therefore, $$\lim_{n\to \infty} \frac{f_n(x+h)+f_n(x-h)-2f_n(x)}{h^2}\geq 0$$ for all $x\in \mathbb{R}$ and $h &gt; 0$, which implies that $$g''(x) = \lim_{h\to 0} \lim_{n\to \infty} \frac{f_n(x+h)+f_n(x-h)-2f_n(x)}{h^2}\geq 0$$ for all $x\in \mathbb{R}$.</p>
"
"2380974","2381002","<p>Here are two examples by changing our underlying logic (perhaps someone else can comment on ""Inter-universal TeichmÃ¼ller theory"" since I know nothing about it except that it is hard to get into):</p>

<ul>
<li><strong>constructive and intuitonistic mathematics</strong>: is probably not easy to get into for the average mathematician, since there are many possible pitfalls. Just look at the notion of ""finite set"". There are 5 different notions given on <a href=""https://ncatlab.org/nlab/show/finite+set"" rel=""nofollow noreferrer"">the nLab</a> that are all equivalent in $\mathsf{ZFC}$ but generally not equivalent in constructive mathematics. Parts of measure theory and topology get extra complicated since the classical notions start to ""fall apart"" (c.f. <a href=""https://ncatlab.org/nlab/show/Cheng+space"" rel=""nofollow noreferrer"">Cheng spaces</a> and <a href=""https://ncatlab.org/nlab/show/locale"" rel=""nofollow noreferrer"">locales</a>). The situation gets even spicier if we start to contradict the law of excluded middle (c.f. 
<a href=""https://en.wikipedia.org/wiki/Smooth_infinitesimal_analysis"" rel=""nofollow noreferrer"">smooth infinitesimal analysis</a>).</li>
<li><strong>inconsistent / paraconsistent mathematics</strong>: this even much more obscure than constructive mathematics. The basic idea is the if we get rid of <em>ex falso quodlibet</em> then we are allowed to talk about contradictory statements without our logic becoming trivial (note that this usually stops the disjunctive syllogism from working). You can try to <a href=""https://plato.stanford.edu/entries/mathematics-inconsistent/"" rel=""nofollow noreferrer"">do mathematics in this setting</a>. If I'm not mistaken: one motivation is to talk about ""real numbers"" $\varepsilon$ with $\varepsilon &gt; 0$ <em>and</em> $\varepsilon = 0$.</li>
</ul>
"
"2380984","2380989","<p>Yes. If $x\in I$, then $\{x\}$ is an open subset of $F$. Since $I=\bigcup_{x\in I}\{x\}$, $I$ is an open subset of $F$ and therefore $F\setminus I$ is closed.</p>
"
"2380994","2381036","<p>Consider the <a href=""https://en.wikipedia.org/wiki/Cantor_set"" rel=""nofollow noreferrer"">Cantor set</a>. This is closed, has no isolated point and has empty interior. Therefore the Cantor set is not regular closed.</p>
"
"2381000","2381009","<p>We prove by induction that $f(n)=n$ for all $n \in \mathbb N_0$:</p>

<p>The cases $n=0$ and $n=1$ are clear.</p>

<p>Now suppose $f(n)=n$ for some $n$. Then, use $f(m^2 + f(n))=(f(m))^2 + n$ with $m=1$,</p>

<p>$f(n+1)=f(f(n)+1^2)=f(1)^2+n=n+1$.</p>

<p>From $(4)$ you get</p>

<p>$f(m)=m$ for all $m \in \mathbb Z$.</p>
"
"2381018","2381031","<p>You basically want to show that there exists some $n_o \in \mathbb{N}$, such that for all $n \geq n_o$, we have $n^{1000} \leq 5^{\sqrt{n}}$.</p>

<p>Taking log of both terms, we have, $1000\log(n)$ and $\sqrt{n} \log (5)$. At this step it is clear that $1000\log(n)$ will be dominated by $\sqrt{n} \log (5)$ for some $n_o$, since $\sqrt{n}$ is known to dominate over $\log(n)$ in the long run.</p>

<p>Then this implies the original inequality is true, and hence $n^{1000} \in \mathcal{O}(5^{\sqrt{n}})$.</p>

<p><em>Code</em>:</p>

<blockquote>
  <p>sqrt(100000000000000000)*log(5)</p>
  
  <p>ans =</p>
  
  <p>5.0895e+08</p>
  
  <p>log(100000000000000000)*1000</p>
  
  <p>ans =</p>
  
  <p>3.9144e+04</p>
</blockquote>
"
"2381023","2381032","<p>Since the Taylor series of $\frac1{\sqrt{1-x}}$ is$$1+\frac12x+\frac12\times\frac34x^2+\frac12\times\frac34\times\frac56x^3+\cdots,$$and this series converges (to $\frac1{\sqrt{1-x}}$) when $|x|&lt;1$, you have$$|x|&lt;1\Longrightarrow\frac1{\sqrt{1-x^2}}=1+\frac12x^2+\frac12\times\frac34x^4+\frac12\times\frac34\times\frac56x^6+\cdots$$Therefore,\begin{multline*}1+\frac12\left(\frac{2x}{1+x^2}\right)^2+\frac12\times\frac34\left(\frac{2x}{1+x^2}\right)^4+\frac12\times\frac34\times\frac56\left(\frac{2x}{1+x^2}\right)^6+\cdots=\\=\frac1{\sqrt{1-\left(\frac{2x}{1+x^2}\right)^2}}=\frac{1+x^2}{1-x^2}.\end{multline*}</p>
"
"2381035","2381197","<p>When you design logic circuits, you usually start with some specification of desired functionality, and then use any number of techniques to capture that with a logical expression.  Those techniques often rely on useful logical equivalences so that you can, for example, use boolean algebra to rewrite and simplify expressions. </p>

<p>And, as it turns out, the AND and the OR are each other's <em>dual</em> operators, which means that not only is it true that you often find yourself going back and forth between them (think Demorgans's Laws!), but they share lots of logical properties, like Commutation, Association, Distribution, Absorption, Reduction, Idempotence, Adjacency, etc.  Whereas if you were to work with an AND and an XOR, you would need to remember a different set of such logical properties for each of them. In fact, just the very fact that many of these logical principles just mentioned involve both the AND and the OR is a point in favor of using AND and OR as your 'basic' operators, whereas I doubt you would find just as many useful principles involving the AND and the XOR.</p>
"
"2381048","2381065","<p>Another way of proving that $f$ is differentiable at $0$ is simply to observe that$$\lim_{z\to0}\frac{|z|^2}z=\lim_{z\to0}\overline z=0.$$Besides, if $z_0\neq0$, then$$\lim_{z\to z_0}\frac{|z|^2-|z_0|^2}{z-z_0}=\lim_{z\to z_0}\frac{|z|-|z_0|}{z-z_0}\bigl(|z|+|z_0|\bigr).$$Now, if $z$ approaches $z_0$ along the circle centered at $0$ passing through $z_0$, then the previous limit is $0$. And if $z$ approaches $z_0$ along the ray $\bigl\{\lambda z_0\,|\,\lambda\in(1,+\infty)\bigr\}$, then the previous limit is $2\overline{z_0}\neq0$. Therefore the limit does not exist.</p>
"
"2381053","2381063","<p>You can prove  by induction that</p>

<p>$$\sum_{n=1}^N \frac{1}{\ln(n+2)} - \frac{1}{\ln(n+1)} = \frac{1}{\ln(N+2)} -\frac{1}{\ln 2}$$</p>

<p>which means that</p>

<p>$$\sum_{n=1}^\infty \frac{1}{\ln(n+2)} - \frac{1}{\ln(n+1)} \\= \lim_{N\to\infty}\left(\sum_{n=1}^N \frac{1}{\ln(n+2)} - \frac{1}{\ln(n+1)}\right) \\= \lim_{N\to\infty}\frac{1}{\ln(N+2)} -\frac{1}{\ln 2}$$</p>

<p>which should be easy to calculate.</p>
"
"2381066","2381102","<p>If you let $z=x+iy$, then your mapping is $(x,y) \mapsto z \mapsto \dfrac{z}{1+|z|}$. </p>

<p>Let $\dfrac{z}{1+|z|}=\dfrac{w}{1+|w|}$. Then taking magnitudes of each side you get: $\dfrac{|z|}{1+|z|}=\dfrac{|w|}{1+|w|}$.  </p>

<p>The derivative of $\dfrac{r}{1+r}$ is  $\dfrac{1}{(1+r)^2}$ which is clearly positive, so since $\dfrac{r}{1+r}$ is strictly increasing for all $r \geq 0$. It is 1-to-1. </p>

<p>This gives $|z|=|w|$ and thus $\dfrac{z}{1+|z|}=\dfrac{w}{1+|w|}=\dfrac{w}{1+|z|}$ and so $z=w$.</p>
"
"2381068","2381073","<p>If $L(D)=\sum_{|i|=1}^m a_i D^i$, then the leading part is $$L'(D)=\sum_{|i|=m}a_i D^i.$$</p>

<p>It's ellptic if $L'(z)\neq 0$ for $z\neq 0$ real (not if $L'=L$). </p>

<p>For example, $L(D)=[D_1,D_2]=D_1D_2-D_2D_1$ is not elliptic, whereas $L(D)=D_1D_2-2D_2D_1$ is elliptic.</p>
"
"2381069","2381081","<p>We can write the definition of the chain rule as
$$\frac{dz}{dt} = \frac{dz}{dx} \cdot \frac{dx}{dt}.$$
Now, take $z={dy\over dx}$ and plug it in the previous formula:
$${d\over dt}{dy\over dx}=\left({d\over dx}{dy\over dx}\right){dx\over dt}\quad\text{(1)}$$
You get the expression of your question.</p>

<p>In particular, $\text{(1)}$ is not a <em>definition</em> of the chain rule, but an application of it.</p>
"
"2381072","2381078","<p>Without going into details, this is expected.  </p>

<p>Details lite: If your random point is $x=(x_1, x_2,\ldots,x_n)$ and your fixed vector is $c=(1,0,\ldots,0)$ (and it might as well be), the angle is determined by the square of its cosine, $\cos^2 \theta = (x,c)^2/\|x\|^2$ which is $x_1^2/(x_1^2 + x_2^2 + \ldots x_n^2)$.  The term $x_1^2$ gets washed out by the others more and more as $n$ gets bigger.  As you move from $n=2$ to $3$, and so on.</p>
"
"2381077","2381101","<p>$$f(x)=\left(1-\frac{1}{x} \right)^x$$
Change of variable :
$$x=\frac{1}{\epsilon}\quad\to\quad f(x)=g(\epsilon)=\exp\left(\frac{\ln(1-\epsilon)}{\epsilon} \right)$$
$\frac{\ln(1-\epsilon)}{\epsilon}=\frac{1}{\epsilon}\left(-\epsilon -\frac{\epsilon^2}{2}-\frac{\epsilon^3}{3}-...\right)=-1-\frac{\epsilon}{2}-\frac{\epsilon^2}{3}-...$</p>

<p>$x\to\infty \qquad \epsilon \to 0 \qquad \frac{\ln(1-\epsilon)}{\epsilon} \to -1$
$$f(x\to\infty)=g(\epsilon\to 0)\to \exp(-1)$$</p>
"
"2381082","2381089","<p>If $a \ge 1 $ and $b \ge 1$ we have</p>

<p>$(x_1 - x_2)^2 + (a - 1)x_1^2 + (b - 1)x_2^2 \ge 0$</p>

<p>since $(x_1 - x_2)^2 \ge 0$, $(a - 1)x_1^2  \ge 0 $ and $(b - 1)x_2^2 \ge 0$.</p>
"
"2381087","2381091","<p>Suppose there is $y \geqslant 0$ such that $f(y) &gt; 1$ and let </p>

<p>$$x = \sup \{ x \in [0, y] : f(x) \leqslant 1 \}$$</p>

<p>so from continuity $f(x) = 1$. But from the MVT there is $\xi \in (x, y)$ such that </p>

<p>$$f'(\xi) = \frac{f(y)-f(x)}{y-x} &gt; 0$$</p>

<p>but $f(\xi) &gt; 1$ which contradicts one of the assumptions. </p>
"
"2381100","2381108","<p>Let $||\cdot||$ be a matrix norm on $M_{n}(\mathbb{R})$. </p>

<p>For every matrices $A$, $B$ and $M$ we have </p>

<p>$\rho_A(M) - \rho_B(M) = (A - B)^T M A + B^T M (A - B)$ and so 
$\left\|\rho_A(M) - \rho_B(M)\right\| \le \left\|(A - B)^T\right\| \left\|M\right\|\left\| A\right\| + \left\|B^T\right\|\left\| M \right\| \left\|(A - B)\right\|$ dividing by $\left\|M\right\|$ and taking the supremum we have : </p>

<p>$\left\|\rho_A - \rho_B\right\| \le  \left\|(A - B)^T\right\|\left\| A\right\| + \left\|B^T\right\|\left\|(A - B)\right\|$. Finally using the continuity of $M \mapsto M^T$ we can conclude the continuity of $\rho$.</p>
"
"2381107","2381121","<p>Looks good.</p>

<p>For your information, the other absorption law is $$x(x+w) = x,$$ so your simplification result is right by letting 
$$w = \bar y + \bar z.$$</p>
"
"2381110","2381117","<p>Let $f(x,y)=\frac{y\cdot \ln(1+x^2)}{y^2+\ln(1+x^2)^2}$</p>

<p>Show that</p>

<ol>
<li>$f(x,0) \to 0$ for $x \to 0$</li>
</ol>

<p>and</p>

<ol start=""2"">
<li>$\lim_{y \to 0+}f(\sqrt{y},y) \ne 0$  </li>
</ol>

<p>(d'Hospital !).</p>

<p>Consequence ?</p>
"
"2381111","2381288","<p>The equation to be considered for $n$ is
$$
\sum_{k=1}^{n-2} k \left\lfloor \frac{n}{k} \right\rfloor = 1 + \sum_{k=1}^{n-1} k \left\lfloor \frac{n-1}{k} \right\rfloor
$$
and we assume $n&gt;2$.</p>

<p>If we add $(n-1)\left \lfloor \frac{n}{n-1} \right\rfloor + n \left\lfloor \frac{n}{n}\right \rfloor + (n-1)\left\lfloor \frac{n-1}{n}\right\rfloor= (n-1) + (n) + (0) = 2 n - 1$ for $n&gt;2$ on both the left and right hand side, we can rewrite the equations as 
$$
\sum_{k=1}^{n} k \left\lfloor \frac{n}{k} \right\rfloor = 1 + (2n-1) + \sum_{k=1}^{n} k \left\lfloor \frac{n-1}{k} \right\rfloor
$$
Bringing both sums to the left and combining them then gives:
$$
\sum_{k=1}^{n-2} k \left( \left\lfloor \frac{n}{k} \right\rfloor - \left\lfloor \frac{n-1}{k} \right\rfloor\right)= 2 n.
$$
On the left hand side we now have a sum over terms of the form:
$$
\left\lfloor \frac{n}{k} \right\rfloor - \left\lfloor \frac{n-1}{k} \right\rfloor
$$
Since $n$ and $n-1$ only differ by 1, these terms can only be either 1 or 0 whenever $k$ divides $n$ respectively when it doesn't. So only for $k|n$ there is a non-zero contribution on the left. We therefore find the equation
$$
\sum_{k|n}^{n} k \left( \left\lfloor \frac{n}{k} \right\rfloor - \left\lfloor \frac{n-1}{k} \right\rfloor\right)= \sum_{k|n}^{n} k = 2 n
$$
which is just the definition of a perfect number and establishes the equivalence.</p>
"
"2381114","2381215","<p>Claim: The sequence $x_n \rightarrow a$ if and only if for every (open) neighborhood $U$ of $a$, there is an $N$ such that for all $n \ge N$, $x_n \in U$.</p>

<p>Note: The definition you are using is, that $x_n \rightarrow a$ if for every $\epsilon &gt; 0$, there exists an $N$ such that $d(x_n,a) &lt; \epsilon$ <strong><em>for all</em></strong> $n \ge N$. </p>

<p>Proof:</p>

<p>$(\Rightarrow)$ Suppose that $x_n \rightarrow a$. </p>

<ul>
<li><p>Now consider any arbitrary open neighborhood $U$ around $a$. Since $U$ is open, there exists some $\epsilon &gt; 0$ such that $B(a,\epsilon) \subset U$. Note I am using your notation that $B(a,\epsilon)$ is simply the open ball of radius $\epsilon$ around $a$. </p></li>
<li><p>Since $x_n \rightarrow$ by hypothesis, there is an $N$ such that $x_n \in B(a,\epsilon)$ for all $n \ge N$.</p></li>
<li><p>Since $B(a,\epsilon) \subset U$, we also have $x_n \in U$ for all $n \ge N$.</p></li>
</ul>

<p>$(\Leftarrow)$ Suppose that for every open neighborhood $U$ of $a$, there is an $N$ such that for all $n \ge N$, $x_n \in U$.</p>

<ul>
<li>Fix an arbitrary $\epsilon &gt; 0$. Clearly, $B(a, \epsilon)$ is an open neighborhood of $a$. Therefore by hypothesis, there exists an $N$ such that for all $n \ge N$, $x_n \in U$.</li>
<li>By definition of $B(a, \epsilon)$, this means that for all $n \ge N$, we have $d(x_n,a) &lt; \epsilon$. $\square$</li>
</ul>
"
"2381123","2387586","<p>Let $L_t$ denote the arrival time of the last passenger to arrive before time $t$, then $0\leqslant L_t&lt;t$ almost surely, and $t-L_t$ is distributed as $\min\{X,t\}$ where $X$ is exponential with parameter $\lambda$. Thus, the waiting time $W_t=t-L_t$ of the last passenger is such that $$E(W_t)=E(\min\{X,t\})=tP(X&gt;t)+\int_0^txf_X(x)dx$$ that is, $$E(W_t)=te^{-\lambda t}+\int_0^tx \lambda e^{-\lambda x}dx=\frac{1-e^{-\lambda t}}{\lambda}$$ Now, plug in $t=15$ minutes.</p>

<p><strong>Edit answering a new question asked in the comments below:</strong> The average waiting time of the passengers taking the bus can be computed as follows. The number of passengers taking the bus is $1+N_t$ where $N_t$ is Poisson with parameter $\lambda t$. The waiting time of one passenger equals $t$, the mean waiting time of every other passenger, if any, is $\frac12t$, thus, the mean waiting time $w_t$ of the passengers is $$w_t=E\left(\frac{t+\frac12tN_t}{1+N_t}\right)=\frac12t\left(1+E\left(\frac1{1+N_t}\right)\right)=\frac12t\left(1+\frac{1-e^{-\lambda t}}{\lambda t}\right)$$
Predictably, $\frac12t&lt;w_t&lt;t$ for every positive $\lambda$, $w_t\to t$ when $\lambda\to0$ (can you say why?) and $w_t\to\frac12t$ when $\lambda\to\infty$ (can you say why?).</p>
"
"2381124","2381265","<p>The correct divisor is $6$.  To get the average, it doesn't matter how you got the terms.  Another way to get the squares is to add up the odd numbers, so you could do $\frac 16\left((1)+(1+3)+(1+3+5)+(1+3+5+7)+(1+3+5+7+9)+(1+3+5+7+9+11)\right)$  You wouldn't want the denominator to change to $21$ because you got the squares this way, would you?</p>
"
"2381126","2381156","<p>Drop an altitude onto $AB$ from $D$ and from $C$. Since $AB\parallel DC$, the altitudes have the same length. Also $AD=BC$, so</p>

<p>$$\sin \angle A=\sin \angle B$$</p>

<p>Either $\angle A=\angle B$, or $\angle A+\angle B=180^\circ$.</p>
"
"2381141","2382497","<p>Either $a$ or $b$ can be used as $\sqrt{-1}$.</p>
"
"2381144","2381153","<p>I think this comes down to keeping all the units straight, or rather it comes down to choosing good units to work with.</p>

<p>Let's fix a unit of time by saying that in each unit the prisoner travels $8$ steps and his pursuer travels $5$.  </p>

<p>To calculate the amount of time involved, we work in ""prisoner lengths"".  Since each pursuer step is $\frac 52$ of a prisoner step, in one unit of time the pursuer travels $5\times \frac 52$ lengths, and the prisoner travels $8$</p>

<p>Solve for the amount of time it takes to catch the prisoner.  After $n$ units of time we must have $$n\times 5\times \frac 52=n\times 8 +27\implies n=6$$</p>

<p>But of course the pursuer then traveled $6\times 5=30$ ""pursuer steps"".</p>
"
"2381146","2381992","<p>I don't know that there is any clever shortcut to solve it. This problem comes from back in times when people loved their word puzzles and were not afraid to do a little arithmetic by hand.</p>

<blockquote>
  <p>You see, Andrew managed to get possession of just two-thirds of the parcel
   of sugar-plums.</p>
</blockquote>

<p>$A = \frac{2}{3}$</p>

<blockquote>
  <p>Bob at once grabbed three-eighths of these, and Charlie managed to seize three-tenths also.</p>
</blockquote>

<p>$B = \frac{3}{8} A = \frac{1}{4}\,$, $\;C = \frac{3}{10} A = \frac{1}{5}\,$, $\;A = A-B-C = \frac{2}{3}-\frac{1}{4}-\frac{1}{5} = \frac{13}{60}$</p>

<blockquote>
  <p>Then young David dashed upon the scene, and captured all that Andrew had left, except one-seventh, which Edgar artfully secured for himself by a cunning trick.</p>
</blockquote>

<p>$D=\frac{6}{7}A = \frac{13}{70}\,$, $\;E=\frac{1}{7}A = \frac{13}{420}\,$, $\;A = 0\,$</p>

<blockquote>
  <p>Now the fun began in real earnest, for Andrew and Charlie jointly set upon Bob, who stumbled against the fender and dropped half of all that he had,</p>
</blockquote>

<p>$b^{\,'} = \frac{1}{2}B = \frac{1}{2} \frac{1}{4} =\frac{1}{8}\,$, $\;B = B - b^{\,'} = \frac{1}{4}-\frac{1}{8}= \frac{1}{8}$</p>

<blockquote>
  <p>which were equally picked up by David and Edgar, who had crawled under a table and were waiting.</p>
</blockquote>

<p>$D=D+\frac{1}{2} b^{\,'}=\frac{13}{70}+\frac{1}{16} = \frac{139}{560}\,$, $\;E=E+\frac{1}{2}b^{\,'} = \frac{13}{420}+\frac{1}{16}=\frac{157}{1680}\,$</p>

<blockquote>
  <p>Next, Bob sprang on Charlie from a chair, and upset all the latter's collection on to the floor.</p>
</blockquote>

<p>$c^{\,'}=C=\frac{1}{5}\,$, $\;C=0$</p>

<blockquote>
  <p>Of this prize Andrew got just a quarter, Bob gathered up one-third, David got two-sevenths,</p>
</blockquote>

<p>$A=A+\frac{1}{4}c^{\,'} = \frac{1}{20}\,$, $\;B=B+\frac{1}{3}c^{\,'}=\frac{1}{8}+\frac{1}{15}=\frac{23}{120}\,$, $\;D=D+\frac{2}{7}c^{\,'}=\frac{139}{560}+\frac{2}{35}=\frac{171}{560}\,$</p>

<blockquote>
  <p>while Charlie and Edgar divided equally what was left of that stock.</p>
</blockquote>

<p>$\;c^{\,''}=\left(1-\frac{1}{4}-\frac{1}{3}-\frac{2}{7}\right)c^{\,'}=\frac{11}{84}c^{\,'}=\frac{11}{420}\,$, $\;C=C+\frac{1}{2}c^{\,''}=\frac{11}{840}\,$, $\;E=E+\frac{1}{2}c^{\,''}=\frac{157}{1680}+\frac{11}{840}=\frac{179}{1680}$</p>

<blockquote>
  <p>They were just thinking the fray was over when David suddenly struck out in two directions at once, upsetting three-quarters of what Bob and Andrew had last acquired.</p>
</blockquote>

<p>$b^{\,''}=\frac{3}{4} \frac{1}{3}c^{\,'}=\frac{1}{20}\,$, $\;a^{\,''}=\frac{3}{4}\frac{1}{4}c^{\,'}=\frac{3}{80}\,$, $\;B=B-b^{\,''}=\frac{23}{120}-\frac{1}{20}=\frac{17}{120}\,$, $\;A=A-a^{\,''}=\frac{1}{20}-\frac{3}{80}=\frac{1}{80}\,$</p>

<blockquote>
  <p>The two latter, with the greatest difficulty, recovered five-eighths of it in equal shares, but the three others each carried off one-fifth of the same.</p>
</blockquote>

<p>$d^{\,''}=b^{\,''}+a^{\,''}=\frac{1}{20}+\frac{3}{80}=\frac{7}{80}\,$, $\;A=A+\frac{1}{2}\frac{5}{8}d^{\,''}=\frac{1}{80}+\frac{7}{256}=\frac{51}{1280}\,$, $\;B=B+\frac{1}{2}\frac{5}{8}d^{\,''}=\frac{17}{120}+\frac{7}{256}=\frac{649}{3840}\,$, $C=C+\frac{1}{8}d^{\,''}=\frac{11}{840}+\frac{7}{640}=\frac{323}{13440}\,$, $\;D=D+\frac{1}{8}d^{\,''}=\frac{171}{560}+\frac{7}{640}=\frac{1417}{4480}\,$, $\;E=E+\frac{1}{8}d^{\,''}=\frac{179}{1680}+\frac{7}{640}=\frac{1579}{13440}\,$</p>

<blockquote>
  <p>Every sugar-plum was now accounted for, and they called a truce, and divided equally amongst them the remainder of the parcel.</p>
</blockquote>

<p>$z=\frac{1}{5}\frac{1}{3}=\frac{1}{15}\,$:</p>

<p>$$
A=A+z=\frac{51}{1280}+\frac{1}{15}=\frac{409}{3840}=\frac{\color{red}{2863}}{26880} \\[5px]
$$
$$
B=B+z=\frac{649}{3840}+\frac{1}{15}=\frac{181}{768}=\frac{\color{red}{6335}}{26880} \\[5px]
$$
$$
C=C+z=\frac{323}{13440}+\frac{1}{15}=\frac{1219}{13440}=\frac{\color{red}{2438}}{26880} \\[5px]
$$
$$
D=D+z=\frac{1417}{4480}+\frac{1}{15}=\frac{5147}{13440}=\frac{\color{red}{10294}}{26880} \\[5px]
$$
$$
E=E+z=\frac{1579}{13440}+\frac{1}{15}=\frac{165}{896}=\frac{\color{red}{4950}}{26880} \\[5px]
$$</p>

<blockquote>
  <p>What is the smallest number of sugar-plums there could have been at the start, and what proportion did each boy obtain?</p>
</blockquote>

<p>$\operatorname{lcm}(3840, 768, 13440, 13440, 896) = 26880\,$.</p>
"
"2381160","2381552","<ol>
<li><p>Since $y$ is a regular value of $f$, we should have that $X$ is discrete, i.e., every point of $X$ is isolated. In particular $X$ is finite and you can <em>enumerate</em> points of $X$ in ascending order.</p></li>
<li><p>There exist $n \geq 0$, a neighborhood $V$ of $y$ and smooth functions $\psi_i : V \to U_i$ for $i = 1, \cdots, n$ such that $U_i$ are disjoint open subsets of $I$ and</p>

<p>$$f^{-1}(\{y'\}) = \{ \psi_1(y') &lt; \cdots &lt; \psi_n(y')\}$$</p>

<p>for each $y' \in V$. That is, $\psi_i$ are local inverse of $f$. Assuming that your construction is given by</p>

<p>$$ I = \begin{cases}
(0, \psi_1(y)) \cup \cdots \cup (\psi_n(y), 1) &amp; \text{if $n$ is even} \\
(0, \psi_1(y)) \cup \cdots \cup (\psi_{n-1}(y), \psi_n(y)) &amp; \text{if $n$ is odd}
\end{cases}, $$</p>

<p>i.e. collecting only odd-th subintervals of the open set $(0,1)\setminus f^{-1}(\{y\})$ and taking union on them, then it follows that</p>

<p>$$ \frac{d}{dy} \int_{I} g(x) \, dx
= \sum_{i=1}^{n} (-1)^{i-1} g(\psi_i(y))\psi_i'(y)
= \sum_{i=1}^{n} (-1)^{i-1} \frac{g(\psi_i(y))}{f'(\psi_i(y))}. $$</p>

<p>In your notation, $x_i = \psi_i(y)$ and so it is slightly more succinctly written as</p>

<p>$$ \frac{d}{dy} \int_{I} g(x) \, dx = \sum_{i=1}^{n} (-1)^{i-1} \frac{g(x_i)}{f'(x_i)}. $$</p></li>
</ol>
"
"2381163","2381166","<p>You can get non-differentiability at a countable infinite set
by considering
$$\sum_{n=1}^\infty a_n|x-c_n|$$
where $a_n&gt;0$ and $(a_n)$ goes to zero rapidly.</p>
"
"2381167","2381175","<p>$|(a+b)-13|=|(a-5)+(b-8)| \le |a-5|+|b-8| &lt; \frac{1}{2}+\frac{1}{2}=1$</p>

<p>(triangle inequality !)</p>
"
"2381168","2381304","<p>The Lagrangian function is a purely formal object not having any intuitive interpretation. Setting $\nabla L=0$ together with the constraint furnishes the points ${\bf x}\in S$ (the manifold defined by the constraint) where
$$\nabla f({\bf x})=\lambda\nabla g({\bf x})\tag{1}$$
for some factor $\lambda$. These points are the <em>conditionally stationary points</em> of $f$ on $S$.</p>

<p>Now the condition $(1)$ has an intuitive geometric meaning: Consider a point ${\bf p}\in S$. Since $S$ is a level surface of the constraint function $g$ the gradient $\nabla g({\bf p})$ (assumed to be $\ne{\bf 0}$) is orthogonal to $S$ at ${\bf p}$, or more precisely: is the normal of the tangent hyperplane $S_{\bf p}$. </p>

<p>On the other hand, if ${\bf p}$ is a conditionally stationary point of $f$, then  the directional derivative $$\lim_{t\to0+}{f({\bf p}+t{\bf A})-f({\bf p})\over t}=\nabla f({\bf p})\cdot{\bf A}$$ of $f$ at ${\bf p}$ is $=0$ in all <em>allowed directions</em>, i.e., in all directions ${\bf A}\in S_{\bf p}$. This  means that  $\nabla f({\bf p})\perp S_{\bf p}$, hence $\nabla f({\bf p})$ is parallel to $\nabla g({\bf p})$, and this is what $(1)$ is saying.</p>
"
"2381179","2381719","<p>The matrix $\mathbf C$ defines a family of hyperbolas $\mathbf x^T\mathbf C\mathbf x=k$ with common principal axes and asymptotes. The degenerate member of this family $\mathbf x^T\mathbf C\mathbf x=0$ consists of those asymptotes. The asymptotes of a hyperbola in standard position are $\frac xa\pm \frac yb=0$, so they have direction vectors $$\left(\frac1a,\pm\frac1b\right)=\frac1a(1,0)\pm \frac1b(0,1).$$ This has an obvious generalization to a rotated hyperbola: if $\mathbf v_{\text{maj}}$ and $\mathbf v_{\text{min}}$ are unit vectors that give the major (transverse) and minor axes, respectively, with corresponding half-axis lengths $a$ and $b$, its asymptotes have direction vectors $\frac1a\mathbf v_{\text{maj}}\pm\frac1b\mathbf v_{\text{min}}$. (We donât really need unit vectors for this, but we must have $\|\mathbf v_{\text{maj}}\|=\|\mathbf v_{\text{min}}\|$.)  </p>

<p>The eigenvectors of the matrix $\mathbf C$ are the conicâs principal axes and its eigenvalues are the reciprocal squares of the corresponding half-axis lengths, from which the method you found immediately follows.  </p>

<p>Thereâs another method for decomposing such a matrix that doesnât involve computing eigenvalue and eigenvectors. The idea is to find a skew-symmetric matrix $\mathbf M$ such that $\mathbf C+\mathbf M$ is a scalar multiple of $\mathbf g\mathbf h^T$. The lines can then be read directly from this rank-one matrix: they are a row and column of $\mathbf C+\mathbf M$ that correspond to a non-zero diagonal entry.</p>
"
"2381181","2381185","<p>There's no contradiction, there's just ill-defined terms. </p>

<p>The row</p>

<blockquote>
  <p>$3=\sqrt{1+2\cdot \sqrt{1+3 \cdot \sqrt{1+4\cdot \sqrt36}} }$  </p>
</blockquote>

<p>is correct, but the following </p>

<blockquote>
  <p>$\vdots$  </p>
  
  <p>$3=\sqrt{1+2\cdot \sqrt{1+3 \cdot \sqrt{1+4\cdot \sqrt{1+ \cdots}}} }$  </p>
</blockquote>

<p>is meaningless.</p>

<p>There is no clear way to define what the three dots at the end of that expression mean.</p>

<hr>

<p>To provide a counterexample, when a mathematitian writes $\dots$, he is usually capable of defining exactly what those dots mean. For example, writing</p>

<p>$$1+\frac12+\frac 14 + \cdots$$</p>

<p>is the same as</p>

<p>$$\lim_{n\to\infty} 1+\frac12+\frac14+\cdots + \frac{1}{2^n}.$$</p>

<p>This is a well defined expression, because</p>

<ol>
<li>The concept of limit is well defined using $\epsilon-\delta$ definitions</li>
<li>The concept of a finite sum of $n$ elements is defined through induction</li>
</ol>

<p>If you want to go down deeper, every concept can be broken down into small pieces until only axioms and definitions remain.</p>

<hr>

<p>On the other hand, there is no clear way how to strictly mathematically (using more basic concepts) write down what </p>

<p>$$\sqrt{1+2\cdot \sqrt{1+3 \cdot \sqrt{1+4\cdot \sqrt{1+ \cdots}}} }$$
represents, and until you write it down strictly, you shouldn't be surprised when you get ""weird"" results. But those weird results aren't strictly speaking ""contradictions"", since they are not real mathematical expressions.</p>

<hr>

<p>Another example of why you can't just write down three dots and hope everything sticks in infinity:</p>

<p>$$0.9\neq 1
\\0.99\neq 1
\\0.999\neq 1
\\\vdots
\\0.999999\dots \neq 1$$</p>

<p>is also not true, because the conclusion is false.</p>

<p>Or, even more simply</p>

<ul>
<li>$\{1\}$ is finite</li>
<li>$\{1,2\}$ is finite</li>
<li>$\{1,2,3\}$ is finite</li>
<li>$\vdots$</li>
<li>$\{1,2,3,\dots\}$ is finite.</li>
</ul>
"
"2381183","2381798","<p>Take the two end points <em>A</em> and <em>B</em> as a line segment and find the perpendicular bisector. The center of the arc is going to be on this line. Now find the point of the line which is a distance $r$ (the radius) from the end point.</p>

<p><a href=""https://i.stack.imgur.com/IjaYE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IjaYE.png"" alt=""pic1""></a></p>

<ul>
<li><p>You can find the distance $ m = \frac{1}{2} \| AB \| $ and then the distance $d = \sqrt{r^2-m^2}$ to find the point <em>C</em>.</p></li>
<li><p>Or you can find the angle $\theta = \sin^{-1} \frac{m}{r}$ and then set $d = r \cos \theta$.</p></li>
</ul>

<h2>Edit 1</h2>

<p>I think the follow-up question asks to find the red distance $r$ and angle $\theta$ below from the blue $R$ and $\varphi$:</p>

<p><a href=""https://i.stack.imgur.com/6AmnC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6AmnC.png"" alt=""Polar1""></a></p>

<p>These are found with a little trigonometry as</p>

<p>$$ \begin{aligned} 
  r \sin \theta &amp; = R \sin \varphi \\
  r_O + r \cos\theta &amp;= R \cos \varphi
\end{aligned} \Rightarrow
 \begin{aligned}
  r &amp;= \sqrt{ R^2+r_O^2-2 R r_O \cos \varphi} \\
  \theta &amp; = \tan^{-1} \left( \frac{R \sin \varphi}{R \cos\varphi -r_O} \right)
\end{aligned} $$</p>

<p>where $r_O$ is the radius of the outer circle. The arc radius is $R=r_O+r_I$ it turns out (I think).</p>
"
"2381190","2381274","<p>Taking the derivative $n$ times and setting $x = 0$ gives you the coefficient of $x^n$ in the exponential generating function:</p>

<p>$$ f(x) = \sum_{k = 0}^\infty f^{(k)}(0) \frac{x^k}{k!}. $$</p>

<p>It is typical to write this as in terms of the coefficient operator $\left[ \frac{x^n}{n!} \right]$ which when applied to a power series $f(x)$ gives you the coefficient of $\frac{x^n}{n!}$ in $f(x)$. In other words</p>

<p>$$\left[ \frac{x^n}{n!} \right] \sum_{k = 0}^\infty a_k \frac{x^k}{k!} = a_n. $$</p>

<p>Then</p>

<p>\begin{align}
\sum_{k = 0}^n \binom{n}{k} (-1)^{n - k}k^n &amp;= \sum_{k = 0}^n \binom{n}{k} (-1)^{n - k} \left[ \frac{x^n}{n!} \right] e^{kx} \\
&amp;= \left[ \frac{x^n}{n!} \right]\sum_{k = 0}^n \binom{n}{k} (-1)^{n - k}  e^{kx} \\
&amp;= \left[ \frac{x^n}{n!} \right] (e^x - 1)^n.
\end{align}</p>

<p>In the theory of combinatorial species, $e^x - 1$ is the (exponential) generating function for non-empty sets. That is, of the map which takes in a set $X$ and spits out the set</p>

<p>$$ \mathcal{E}(X) = \begin{cases} \{X\} &amp; \text{if } X \ne \emptyset \\ \emptyset &amp; \text{if } X = \emptyset \end{cases}. $$</p>

<p>Notice that</p>

<p>$$ |\mathcal{E}(X)| = \begin{cases} 1 &amp; \text{if } X \ne \emptyset \\ 0 &amp; \text{if } X = \emptyset \end{cases} $$</p>

<p>We define the generating function for $\mathcal{E}$ to be</p>

<p>$$ \sum_{n = 0}^\infty |\mathcal{E}([n])| \frac{x^n}{n!} = e^x - 1 $$</p>

<p>where $[n] = \{1, 2, \dots, n\}$.</p>

<p>If $\mathcal{F}$ is a combinatorial species then we define the power $\mathcal{F}^k$ to be the map</p>

<p>$$ \mathcal{F}^k(X) = \bigsqcup_{(S_1,\dots,S_k)} \mathcal{F}(S_1) \times \dots \times \mathcal{F}(S_k) \tag{1}$$</p>

<p>where the union is over all (ordered) partitions of $X$ into $k$ subsets $S_1,\dots,S_k$. That is, $S_1 \cup \dots \cup S_k = X$ and $S_1,\dots,S_k$ are disjoint.</p>

<p>One can show that if $F(x) = \sum_n |\mathcal F([n])| x^n/n!$ is the generating function for $\mathcal{F}$ then $F(x)^k$ is the generating function for $\mathcal{F}^k$.</p>

<p>Bringing this back to $\mathcal{E}^n$ we can say that</p>

<p>$$ \left[ \frac{x^n}{n!} \right] (e^x - 1)^n = |\mathcal{E}^n([n])| $$
is the number of ways of partitioning $[n]$ into $n$-non-empty sets because only the terms in $(1)$ in which $S_1,\dots,S_n$ are all non-empty will contribute to the union.</p>

<p>Partitioning $[n]$ into $n$ singletons is the same thing as a permutation of $n$ and there are $n!$ of these. Hence</p>

<p>$$n! = \left[ \frac{x^n}{n!} \right] (e^x - 1)^n = \sum_{k = 0}^n \binom{n}{k} (-1)^{n - k}k^n. $$</p>
"
"2381192","2381285","<p>I presume you are referring to 
<a href=""https://i.stack.imgur.com/jUzI9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jUzI9.png"" alt=""enter image description here""></a>
(available at link below)</p>

<p>Here the author is using the two  symbols (which I can't replicate with Latex) to represent two different <strong>partial orders</strong> (not to differentiate between less than and much less than).</p>

<p>P.S. the answer to the question may clarify what is going on. The partial order on line two is stronger than the first since $a \le x \text{ and } b \le y \implies a \lt x \text{ or } (a = x \text{ and } b \le y)$</p>

<p><a href=""https://books.google.co.za/books?id=ZuIgAwAAQBAJ&amp;pg=PR4&amp;lpg=PR4&amp;dq=P.+Szekeres,+A+Course+in+Modern+Mathematical+Physics&amp;source=bl&amp;ots=rJ9u7UhlJm&amp;sig=nSE68IYYcOzm36rABaui66STjUg&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwihrtj9m7vVAhXiJMAKHTlRCwsQ6AEIUTAJ#v=onepage&amp;q=P.%20Szekeres%2C%20A%20Course%20in%20Modern%20Mathematical%20Physics&amp;f=false"" rel=""nofollow noreferrer"">https://books.google.co.za/books?id=ZuIgAwAAQBAJ&amp;pg=PR4&amp;lpg=PR4&amp;dq=P.+Szekeres,+A+Course+in+Modern+Mathematical+Physics&amp;source=bl&amp;ots=rJ9u7UhlJm&amp;sig=nSE68IYYcOzm36rABaui66STjUg&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwihrtj9m7vVAhXiJMAKHTlRCwsQ6AEIUTAJ#v=onepage&amp;q=P.%20Szekeres%2C%20A%20Course%20in%20Modern%20Mathematical%20Physics&amp;f=false</a>)</p>
"
"2381200","2381236","<p>If I have understood your notation correctly, in Mathematica I get
$$
f(x)=\frac{96}{x^7}\left((x^2-15)x \cos(x) + 3(5-2x^2)\sin(x) \right)
$$
you can change in bounds on the inverse transform to be between $-1$ and $1$, because of the limited domain.
$$
f(x) = \frac{1}{2\pi}\int_{-1}^1 (1-t^2)^3 \exp(i t x)\;dt
$$
note that the constant of $2 \pi$ can vary on the definition used, and the inverse transform has the opposite sign in the exponential to the forward transform.</p>
"
"2381210","2381233","<p>Just use that $\sqrt{\cdot}$ is continuous at $0$ and $\lim_{n \to \infty} \frac{1}{n} = 0$. Then we get $$\lim_{n \to \infty} \frac{1}{\sqrt{n}} = \sqrt{\lim_{n \to \infty} \frac{1}{n}} = \sqrt{0} = 0.
$$</p>
"
"2381212","2381235","<p><strong>hint</strong></p>

<p>$f $ is decreasing at $\Bbb R $, thus</p>

<p>For $1\le j\le 10$</p>

<p>$$f (x_j)\le f (\frac {x_{j-1}+x_j}{2})\le f (x_{j-1}) $$
and
$$\forall x\in [x_{j-1},x_j] \;\; f (x)\le f (x_{j-1}) $$</p>

<p>hence, the answer is $B $.</p>
"
"2381231","2381294","<p>You can reduce the problem to the case of $n=0$. If $P(P(P(n)))=n$, define $P_0(x)=P(x+n)-n$. Then $P_0(P_0(P_0(0)))=P(P(P(n)))-n$, so, if the theorem is true for $0$, then $P_0(0)=P(n)-n=0$.</p>

<p>Assume $P(0)\neq 0$ and $P(P(P(0)))=0$.</p>

<p>We also know that $P(P(0))\neq 0$ or else $P(P(P(0)))=P(0)\neq 0$. We also have that $P(0)\neq P(P(0))$, because otherwise, $0=P(P(P(0)))=P(P(0))\neq 0$.</p>

<p>We will now use the following standard result:</p>

<blockquote>
  <p>For $q(x)\in\mathbb Z[x]$ you have that $q(m)\equiv q(n)\pmod{m-n}$ for all integers $m,n$.</p>
</blockquote>

<p>We will prove that $P(0)\mid P(P(0))$ and $P(P(0))\mid P(0)$. From this we deduce that $P(P(0))=-P(0)$ and hence that $2P(0)\mid P(P(0))$, which is only possible if $P(0)=P(P(0))=0$.</p>

<p>Proof:</p>

<p>(1) With $q(x)=P(P(x))$ and $m=P(0), n=0$ you get that $$P(0)\mid P(P(0)).$$</p>

<p>(2) With $q=P(x),m=P(P(0)), n=0$ you $$P(P(0))\mid P(0).$$</p>

<p>(3) So $P(0)=\pm P(P(0))$. If $P(P(0))=P(0)$ then $0=P(P(P(0))=P(0)$. So assume $P(0)=-P(P(0))$.</p>

<p>(4) Now, letting $q(x)=P(x), m=P(0), n=P(P(0))=-P(0)$, you get $P(P(0))\equiv 0\pmod{2P(0)}.$</p>

<p>So $2P(0)\mid P(P(0))$ which contradicts $P(0)=\pm P(P(0))$ unless $P(0)= 0$.</p>

<hr>

<p>This proof shows that this works for any function $f:\mathbb Z\to\mathbb Z$ such that $f(n)\equiv f(m)\pmod{n-m}$ for all integers $n,m$. There are uncountably many non-polynomial $f$, but there are also some polynomials with non-integer coefficients, like $f(x)=\frac{x^4+x^2}{2}$.</p>

<hr>

<p>So the short proof is (writing $P^2(x)=P(P(x)), P^3(x)=P(P(P(x)))$):</p>

<p>Assume $P^3(n)= n.$ Then:</p>

<p>$$\begin{align}P^2(n)&amp;\equiv P(n)\equiv n\pmod{P(n)-n}\\
n= P^3(n)&amp;\equiv P(n)\pmod{P^2(n)-n}\\
n=P^3(n)&amp;\equiv P^2(n)\pmod{P^2(n)-P(n)}
\end{align}$$</p>

<p>The first two lines mean that $P^2(n)-n=\pm \left(P(n)-n\right).$</p>

<p>If $P^2(n)-n=P(n)-n$, then $P^2(n)=P(n)$ and hence $n=P(P^2(n))=P(P(n))=P(n).$</p>

<p>If $P^2(n)-n=n-P(n)$ then $P^2(n)-P(n)=2(n-P(n))$. So $$n=P(P^2(n))\equiv P(P(n))\pmod{2(n-P(n))}$$</p>

<p>So we get that $2(n-P(n))\mid (n-P^2(n))$. But $n-P^2(n)=\pm (n-P(n))$, contradiction, unless $n-P^2(n)=0$ and hence $P^2(n)=n$ and then $P(n)=P(P^2(n))=n$.</p>
"
"2381239","2381250","<p>It is not in contradiction with what you obtained : </p>

<p>Using the AM-GM,</p>

<p>$$ 3a +c \geq 2 \sqrt{3ac} &gt; 2\sqrt{4b^2} = 4b $$</p>

<p>Then, C is correct!</p>
"
"2381240","2381255","<p>Take the easiest one defined by</p>

<p>$$(\forall a\in A) \;\;\;\; f (a)=\{a\}. $$</p>

<p>for $a,b\in A $,</p>

<p>$$f (a)=f (b)\implies \{a\}=\{b\}$$
$$\implies a=b $$</p>

<p>this proves $f $ is injective.</p>
"
"2381241","2381243","<p>Since it is continuous, you can't have vertical asymptotes, so the denominator cannot have real roots.  You can have a quadratic on top and quartic on bottom to fit these conditions.  Let
$$f(x)=\frac{ax^2+b}{x^4+1}$$
You can use the first 2 conditions to solve for $a$ and $b$, while satisfying the other conditions.</p>
"
"2381248","2381254","<p>The kind of convergence you refer to is (compact) normal convergence.</p>

<p>This can be shown using <a href=""https://en.wikipedia.org/wiki/Morera%27s_theorem"" rel=""nofollow noreferrer"">Morera's theorem</a> in conjunction with <a href=""https://en.wikipedia.org/wiki/Cauchy%27s_integral_theorem#Statement_of_theorem"" rel=""nofollow noreferrer"">Cauchy's integral theorem</a>, which states:</p>

<blockquote>
  <p>A continuous function $f$ is analytic in an open domain $\Omega$ iff for every
  closed, zero-homological, rectifiable curve $\gamma$ the line integral
  $\oint_\gamma f(z) d z = 0$. (A zero-homological loop is a loop with
  winding number zero.)</p>
</blockquote>

<p>We can use this result to prove the desired property:</p>

<p>Continuity of the limit follows immediately from the uniform convergence. It remains to verify the key element of Cauchy's integral theorem, namely that all line integrals $\oint_\gamma f(z) dz = 0$ for the limit. 
Provided that $f_n\to f$ uniformly on every compact set, you have in particular $f_n\to f$ uniformly on every closed loop $\gamma$ (of finite length). Hence, for any zero-homological loop $\gamma$, and every $\epsilon&gt;0$ there exists some $N(\epsilon)\in \mathbb N$ such that $|f_n(z)-f(z)|&lt;\epsilon$ for all $n&gt;N(\epsilon)$. If $l(\gamma)$ denotes the length of $\gamma$, we then obtain
$$\left|\oint_\gamma f_n(z) dz - \oint_\gamma f(z) dz\right| \le l(\gamma)\epsilon,\quad\forall n&gt;N(\epsilon).$$
Since for every $n$, the integral $\oint_\gamma f_n(z) d z = 0$, this proves that $\oint_\gamma f(z) d z = 0$. Hence $f$ is analytic.</p>
"
"2381249","2382534","<blockquote>
  <p>How can I work out to which point the system will converge for initial states in the neighbourhood of $[0,0,1]$? Or for any initial state $[p_1(0), p_2(0),p_3(0)]$ in the three-trait simplex?</p>
</blockquote>

<p>What you are interested in are called basins of attraction. For non-linear maps such as yours, they can be very complex and impossible to describe analytically. Without intensively investigating the situation myself, I can only give you a road map here:</p>

<ol>
<li><p>Start by visualising the basins for some typical sets of parameters (which should be easy if I understood correctly that your state space is two-dimensional).
To do this, take a grid of points in your state space as initial conditions and colour them according to the fixed point they converge to.</p></li>
<li><p>If you see a simple structure, rejoice.
See whether you can understand it and describe it analytically.</p></li>
<li><p>Otherwise look for symmetries and other structures.
If your basins have simple borders, you can try to understand them.</p></li>
<li><p>Depending on your question, the relevant quantity for you may be the size of the basins.
This can be approximated empirically (just count the pixels) and you can then analyse its dependence on the control parameters.</p></li>
</ol>

<blockquote>
  <p>I understand from a previous questionâs answer that this might involve Lyapunov exponents and the Jacobian</p>
</blockquote>

<p>Probably not. (Local) Lyapunov exponents tell you something about how quickly the state converges to a given fixed point within its vicinity. They cannot tell you <em>to which</em> fixed point the state converges (unless there is only one stable fixed point, in which case you can indeed find out using Lyapunov exponents). If your initial condition is near the border of basins of attraction, it is not in the vicinity of a fixed point (in the sense of the above statement).</p>
"
"2381251","2382637","<p>The confusion about ranges and zero reduced costs might have to do with the fact that, in a basic feasible solution, the values of the basic variables are uniquely determined by the values of the nonbasic variables. A nonzero reduced cost (on a nonbasic variable) tells you what would happen to the objective value if you forced that nonbasic variable to move off whichever bound (lower or upper) it currently sits on. So for Balaji (nonbasic on its lower bound), forcing a small investment in Balaji reduces the overall yield by 0.25% times that investment, while for Amar (nonbasic on its upper bound), forcing the solution to invest a little less in Amar would reduce yield by 0.50% times the reduction in Amar.</p>

<p>The values of basic variables, however, cannot be unilaterally adjusted, since they are completely determined by the values of the nonbasic variables. Perhaps a more accurate way to say this is that modifying the value of a basic variable requires modifying one or more nonbasic variables, and how you do that (combined with the reduced costs of the nonbasic variables you change) will determine the impact on the objective value.</p>

<p>The ranges tell you how much you can tweak any one objective coefficient while preserving optimality of the current basis. If you move an objective coefficient outside its range, a different corner of the feasible region may become optimal.</p>
"
"2381256","2381939","<p><strong>Preliminaries</strong></p>

<p>Using $a^2+b^2=(a+ib)(a-ib)$, we get
$$
\begin{align}
\left(\omega_0^2-\omega^2\right)^2+(\Gamma_0\omega)^2
&amp;=\left(\omega^2+i\Gamma_0\omega-\omega_0^2\right)\left(\omega^2-i\Gamma_0\omega-\omega_0^2\right)
\end{align}
$$
The roots for the two quadratics are
$$
\omega=\frac{\pm i\Gamma_0\pm\sqrt{4\omega_0^2-\Gamma_0^2}}2
$$
Thus, re-pairing the roots gives
$$
\begin{align}
&amp;\left(\omega_0^2-\omega^2\right)^2+(\Gamma_0\omega)^2\\
&amp;=\left(\omega^2+\omega\sqrt{4\omega_0^2-\Gamma_0^2}+\omega_0^2\right)\left(\omega^2-\omega\sqrt{4\omega_0^2-\Gamma_0^2}+\omega_0^2\right)\\
&amp;=\left[\left(\omega+\sqrt{\omega_0^2-\tfrac14\Gamma_0^2}\right)^2+\tfrac14\Gamma_0^2\right]\left[\left(\omega-\sqrt{\omega_0^2-\tfrac14\Gamma_0^2}\right)^2+\tfrac14\Gamma_0^2\right]
\end{align}
$$
Then <a href=""https://en.wikipedia.org/wiki/Partial_fraction_decomposition"" rel=""nofollow noreferrer"">Partial Fractions</a> gives
$$
\begin{align}
&amp;\frac1{\left(\omega_0^2-\omega^2\right)^2+(\Gamma_0\omega)^2}\\[12pt]
&amp;=\frac1{2\omega_0^2}\left[\frac{1+\frac\omega{\sqrt{4\omega_0^2-\Gamma_0^2}}}{\left(\omega+\sqrt{\omega_0^2-\tfrac14\Gamma_0^2}\right)^2+\tfrac14\Gamma_0^2}
+\frac{1-\frac\omega{\sqrt{4\omega_0^2-\Gamma_0^2}}}{\left(\omega-\sqrt{\omega_0^2-\tfrac14\Gamma_0^2}\right)^2+\tfrac14\Gamma_0^2}\right]
\end{align}
$$</p>

<hr>

<p><strong>Integration</strong></p>

<p>Due to the evenness of the integrand,
$$
\begin{align}
&amp;\int_0^\infty\frac1{\left(\omega_0^2-\omega^2\right)^2+(\Gamma_0\omega)^2}\,\mathrm{d}\omega\\[6pt]
&amp;=\frac12\int_{-\infty}^\infty\frac1{\left(\omega_0^2-\omega^2\right)^2+(\Gamma_0\omega)^2}\,\mathrm{d}\omega\\
&amp;=\frac1{4\omega_0^2}\int_{-\infty}^\infty\left[{\small\frac{1+\frac\omega{\sqrt{4\omega_0^2-\Gamma_0^2}}}{\left(\omega+\sqrt{\omega_0^2-\tfrac14\Gamma_0^2}\right)^2+\tfrac14\Gamma_0^2}
+\frac{1-\frac\omega{\sqrt{4\omega_0^2-\Gamma_0^2}}}{\left(\omega-\sqrt{\omega_0^2-\tfrac14\Gamma_0^2}\right)^2+\tfrac14\Gamma_0^2}}\right]\,\mathrm{d}\omega\\
&amp;=\frac1{4\omega_0^2}\int_{-\infty}^\infty\left[{\small\frac{\frac12+\frac\omega{\sqrt{4\omega_0^2-\Gamma_0^2}}}{\omega^2+\tfrac14\Gamma_0^2}
+\frac{\frac12-\frac\omega{\sqrt{4\omega_0^2-\Gamma_0^2}}}{\omega^2+\tfrac14\Gamma_0^2}}\right]\,\mathrm{d}\omega\\[6pt]
&amp;=\frac1{4\omega_0^2}\int_{-\infty}^\infty\frac{\mathrm{d}\omega}{\omega^2+\tfrac14\Gamma_0^2}\\[12pt]
&amp;=\frac\pi{2\omega_0^2\Gamma_0}
\end{align}
$$</p>
"
"2381261","2381282","<p>The residue is given by
$$ \frac{1}{2\pi i}\int_{|z|=1} e^{z-1/z} \, dz, $$
which is probably easier to evaluate than a series. Putting $z=e^{i\theta}$ gives
$$ \frac{1}{2\pi}\int_{-\pi}^{\pi} \exp{(e^{i\theta}-e^{-i\theta})} e^{i\theta} \, d\theta = \frac{1}{2\pi}\int_{0}^{2\pi} e^{2i\sin{\theta}} e^{i\theta} \, d\theta $$
This is a Fourier integral, of course. What is the Fourier expansion of $e^{2i\sin{\theta}}$? The answer is provided by the <a href=""https://en.wikipedia.org/wiki/Jacobi%E2%80%93Anger_expansion"" rel=""nofollow noreferrer"">JacobiâAnger expansion</a>,
$$ e^{iz\sin{\theta}} = \sum_{n = -\infty}^{\infty} J_n(z) e^{in\theta}, $$
from which we see that the integral pulls out the $n=-1$ coefficient, $J_{-1}(2)=-J_1(2)$, and the answer can't be expressed more simply.</p>
"
"2381267","2381278","<p>You've got both $y$ and $\delta y$ wrong. $y=\cos(60^\circ)=\frac{1}{2}$, and $\delta y\approx\frac{\pi \sqrt{3}}{180}.$ Not sure where your magic factor of $45$ comes from. So you get:</p>

<p>$$\cos(62^\circ)=y+\delta y\approx\frac{1}{2}-\frac{\pi\sqrt{3}}{180}.$$</p>
"
"2381272","2381454","<blockquote>
  <p><strong>Lemma:</strong> in the context above, $fg(y)=gf(y)$ for all $y\in I^2$.</p>
  
  <p><em>Proof:</em> Let $i,j\in I$. Then $(fg-gf)(ij)=fg(ij)-gf(ij)=f(g(i)j)-g(if(j))=f(j)g(i)-f(j)g(i)=0$. Since this holds for all $i,j$, it holds for all elements of $I^2$. </p>
</blockquote>

<p>Notice that the above holds for any commutative ring $R$. </p>

<p><em>Proof of your problem</em>:</p>

<blockquote>
  <p>Suppose $(fg-gf)(x)=z\neq 0$ for some $x\in I$. Then $(fg-gf)(x^2)=(fg-gf)(x)x=zx\neq 0$ also. But by the lemma we know this cannot be the case since $x^2\in I^2$.</p>
</blockquote>

<p>Now actually I remember the version of this question I solved long ago:</p>

<blockquote>
  <p><strong>Proposition:</strong> Let $I$ be an ideal of a commutative ring $R$ such that $I$ contains a regular element $r$ (that is, $r$ isn't a zero divisor.)  Then $End(I_R)$ is commutative.</p>
</blockquote>

<p>The proof is the same, except that when you arrive at ""Suppose $(fg-gf)(x)=z\neq 0$ for some $x\in I$"", you take your special regular element $r\in I$ and say that ""$(fg-gf)(xr)=(fg-gf)(x)r=zr\neq 0$, a contradiction.""</p>
"
"2381280","2381293","<p>Let me at least address the specific question you had about convergence:</p>

<p>If $f\in L^2(\omega)$, when is $f\in L^2(\nu)$? (I have replaced $\omega_n$ by any weight function $\nu$, the index does not play a role in the following.)</p>

<p>We find</p>

<p>$$\|f\|_\nu^2 = \int f^2(x)\nu(x) dx = \int f^2(x)\omega(x)\frac{\nu(x)}{\omega(x)}dx \le \|f\|_\omega^2\cdot \left\|\frac{\nu}{\omega}\right\|_{L^\infty},$$
where we obtain the estimate by using HÃ¶lder's inequality with $p=1$ and $q = \infty$. This gives you some information on how $\omega$ and $\nu$ have to behave relatively to each other.</p>

<p>The above calculation already motivates which aspects of a weight function $\omega$ might be relevant in the definition the corresponding weighted $L^2$-space:</p>

<ul>
<li>Poles/unboundedness of $\omega$</li>
<li>Zeros of $\omega$</li>
<li>Asymptotic behavior of $\omega(x)$ as ""$x\to \partial D$"", i.e. when approaching the boundary of your domain. </li>
</ul>

<p>When comparing weighted $L^2$-spaces, these are the aspects you should pay attention to.</p>
"
"2381286","2381630","<p>To keep it understandable albeit inelegant I'll do the passage in two steps</p>

<p>First the translation</p>

<p>$x^2 - xy + y^2 - 3y -1 = 0$</p>

<p>we look for a new centre $(h;\;k)$ so we substitute $x=x'+h;\;y=y'+k$</p>

<p>$-(h+x) (k+y)+(h+x)^2+(k+y)^2-3 (k+y)-1=0$</p>

<p>$x'^2-x' y'+y^2+x' (2 h-k)+y' (-h+2 k-3) +h^2-h k+k^2-3 k-1=0$ </p>

<p>To have no first degree terms we put $2h-k=0;\;-h+2 k-3=0$ which gives</p>

<p>$h=1;\;k=2$ and we plug these values in the previous equation</p>

<p>$x'^2 - x' y' + y'^2=4$</p>

<p>Now we want to get rid of the $x'y'$ term. To do so we have to rotate the axis using these equations</p>

<p>$\begin{aligned}x'&amp;=X\cos \theta -Y\sin \theta \\y'&amp;=X\sin \theta +Y\cos \theta \end{aligned}$</p>

<p>$(X \sin \theta+Y \cos \theta)^2-(X \cos \theta-Y \sin \theta) (X \sin \theta+Y \cos \theta)+(X \cos \theta-Y \sin \theta)^2=4$</p>

<p>collecting terms</p>

<p>$X^2 \left(\sin ^2\theta+\cos ^2\theta-\sin \theta \cos \theta\right)+X Y \left(\sin ^2\theta-\cos ^2\theta\right)+Y^2 \left(\sin ^2\theta+\cos ^2\theta+\sin \theta \cos \theta\right)=4$</p>

<p>as we want the term $XY$ off we set $\sin ^2\theta-\cos ^2\theta=0$</p>

<p>$\tan^2\theta=1\to \theta=\pm \dfrac{\pi}{4}$</p>

<p>if we want the major axis of the ellipse to be horizontal we choose $\theta=\dfrac{\pi}{4}$ and substitute this value in the last equation</p>

<p>$\dfrac{X^2}{8}+\dfrac{3 Y^2}{8}=1$</p>

<p>the equations of the roto-translation altogether are </p>

<p>$\begin{aligned}x&amp;=X\cos \frac{\pi}{4}-Y\sin \frac{\pi}{4} +1\\y&amp;=X\sin \frac{\pi}{4}+Y\cos \frac{\pi}{4} +2\end{aligned}$</p>

<p>or</p>

<p>$\begin{aligned}x&amp;=\frac{\sqrt 2}{2}(X-Y) +1\\y&amp;=\frac{\sqrt 2}{2}(X+Y) +2\end{aligned}$</p>

<p>hope this helps</p>
"
"2381287","2381334","<p>How many points are up for grabs? The total number of points awarded per match is constant, and the number of matches for a round robin of $n$ teams is well-known.</p>

<blockquote class=""spoiler"">
  <p> $\left(\array{9\\2}\right)=36$ matches, with $2$ points per match, so $72$ in total.</p>
</blockquote>

<p>You want to find the threshold to guarantee being in the top $k$ teams, $k=4$ in this case. Let's say that there are $k+1=5$ good teams, and the rest are bad; this creates the most competitive race for the top 4. The bad teams play some matches between each other, so their points total is non-zero. Work out how many points they must have between them at minimum (i.e. assuming they each lose to each of the top 5), which gives you the total number of points the top 5 can have at maximum.</p>

<blockquote class=""spoiler"">
  <p> The bottom 4 teams play $\left(\array{4\\2}\right)=6$ matches between each other so must have at least $12$ points between them, leaving no more than $60$ points between the top 5.</p>
</blockquote>

<p>If you have $x$ points, and the top 5 have $y$ points in total, how do you ensure you're not last out of those 5? Hint: if you're above average, you're not the worst.</p>

<blockquote class=""spoiler"">
  <p> If you have more than $y/5$ points, then the others cannot all have more than or equal to you, because then the total would be more than $5*y/5=y$, but the total <strong>is</strong> $y$. So we need $x &gt; y/5$, and $y\le60$ so we need $x&gt;12$. As $x$ is an integer, you thus need $13$ points to <em>guarantee</em> a top 4 finish. $12$ points isn't sufficient, because you could have each of the top 5 scoring $12$, and you could lose the tiebreak.</p>
</blockquote>
"
"2381301","2381302","<p>You just need to show that $x^2+x+1$ does not have root in $\mathbb{Z}/2$ so it is irreducible since it is a polynomial of degree 2. </p>
"
"2381308","2381322","<p>It is a requirement and is  equivalent to the  analog of the leibniz rule. </p>

<p>For instance, consider
\begin{equation}
d g(v,u) = \nabla g(u, v)+ g(\nabla u, v)+ g(u, \nabla v) \hspace{1cm} 
\end{equation}
$g$  is a $(0,2)$-tensor   and $u,v$ are two vector fields.  In index notation this is gien by </p>

<p>\begin{equation}
{(g_{ij} v^i u^j)}_ {,\   k}= g_{ij, k}
v^i u^j +   g_{ij} v^i_{\   ,k} u^j + 
 g_{ij,k}  v^i u^j_{\   ,k}
\end{equation}</p>

<p>The left hand side here is saying first contract and then differentiate, the right hand side is you first differentiate using the Leibniz rule for the product  and then contract.</p>
"
"2381316","2381331","<p>From the graph, </p>

<p>$$\lim_{x\to 2, x &lt;2}g (x)=+\infty $$</p>

<p>and</p>

<p>$$\lim_{x\to 2, x&gt;2}g (x)=+\infty $$</p>

<p>thus
$$\lim_{x\to 2, x\ne 2}g (x )=+\infty $$</p>
"
"2381317","2381360","<p>The closedness of $C$ is used for</p>

<blockquote>
  <p>Then $A \setminus C = (A \setminus C_k) \cup R_k \cup R_{k+1} \cup \dotsc$.</p>
</blockquote>

<p>Since each $R_k$ consists only of points having a positive distance from $C$, the union also contains only points with positive distance from $C$. Points in $\overline{C} \setminus C$ have distance $0$ from $C$, and hence aren't covered by the union. Thus the argument only works for sets $A$ with $A \cap (\overline{C}\setminus C) = \varnothing$. Having $C$ closed ensures that.</p>
"
"2381321","2381354","<p>The ${C^k([a,b];X)}$-norm is usually defined by</p>

<p>$$\|f\|_{C^k([a,b];X)} =  \sum_{j=0}^k\sup_{t\in[a,b]}\left\|\frac{d^j}{dt^j}f(t,.)\right\|_X,$$
where for every $t\in [a,b]$, $f(t,.)$ is an element of $X$. Instead of the sum you may equivalently take the maximum over all $j$.</p>

<p>The idea behind this is that $f\in C^k(a,b;X)$ can be read as follows: the map $t\mapsto f(t,.)$ maps from $[a,b]$ into $X$. Differentiability of this map means that for every $t\in[a,b]$ the limit
$$\lim_{h\to 0}\frac{f(t+h,.) - f(t,.)}{h}$$
exists in $X$! That is, with respect to the norm $\|.\|_X$. In combination with the continuity of the norm, this means that the limit
$$\left\|\lim_{h\to 0}\frac{f(t+h,.) - f(t,.)}{h}\right\|_X = \left\|\frac{d}{dt}f(t,.)\right\|_X$$
exists. Note that this is a much stronger statement than the existence of $\frac d{dt}\|f(t,.)\|_X$!</p>

<p>Accordingly, the $C^1([a,b];X)$-norm has to be 
$$\|f\|_{C^1([a,b];X)} =  \sup_{t\in[a,b]}\left\|f(t,.)\right\|_X+ \sup_{t\in[a,b]}\left\|\frac{d}{dt}f(t,.)\right\|_X,$$
and with a natural continuation for higher derivatives. </p>

<p>A simple counter-example helps to illustrate why the derivatives need to be <em>inside</em> the norm: Consider $f(t,x)$ such that $\|f(t,.)\|_X$ is constant in $t$. Then $\|f(t,.)\|_X' = 0$, but $t\mapsto f(t,.)$ needs not be constant, and so $t\mapsto f(t,.)'$ will not vanish. Hence, $\|f(t,.)'\|_X$, and not $\|f(t,.)\|_X'$ captures the behavior of the derivative.</p>
"
"2381325","2381336","<p>Actually, $$0.\overline{9} = 0.\overline{3}\cdot3 = \frac{3}{3} = 1$$ </p>

<p>For a more rigorous way, note that $0.\overline{9} = \frac{9}{10} + \frac{9}{100} + \ldots$. By the formula for the sum of an infinite series, $0.\overline{9} = 1$. </p>

<p>Essentially, we need to prove that $\lim_{x\to\infty} \left(\dfrac{1}{10^n}\right) = 0$. By the squeeze theorem, this is clearly $0$. </p>

<p>For another way, let $S = 0.\overline{9}$. Then $10S = 9.\overline{9}$. This means that $9S = 9 \implies S = 1$</p>

<p>Therefore, we have established (in three different ways) that $0.\overline9=1$.</p>

<p>To convince you that there is no infinitesimal, try to construct a number between $0.\overline{9}$ and $1$.</p>
"
"2381332","2381346","<p>You can first check if $b-a \geq 360$; if so, then you're guaranteed to hit the interval. Failing that, note that you're looking to show that $360k \in [a-C,b-C]$. You can divide through by $360$, resulting in $k \in \left[\frac{a-C}{360},\frac{b-C}{360}\right]$. Letting $frac(x)$ represent the fractional part of $x$, i.e. $x - \lfloor x \rfloor$, if $frac\left(\frac{a-C}{360}\right) &gt; frac\left(\frac{b-C}{360}\right)$, then you will hit a multiple of 360 in this interval; otherwise, not.</p>

<p>In both cases, finding the value $k$ can be done by taking $\left\lceil\frac{a - C}{360}\right\rceil$ (If a value exists in the interval, we know the smallest value valid value has to work).</p>
"
"2381335","2381343","<p>If $x=0$, it converges. </p>

<p>if $x\ne 0$, we use the equivalence</p>

<p>$$\ln (1+a)\sim a \;\; (a\to 0) $$</p>

<p>which gives</p>

<p>$$\ln (1+\frac{x}{n^3})\sim \frac {x}{n^3} \;\;(n\to +\infty ) $$</p>

<p>and since $\sum \frac {1}{n^3} $ converges, we conclude that
$\sum \frac {x}{n^3} $ and  $\sum \ln (1+\frac {x}{n^3}) $ converge.</p>
"
"2381345","2382077","<p>Before trying to answer to the question, I would like to clarify a possible mess in the wording of the problem :</p>

<p>$$Ty''-(P_{0}+\gamma x) [1+y'^{2}]^{\frac{3}{2}} = 0; \quad
y(0) = 0;\quad y'(0)\rightarrow\infty \quad ;\quad 0\leq x \leq h \tag 1$$</p>

<p>The ODE is separable :</p>

<p>$$T\frac{y''}{[1+y'^{2}]^{\frac{3}{2}} }=P_0+\gamma x \tag 2$$</p>

<p>Integration leads to: 
$\quad T\int \frac{d(y')}{[1+y'^{2}]^{\frac{3}{2}} }=\int (P_0+\gamma x)dx \tag 3$
$$T\frac{y'}{[1+y'^{2}]^{\frac{1}{2}} }=P_0x+\frac{\gamma}{2}x^2+c \tag 4$$</p>

<p>$y'(0)\rightarrow\infty \quad\implies\quad \frac{y'}{[1+y'^{2}]^{\frac{1}{2}} }\rightarrow 1 \quad\implies\quad T=c \tag 5$
$$T\frac{y'}{[1+y'^{2}]^{\frac{1}{2}} }=P_0x+\frac{\gamma}{2}x^2+T \tag 6$$
$\text{All terms are positive.} \quad \begin{cases} 
T\frac{y'}{[1+y'^{2}]^{\frac{1}{2}} } &lt;T \\
P_0x+\frac{\gamma}{2}x^2+T &gt;T
\end{cases} \tag 7$</p>

<p>Thus the equality $(6)$ is impossible. Is there a typo in $(1)$ ?</p>

<p>IN ADDITION :</p>

<p>So, the problem of equation $(1)$ and given conditions has no real solution.</p>

<p>Nevertheless, just for curiosity, one can continue for complex solution.</p>

<p>$$T^2\frac{y'^2}{1+y'^2}=\left(P_0x+\frac{\gamma}{2}x^2+T\right)^2 \quad\to\quad  y'^2=\frac{\left(P_0x+\frac{\gamma}{2}x^2+T\right)^2}{T^2-\left(P_0x+\frac{\gamma}{2}x^2+T\right)^2} $$</p>

<p>$$y'=\pm\, i\frac{P_0x+\frac{\gamma}{2}x^2+T}{\sqrt{\left(P_0x+\frac{\gamma}{2}x^2+T\right)^2-T^2} }\qquad\qquad \left(P_0x+\frac{\gamma}{2}x^2+T\right)^2-T^2 \geq 0 $$</p>

<p>$$\boxed{ y(x)=\pm\, i\int_0^x \frac{P_0\xi+\frac{\gamma}{2}\xi^2+T}{\sqrt{\left(P_0\xi+\frac{\gamma}{2}\xi^2+T\right)^2-T^2}}d\xi }\qquad\text{so that}\quad y(0)=0$$
The integral could be expressed on closed form in terms of elliptic integral of second kind (Huge formula). In practice, numerical calculus of the integral is probably the simplest method. </p>

<p>The integral is convergent for $x\to 0$ because $\frac{P_0\xi+\frac{\gamma}{2}\xi^2+T}{\sqrt{\left(P_0\xi+\frac{\gamma}{2}\xi^2+T\right)^2-T^2}} \simeq \frac{T}{\sqrt{2TP_0\xi}}$
$$y(x)\simeq \pm\, i\sqrt{\frac{2T}{P_0}}\sqrt{x}\qquad x\text{ small}$$</p>
"
"2381348","2384039","<p>You are correct, and the lecture notes are wrong. For example, </p>

<p>$$\int_{-1}^1 \frac{1}{\sqrt{1-x^2}} \cos^2(2\arccos(x))\ dx = \frac{\pi}{2}$$</p>

<p>(<a href=""http://www.wolframalpha.com/input/?i=integrate%20(cos(2%20arccos(x)))(cos(2%20arccos(x)))%2Fsqrt(1-x%5E2)%20from%20-1%20to%201"" rel=""nofollow noreferrer"">see Wolfram|Alpha</a>), while</p>

<p>$$\int_{-\pi}^{\pi} \cos^2(2\varphi)\ d\varphi = \pi$$</p>

<p>(<a href=""http://www.wolframalpha.com/input/?i=integrate%20from%20-pi%20to%20pi%20cos(4x)cos(4x)%20dx"" rel=""nofollow noreferrer"">again, see W|A</a>). Note that $\cos(x)$ is an even function, so $\cos(nx)$ is even for any $n$, so the integral from $0$ to $\pi$ is exactly $\frac{1}{2}$ of the integral from $-\pi$ to $\pi$.</p>

<p>Also note that, if $m\neq n$, the integral (from $0$ to $\pi$)</p>

<p>$$\frac{m\sin(\pi m)\cos(\pi n)-n\sin(\pi n)\cos(\pi m)}{m^2-n^2} = 0$$</p>

<p>as $\sin(\pi m) = \sin(\pi n) = 0$. (This evaluation can be proven by using the product-to-sum formulas to reduce the integral to a couple integrals of cosines). Only when $m=n$ will this not be valid (i.e. it will result in $\frac{0}{0}$) and the integral will assume a nonzero value. </p>
"
"2381350","2381358","<p>Note that the normal cosine function has a period of $2\pi$. To find the period of this function, we want to find the smallest positive $n$ such that $\frac{n}{8} - \pi$ is a multiple of $2\pi$ off of $\frac{0}{8} - \pi$. Note that setting $\frac{n}{8} - \pi - (0 - \pi) = 2 \pi k$ for arbitrary integer $k$ gives us $\frac{n}{8} = 2\pi k$ so $n = 16\pi k$. Then the minimum positive value at $k=1$ gives us a period of $16 \pi$.</p>

<p>In general, for sinusoids of the form:
$$f(x) = a\cos (bx - h) + k$$</p>

<ul>
<li>$a$ represents amplitude</li>
<li>$b$ represents frequency (so the period is given by $\frac{2\pi}{b}$)</li>
<li>$h$ represents horizontal shift (to the right)</li>
<li>$k$ represents vertical shift (upwards)</li>
</ul>
"
"2381351","2381369","<p>Once you compute the probability, you should be able to compute the entropy.</p>

<p>$$P(Y=y_i) = \sum_{x=0}^2P(Y=y_i|X=x)P(X=x)$$</p>
"
"2381353","2381413","<p>The answer to your question as stated is ""no"".  For instance if $X$ is Poisson of mean $1$, $k = 1$, $p_1 = 1$ and $p_2 = 1/4$, then $P(X &gt; 1) \approx .26$, implying that $$\frac{p_1}{p_2}P(X &gt; 1) &gt; 1.$$</p>
"
"2381363","2383153","<p>To get this notational question wrapped up, here is a summarizing answer.</p>

<p><strong>Generalities.</strong></p>

<p>Needless to say, marking out diagrams as commutative is <em>unusual</em>,
and---if done badly--<em>spoils them</em>.</p>

<p>The beauty and usefulness of commutative diagrams lies in that they contains hardly anything unnecessary.</p>

<p>There can be reasons for a symbolic notation though, in particular if one in a proof works with multiple diagrams, as mathematical objects in their own right, and finds accompanying words lying about somewhere in their periphery inadequately precise.</p>

<p>This is why I, personally, opted not to use any symbol within the diagram, neither the $\circlearrowleft$ kindly suggested by Osama Ghani, nor the $'''$ suggested by Arnaud D., nor the rotated # kindly suggested by Hans Lundmark, in the comments to the OP, nor even the identity 2-cell kindly suggested by Derek Elkins in one of the answers (in the example below, one would need more than one higher cell, which brings about new problems, while the intent is just to mark the diagram in its entirety as commutative). </p>

<p>In my work, I use <em>rectangular light-gray frames around the diagram</em>, annotated with the word ""commutes"", always at the bottom of the rectangle (see below). </p>

<p>For certain purposes, it still seems to me that 
a standardized notation for a diagram being commutative
is useful.</p>

<p>Please note that the below is unusual; I'm just adding it here to suggest
to others one of the infinitely-many solutions one can find to this notational problem.</p>

<p>Your mileage may vary.</p>

<p><strong>Examples.</strong></p>

<p>Model-theoretically speaking, while</p>

<p><a href=""https://i.stack.imgur.com/Dnmoq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dnmoq.png"" alt=""enter image description here""></a></p>

<p>represents a quiver,</p>

<p><a href=""https://i.stack.imgur.com/nU952.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nU952.png"" alt=""enter image description here""></a></p>

<p>represents a <em>set of axioms</em>($=$ set of pairs of words over a signature of category theory).</p>

<p>In this convention, the frame around the diagram changes its type.</p>

<p><strong>Caveat emptor</strong>, this notation is not attested in the literature, for all I know, there may be someting subtly wrong with it eluding me.</p>
"
"2381365","2381423","<blockquote>
  <p>We obtain
  \begin{align*}
\color{blue}{[x^{10}]}&amp;\color{blue}{\left(\frac{1}{x^2}+\frac{1}{x}+1+x+\cdots+x^{10}\right)^{11}}\\
&amp;=[x^{10}]x^{-22}\left(1+x+x^2+\cdots+x^{12}\right)^{11}\\
&amp;=[x^{32}]\left(\frac{1-x^{13}}{1-x}\right)^{11}\\
&amp;=[x^{32}]\left(1-\binom{11}{1}x^{13}+\binom{11}{2}x^{26}\right)\sum_{j=0}^\infty \binom{-11}{j}(-x)^j\tag{1}\\
&amp;=\left([x^{32}]-11[x^{19}]+50[x^{6}]\right)\sum_{j=0}^\infty \binom{10+j}{j}x^j\tag{2}\\
&amp;=\binom{42}{10}-11\binom{29}{10}+55\binom{16}{6}\tag{3}\\
&amp;\color{blue}{=1\,251\,553\,303}
\end{align*}</p>
</blockquote>

<p><em>Comment:</em></p>

<ul>
<li><p>In (1) we expand the first three terms of the numerator only, since all other terms do not contribute to $[x^{32}]$ and we apply the <em><a href=""https://en.wikipedia.org/wiki/Binomial_series"" rel=""noreferrer"">binomial series expansion</a></em>.</p></li>
<li><p>In (2) we use the binomial identity $\binom{-p}{q}=\binom{p+q-1}{q}(-1)^q$ and the linearity of the <em>coefficient of</em> operator $[x^k]$.</p></li>
<li><p>In (3) we select the coefficients of $x^k$ accordingly and apply the binomial identity $\binom{p}{q}=\binom{p}{p-q}$.</p></li>
</ul>
"
"2381379","2381389","<p>It is equivalent to assume that $Q$ and $R$ are false and then prove the new theorem </p>

<blockquote>
  <p>$$\text{[$A$ and (not $Q$) and (not $R$)] $\rightarrow$ $P$.}$$</p>
</blockquote>

<p>You can see this by explicitly writing out the truth table for these quantities. You can also negate any two of the three $P,Q,R$ and add it to your premise to deduce the other. The intuitive reason this works is that any ""or"" statement is true so long as at least one of the components is true, so it doesn't hurt to assume that all but one is false.</p>

<p>To make the writing of the truth table easier, you can rewrite [$P$ or $Q$ or $R$] as [$P$ or ($Q$ or $R$)] and then adding [not ($Q$ or $R$)] to your assumptions. By de Morgan's law, [not ($Q$ or $R$)] $\leftrightarrow$ [(not $Q$) and (not $R$)].</p>
"
"2381382","2381391","<p>No. Think about
$$
f(x)=e^{ix}.
$$
Note that
$$
f(2\pi)-f(0)=0
$$
but $|f'(x)|=1$ for all $x\in{\bf R}$.</p>
"
"2381386","2381398","<p>Not correct.</p>

<p>a) should be $5^{10}$: person 1 has 5 options, person 2 has 5 options, ...</p>

<p>b) is also not correct.  Try calculating the number of ways in which <em>some</em> call center does <em>not</em> get anyone.</p>
"
"2381388","2381533","<p>A space is first-countable if for each point $x$ there is a <em>single</em> sequence of neighborhoods such that <em>every</em> neighborhood of $x$ contains in some neighborhood in the sequence. Although for each specific neighborhood $V$ we can easily find a sequence with some element contained in $V$, we can't necessarily find a single sequence which works for every such neighborhood. (This is similar to the reason why the reals are uncountable, even though any <em>specific</em> real can be put on a list.)</p>

<p>This just shows why first-countability is not obviously trivial. It turns out that it is <em>really</em> nontrivial: there are lots of non-first-countable sequences. For example, consider the <a href=""https://en.wikipedia.org/wiki/Order_topology#Ordinals_as_topological_spaces"" rel=""nofollow noreferrer"">order topology on $\omega_1+1$</a>. Every open neighborhood of $\omega_1$ here contains a neighborhood of the form $(\alpha, \omega_1]$ for $\alpha$ a countable ordinal. But the supremum of countably many countable ordinals is countable, so for any sequence $U_i$ ($i\in\mathbb{N}$) of neighborhoods of $\omega_1$ contains a neighborhood of the form $(\beta, \omega_1]$ for some countable ordinal $\beta$. But then the neighborhood $(\beta+1, \omega_1]$ doesn't contain any of the neighborhoods $U_i$.</p>

<hr>

<p>EDIT: here's a much better example, not requiring ordinals at all: the <a href=""https://en.wikipedia.org/wiki/Cocountable_topology"" rel=""nofollow noreferrer""><em>cocountable topology</em></a> on any uncountable set $S$. For any sequence of neighborhoods $(U_i)_{i\in\mathbb{N}}$ of a point $x$, the intersection $\bigcap U_i$ is again cocountable, and hence uncountable (since $S$ is uncountable). Pick $s\in \bigcap U_i$, $s\not=x$; then $(\bigcap U_i)\setminus\{s\}$ is a neighborhood of $x$ not containing any of the $U_i$s.</p>

<hr>

<p>Interestingly, first countability is <strong>nontrivially nontrivial</strong>: showing that a space is not first countable generally requires using a principle of the form ""the union of countably many ""small"" sets is ""small,"""" and such principles are generally not true without some amount of the axiom of choice. For instance, it is consistent with ZF that <em>every infinite set is a countable union of sets of strictly smaller cardinality</em>! In fact, I don't offhand know of an example of a space which is provably non-first-countable <em>in ZF alone</em> (and I've now <a href=""https://math.stackexchange.com/questions/2381554/universal-first-countability-in-zf"">asked a question about this</a>).</p>

<p>(Incidentally, there are some neat examples if choice fails sufficiently badly - e.g. the cofinite topology on any <a href=""https://en.wikipedia.org/wiki/Amorphous_set"" rel=""nofollow noreferrer"">amorphous set</a>, if such a set exists.)</p>
"
"2381396","2381878","<p>Note that by definition, the maps $X,Y \rightrightarrows X \sqcup_f Y$ are induced by the inclusions $X,Y \rightrightarrows X \sqcup Y$. Suppose you have another solution $(Z,u,v)$, and a (continuous) map $\phi:X \sqcup_f Y \to Z$ making the diagram commute. Composing with the quotient map we get a map $\bar\phi : X \sqcup Y \to Z$ or, equivalently, two maps $X,Y \rightrightarrows Z$. Commutativity of the diagram then dictates that $\bar\phi (x) = u(x)$ and $\bar\phi(y) = v(y)$ for all $x \in X, y \in Y$. This means that the only way to define $\phi$ is to let $\phi([x]) = u(x)$ and $\phi([y]) = v(y)$. Moreover, since $u \circ i = v  \circ f$, we have $$\phi([f(a)]) = v(a) = u(a) = \phi([a])$$ for any $a \in A$, so $\phi$ is a well-defined continuous map.</p>

<p>Note that it doesn't matter whether $A \subset X$ is closed or not. (But in case it is closed, the parallel map $Y \to X \sqcup_f Y$ is also a closed embedding.)</p>
"
"2381404","2381430","<p>If you write the sum as 
$$
\sum_{k=0}^n\frac{1}{n}\frac{1}{1+(\tfrac{k}{n})^2}=\left(\sum_{k=0}^{n-1}\frac{1}{n}\frac{1}{1+(\tfrac{k}{n})^2}\right)+\frac{1}{2n}
$$
then by additivity of limits, you will get 
\begin{align*}
\lim_{n\to\infty}\sum_{k=0}^n\frac{1}{n}\frac{1}{1+(\tfrac{k}{n})^2}&amp;=\lim_{n\to\infty}\left(\sum_{k=0}^{n-1}\frac{1}{n}\frac{1}{1+(\tfrac{k}{n})^2}\right)+\lim_{n\to\infty}\frac{1}{2n}\\&amp;=\lim_{n\to\infty}\sum_{k=0}^{n-1}\frac{1}{n}\frac{1}{1+(\tfrac{k}{n})^2}
\end{align*}
so yes, it turns out that this is essentially just the Riemann sum as you said.</p>
"
"2381405","2381428","<p>1) What is the recurrence relation? i.e. if $x_{n}$ is owed at the end of the $n$th year, with $x_{0}=150000$, what is $x_{n+1}$ in terms of $x_{n}$? You should be able to work this out.</p>

<blockquote class=""spoiler"">
  <p> It's $x_{n+1}=1.06x_{n}-10000$.</p>
</blockquote>

<p>2) Can you use this to come up with a closed form expression for $x_{n}$?</p>

<p>Hint 1:</p>

<blockquote class=""spoiler"">
  <p> Try $x_{n}=a\cdot1.06^{n}+c$. Substitute into the recurrence relation and then substitute in $x_{0}$ to get the two constants.</p>
</blockquote>

<p>Answer:</p>

<blockquote class=""spoiler"">
  <p> $x_{n+1} = 1.06x_{n}-10000 = 1.06\cdot a\cdot1.06^{n}+1.06c - 10000 = a\cdot1.06^{n+1}+1.06c-10000 = a\cdot1.06^{n+1}+c \Rightarrow 0.06c = 10000 = 10^{6}/6 \\ x_{0}= 150000 = a + 10^{6}/6 \Rightarrow a = (900000-10^{6})/6 = -10^{5}/6 \\ \therefore x_{n} = \frac{10^{5}}{6}\left(10 - 1.06^{n}\right)$<br>You can find the number of years to pay the mortgage by finding the smallest integer $n$ for which $x_{n}\le 0$, which is $\lceil\ln{10}/\ln{1.06}\rceil = 40$.</p>
</blockquote>
"
"2381406","2381411","<p>To answer the question in the title, the two rings are different because $y$ has an inverse in $k(y)[x]$ but not in $k[x,y]$.</p>
"
"2381409","2381443","<p>The maps $\Phi$ and $\Psi$ are typically neither injective nor surjective.   For a simple example, let $X=S^{n+1}\sqcup S^n\sqcup S^n$, let $A$ be the union of the northern hemisphere of $S^{n+1}$, the first copy of $S^n$, and the northern hemisphere of the second copy of $S^n$, and let $B$ be the the union of the southern hemisphere of $S^{n+1}$, the first copy of $S^n$, and the southern hemisphere of the second copy of $S^n$.</p>

<p>Then $A\cap B=S^n\sqcup S^n\sqcup S^{n-1}$, but $\Phi$ kills the generator of $H^n(A\cap B)$ coming from the first $S^n$ since it is nullhomotopic in both $A$ and $B$.  Also, $\Phi$ is not surjective since $H^n(A)\oplus H^n(B)$ has two copies of the second $S^n$.</p>

<p>The two copies of the second $S^n$ in $H^n(A)\oplus H^n(B)$ also make $\Psi$ not injective.  But $\Psi$ is not surjective either, since it does not hit the generator of $H^n(X)=H^n(S^{n+1}\sqcup S^n\sqcup S^n)$ coming from the second $S^n$.</p>

<p>On the chain level, the map $\psi:C_n(A)\oplus C_n(B)\to C_n(X)$ is typically not surjective (assuming you're talking about singular chains): there can be singular simplices in $X$ that are not contained in either $A$ or $B$.  However, it is ""surjective up to homotopy"", in that the inclusion of the image of $\psi$ into $C_n(X)$ is a chain homotopy equivalence.</p>
"
"2381440","2381464","<p>The $T$ used in the construction of an alternating tensor is arbitrary. It says ""$T$ is any $p$-tensor"". Whereas the equality $T^{\pi} = (-1)^{\pi}T$ is satisfied exactly when $T$ is alternating, i.e., an arbitrary $p$-tensor doesn't have to satisfy the above equality, hence you can't use the equality $T^{\pi} = (-1)^{\pi}T$ to simplify the RHS of the definition of $\text{Alt}(T)$.</p>
"
"2381461","2381666","<p>The right triangle $\triangle CBD$ has fixed-size legs of lengths $a_3$ and $d$ respectively, so its hypotenuse will have fixed length $\sqrt{a_3^2+d^2}$ by Pythagoras' theorem.</p>

<p>Given the fixed point $D$ and the known distances $a_2$ and $\sqrt{a_3^2+d^2}\,$, the angles $\theta_2$ and $\theta_3^{\,'}$ can be determined as shown in the posted solution to the first problem, where $\theta_3^{\,'}$ is the angle between $BD$ and the dashed line marked with an arrow.</p>

<p>The last step is to note that $\theta_3 - \theta_3^{\,'} = \angle CBD=\arctan(d / a_3)\,$, so $\theta_3 = \theta_3^{\,'} + \arctan(d / a_3)\,$.</p>
"
"2381471","2381479","<p>Definitions are not set in stone. Whoever wrote the Wikipedia article must think that the definition is better if we allow repeated subsets, while Harary does not. Just keep this fact in mind while reading any literature where the author talks about <em>intersection graphs</em> that there is a difference of opinion as to whether duplicate subsets should be allowed in the vertex set. Usually, though, the precises definition will not matter that much, or will be clear from the context.</p>
"
"2381478","2381864","<p>First of all, when you said that</p>

<blockquote>
  <p>I first noted that $x+2x^2+3x^3+\ldots=\sum_{n=1}^{\infty}nx^n$, which is just the derivative of the geometric series,</p>
</blockquote>

<p>you effectively <strong>integrated</strong> the given series. You were able to do it quickly by observation, but you integrated nevertheless. So your first step is the same as in the book's solution!</p>

<p>Next, you're right that for the geometric series we already know that its interval of convergence is $(-1,1)$. Making this observation is indeed more efficient than applying the Ratio Test &mdash; which is perfectly fine as well, but certainly is more time consuming than simply alluding to a known fact.</p>

<p>But then you make a mistake when you claimed that</p>

<blockquote>
  <p>Hence, the domain of $f$ is $(â1,1)$.</p>
</blockquote>

<p>Termwise integration or differentiation of power series preserves their <strong>open intervals of convergence</strong>, but the <strong>behavior at the endpoints may change</strong>. So knowing that the original series (the geometric series in this example) converges on $(-1,1)$, you can <strong>NOT</strong> immediately deduce that its termwise integral series converges on $(-1,1)$ as well. Its interval of convergence is still from $-1$ to $1$, but we don't know yet whether the endpoints are included or not. That's why the endpoints have to be examined now. It's only a lucky coincidence that in this example the interval is again $(-1,1)$.</p>
"
"2381501","2381581","<p>The construction here is to take the preorder of monomorphisms into some object and reduce it to a partial order (the elements of which we call the ""subobjects""). The reason we want to do this is because the preorder of monomorphisms can be a proper class, but the partial order will often be a set. Think of how many monomorphisms there are into a given non-empty set $A$, compared to how many subsets there are of $A$.</p>

<p>The reason we want to do this is that when the subobjects of the objects of a finitely complete category $\mathcal{C}$ form a set, we can define a functor $Sub(-):\mathcal{C}^{op}\to \mathbf{Set}$. It is in terms of this functor that we can most easily start talking about the internal logic of a category, and is of key interest in topos theory. For example, $\mathcal{C}$ has a subobject classifier if and only if $Sub(-)$ is representable.</p>
"
"2381507","2381514","<p>We see that the equation $$ax^2+(a+d)x+a+2d=0$$
has integer roots, which says $1+\frac{d}{a}\in\mathbb Z$.</p>

<p>Let $\frac{d}{a}=k$.</p>

<p>Hence, we have $$x^2+(1+k)x+1+2k=0$$
has integer roots, which gives
$$(1+k)^2-4(1+2k)=n^2,$$ where $n$ is non-negative integer number,
which gives
$$k^2-6k-3=n^2$$ or
$$(k-3)^2=n^2+12$$ or
$$(k-3-n)(k-3+n)=12$$ and the rest is smooth:</p>

<p>Since $n$ is non-negative integer, we obtain two cases only.</p>

<ol>
<li><p>$n-k+3=6$ and $n+k-3=-2$, which gives $k=-1$;</p></li>
<li><p>$n-k+3=-2$ and $n+k-3=6$, which gives $k=7$.</p></li>
</ol>
"
"2381513","2381705","<p>The product rule for divergences (see for example <a href=""https://en.wikipedia.org/wiki/Del#Divergence"" rel=""nofollow noreferrer"">here</a>) gives the identity
$$\nabla \cdot (u_t\nabla u) = \nabla u\cdot \nabla u_t + u_t\Delta u$$
i.e.
$$\nabla u\cdot \nabla u_t = - u_t\Delta u + \nabla \cdot (u_t\nabla u). \tag{*}$$</p>

<p>On the other hand, the divergence theorem gives
$$\int_{|x-x_0|\leq t_0 - t} (\nabla \cdot (u_t\nabla u))dx = \int_{|x-x_0|= t_0 - t} (u_t\nabla u \cdot n)dS.$$</p>

<p>Integrating (*) over $|x-x_0|\leq t_0 - t$ and applying this form of the divergence theorem gives
$$\int_{|x-x_0|\leq t_0 - t}(\nabla u\cdot \nabla u_t)dx =
 - \int_{|x-x_0|\leq t_0 - t}(u_t\Delta u)dx +
\int_{|x-x_0|= t_0 - t}(u_t\nabla u \cdot n)dS.$$</p>

<p>Correcting the typos in the expression for $de/dt$,
\begin{align*}
\frac{de}{dt} &amp;= \int_{|x-x_0|\leq t_0 - t}(u_t u_{tt} + \nabla u \cdot\nabla u _t)dx - \frac{1}{2}\int_{|x-x_0| = t_0 - t}(u_t^{2} +|\nabla u |^2) dS\\
 &amp;= \int_{|x-x_0|\leq t_0 - t}(u_t u_{tt} + \nabla u \cdot\nabla u _t)dx + \int_{|x-x_0| = t_0 - t}(-\frac{1}{2}u_t^{2} -\frac{1}{2}|\nabla u |^2) dS\\
&amp;= \int_{|x-x_0|\leq t_0 - t}(u_t u_{tt} - u_t\Delta u)dx + \int_{|x-x_0| = t_0 - t}(u_t\nabla u\cdot n - \frac{1}{2}u_t^{2} - \frac{1}{2}|\nabla u|^2)dS.
\end{align*}</p>
"
"2381515","2381546","<p>With the definition of the fractional derivative in term of convolution or the Fourier transform.</p>

<hr>

<p>The Fourier transform of $\text{sign}(x)$ is $\frac{1}{i\pi}\frac{d}{d\xi}\log |\xi|$.</p>

<p>Thus the inverse FT of $\frac{1}{i\pi}D^{1/2}\log |\xi|$  is $(2i \pi x)^{-1/2}\text{sign}(x)$. </p>

<p>Therefore, as distributions, for $\xi &gt; 0$ $$\frac{1}{i\pi} D^{1/2}\log |\xi| = (2i \pi)^{-1/2}\int_0^\infty x^{-1/2}e^{-2i \pi \xi x}dx+(-2i \pi)^{-1/2})\int_0^\infty x^{-1/2}e^{2i \pi \xi x}dx$$
$$ = 2 \xi^{-1/2} \Gamma(1/2)$$</p>
"
"2381523","2381532","<p>When it talks about an algorithm X, it is talking about a set of instructions ... for example, when I tell you how to do addition of two numbers, I might say 'start with the two digits on the right, add those up, and write down the sum and the possibly carry-over. Then, ....'</p>

<p>Now, notice that these instructions are expressed in English, but I could have used any other language as well. Computers of course have their own language too, usually binary. Anyway, it is <em>some</em> language we ar using, and the computer program is a string of symbols following that language.</p>

<p>So, don't think of X as the actual 'running program', but rather as the <em>description</em> of what needs to be done: a program is just a description of a process, and only when the program is being executed will that process actually take place.</p>

<p>In sum: the algorithm/computer program X is a string of characters that happens to be a set of instructions.</p>

<p>But the 'data set' D is <em>any</em> kind of string of symbols. That of course does not mean that D is a set of instructions, but it <em>could</em> be.  Hence, you can use X as a specific kind of data set; a data set that happens to be a computer program.</p>

<p>And so we can pass X as an argument to the CheckHalt routine, which will treat the first argument X as a computer program, and the second argument (also X) as a string of symbols that the computer program X is supposed to work with (that is, CheckHalt is going to try and determine whether the computer program X, when it is being executed and will work on input symbol string X will halt or not)</p>
"
"2381527","2381563","<p>You're overthinking it. ""Every nonzero finitely generated module $M$"" is cleverly hiding the relevant subclass of simple modules.</p>

<p>$IM=\{0\}$ for every simple module, hence $I\subseteq J(R)$.</p>
"
"2381531","2381614","<p>Lets start with your second question.</p>

<p>Let $\mathfrak{F}(\Bbb R)$ be the collection of smooth functions $\Bbb R\to\Bbb
R$ and let $Q$ be a $n\times n$ matrix over $\mathfrak{F}(\Bbb R)$. This means
that $Q$ is of the form
$$
Q=
\begin{bmatrix}
q_{11} &amp; q_{12} &amp; \dotsb &amp; q_{1n} \\
q_{21} &amp; q_{22} &amp; \dotsb &amp; q_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
q_{n1} &amp; q_{n2} &amp; \dotsb &amp; q_{nn} \\
\end{bmatrix}
$$
where each $q_{ij}$ is a smooth function $q_{ij}:\Bbb R\to\Bbb R$.</p>

<p>Next, let $\{y_1,\dotsc,y_n\}$ be functions $\Bbb R\to\Bbb R$ satisfying
$Y^\prime=QY$ where
$$
Y=
\begin{bmatrix}
  y_1 \\ y_2 \\ \vdots \\ y_n
\end{bmatrix}\tag{1}
$$
Note that the equation $Y^\prime=QY$ gives the system of first-order
differential equations
$$
\begin{array}{rcrcrcrcrcrcrcrcrc}
  y_1^\prime &amp; = &amp; q_{11}\cdot y_1 &amp; + &amp; q_{12} \cdot y_2 &amp; + &amp; \dotsb &amp; + &amp; q_{1n}\cdot y_n \\
  y_2^\prime &amp; = &amp; q_{21}\cdot y_1 &amp; + &amp; q_{22} \cdot y_2 &amp; + &amp; \dotsb &amp; + &amp; q_{2n}\cdot y_n \\
  \vdots     &amp;   &amp; \vdots          &amp;   &amp; \vdots           &amp;   &amp; \ddots &amp;   &amp; \vdots          \\
  y_n^\prime &amp; = &amp; q_{n1}\cdot y_1 &amp; + &amp; q_{n2} \cdot y_2 &amp; + &amp; \dotsb &amp; + &amp; q_{nn}\cdot y_n \\
\end{array}\tag{$\ast$}
$$
So, solving $Y^\prime=QY$ is equivalent to solving the system ($\ast$).</p>

<blockquote>
  <p><strong>Example.</strong> Let $Q=\begin{bmatrix}  x^2 &amp; e^x \\  4   &amp; \sin(x)\end{bmatrix}$. Then $Y^\prime=QY$ is the system of first-order differential equations $$\begin{array}{rcrcrcrcrcrcrcrcrc}  y_1^\prime &amp; = &amp; x^2\cdot y_1 &amp; + &amp; e^x\cdot y_2 \\  y_2^\prime &amp; = &amp; 4\cdot y_1  &amp; + &amp; \sin(x)\cdot y_2\end{array}$$</p>
</blockquote>

<p>Now, note that any given $Y$ of the form (1) is an element of the vector space
$\mathfrak{F}(\Bbb R)^n$. Let $S_Q$ be the collection of all $Y$ satisfying
$Y^\prime=QY$. We wish to show that $S_Q$ is a subspace of $\mathfrak{F}(\Bbb
R)^n$. To do so, we may use the one-step vector subspace test, noting that
$S_Q\neq\varnothing$ since $\mathbf{0}\in S_Q$. To use the one-step vector
subspace test, suppose that $Y,Z\in S_Q$ and that $\lambda\in\Bbb R$. Then
$$
(Y+\lambda\cdot Z)^\prime
= Y^\prime + \lambda\cdot Z^\prime
= QY + \lambda\cdot QZ
= Q(Y+\lambda\cdot Z)
$$
Hence $Y+\lambda\cdot Z\in S_Q$, proving that $S_Q$ is indeed a subspace of
$\mathfrak{F}(\Bbb R)^n$.</p>

<p>So, to summarize, the answers to your questions are:</p>

<ol>
<li><p>$S_Q$ is <em>not</em> ``a subspace of the matrix $Q$'' (this language does not make sense). Rather, $S_Q$ is a subspace of the vector space $\mathfrak{F}(\Bbb R)^n$.</p></li>
<li><p>The equation $Y^\prime=QY$ is a succinct way to write the system of first-order linear differential equations ($\ast$).</p></li>
</ol>
"
"2381535","2381580","<p>Hints: Let $g(t)=t^\lambda f(t).$ It's enough to show $g\equiv 0.$ From our hypothesis there exists $N\in \mathbb N$ such that</p>

<p>$$\int_0^1 g(t)t^{n\lambda}\,dt = 0,\,\, n=N,N+1,\cdots.$$</p>

<p>It follows that the above integral is $0$ for $n\in \{N,2N,3N,\dots\}.$ By Stone-Weierstrass, polynomials in $t^{N\lambda}$ are dense in $C[0,1].$ So there are polynomials $p_k$ such that $p_k(t^{N\lambda})\to g(t)$ uniformly on $[0,1].$ Consider $p_k(t^{N\lambda})-p_k(0).$</p>
"
"2381538","2382099","<p>If you want to do the same for an $n \times n$ matrix $A$, let $P=(p_{i,j})$ be a matrix such that $P^{-1} A P$ is diagonal. Then considering the adjugate matrix $\text{adj}(P) = ((-1)^{i+j}P_{j,i})$, one has for $i\ne j$,
$$\sum_{k,l} (-1)^{i+k} P_{k,i} a_{k,l} p_{l,j} = 0$$
where $P_{i,j}$ is the $(i, j)$ minor of $P$. These $n(n-1)$ relations are homogeneous polynomials of degree $n$ in the coefficients $p_{i,j}$.
I don't know where you want to go from here.</p>
"
"2381543","2382356","<p>Qiaochu Yuan has brought me to the solution of the paradox, I believe. </p>

<p>Consider the category of finite dimensional $G$-graded $k$-vector spaces. It is a $k$-linear, semisimple category with finitely many simple objects, which correspond to the elements of the group. Call them $k_g, g \in G$. Their tensor product is then $k_{g_1} \otimes k_{g_2} = k_{g_1 g_2}$.</p>

<p>To choose an associator for the whole category, it suffices to define it on the simple objects. This means we have isomorphisms $\alpha_{g_1,g_2,g_3}\colon (k_{g_1} \otimes k_{g_2}) \otimes k_{g_3} \to k_{g_1} \otimes (k_{g_2} \otimes k_{g_3})$ satisfying the pentagon axiom. Since $k_{g_1 g_2 g_3}$ is a simple object, these correspond to invertible numbers in $k$ which we will call $\omega(g_1,g_2,g_3)$, and the pentagon axiom turns out to be the multiplicative cocycle condition:</p>

<p>$$ \omega(g_1, g_2, g_3) \omega(g_1, g_2 g_3, g_4) \omega(g_2, g_3, g_4) = \omega(g_1 g_2, g_3, g_4) \omega(g_1, g_2, g_3 g_4) $$</p>

<p>Furthermore, a monoidal equivalence between two copies of the same category with different associators is given by a coboundary between the two cocycles, so indeed any category of $G$-graded vector spaces is given by a class in $H^3(G,k^*)$.</p>

<p>Now let us deform the associator. Replace $k$ by $k[\varepsilon]/\varepsilon^2$. An associator is now given by $\omega(g_1, g_2, g_3) + \varepsilon \omega'(g_1, g_2, g_3)$, where $\omega$ satisfies the cocycle condition as before, and $\omega'$ satisfies the <em>additive</em> cocycle condition:</p>

<p>$$ \omega'(g_1, g_2, g_3) + \omega'(g_1, g_2 g_3, g_4) + \omega'(g_2, g_3, g_4) = \omega'(g_1 g_2, g_3, g_4) + \omega'(g_1, g_2, g_3 g_4) $$</p>

<p>It is, in a sense, the derivative of the multiplicative cocycle condition.</p>

<p>Up to equivalence of categories, we have in particular $[\omega] \in H^3(G, k^*)$ and $[\omega'] \in H^3(G, k)$, in the latter considering the underlying additive group of $k$. $H^3(G, k)$ classifies in which directions we can ""continuously deform"" the associator.</p>

<p>Now for $k$ having characteristic 0 (or in general, as Qiaochu says, if the characteristic of $k$ doesn't divide the order of the group), $H^3(G,k) = 0$, which means that we cannot deform the associator at all. In particular, there are only finitely many different possible associators. This is a special case of a statement called ""Ocneanu rigidity"".</p>
"
"2381545","2381548","<p>You can deformation retract the real line component of this space onto the space $S$. From there, it shouldn't be hard to see that its fundamental group is that of the circle.</p>

<p>The desired deformation retraction is
$$
H(x,t) = \begin{cases}
x, &amp; \text{if $x\in S$}\\
(1-t)x, &amp; \text{if $x\in R$}.
\end{cases}
$$
Since $X$ is the union of $S$ and $R$, both of which are closed subspaces of $X$, the homotopy $H$ is continuous by the pasting lemma.</p>
"
"2381547","2381652","<p>A wee bit of manipulation and we get</p>

<p>${n \choose 3} = 2^{n-2}$ or</p>

<p>$\frac {(n-2)(n-1)n}{6} = 2^{n-2}$</p>

<p>$n(n-1)(n-2) = 2^{n-1}*3$</p>

<p>So $n, n-1, n+2$ must have no other factors than $2$ or $3$.  A and there must be exactly one power of $3$ as a factor.  So the only possible odd numbers among $n, n-1,$ and $n-2$ (of which there must be at least one) allowed would be $3$ or $1$.  Assuming $n\ge 3$ and natural (<em>Is</em> that an assumption?) we must have at least one odd and it must be greater than one. So we can only have one odd and it must equal $3$.</p>

<p>Furthermore the two other terms must be even and they must powers of $2$.   </p>

<p>Hence the only possible odd among $n, n-1, n-2$ is equal to $3$.   $n, n-2$ have the same parity so they must be even and $n-1$ must be odd and $n-1 =3$. </p>

<p>This is good as $n=4$ and $n-2 = 2$  are powers of $2$ (and that is the only two powers of $2$ where $2^k - 2^m = 2$).  $\frac {2*3*4}6 = 4 = 2^2 = 2^{4-2}$ so this is the only acceptable solution.</p>
"
"2381554","2381572","<p>Let $X=\mathbb{N}\times\mathbb{N}\cup\{\infty\}$, topologized such that a nonempty set is open iff it contains $\infty$ and contains cofinitely many points of $\{n\}\times\mathbb{N}$ for each $n\in\mathbb{N}$.  Then $X$ is not first-countable at $\infty$.  Indeed, suppose $(U_n)$ is a local base at $\infty$.  For each $n$, let $j_n$ be the least $j$ such that $(n,j)\in U_n$ (since $U_n$ is nonempty and open, it must contain $(n,j)$ for all but finitely many $j$).  Now let $$U=\{\infty\}\cup\{(n,j)\in\mathbb{N}\times\mathbb{N}:j&gt;j_n\}.$$ Then $U$ is an open neighborhood of $\infty$.  However, $U$ does not contain any $U_n$, since for each $n$, $(n,j_n)\in U_n$ but $(n,j_n)\not\in U$.</p>

<p>This is not just a pathological example--similar examples come up very naturally when you glue together infinitely many spaces at a point, for instance, when considering non-locally finite CW complexes in algebraic topology.  As a very concrete example, a wedge sum of countably infinitely many copies of $[0,1]$ can be proven (in ZF) to not be first-countable at the basepoint by essentially the same argument.</p>
"
"2381564","2381845","<p>Both definitions are commonly used: either you can require that a Dedekind domain have dimension exactly $1$, or you can require that a Dedekind domain have dimension $\leq 1$.  In the first case fields are not Dedekind domains and only PIDs which are not fields are Dedekind domanis, and in the second case all PIDs are Dedekind domains.  The case of a field is pretty trivial, so it doesn't really matter which definition you use, though I would consider the second definition to be more natural.</p>
"
"2381565","2381673","<p>Consider linear $f$ first: $f(s) = As+B$. Then $\|f\|$ is a convex function, so <a href=""https://math.stackexchange.com/q/1408231"">one-sided derivatives exist</a>. </p>

<p>General case: $f$ has right derivative $A$ at $t$, meaning $$f(s) = f(t)+A(s-t)+o(|s-t|),\quad s\to t^+$$ Since the norm is a Lipschitz function, it follows that $$\|f(s)\| = \|f(t)+A(s-t)\| + o(|s-t|),\quad s\to t^+$$
Here  $\|f(t)+A(s-t)\|$ is right-differentiable by the linear case, and the $o$-term is negligible.</p>
"
"2381574","2382083","<p>Answering the first question on finding a non-primitive irreducible polynomial of degree $p$, when factors of $2^p-1$ and a primitive polynomial $p(x)$ are known. The rest of the questions are more difficult, and I have nothing useful to say about them.</p>

<p>Let $\alpha$ be the corresponding primitive element, i.e. a zero of $p(x)$. Let's use the basis $\mathcal{B}=\{1,\alpha,\alpha^2,\ldots,\alpha^{p-1}\}$ for $GF(2^p)$ over $GF(2)$. Let $\beta=\alpha^d$. </p>

<p>The minimal polynomial $m(x)=\sum_{i=0}^pm_ix^i$ of $\beta$ can be found as follows.</p>

<ol>
<li>Calculate the powers $\beta^j,j=0,1,\ldots,p$ in terms of the basis $\mathcal{B}$. This can be done efficiently with square-and-multiply.</li>
<li>Because $m(x)$ is irreducible of degree $p$, we know that $m_0=m_p=1$. Using this and the result of the earlier calculation we can write the sum
$$S=\sum_{i=0}^pm_i\beta^i$$ in terms of $\mathcal{B}$. We treat the coefficients $m_1,m_2,\ldots,m_{p-1}$ as unknown variables. Observe that $S$ is linear in all these variables.</li>
<li>Solve the unknowns $m_1,\ldots,m_{p-1}$ from the system $S=0$.</li>
</ol>

<p>A way to accomplish this with Mathematica is the following (ask Mathematica gurus for tips to possibly make this more efficient). I did the example of
$$
\begin{aligned}
p&amp;=67,\\
p(x)&amp;=1+x+x^2+x^5+x^{67},\\
d&amp;=193707721.
\end{aligned}
$$</p>

<pre><code>In[1]:=powers = Table[PolynomialRemainder[x^(j*193707721), 1 + x + x^2 + x^5 + x^67, x, Modulus -&gt; 2], {j, 0, 67}];
In[2]:=coeffs = Table[m[j], {j, 0, 67}];
In[3]:=prod = coeffs.powers;
In[4]:=terms = CoefficientList[prod, x];
In[5]:=m[0] = 1;
In[6]:=m[67] = 1;
In[7]:=Solve[Table[terms[[i]] == 0, {i, 1, 67}], Table[m[i], {i, 1, 66}], 
 Modulus -&gt; 2];
In[8]:=minimal = coeffs/.%[[1]];
In[9]:=minpol = Sum[minimal[[i]] x^(i - 1), {i, 1, 68}]
</code></pre>

<p>And in the last step Mathematica kindly gave the output
$$m(x)=x^{67}+x^{66}+x^{63}+x^{62}+x^{61}+x^{60}+x^{59}+x^{56}+x^{55}+x^{53}+x^{5
   2}+x^{46}+x^{45}+x^{44}+x^{43}+x^{41}+x^{40}+x^{39}+x^{38}+x^{35}+x^{32
   }+x^{31}+x^{29}+x^{25}+x^{24}+x^{22}+x^{19}+x^{18}+x^{16}+x^{15}+x^{12}
   +x^{11}+x^{10}+x^8+x^6+x^5+x^4+x^2+1$$
as the minimal polynomial of $\beta$.</p>

<p>As a final check I calculated that the remainder of $x^{761838257287}$ modulo $m(x)$ is equal to $1$ as it should, because the multiplicative order of
$\beta$ is $(2^p-1)/d$.</p>

<p>You can similarly find the minimal polynomial of any power of $\beta$. Unfortunately I don't know how to find an exponent $s$ such that the minimal polynomial of $\beta^s$ would have a low number of terms. The question of finding low weight primitive polynomials has received quite a bit of attention, but I'm afraid I haven't followed that work at all.</p>
"
"2381579","2381589","<p>My first instinct in factoring is completing the square. $x^4-7x^2 + 1$ naturally becomes $(x^2-1)^2-5x^2$ or $(x^2+1)^2-9x^2$. Of the two the latter is a difference of squares and is thus more useful. Thus $$x^4-7x^2 + 1 = (x^2+1)^2-9x^2 = (x^2 + 1 + 3x)(x^2+1-3x)$$</p>
"
"2381599","2381608","<p>We have that $$(n_1,n_2,n_3)\begin{pmatrix}T_1 &amp; 0&amp; 0\\ 0&amp; T_2 &amp; 0\\ 0&amp; 0&amp; T_3\end{pmatrix}\begin{pmatrix}c_1\\ c_2\\ c_3\end{pmatrix}=T_1c_1n_1+T_2c_2n_2+T_3c_3n_3.$$ So the maximum value is obtained when</p>

<p>$$(n_1,n_2,n_3)=\frac{1}{\sqrt{T_1^2 c_1^2+T_2^2 c_2^2+T_3^2 c_3^2}}(T_1c_1,T_2c_2,T_3c_3).$$</p>

<p>Why? We have that $$(a_1,a_2,a_3)\cdot(b_1,b_2,b_3)=\sqrt{a_1^2+a_2^2+a_3^2}\sqrt{b_1^2+b_2^2+b_3^2}\cos(\widehat{a,b}).$$ If $b$ is given, when is maximum the product? If we assume $a$ is a unit vector then we achive the maximum when the angle between $a$ and $b$ is zero. That is, when $(a_1,a_2,a_3)=\frac{1}{\sqrt{b_1^2+b_2^2+b_3^2}}(b_1,b_2,b_3).$ </p>
"
"2381602","2381648","<p>In your other post, you said that you were mistaken to tag it with ""order statistics"". But that is what this is. Looking at the original text (which you link to in your comment above), you seem to me to have transcribed the problem incorrectly: it seems to me to read $z_iF^{z_i-1}(x)f(x)$, not $z_iF^{z_i^{-1}}(x)f(x)$. The former is the pdf of the maximum of $z_i$ independent draws from a random variable distributed with cdf $F$.</p>

<p>So, essentially, the text is saying that the amount of ""effort"" $z_i$ exerted by a firm is the number of draws they make from a random variable distributed by $F$. Accordingly, the result of their draws -- the ""innovation"", as that text has it -- is the maximum of their draws. Therefore, the random variable we are interested in is the maximum of $z_i$ draws, or in other words, the $z_i^{th}$ order statistic of $z_i$ independent draws from a $F$-distributed random variable. Let's call this random variable $Y$, so that $Y$ is the maximum of $z_i$ draws from another random variable, $X$, where $X\sim F$.</p>

<p>As they say in the text, the cdf of $Y$ is given by $F^{z_i}(x)$. Of course, the pdf of this random variable is the derivative of its cdf. Since
$$ \frac d{dx}F^{z_i}(x) = z_i F^{z_i-1}(x)\frac d{dx} F(x)=z_i F^{z_i-1}(x)f(x)$$
we have that this is the pdf of $Y$. So to answer your question, $z_iF^{z_i-1}(x)f(x)$ is the pdf of the random variable $Y$, where $Y$ itself the maximum of $z_i$ independent draws from the random variable $X$.</p>
"
"2381604","2381612","<p><em>If $p$ is constrained to be positive ($p&gt;0$ no matter what):</em></p>

<p>Take $(a_n)_n$, $(b_n)_n$ defined by $a_n = n$ and $b_n=\frac{1}{n^2\ln^2 n}$.</p>
"
"2381605","2381616","<p>There is no solution as</p>

<p>$$c^{30} = 9^{10} = 5$$</p>

<p>but $$c^{30} = 1 \ \ \forall c \not =0$$</p>
"
"2381607","2382971","<p>To begin with, you want both disk bundles to be orientable or nonorientable simultaneously (a bundle is orientable iff it is trivial over the 1-skeleton of the base). For orientable bundles with orientable base (""totally orientable""), the $\pm$ Euler number is the complete invariant; otherwise the Euler number is not even well-defined. However, one can define the ""virtual Euler number"", obtained by taking a finite totally orientable cover $p: X'\to X$ and setting $e(X):= e(X')/deg(p)$. I think, this is a complete invariant. Check <a href=""https://homepages.warwick.ac.uk/~masgar/Teach/2012_MA4J2/geometry.pdf"" rel=""nofollow noreferrer"">""The geometries of 3-manifolds""</a>,  Bulletin of the London Mathematical Society (1983), by Peter Scott. </p>
"
"2381613","2382297","<p>To solve a ODE along a curve in the complex plane, you have to first transform it into an ODE on a real interval. Let me demonstrate this procedure using your example. We first parametrize the line from $0$ to
$3 + 2 \mathrm{i}$ by
$$
t = s \cdot (3 + 2 \mathrm{i})
\quad \text{for} \quad
s \in [0, 1]
$$
and introduce the function
$$
w(s) = y(t) = y(s \cdot (3 + 2 \mathrm{i}))
\,.
$$
The chain rule gives the relation that
$$
\frac{dw}{ds} = \frac{dy}{dt} \frac{dt}{ds}
= \frac{dy}{dt} (3 + 2\mathrm{i})
\,.
$$
Hence,
$$
\frac{dy}{dt} = \frac{1}{3 + 2\mathrm{i}} \frac{dw}{ds}
= \frac{3 - 2\mathrm{i}}{13} \frac{dw}{ds}
$$
Replacing $y(t)$ and $\frac{dy}{dt}$ by 
$w(s)$ and $\frac{3 - 2\mathrm{i}}{13} \frac{dw}{ds}$ 
in the ODE, we see that
$$
   \frac{3 - 2\mathrm{i}}{13} \frac{dw}{ds}
   = w
   \,, \quad
   w(0) = 0
   \,.
$$
This equation is now an ODE in a real variable. Hence, you can use Matlab to solve it. Since
$$
s = \frac{t}{3 + 2\mathrm{i}}
\,,
$$
you obtain your solution $y(t)$ by
$$
y(t) = w(s) = w(\frac{t}{3 + 2\mathrm{i}})
$$
for points $t$ on the line.</p>
"
"2381622","2381634","<p>The speed keeps doubling as long as the speed is below 340 m/s, so that would be 512 m/s ...  and 400km is way more than enough length for the first 8 bell rings to catch up with the horse.  Even when the horse runs at 256 m/s, and assuming the distance between the bell and ears is, say, 3 meters, then it will only take a fraction of a second for the sounds to reach the horse's ears, and hence the horse will have traveled less than 256 meters before hearing the sound .. so yeah, 400km is <strong>plenty</strong> to reach 512 m/s</p>
"
"2381629","2381861","<p>If you mean proper isotopies (An isotopy of $D^2$ in $B^4$ which restricts to an isotopy of $S^1$ inside $S^3$), the answer is no. Take for instance the flat disk bounding the unknot (or more generally any doubly-slice disk) and connect sum any non-trivial 2-knot (an embedded $S^2$ in $S^4$ not smoothly isotopic to a longitudinal $S^2$). If this disk was isotopic to the flat disk, then it's easy to construct an isotopy from this 2-knot to the standard longitudinal $S^2$. The first example of such a 2-knot is due to E. Artin, and I am having trouble finding a relatively accessible survey focusing on 2-knots. Maybe Hillman's book is a decent place to start (though it is certainly a large amount of material) <a href=""https://arxiv.org/pdf/math/0212142.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/math/0212142.pdf</a> .</p>

<p>If for some strange reason you mean isotopies which aren't necessarily proper then they are always isotopic, as you can just shrink along a radial vector field until the derivative approximates the embedding (so every embedding is isotopic to a flat disk.</p>
"
"2381631","2381635","<p>Since $x_0\in S$, if $y\in x$ for all $x\in S$, then $y\in x_0$.</p>
"
"2381636","2381665","<p>Since the plane must contain the first line we get that $(-1,5,2)$ is a point of the plane and $(3,3,1)$ gives a direction.</p>

<p>Since the plane is parallel to the intersection of the other two planes we have that $$(1,-2,3)\times (-2,-1,0)=(3,-6,-5)$$ is another direction of the plane. </p>

<p>Thus its equation is </p>

<p>$$\begin{vmatrix}x+1 &amp; y-5 &amp; z-2\\ 3&amp;3&amp;1\\ 3&amp;-6&amp;-5\end{vmatrix}=0.$$</p>
"
"2381658","2384167","<p>What you are looking for is a corollary of [a certain variation of] the $9$-lemma (also known as the $3\times 3$-lemma); the name ""cross lemma"" might not be so widely used in the literature.</p>

<p>You may consider the factorizations of $\psi_2\circ \phi_1$ and $\phi_2\circ\psi_1$ through their respective images, and complete the arrows $\phi_1$, $\psi_1$ to a square by taking a pullback, and $\phi_2$, $\psi_2$ by taking a pushout. Then you obtain a diagram with exact rows and columns</p>

<p>$\require{AMScd}
\begin{CD}
@. 0 @. 0 @. 0 @. \\
@. @VVV @VVV @VVV @.\\
0 @&gt;&gt;&gt; \bullet @&gt;&gt;&gt; A @&gt;&gt;&gt; \bullet @&gt;&gt;&gt; 0 \\
@. @VVV @VV\phi_1 V @VVV @. \\
0 @&gt;&gt;&gt; D @&gt;&gt;\psi_1&gt; B @&gt;&gt;\psi_2&gt; E @&gt;&gt;&gt; 0\\
@. @VVV @VV\phi_2 V @VVV @. \\
0 @&gt;&gt;&gt; \bullet @&gt;&gt;&gt; C @&gt;&gt;&gt; \bullet @&gt;&gt;&gt; 0 \\
@. @VVV @VVV @VVV @. \\
@. 0 @. 0 @. 0 @.
\end{CD}$</p>

<p>I believe there are some modern references for this statement, but it appears precisely in this form as Proposition 16.5 of <a href=""http://www.maths.ed.ac.uk/~aar/papers/mitchell.pdf"" rel=""nofollow noreferrer"">Mitchell's ""Theory of Categories""</a> (1965).</p>
"
"2381664","2381750","<p>In the question as asked, your mistake is the following:</p>

<blockquote>
  <p>if the logic has two non-isomorphic models, then this is rather trivial, or? For then just choose a proposition which is true in one model, but false in the other...</p>
</blockquote>

<p>Just because $\mathcal{A}\not\cong\mathcal{B}$ does <strong>not</strong> mean that there is a sentence true in one and false in the other! There are non-isomorphic structures which <em>are</em> <a href=""https://en.wikipedia.org/wiki/Elementary_equivalence"" rel=""nofollow noreferrer"">elementarily equivalent</a>. One easy way to see this is just by a cardinality argument: in a given language $L$, there are only $2^{\vert L\vert+{\aleph_0}}$-many sets of sentences in the language $L$ but a proper class of structures, so we have to be able to find non-isomorphic structures with the same theory. More satisfyingly, the <a href=""https://en.wikipedia.org/wiki/Compactness_theorem"" rel=""nofollow noreferrer"">compactness theorem</a> (or rather, its corollary the <a href=""https://en.wikipedia.org/wiki/L%C3%B6wenheim%E2%80%93Skolem_theorem#Upward_part"" rel=""nofollow noreferrer"">upwards Lowenheim-Skolem theorem</a>) tells us that for any structure $\mathcal{A}$ there is a nonisomorphic, elementarily equivalent structure $\mathcal{B}$. And in particular, if $T$ is a complete theory with an infinite model, then $T$ has lots of nonisomorphic but elementarily equivalent models (since any two models of $T$ are elementarily equivalent since $T$ is complete). <em>(Exercise: ""has non-elementarily-equivalent models"" is <strong>exactly the same as</strong> ""is incomplete."")</em></p>

<p>So Godel's incompleteness theorem says <strong>far more</strong> than just that there are non-isomorphic models of any ""reasonable"" theory of arithmetic: it says that no reasonable theory of arithmetic even pins down the true natural numbers up to elementary equivalence!</p>

<p>It's worth considering, at this point, some examples of <em>complete</em> theories.</p>

<ul>
<li><p>The theory of dense linear orders without endpoints is computably (indeed, finitely) axiomatizable and is complete; this is a standard exercise in most first-semester logic classes. </p></li>
<li><p>A more interesting example is the following. If we consider the structure $(\mathbb{N}; +)$, it turns out that Godel <em>fails</em>: the theory <a href=""https://en.wikipedia.org/wiki/Presburger_arithmetic"" rel=""nofollow noreferrer"">Presburger arithmetic</a> is complete (it axiomatizes $Th(\mathbb{N}; +)$). It's also decidable, since Presburger arithmetic is computably axiomatized.</p></li>
<li><p>Similarly, it turns out that the theory of real closed fields is complete (and computably axiomatized); this is due to <a href=""https://en.wikipedia.org/wiki/Real_closed_field#Model_theory:_decidability_and_quantifier_elimination"" rel=""nofollow noreferrer"">Tarski</a>.</p></li>
</ul>

<hr>

<p>You also asked about <em>truth</em> (although truth doesn't even show up in your statement of Godel's theorem). The issue here - as in <a href=""https://math.stackexchange.com/questions/2346305/the-entscheidungsproblem-and-the-notion-of-decidability-in-first-order-logic"">your other question</a> - is that you are <em>conflating two notions of truth</em>. The truthy version of Godel's incompleteness theorem says</p>

<blockquote>
  <p>For any ""reasonable"" theory $T$, there is a true sentence $\varphi$ which is not provable in $T$.</p>
</blockquote>

<p>""True"" here means ""true <em>in the structure $(\mathbb{N}; +, \times)$</em>;"" this has nothing to do with truth <em>in all models of $T$</em>. A better statement of the ""truthy"" incompleteness theory is:</p>

<blockquote>
  <p>For any ""reasonable"" theory $T$, there is a sentence $\varphi$ such that $\varphi$ is not provable in $T$ but $(\mathbb{N}; +,\times)\models\varphi$.</p>
</blockquote>

<p>This makes it very clear what's going on, and why Godel's completeness theorem is irrelevant. Again, it boils down to the fact that any reasonable theory of arithmetic $T$ will have lots of non-elementarily-equivalent models; the sentence $\varphi$ above is <em>not true in all of them</em>, hence (by Completeness) <em>not provable in $T$</em>, but <em>is true in the standard model</em>.</p>

<p>(Keep in mind that ""reasonable"" here means, among other things, ""computably axiomatizable""; of course the set of all sentences true in $(\mathbb{N}; +,\times)$ is a complete theory, but Incompleteness doesn't apply to it since it's not computably axiomatizable.)</p>
"
"2381669","2381674","<p>Assume $\phi(x,y)=\phi(u,v).$ Thus we have</p>

<p>$$|x-u|=|f(v)-f(y)|\le k |v-y|$$ and </p>

<p>$$|y-v|=|f(u)-f(x)|\le k|x-u|.$$ Since $k&lt;1$ this is only possible if $x=u$ and $y=v.$ This shows injectivity.</p>
"
"2381683","2381696","<p>$(A)$</p>

<p>Fix $\epsilon&gt;0$. By uniform continuity, there is a $\delta&gt;0$ such that when $x,y \in (b-\delta,b)$ we have $|f(x) - f(y)| &lt; \epsilon$.  </p>

<p>Suppose $\{a_n\}_{n \geq 1}$ is an arbitrary sequence in $(a,b)$. Suppose also that it converges to $b$. </p>

<p>Consider the sequence $\{f(a_n)\}_{n \geq 1}$. If we choose $M$ large enough so that for all $k,m &gt; M$, $a_k, a_m  \in (b-\delta, b)$, we have</p>

<p>$$|f(a_k) - f(a_m)| &lt; \epsilon$$</p>

<p>This implies $\{f(a_n)\}$ is Cauchy. Hence, the sequence converges with limit $L$. This limit $L$ must be independent of the sequence converging. If $\{b_n\}$ is another sequence converging to $b$ but $\{f(b_n)\}$ converges to some $L_1 \neq L$, the sequence $$c_n = 
\begin{cases}
a_n  &amp; n \ \text{odd} \\
b_n &amp; n \ \text{even}
\end{cases}$$</p>

<p>converges to $b$ but $\{f(c_n)\}$ doesn't converge, which yields a contradiction. </p>

<p>Hence, we can see that for every sequence $\{a_n\}$ in $(a,b)$ converging to $b$, $\{f(a_n)\}$ converges to the same number $L$, which means</p>

<p>$$\lim_{x \to b^{-}} f(x) = L$$</p>

<p>$(B)$</p>

<p>No. Consider $f(x) = \sin \left(\frac{1}{1-x}\right)$ on $(0,1)$. </p>
"
"2381684","2391455","<p>It might be interesting to note the condition $f(0)\in \mathbb R$ is not needed. I.e., any holomorphic map $f:\mathbb D\to \Omega$ satisfies $|f'(0)|\le 2.$ That's because the derivative estimate in the Schwarz Lemma can be improved from the one usually given: If $g:\mathbb D \to \mathbb D $ is holomorphic, then $|g'(0)|\le 1-|g(0)|^2.$ For the proof of this, let $a= g(0)$ and apply the usual SL to $\varphi_a\circ g,$ where</p>

<p>$$\varphi_a(z) = \frac{a-z}{1-\bar a z}.$$</p>

<p>Since $\varphi_a\circ g(0)=0,$ we get</p>

<p>$$|\varphi_a(g(0))g'(0)| = |\varphi_a(a))g'(0)| = \frac{|g'(0)|}{1-|a|^2} \le 1.$$</p>

<p>The estimate on $|g'(0)|$ follows. A corollary is that no matter where $g(0)$ is, $|g'(0)|\le 1.$</p>

<p>If we now have $f:\mathbb D\to \Omega,$ we can consider the map $ f(z) - \text { Re }f(0).$ (Subtracting off the real part makes the estimate easier.) Go through the same process that @ts375_zk26 presented, use the $|g'(0)|\le 1$ corollary above, and $|f'(0)|\le 2$ should fall out after some computation.</p>
"
"2381685","2381727","<blockquote>
  <p>First case </p>
</blockquote>

<p>$$\lim_{x\to -\infty}f (x)=L\in \Bbb R $$</p>

<p>the negation is
$$(\exists \epsilon&gt;0) \; \;( \forall A &lt;0)\;\;(\exists x\in \Bbb R) \;\;:$$
$$ x &lt;A \; \land\; |f (x)-L|\ge \epsilon $$</p>

<blockquote>
  <p>Second case</p>
</blockquote>

<p>$$\lim_{x\to -\infty}f (x)=+\infty $$</p>

<p>the negation is
$$(\exists B&gt;0) \;\; (\forall A &lt;0) \;\;(\exists x\in \Bbb R) \;\;:$$
$$x &lt;A \;\;\land f (x)&lt;B $$</p>

<p>The third for you.</p>
"
"2381693","2381707","<p>The answer to the question in your title is yes.</p>

<p>Ãgoston, I.; Demetrovics, J.; HannÃ¡k, L. On the number of clones containing all constants (a problem of R. McKenzie). Lectures in universal algebra (Szeged, 1983), 21â25, Colloq. Math. Soc. JÃ¡nos Bolyai, 43, North-Holland, Amsterdam, 1986.</p>

<p>''Answering a question raised by McKenzie, the authors prove that over any finite set with
at least three elements there are continuously many clones containing all constants. Here
a clone over some set
A
is a set of finitary functions over
A
containing all projections
and closed with respect to composition.''</p>

<p>Images of the article from <a href=""https://books.google.de/books?id=1MfSBQAAQBAJ&amp;pg=PA21&amp;lpg=PA21&amp;dq=%22On+the+number+of+clones+containing+all+constants%22&amp;source=bl&amp;ots=341ZB8ANH_&amp;sig=jebu9zwxKJkFTfE8So-ANnRRmVM&amp;hl=en&amp;sa=X&amp;redir_esc=y#v=onepage&amp;q=%22On%20the%20number%20of%20clones%20containing%20all%20constants%22&amp;f=false"" rel=""nofollow noreferrer"">Google Books</a>:</p>

<blockquote>
  <p><a href=""https://i.stack.imgur.com/5Ibp9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5Ibp9.png"" alt=""enter image description here""></a>
  <a href=""https://i.stack.imgur.com/70nzM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/70nzM.png"" alt=""enter image description here""></a>
  <a href=""https://i.stack.imgur.com/bo6Ae.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bo6Ae.png"" alt=""enter image description here""></a>
  <a href=""https://i.stack.imgur.com/nYVCj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nYVCj.png"" alt=""enter image description here""></a>
  <a href=""https://i.stack.imgur.com/kNnPF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kNnPF.png"" alt=""enter image description here""></a></p>
</blockquote>
"
"2381699","2381772","<p>Some observations: </p>

<ul>
<li>if $y$ works, then considering $x$ as the vector whose $n$-th coordinate is $1$ and all the others $0$, we get that $1\leqslant \left\lvert y_n\right\rvert$.</li>
<li>Consider $x_n= \left\lvert y_n\right\rvert^{-1}$ for $0\leqslant n\leqslant N$, and zero for the others $n$. Then the $\ell^p$ norm of $x$ is $\left(\sum_{n=0}^N  \left\lvert y_n\right\rvert^{-p}\right)^{1/p}$ while $\left\lVert xy\right\rVert_\infty =1$. Consequently, we should have $\sum_{n=0}^N  \left\lvert y_n\right\rvert^{-p}\leqslant 1   $ and since $N$ is arbitrary, we get 
$$\tag{*}   \sum_{n=0}^{+\infty}    \left\lvert y_n\right\rvert^{-p}\leqslant 1.$$</li>
</ul>

<p>Actually, any sequence $\left(y_n\right)_{n\geqslant 1}$ satisfying (*) does the job, since 
$$\sum_{n=0}^{ +\infty}\left\lvert x_n\right\rvert^p=\sum_{n=0}^{ +\infty}\left\lvert x_n\right\rvert^p \left\lvert y_n\right\rvert^p \frac 1{\left\lvert y_n\right\rvert^p}\leqslant\left\lVert xy\right\rVert_\infty^p\sum_{n=0}^{ +\infty} \frac 1{\left\lvert y_n\right\rvert^p}\leqslant  \left\lVert xy\right\rVert_\infty^p .              $$       </p>
"
"2381701","2381711","<p>Set $I = \displaystyle \int_0^{\pi/2}\log(\cos(x))dx$. Then </p>

<p>\begin{align}
I &amp;= \displaystyle \frac{I+I}{2}\\ \\
&amp;=\frac12\int_0^{\pi/2}\log\big(\sin(x))+\log(\cos(x)\big)dx \\ \\
&amp;=\frac12\int_0^{\pi/2}\log\big(\sin(x)\cos(x)\big)dx\\ \\
&amp;=\frac12\int_0^{\pi/2}\log\left(\frac12\sin(2x)\right)dx\\ \\
&amp;=\frac\pi4 \log\left(\frac12\right) + \frac12\int_0^{\pi /2}\log(\sin(2x))dx\\ \\
&amp;=\frac\pi4 \log\left(\frac12\right) + \frac I4 + \frac14\int_0^{\pi/2}\log(\cos(x))dx\\ \\
&amp;=\frac\pi4 \log\left(\frac12\right) + \frac I4 + \frac I4\\ \\
&amp;= \frac\pi4 \log\left(\frac12\right) + \frac I2.
\end{align}</p>

<p>Hence $I = \displaystyle \frac\pi 2 \log\left(\frac12\right).$</p>

<p>Edit: perhaps my choice to attack the problem as I did is unclear. Here's maybe a little intuition for why the integrals for $\log\circ \sin$ and $\log \circ \cos$ should be equal on this interval. Look at the Riemann sums. Take a rectangle on the left half, while looking at $\log \circ \cos$. It will correspond exactly to a rectangle on the right half of the interval for $\log \circ \sin$ by the symmetry of $\sin$ and $\cos$ about $\displaystyle \frac\pi4$.</p>

<p>Here is a full calculation, with all the gory details.</p>

<p>Set $u= x-\displaystyle\frac\pi2$, so $\cos(x) = -\sin(u)$. Then we have 
$$\int_0^{\pi/2}\log(\cos(x))dx = \int_{-\pi /2}^0 \log(-\sin(u))du = \int_{-\pi/2}^0 \log(\sin(-u))du $$
$$= \int_{\pi/2}^0 \log(\sin(u))(-du) = \int_0^{\pi/2}\log(\sin(u))du$$
as desired.</p>
"
"2381703","2381709","<p>It's even possible to have $\lim_{t\to\infty}\frac{f(t)}{t}=0$ and $\limsup_{t\to\infty}f^{\prime}(t)=\infty$, as the example $f(t)=\sin(t^2)$ shows.</p>
"
"2381704","2381884","<p>It is useful to identify the <em>interval</em> that contains the median, and that
can be done without making unwarranted assumptions. For example, consider the
$n = 25$ scores below, which have been sorted from smallest to largest.</p>

<pre><code> 75    76    80    82    88    90    91    92    94    95
 99   100   102   103   103   105   106   107   109   113   
113   115   116   116   119
</code></pre>

<p>Their exact median is $H = 102,$ the thirteenth observation in the sorted list.</p>

<p>A frequency histogram below based on the following cutpoints (bin boundaries): </p>

<pre><code>74.5, 84.5, ..., 114.5, 125.5.
</code></pre>

<p>These boundaries were chosen so that no (integer) score can fall exactly
on a boundary.</p>

<p>The five interval midpoints are 79.5, 89.5, 99.5, 109.5, and 119.5. We can see
from this frequency histogram below, that the corresponding frequencies
are 4, 5, 6, 6, and 4.</p>

<p>Just looking at the histogram (or at a table of interval boundaries, midpoints,
and frequencies), and <em>without knowledge</em> of the exact values of the $n = 25$
observations, all we can say about the median is that it falls in the
interval $(94.5, 104.5)$ with midpoint $99.5$ and frequency $6.$ This
interval is called the <em>median interval.</em></p>

<p>In practice, grouped data tables and histograms are used mainly for samples
that are at least moderately large. For a large sample it would ordinarily
be sufficient to say that the median falls in the interval with midpoint $99.5.$</p>

<p><a href=""https://i.stack.imgur.com/YqB5P.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YqB5P.png"" alt=""enter image description here""></a> </p>

<p>A favorite exercise in elementary statistics books is to try to approximate the
exact value of the median from a histogram or from grouped data. Doing so
requires one to make the assumption (seldom true) that the observations within the median
interval are evenly spaced (or uniformly distributed).</p>

<p>One formula for approximating the exact median $H$ is</p>

<p>$$ H = L + \frac{w}{f_m}(.5n - cf_b),$$</p>

<p>where $L = 94.5$ is the lower limit of the median interval, $f_m = 6$ is the
frequency of the median interval, $cf_b = 9$ is the number of observations
in intervals below the median interval, $w = 10$ is the (common) interval
width, and $n = 25$ is the total sample size. This kind of formula is
sometimes called an 'interpolation' formula. </p>

<p>For our data,</p>

<p>$$ H = 94.5 + (10/6)(25/2 - 9) = 100.3333.$$</p>

<p>This procedure is seldom used in serious statistical analysis, and formulas
for it can differ a bit from one textbook to another. I do not know the formula in
your book, or why you wonder about a distinction between even and odd
sample sizes $n$. </p>

<p>I hope this answer is helpful. If you are using a different formula to
approximate the median, or if you have further questions, please leave me a Comment and edit your
Question to be a little more specific. Then perhaps one of us can be of
further help. </p>

<p><em>Notes</em>: (1) The 25 observations are simulated from $\mathsf{Norm}(\mu = 100,\,\sigma = 15)$ and rounded to integers. So the median of the <em>population</em> from which
the data were drawn is $\eta = 100.$ (2) It is not usually a good idea to
'group' datasets with $n$ as small as 25, or to make histograms of such
small datasets. I chose this particular illustration because I thought it
would make the application of the interpolation formula easy to follow.</p>
"
"2381710","2381732","<p>No, take for example the constant presheaf (obviously flasque), its sheafification is the constant sheaf which is not flasque in general (and in fact has very interesting cohomology).</p>
"
"2381712","2382044","<p>Elaborating on the comment by Daniel Fischer:</p>

<p>$$\begin{align}
 f(z) &amp;= \frac{1}{z^2\sin z} = \frac{1}{z^3(1-\frac{z^2}{6}+O(z^4))}=\frac{1}{z^3}(1-\frac{z^2}{6}+O(z^4))^{-1}\\
&amp;=\frac{1}{z^3}(1+\frac{z^2}{6}+O(z^4)) = \frac{1}{z^3}+\frac{1}{6z}+O(z).
\end{align}$$</p>

<p>Where I have used the expansion $(1+z)^\alpha = 1 +\alpha z +\frac{\alpha(\alpha-1)}{2}z^2 +\dots$ for $\alpha=-1$.
The coefficient of the $1/z$ term is the residue, which in this case is $1/6$.</p>

<p>Because $z^3f(z)$ is for this function analytic at $z=0$, the function has a pole of order 3. You could also use the general formula for poles of order $m$, with $m=3$ to get the residue at $z=0$.</p>

<p>$$ \mathrm{Residue} = \lim_{z\to0}\frac{1}{2}\frac{d^2}{dz^2}z^3f(z),$$</p>

<p>which after a longer calculation will also give you 1/6 (easier and shorter with the expansion suggested by Daniel Fisher).</p>
"
"2381716","2381831","<p>Consider $K=(J:I)$, the set of elements which take $I$ into $J$. Then, I claim that $I+K=R$. If not, it is contained in a maximal ideal $M$.But, when you localize at $M$, we have $I_M=J_M$, since $I=J+I^2$ and Nakayama. So, there exists an $s\not\in M$ with $s\in K$, a contradiction.</p>

<p>So, we can find $h\in I$ with $1-h\in K$. I claim that $I=J+hR=I'$, which will prove what you need. So, let $a\in I$. So, we an write, $a=ah+a(1-h)$. Then, $ah\in I'$ and $a(1-h)\in I\subset I'$, since $1-h\in K$. Thus we are done.</p>
"
"2381717","2381739","<p>Consider the pairs of vertices $(v_{i-1}, v_i)$ for $2 \leq i \leq n-1$. There are $n-2$ of them. There are $d(v_1)$ edges between $v_1$ and some of the vertices $v_k$ for $2 \leq k \leq n-1$. In this way, $v_1$ touches $d(v_1)$ of the pairs $(v_{i-1}, v_i)$ ('touches' means $v_1$ is adjacent to $v_i$). Similarly $v_n$ touches $d(v_n)$ of the pairs $(v_{i-1}, v_i)$ (here 'touches' means $v_n$ is adjacent to $v_{i-1}$). Thus $d(v_1) + d(v_n) \geq n$ pairs are touched by $v_1$ or $v_n$. This is more than the total number of pairs, so by the pigeonhole principle, there is some pair $(v_{i-1}, v_i)$ that $v_1$ and $v_n$ both 'touch', as desired.</p>
"
"2381724","2381748","<p>My example is probably not the best but it still show you that the linear regression cost function applied to logistic regression can give local minima.</p>

<p>Let us consider a training set with 3 examples in 1D:
$$\{(-1,2), (-20,-1), (-5,5)\}$$</p>

<p>For the sake of simplicity let us consider a model without constant term so that we get the activation function $$h_{\theta}(x)=\dfrac{1}{1+e^{\theta \cdot x}}$$</p>

<p>If we use the cost function of a linear regression we would get</p>

<p>$$J(\theta)=\dfrac{1}{6}\left(\left(\dfrac{1}{1+e^{-\theta}}-2\right)^2 + \left(\dfrac{1}{1+e^{-20\theta}}+1\right)^2+\left(\dfrac{1}{1+e^{-5\theta}}-5\right)^2\right)$$</p>

<p>Let us now plot this function. You need to zoom a bit around $0$ to see the following graph</p>

<p><a href=""https://i.stack.imgur.com/MAoX4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MAoX4.png"" alt=""graph""></a></p>

<p>There is clearly a local minimum that we could reach.
This is one of the reason you should use the classical cost function for logistic regression.</p>

<p>Besides I am sure that you should be able to find a nicer graph with more local minima when using a bigger value for $n$.</p>
"
"2381728","2381730","<p>Guide:$$\theta^2 (1-\theta)^8 = a$$
and
$$\theta^3(1-\theta)^7 = b$$</p>

<p>implies $$\frac{1-\theta}{\theta}=\frac{a}{b}$$</p>

<p>which can be converted into a linear equation in $\theta$. </p>

<p>After you solve for for $\theta$, you should be able to solve the problem.</p>
"
"2381736","2381749","<p>To see why $\displaystyle \bigcap_{i\,\in\,\varnothing} S_i = \text{the âuniverse''},$ it is not necessary to know how to view intersection as a product.</p>

<p>To see why $\displaystyle\sum_{i\,\in\,\varnothing} x_i = 0$ it is only necessary to see that if you add no numbers to the subtotal that you already have, the result is the same as if you had added the number $0.$</p>

<p>To see why $\displaystyle\prod_{i\,\in\,\varnothing} x_i = 1$ it is only necessary to see that if you multiply the product that you already have by no numbers, the result is the same as if you had multiplied it by the number $1.$</p>

<p>Likewise $\displaystyle \cdots\cap X\cap Y\cap Z\cap \bigcap_{i\,\in\,\varnothing} S_i = \cdots\cap X \cap Y\cap Z.$ </p>

<p>One can also say that every time you add a new term to an intersection $\cdots\cap X\cap Y\cap Z$ you are excluding more things from membership in the intersection. If there are no such terms, then nothing has been excluded.</p>

<p>It is somewhat like the fact that $\sup\varnothing = -\infty.$ Every time you add more members to the set $A$, you add more things that $\sup$ cannot fall below. It falls as far downward as those constraints allow. That is why it is called the <b>smallest</b> upper bound. If there are no members, then there is nothing that it cannot fall below.</p>

<p>To view intersection as multiplication, think of indicator functions $\chi_A$ of sets $A$:
$$
\chi_A(x) = \begin{cases} 1 &amp; \text{if } x\in A, \\ 0 &amp; \text{if } x\notin A. \end{cases}
$$
Then the indicator function of the intersection of sets is the product of the indicator functions:
$$
\chi_{A\,\cap\,B}(x) = \chi_A(x) \chi_B(x).
$$</p>
"
"2381758","2381763","<p>Hint 1:
$$P(\text{more heads than tails}) + P(\text{more tails than heads}) + P(\text{same number of heads and tails}) = 1$$</p>

<p>Hint 2:
$$P(\text{more heads than tails}) = P(\text{more tails than heads})$$</p>

<p>Hint 3:
$$P(\text{same number of heads and tails}) = \binom{8}{4}\frac{1}{2^8}$$
(by the binomial distribution).</p>

<hr>

<p>Alternatively, the answer can be computed as
$$P(\text{5 heads}) + P(\text{6 heads}) + P(\text{7 heads}) + P(\text{8 heads}) =\binom{8}{5} \frac{1}{2^8} + \binom{8}{6} \frac{1}{2^8} + \binom{8}{7} \frac{1}{2^8} + \binom{8}{8} \frac{1}{2^8}$$</p>
"
"2381760","2381780","<p>An easier approach is to do the <a href=""https://en.wikipedia.org/wiki/Laplace_expansion"" rel=""nofollow noreferrer"">Laplace expansion</a> on the bottom row of the original matrix. This yields the result without appealing to the SVD decomposition. Regarding your question about whether $PB$ exists or not, I think the dimension-matching is fine because $A$ is a square matrix.</p>
"
"2381764","2381842","<p>The typical method of proof goes something like this (sorry that I have not finished the problem here, but perhaps someone could),</p>

<p>Consider the energy
$$ e(t) = \frac{1}{2}\int_{\Omega(t)} u_t^2 + |\nabla u|^2 \; dx.$$
Since we have that $e(0) = 0$ by hypothesis, the idea is to then show that ${\dot e}(t) \leq 0.$</p>

<p>Compute,
\begin{align}
{\dot e}(t) &amp;= \int_{\Omega(t)} u_tu_{tt} + \nabla u_t \cdot \nabla u \; dx -\frac{1}{2}\int_{\partial\Omega} u_t^2 + |\nabla u|^2 \; dS \\
&amp;= \int_{\Omega(t)} u_t(u_{tt} - \Delta u)\; dx + \int_{\partial\Omega(t)} u_t\frac{du}{dn} \; dS -\frac{1}{2}\int_{\partial\Omega} u_t^2 + |\nabla u|^2 \; dS\\
&amp;= \int_{\Omega(t)} -\phi u u_t\; dx \int_{\partial\Omega(t)} u_t\frac{du}{dn} \; dS -\frac{1}{2}\int_{\partial\Omega} u_t^2 + |\nabla u|^2 \; dS.
\end{align}
Using the fact that $$ \bigg|\frac{du}{dn} u_t\bigg| \leq \frac{1}{2} u_t^2 + \frac{1}{2} |\nabla u|^2,$$
we have that $$\int_{\partial\Omega(t)} u_t\frac{du}{dn} \; dS -\frac{1}{2}\int_{\partial\Omega} u_t^2 + |\nabla u|^2 \; dS \leq 0$$
Thus,
\begin{align}
{\dot e}(t) &amp;\leq \int_{\Omega(t)} -\phi u u_t\; dx\\
\end{align} </p>

<p>Here is where I get stuck since $\phi$ can take both positive and negative values in $\Omega$ and not much can be said about $u$ either.  I was hoping for the inequality, ${\dot e}(t) \leq e(t)$ so then $e(t) \leq e(0) \implies e(t) = 0.$</p>
"
"2381767","2381829","<p>I'm not sure this is quite what you're looking for, since it's really just a proof that a finitely generated algebra over a Jacobson ring is Jacobson, specialized to the case of $\mathbb{Z}$.  But here goes.  The key tool is the <a href=""https://en.wikipedia.org/wiki/Artin%E2%80%93Tate_lemma"" rel=""nofollow noreferrer"">Artin-Tate lemma</a> (you can use Noether normalization instead, but Artin-Tate gives what I would consider a more elementary argument).</p>

<p>First, let us prove that any field $K$ which is finitely generated as a ring is a finite field.  Let $k$ be the prime subfield of $K$.  Since $K$ is finitely generated as a $k$-algebra, it suffices to show $K$ has positive characteristic and is finite over $k$.  Let $B\subset K$ be a transcendence basis and note that $K$ is algebraic and hence finite over $k(B)$.  By the Artin-Tate lemma applied to the $\mathbb{Z}$-algebras $k(B)\subseteq K$, $k(B)$ is finitely generated as a ring.  This implies $B=\emptyset$ (any finitely generated subring of $k(B)$ has only finitely many irreducible polynomials which are factors of denominators of its elements, but there are infinitely many different irreducible polynomials in $k[B]$ if $B$ is nonempty).  Moreover, since $\mathbb{Q}$ is not a finitely generated ring, $k$ must have positive characteristic.  Thus $K$ is algebraic over $k=\mathbb{F}_p$ for some $p$ and hence is a finite field.</p>

<p>Now suppose $R$ is a finitely generated domain and $f\in R$ is nonzero.  The ring $S=R[f^{-1}]$ is then also a finitely generated domain, and so it has a maximal ideal $M$.  I claim that $M\cap R$ is a maximal ideal of $R$, and thus there exists a maximal ideal of $R$ which does not contain $f$.</p>

<p>To prove this, observe that field $S/M$ is finitely generated as a ring, and hence finite.  But $R/(M\cap R)$ is naturally a subring of $S/M$, and any subring of a finite field is a field.  Thus $R/(M\cap R)$ is a field, so $M\cap R$ is maximal.</p>
"
"2381769","2381775","<p>The identity $$\lim(f(x)+g(x))=\lim f(x)+\lim g(x)$$ which you use from line 2 to line 3, is only valid when all those three limits (or at the very least the two on the right) exist and are finite.</p>
"
"2381790","2381800","<p>So it sounds like you are trying to solve
$$p(x) = C$$
where $p(x)$ is a cubic function, and $C$ is a given constant.</p>

<p>You say that you know how to factor $p(x)$.  That's good -- but it's actually not particularly helpful in this case, as it doesn't help you get to a solution.</p>

<p>Instead, rewrite the equation as
$$p(x) - C = 0$$
and see if you can factor <em>the new polynomial that appears on the left-hand side</em>.  If you can factor <em>it</em>, you can easily find the zeroes of $p(x)-C$, which are then the solutions of your equation.</p>
"
"2381791","2381848","<p>Instead of thinking of it as each man gets a bunch of shillings, puts them in his pocket, then takes out a bunch of shillings from his pocket and gives them to the next man; think of it as a man with $k$ shillings gets a basket containing $w$ shillings.  The man adds a shilling to it and passes the basket on.  The man now has $k -1$ shillings and the basket now has $w + 1$ shillings in it.</p>

<p>Let it start with there being $m$ men, the last man is the poorest with $k$ shillings (and the first man is the richest with $m+k -1$ shillings).  The basket starts with $0$ in it.  But each man puts in $1$ shilling so at the end of a ""circuit"" (when it gets to the last man) every man has put in a shilling so the basket has $m$ shillings and each man is a shilling poorer.  The last/poorest man has $k -1$ shilling and the first richest has $m+k -2$.</p>

<p>Each round the basket grows by $m$ and each man (in particular the last man) gets poorer by $1$.  So after $j$ rounds the basket with have $m*j$ shillings and the last man will have $k - j$ shillings.  </p>

<p>At the end of  $k$ rounds the last man puts his last shilling in, and the basket has $mk$ shillings.  He passes it on for the $k+1$ round.  At the end of <em>that</em> round, the last man can't do anything and the whole thing ends.  The basket has $km + (m-1)$ shillings as everybody but the last man put a last shilling in.</p>

<p>The last man breaks the basket open and keeps the $km + m -1$ shillings.  The man before him has no shillings as that second man had only one more than the last man.  The first man started with $m+k -1$ shillings and he put in $k+1$ shillings in so the first man has $m-2$ shillings.</p>

<p>We are told a neighbor has $4$ times as much as another, but each neighbor has $1$ more that the neighbor ahead of him.  Except the last man and the first man.</p>

<p>So either the first man has $4$ times as much as the last man and $m-2 = 4(km + m - 1$ or the last man has $4$ times as mch as the first and $km + m -1 = 4(m-2)$.</p>

<p>So $k$ is presumably positive it must be the latter.  And </p>

<p>$km + m - 1 = 4(m-2)$</p>

<p>$(k+1)m -1 = 4m - 8$</p>

<p>$(k-3)m = -7$ </p>

<p>$(3-k)m = 7$</p>

<p>Either $3-k = 7$ and $m =1$ Which is ... silly... as we know there were more than one man.  Or $3-k = 1; k = 2$ and $m=7$.</p>

<p>So:  Beginning:  First man has $8$ shillings and last man has $2$. Basket has $0$ shillings.</p>

<p>End of first round:  Basket has $7$ shillings.  First man has $7$ and last an has $2$.</p>

<p>End of second round:  Basket has $14$ shilings. First man has $6$ and last has $0$.</p>

<p>Final round:  First man has $5$ shillings.  Men $1-6$ have added shillings so the basket has $20$ shillings in it.  It gets passed to the last man who is supposed to add a shilling to it but he can't because he doesn't have any. </p>

<p>So the last man keeps the basket.  The last man has $20$ shillings.  The first man has $5$.  (And men $2-6$ have $4,3,2,1,0$ respectively.</p>
"
"2381799","2381804","<p>In the context of differential equations, the constant goes away since usually, any function that differentiates to the integrand will be a solution so people just choose $C = 0$ for convenience. When you keep the constant as a free variable, then you get something like
$$
e^{x^2/2+C_0} = e^{C_0}e^{x^2/2} = C_1e^{x^2/2},
$$
which indicates that any positive constant multiple of the solution will also be a solution. </p>
"
"2381805","2381807","<p>Yes: note that
$$ 0\leq \frac{2^n}{n!}\leq 2\Big(\frac{2}{3}\Big)^{n-2}$$
for $n\geq 3$, and then use the squeeze theorem.</p>
"
"2381821","2381828","<p>This happens because:
$$
c_i = \frac{\sum_{j \in M, j \neq i}^N z_j}{[\sum_{j \in M}^N z_j]^2} = \frac{\sum_{j \in M}^N z_j}{[\sum_{j \in M}^N z_j]^2} - \frac{z_i}{[\sum_{j \in M}^N z_j]^2}
$$</p>

<p>Now, summing over all possible $i \in M$,
$$
\sum_{i \in M} c_i = \frac{|M|\sum_{j \in M}^N z_j}{[\sum_{j \in M}^N z_j]^2} - \frac{\sum_{i \in M}z_i}{[\sum_{j \in M}^N z_j]^2} = \frac{(|M| - 1)\sum_{j \in M}^N z_j}{[\sum_{j \in M}^N z_j]^2}
$$</p>

<p>From where the statement follows by rearrangement.</p>
"
"2381822","2381876","<p>Using the continuity of $L$, pick $\delta &gt; 0$ such that $|L(f)|&lt;1$ whenever $d(f,0)&lt;\delta$. One sufficient condition for $d(f,0)&lt;\delta$ is that $f$ satisfy the following property:</p>

<blockquote>
  <p><strong>Property $P_{\delta}$.</strong> The support of $f$ is contained in an interval of length $&lt;\delta$. To be precise, there exists an interval $I\subseteq [0,1]$ of length $&lt;\delta$ such that $ \{x : f(x) \neq 0 \} \subseteq I $.</p>
</blockquote>

<p>If $f$ has this property, then so is <em>any</em> scalar multiple of $f$, hence</p>

<p>$$ |L(f)| = \frac{1}{n}|L(nf)| \leq \frac{1}{n} $$</p>

<p>and we have $L(f)=0$ by letting $n\to\infty$.</p>

<p>Now the key idea is that any continuous function $f$ can be uniformly approximated by linear combinations of functions having poperty $P_{\delta}$.</p>

<p>For instance, for each $n$ satisfying $n&gt;2/\delta$, we notice that the linear interpolation $f_n$ of points $(\frac{k}{n}, f(\frac{k}{n}))$ for $k = 0, \cdots, n$ can be written by</p>

<p>$$f_n(x) = \sum_{k=0}^{n} f\left(\frac{k}{n}\right)\varphi_{n,k}(x), \qquad \varphi_{n,k}(x) := \max\left\{0, 1-n\left|x-\frac{k}{n}\right|\right\}.$$</p>

<p>Since each $\varphi_{n,k}$ is supported on the interval $[\frac{k-1}{n}, \frac{k+1}{n}]$ whose length is $=\frac{2}{n} &lt; \delta$, it has property $P_{\delta}$ and hence $L(\varphi_{n,k})=0$. So it follows that $L(f_n) = 0$. On the other hand, we can easily check that $f_n \to f$ uniformly and hence $f_n \to f$ under the metric $d$. So we have $L(f)=0$ as desired.</p>
"
"2381826","2381871","<p>We could simply say as convention that all operations of more than two operands must absolutely have parenthesis around every pair and that if the parenthesis are missing they expression is meaningless.</p>

<p>So $a + b*c + d*e +f$ (as we know it) must be written as $(((a + (b*c)) + (d*e))+f)$  This is, of course, tedious.</p>

<p>We could have as a convention that we always go from left to right, always.  So the expression $a + b*c + d*e + f$ would now mean $((a+b)*c + d)*e + f$ and to write the expression we meant we'd have to writh $a + (b*c) + (d*e) + f$.</p>

<p>This is perfectly acceptable.</p>

<p>However given that we have the distributive law that $a(b+c) = (ab) + (ac)$ there is an incentive to view $a*c + b*c$ so $(a*c) + (b*c) = (a+b)*c$ rather than as $(a*c + b)*c = a*c^2 + (b*c)$.  Our convention is consistent, less capricious, and usefl and insightful.</p>
"
"2381833","2381851","<p>First we have to determine what $f(0)$ is. Suppose $f(0)=0$. Then $f(0)=f(0+f(0))=f(0)+1$, a contradiction. Now suppose $f(0)=k$ for some $k&gt;1$. Then $f(k)=f(0+f(0))=f(0)+1=k+1$. But note that by the first property, we have $$k=f(0)&lt;f(1)&lt;\cdots&lt;f(k)=k+1.$$ Since $0&lt;1&lt;k$, there is no valid value for $f(1)$, a contradiction.</p>

<p>We deduce that $f(0)=1$. This and the second condition suggests that perhaps $f(n)=n+1$ holds for all $n$. Indeed, the function $n\mapsto n+1$ satisfies all of the above conditions, so is a candidate for the solution. We would like to prove that this is the only such function.</p>

<p>Let's prove this by induction. Suppose $f(m)=m+1$ holds for all $0\leqslant m&lt;n$. In particular, $n=f(n-1)$. Thus, $$f(n)=f(0+f(n-1))=f(0)+n-1+1=f(0)+n=n+1.$$</p>

<p>Therefore, by induction, $f(n)=n+1$ holds for all $n$. In particular, $f(2017)=2018$.</p>
"
"2381844","2381857","<p>I'm going to use red, green, and blue as my colors (since that feels more natural to me).  One side needs to be red, two sides are going to be green, and three sides are going to be blue (since I like blue, I want the cube to be mostly blue).</p>

<p>I'm going to start by paining one face red.  Next, I am going to paint the opposite face.  I have two choices:  I can paint that face either green, or blue.</p>

<ol>
<li><p>If I paint the opposite face green, then I have to paint one of the remaining face green, and the other three remaining face blue.  There is only one way to do this.</p></li>
<li><p>If I paint the opposite face blue, then I have to color two of the remaining faces green.  There are two ways to do this:  either I paint two adjacent faces green, or I paint two opposite faces green.</p></li>
</ol>

<p>As these two cases exhaust all possibilities, there are a total of three different ways of painting a cube subject to the constraints you gave.</p>
"
"2381858","2381865","<p>$\overline{A} \cup \overline{B}$ is a closed set containing $A \cup B$. Hence $\overline{A \cup B} \subseteq \overline{A} \cup \overline{B}$.</p>
"
"2381873","2381917","<p>Define 
$$
Z := \sum_{j\in M} z_j \hspace{4mm}\mbox{ and }\hspace{4mm} C := \sum_{i\in M} c_i.
$$
Then by substituting $Z$ and $C$ appropriately into 
$$
\sum_{i \in M} c_i = \frac{Q(|M| - 1)\displaystyle{\sum_{j \in M}} z_j}{\left(\displaystyle{\sum_{j \in M}} z_j\right)^2}, 
$$
we obtain 
$CZ=Q(|M|-1)$. This implies 
$$
Z=\frac{Q(|M|-1)}{C}. 
$$
Furthermore, 
for $i \in M$, 
$$
c_i =\frac{Q\displaystyle{\sum_{j\in M, j\ne i}z_j}}{\left(\displaystyle{\sum_{j\in M}}z_j\right)^2}
$$
can be rewritten as 
$$
c_i = \frac{Q(Z-z_i)}{Z^2}. 
$$
Solving for $z_i$, we have 
\begin{align*}
z_i &amp;= \frac{QZ-c_iZ^2}{Q} \\ 
&amp;= Z-\frac{c_i}{Q}Z^2 \\ 
&amp;= \frac{Q(|M|-1)}{C} - \frac{c_i}{Q}\left(\frac{Q(|M|-1)}{C} \right)^2 \\
&amp;= \frac{Q(|M|-1)}{C} - \frac{c_iQ(|M|-1)^2}{C^2} \\ 
&amp;=  \frac{Q(|M|-1)}{C} \left(1-\frac{c_i (|M|-1)}{C} \right) \\ 
&amp;= \frac{Q(|M| - 1)}{\displaystyle{\sum_{j \in M}} c_j}\bigg(1-\frac{c_i(|M| - 1)}{\displaystyle{\sum_{j \in M}} c_j}\bigg).\\ 
\end{align*}</p>
"
"2381875","2381877","<p>$$x \in (A \cap B)^c \\\iff x \notin A \cap B \\\iff x \notin A \text{ or } x \notin B\\\iff x\in A^c \text{ or } x \in B^c\\\iff x \in A^c \cup B^c$$</p>
"
"2381881","2381890","<p>A simple approach involves noticing that</p>

<p>$$a&lt;x&lt;1\implies(1-x)\ln(a)&lt;\ln(x)&lt;x-1$$</p>

<p>This follows from the simple fact that $\ln(x)$ is concave, and thus, it is bounded above by it's tangent line (which is $x-1$) and bounded below by it's secant line (which is $(1-x)\ln(a)$).</p>

<p>Thus, since $\ln(1/2)=-\ln(2)$,</p>

<p>$$\frac12&lt;b&lt;1\implies\int_{1/2}^b\frac1{x-1}~\mathrm dx&lt;\int_{1/2}^b\frac1{\ln(x)}~\mathrm dx&lt;\frac1{\ln(2)}\int_{1/2}^b\frac1{x-1}~\mathrm dx$$</p>

<p>And as $b\to1^-$, all three integrals diverge.</p>
"
"2381891","2381893","<p>On $[0,1]$, try $$f(x) = \cases{x &amp; if $x &lt; 1$\cr
                                0 &amp; if $x = 1$\cr}$$</p>
"
"2381896","2381908","<p>Intuitively, since $a_n$ is non-negative and monotone, there exists $f(x)$ that is non-negative, monotone, and $f:\mathbb R^+\mapsto\mathbb R^+$ such that $f(n)=a_n$, and we may have$$\frac1n\sum_{k=1}^na_k\sim\frac1n\int_1^nf(x)~\mathrm dx$$And then by L'Hospital's rule, we find that$$\lim_{n\to\infty}\frac{\int_1^nf(x)~\mathrm dx}n=\lim_{n\to\infty}f(n)=\lim_{n\to\infty}a_n$$To prove the integral limit equals the given limit, we may use the squeeze theorem:$$\frac1n\int_1^nf(x)~\mathrm dx\le\frac1n\sum_{k=1}^na_k\le\frac1n\int_1^{n+1}f(x)~\mathrm dx$$which follows from $f(x)$ being monotone.</p>
"
"2381903","2381910","<p>The probability that person $a$ has a birthday on the date and all others don't is $\frac 1{365}*(\frac{364}{365})^{249}$.  Those events are independent so the odds that exactly one person has that birthday are $250*\frac 1{365}*(\frac{364}{365})^{249}\approx .34$.</p>
"
"2381907","2381922","<p>Yes, $g$ is a group isomorphism. But in order for this statement to make any sense, we first need to define the group structure on $p^{-1}(b_0)$!</p>

<p>One sensible way to do this is as follows. Let $H$ be the group of <em>deck transformations</em> for the covering $p : E \to B$. In other words, each element $h \in H$ is a homeomorphism $h : E \to E$ that is compatible with the projection $p : E \to B$, i.e. that satisfies $p \circ h = p$. It is immediate from this definition that every $h \in H$ sends $e_0$ to some point in $ p^{-1}(b_0)$, and with more work (see Hatcher pages 70-71), one can verify that this correspondence between the elements of $H$ and the points in $p^{-1}(b_0)$ is one-to-one and onto. Having thus identified the set $p^{-1}(b_0)$ with the group $H$, it is now meaningful to ask whether or not your map $g : \pi_1(B, b_0) \to p^{-1}(b_0)$ is a group isomorphism, and the answer is yes.</p>

<p>As I mentioned, details of this are provided in Hatcher on pages 70-71, though you may as well prove it yourself. You already know that $g$ is a bijection, so it only remains to check that $g$ is a group homomorphism, and this follows by an argument using uniqueness of path lifts. A few months ago, I wrote <a href=""https://math.stackexchange.com/questions/2206920/e-oversetp-longrightarrowx-covering-space-connected-and-locally-path-con/2206983#2206983"">an answer</a> about a more general version of this result - see the fourth bullet point in that answer for a hint on how to complete the proof.</p>
"
"2381914","2382827","<p>Yes, it can. Consider $M=S^1 \times K^2$ where $K^2$ is the Klein bottle. This manifold fibers over both $T^2$ (since $K^2$ fibers over $S^1$) and over $K^2$. Of course, if the base of the fibration is required to have negative Euler characteristic, then $M$ cannot fiber over nonhomeomorphic surfaces. </p>
"
"2381918","2381938","<p>$f(p) = 1-(p/p_\mathrm{max})^2$.</p>
"
"2381942","2381955","<p>You're not going to get much in the way of ""efficient"" here. Usually the point of these proofs is to demonstrate that you can spell out all the little steps in a cohesive manner. You need to learn to walk before you learn to run.</p>

<p>There are significant problems with your proof. Specifically, you're confusing the sum of two linear functions with summing their arguments (i.e. the vectors you substitute into them).</p>

<p>Let's start by explicitly defining the sum and scalar product of linear transformations. Suppose $T_1, T_2 : V \rightarrow W$ are linear, and $\lambda \in \mathbb{F}$ (the common scalar field for $V$ and $W$, usually $\mathbb{R}$ or $\mathbb{C}$ in undergraduate courses). Then,</p>

<p>\begin{align*}
T_1 + T_2 &amp;: V \rightarrow W : v \mapsto T_1(v) + T_2(v) \\
\lambda T_1 &amp;: V \rightarrow W : v \mapsto \lambda T_1(v).
\end{align*}</p>

<p>Notated more simply, we have for all $v \in V$,</p>

<p>\begin{align*}
(T_1 + T_2)(v) = T_1(v) + T_2(v) \\
(\lambda T_1)(v) = \lambda T_1(v).
\end{align*}</p>

<p>What you have to realise here is that the $+$ between $T_1$ and $T_2$ is a different operation to the $+$ between $T_1(v)$ and $T_2(v)$. The first plus is the addition operation that we're defining; the one that takes two linear maps $T_1, T_2$ and produces a third linear map $T_1 + T_2$. The second plus is the addition in $W$, between two vectors in $W$: $T_1(v)$ and $T_2(v)$. They are different operations on different vector spaces.</p>

<p>So, let's try proving, say, distributivity of scalar multiplication over addition. Suppose $T_1, T_2,$ and $\lambda$ are as above. We wish to show that $\lambda(T_1 + T_2) = \lambda T_1 + \lambda T_2$. In order to prove this, we must show that both functions, on the left and right, do the same thing to every vector $v$. So, we must show that, for all $v \in V$,</p>

<p>$$(\lambda(T_1 + T_2))(v) = (\lambda T_1 + \lambda T_2)(v)$$</p>

<p>Using only the definition of the sum of linear functions as described above, we have,</p>

<p>\begin{align*}
(\lambda(T_1 + T_2))(v) &amp;= \lambda((T_1 + T_2)(v)) &amp; \text{(Definition of scalar multiplication on $L$)} \\
&amp;= \lambda(T_1(v) + T_2(v)) &amp; \text{(Definition of addition on $L$)} \\
&amp;= \lambda T_1(v) + \lambda T_2(v) &amp; \text{(Distributivity in $W$)} \\
(\lambda T_1 + \lambda T_2)(v) &amp;= (\lambda T_1)(v) + (\lambda T_2)(v) &amp; \text{(Definition of addition on $L$)} \\
&amp;= \lambda T_1(v) + \lambda T_2(v) &amp; \text{(Definition of scalar multiplication on $L$)}
\end{align*}
Therefore, $(\lambda(T_1 + T_2))(v) = (\lambda T_1 + \lambda T_2)(v)$ as needed, so $\lambda(T_1 + T_2) = \lambda T_1 + \lambda T_2$.</p>

<p>When showing closure of addition, you'll need to show that $T_1 + T_2$ is a linear function from $V$ to $W$. That is, you'll need to use the definition of $T_1 + T_2$ to show it's linear, specifically for all $v, w \in V$ and $\lambda \in \mathbb{F}$,</p>

<p>\begin{align*}
(T_1 + T_2)(v + w) &amp;= (T_1 + T_2)(v) + (T_1 + T_2)(w) \\
(T_1 + T_2)(\lambda v) &amp;= \lambda(T_1 + T_2)(v)
\end{align*}</p>

<p>Similarly for scalar multiplication. You should only need the definitions of addition in $L$ and basic properties of $V$ and $W$ guaranteed by $V$ and $W$ being vector spaces.</p>

<p>Also think about the additive identity and additive inverses. The additive identity must be a linear transformation from $V$ to $W$. What does it do to an arbitrary vector $v \in V$? These are things you need to make clear!</p>

<p>Good luck!</p>

<p>EDIT: There are still issues with the proof. Let's take a look at your proof of associativity, for example. Associativity of addition states that $(u + v) + w = u + (v + w)$, where $u, v, w$ are elements of the proposed vector space. The proposed vector space you're dealing with is $L$. You should be showing this is true for three linear transformations, $T_1, T_2, T_3$ from $V$ to $W$. You need to prove that
$$(T_1 + T_2) + T_3 = T_1 + (T_2 + T_3).$$
In order to do this, you'll need to show, for an arbitrary $v \in V$, that
$$((T_1 + T_2) + T_3)(v) = (T_1 + (T_2 + T_3))(v)$$
You should be able to show this only with the definition of addition on $L$ (as I described above) and associativity on $W$. I advise writing out reasons for the steps, much like I did with the distributivity. Let me get you started with the first step:
$$((T_1 + T_2) + T_3)(v) = (T_1 + T_2)(v) + T_3(v) \ \ldots \ \text{Definition of addition on }L$$
With all of these axioms that require showing an equality of vectors (everything except closure and possibly additive identity and inverses), you'll only need one vector $v \in V$ to show equality (as you just need to show that when each transformation is applied to a given vector, it returns the same result). As soon as you introduce $u$ or $w$, you're proving the wrong thing.</p>

<p>I'll get to closure in a moment, but let's look at the additive identity. Remember, you want to find an element of $L$, a <strong>linear transformation</strong> from $V$ to $W$, that adds to other transformations in $L$ without changing them. I'll do the additive identity:</p>

<p>Let $\mathbf{0} : V \rightarrow W : v \mapsto 0_W$, where $0_W \in W$ is the additive identity of $W$. So $\mathbf{0}$ is a constant function, taking every $v \in V$ and returning $0_W$. We should probably verify that this function I've made up is really in $L$. It obviously maps from $V$ to $W$, but is it linear? Let's show that now.</p>

<p>Suppose $u, v \in V$ and $\lambda \in \mathbb{F}$. Then
$$\mathbf{0}(u + v)= 0_W = 0_W + 0_W =\mathbf{0}(u) + \mathbf{0}(v)$$
$$\mathbf{0}(\lambda v) = 0_W = \lambda 0_W = \lambda \mathbf{0}(v).$$
Therefore, by definition, $\mathbf{0}$ is linear, and thus belongs to $L$.</p>

<p>Let's show $\mathbf{0}$ is indeed the additive identity. So, we must show $T + \mathbf{0} = T$. Again, we show this is true by applying both sides to an arbitrary vector $v \in V$, and verifying we get the same thing. We have,
\begin{align*}
(T + \mathbf{0})(v) &amp;= T(v) + \mathbf{0}(v) &amp; \text{(Definition of addition on $L$)} \\
&amp;= T(v) + 0_W &amp; \text{(Definition of $\mathbf{0}$)} \\
&amp;= T(v) &amp; \text{(Additive identity of $W$)}
\end{align*}
Thus, as required, $T + \mathbf{0} = T$. I'm hoping this is starting to click. We are only really interested in the addition and scalar multiplication operations on $L$, between linear transformations. We do need addition and scalar multiplication on $W$, but only to show the properties we want of addition and scalar multiplication on $L$.</p>

<p>Finally, I think I'll go through closure, as it's a bit of an exception. I'll do closure under scalar multiplication. Basically, given any $T \in L$ and $\lambda \in \mathbb{F}$, we need to show that $\lambda T$ is linear. Suppose $u, v \in V$ and $\mu \in \mathbb{F}$. We have,
\begin{align*}
(\lambda T)(u + v) &amp;= \lambda T(u + v) &amp; \text{(Definition of scalar multiplication on $L$)} \\
&amp;= \lambda (T(u) + T(v)) &amp; \text{($T$ is linear)} \\
&amp;= \lambda T(u) + \lambda T(v) &amp; \text{(Distributivity in $W$)} \\
&amp;= (\lambda T)(u) + (\lambda T)(v) &amp; \text{(Definition of scalar multiplication on $L$)} \\
(\lambda T)(\mu v) &amp;= \lambda T(\mu v) &amp; \text{(Definition of scalar multiplication on $L$)} \\
&amp;= \lambda (\mu T(v)) &amp; \text{($T$ is linear)} \\
&amp;= (\lambda \mu) T(v) &amp; \text{(Associativity of scalar multiplication on $W$)} \\
&amp;= (\mu \lambda) T(v) &amp; \text{(Commutativity of multiplication on $\mathbb{F}$)} \\
&amp;= \mu (\lambda T(v)) &amp; \text{(Associativity of scalar multiplication on $W$)} \\
&amp;= \mu (\lambda T)(v) &amp; \text{(Definition of scalar multiplication on $L$)}
\end{align*}</p>

<p>These two properties mean that $\lambda T$ is linear, hence $\lambda T \in L$ as required.</p>

<p>I think my answer is long enough as it is. If you want to make further attempts, I'm happy to check them, but I advise making a new question, linking it back to this one, and send me a link to it, via a comment.</p>
"
"2381949","2381956","<p>The image you are looking for is: $F(S) {~=~ \{(xy, y)\mid 1\leq x\leq 2, 2\leq y\leq 3\} \\ ~=~ \{(z,y)\mid\boxed{{y}}\leq z\leq\boxed{{2y}}, 2\leq y\leq 3\}\\ ~=~ \{(u,v)\mid\boxed{{v}}\leq u\leq\boxed{{2v}}, 2\leq v\leq 3\} \\~=~\{(u,v)\mid 2\leq v \leq u\leq 2v\leq 6\}}$</p>

<p>Fill in the boxes.</p>

<p>Edit: You got it.</p>
"
"2381950","2381959","<p>On the plane, if you specify an angle $\theta$ , there is a unique point on the unit circle corresponding to that angle. We call its $x$-coordinate $\cos(\theta)$ and its $y$-coordinate $\sin(\theta)$. This can be regarded as the geometric definition of $\cos$ and $\sin$.
$$(x,y) = (\cos(\theta), \sin(\theta))$$</p>

<p>In three dimensions, a single angle will not define a point on the unit sphere. <a href=""https://en.wikipedia.org/wiki/Spherical_coordinate_system"" rel=""nofollow noreferrer"">You need two angles, the polar and azimuth angles</a>.
The 3D analogue of sine and cosine would be three functions $X(\theta, \phi)$, $Y(\theta, \phi)$ and $Z(\theta, \phi)$ which define the coordinates based on your angles. But these can be expressed in terms of sine and cosine. Therefore there isn't a need to invent new functions.
The $x$, $y$, and $z$-coordinates are
$$(x,y,z) = (\sin(\theta)\cos(\phi), \sin(\theta)\cos(\phi), 
 \cos(\theta))$$</p>
"
"2381951","2382669","<p>If you know the intermediate value theorem, then the proof is very easy. Just observe that if $f'(a)*f'(b)&lt;0$, at one endpoint the derivative is positive and on the other endpoint it is negative.</p>

<p>Concerning your proof:<br>
If $f$ is differentiable and monotonic, then the derivative $f'$ does not change its sign. If $f$ is also stric monotonic, then also $f' \ne 0$ and thus $f'(a)*f'(b)&gt;0$. Additionally, if  $f$ has no local extremum, then $f$ is stric monotonic.</p>

<p>Now assume, that $f$ is differentable on $[a,b]$ and monotonically decreasing on $(a,b)$. Then 
$$ \lim_{x \searrow a} \frac {f(x)-f(a)}{x-a}\le0$$
because $f(x)&lt;f(a)$ if $x&gt;a$ and the limit exists per assumption.</p>
"
"2381966","2381986","<p>Let $A(n) = \# \{ a_m \le n\}$ and use summation by parts $$\sum_{m=1}^\infty \frac{1}{a_m}  = \sum_{n=1}^\infty \frac{A(n)-A(n-1)}{n} =\lim_{N \to \infty} \frac{A(N)}{N}+\sum_{n=1}^{N-1} A(n)(\frac{1}{n}-\frac{1}{n+1})$$ </p>
"
"2381968","2382206","<p>Let $\{f_n\}$ be given, such that $f_n \rightharpoonup f$ in $W^{1/2,2}(\partial\Omega)$.
We have to show $\| f_n - f\|_{L^q(\partial\Omega)} \to 0$.</p>

<p>By HÃ¶lder's inequality, we have
$$
\| f_n - f \|_{L^q(\partial\Omega)}
\le
\| f_n - f \|_{L^1(\partial\Omega)}^{\lambda}\,
\| f_n - f \|_{L^{q^*}(\partial\Omega)}^{1-\lambda},
$$
where $\lambda \in (0,1)$ is determined by $1/q = \lambda/1 + (1-\lambda)/q^*$.
This inequality is typically called an 'interpolation inequality, because you can interpolate between the norms in $L^1$ and $L^{q^*}$.</p>

<p>Now, since the embedding to $L^1$ is compact, you have
$$
\| f_n - f \|_{L^1(\partial\Omega)} \to 0
$$
and since the embedding to $L^{q^*}$ is bounded, you have
$$
\| f_n - f \|_{L^{q^*}(\partial\Omega)} \le C
$$
for some $C &gt; 0$.</p>

<p>Putting things together, you have the desired 
$\| f_n - f\|_{L^q(\partial\Omega)} \to 0$.</p>
"
"2381969","2382295","<p>This is what I got from <em>Mathematica</em>
$$G_{1,4}^{2,0}\left(z\left|
\begin{array}{c}
 1 \\
 0,0,\frac{1}{2},\frac{1}{2} \\
\end{array}
\right.\right)=-\frac{1}{\pi}\left[3\gamma -4 z \, _2F_4\left(1,1;\frac{3}{2},\frac{3}{2},2,2;-z\right)+\log 16z \right]$$</p>

<p>Hope it helps</p>

<p>Edit.</p>

<p>Mathematica code</p>

<pre><code>MeijerG[{{},{1}},{{0,0},{1/2,1/2}},z]//FunctionExpand
</code></pre>
"
"2381981","2382525","<p><a href=""http://www.ams.org/journals/proc/1973-041-02/S0002-9939-1973-0334161-3/S0002-9939-1973-0334161-3.pdf"" rel=""nofollow noreferrer"">This paper</a> by Mardesic shows:</p>

<blockquote>
  <p>Let $X \subseteq \mathbb{R}^n$ which is compact and $k$-dimensional. Then there are coordinates $1 \le i_1 &lt; i_2 \ldots i_k \le n$ such that $\operatorname{int}(\pi_{i_1, \ldots, i_k}[X]) \neq \emptyset$.</p>
</blockquote>

<p>Now, your $K$ does not obey $\dim(K)= 0$ because that implies that $K$ is totally disconnected while it is connected. $\dim(K) =2$ is ruled out by the above theorem (or even on its quoted inspiration: a subset of $\mathbb{R}^n$ has dimension $n$ iff it has non-empty interior) as $K$ has empty interior.</p>

<p>So $\dim(K) = 1$ is forced.</p>
"
"2381994","2382004","<p>$(x+a)(x-a)=x^2-a^2$ so collect these terms to obtain a third degree polynomial in $x^2$.  Solve that as normal.</p>

<p>${(x+1)(x+2)(x+3)(x-1)(x-2)(x-3) \\\qquad=~ (x^2-1)(x^2-4)(x^2-9) \\\qquad=~ x^{2\cdot 3}-(\phantom{1+4+9})x^{2\cdot 2}+(\phantom{4\cdot 9+4+9})x^2-1\cdot 4\cdot 9 \\ \qquad\quad\ddots}$</p>
"
"2381999","2382009","<p>The triangle of coefficients you have is OEIS sequence <a href=""http://oeis.org/A185296"" rel=""noreferrer"">A185296</a> and there is a falling factorial formula which is very close to binomial coefficients as you suspected. A signed version is OEIS sequence <a href=""http://oeis.org/A059343"" rel=""noreferrer"">A059343</a> which is the coefficients of the Hermite 
polynomials $H_n(x)$.</p>
"
"2382001","2382003","<p>Just apply the identity matrix and convolve with a Gaussian filter of order $n$:</p>

<p>$$ \left[\begin{matrix} 00 &amp; 01 &amp; 02 \\ 10 &amp; 11 &amp; 12 \\ 20 &amp; 21 &amp; 22 \end{matrix}\right] $$</p>
"
"2382006","2382022","<p>The set $\mathbb C\cup \{\infty\}$ is homeomorphic to $S^2$, for instance by <em>stereographic projection</em>.  Under this homeomorphism,  $\{\infty\} $ gets sent to the ""north pole"".  The upper hemisphere corresponds precisely to $\{z\in \mathbb C:|z|\ge1\} $.  To do this draw the line through the north pole and each point of the sphere until you hit the complex plane. ..</p>
"
"2382007","2382039","<p>Well, for $0\leq z\leq 1$ and $b&gt;0, c&gt;0$ we have:
$$\int_0^{z} y^{b-1} (z-y)^{c-1}\mathrm d y = \dfrac{\Gamma(b)\Gamma(c) z^{b+c-1}}{\Gamma(b+c)}$$</p>

<p>So for $0\leq x\leq 1$ and $p_1,p_2,p_3\in\Bbb R^+$</p>

<p>$$\int_0^{1-x} \dfrac{\Gamma(p_1+p_2+p_3)x^{p_1-1}y^{p_2-1} (1-x-y)^{p_3-1}}{\Gamma(p_1)\Gamma(p_2)\Gamma(p_3)}\mathrm d y = \dfrac{\Gamma(p_1+p_2+p_3) x^{p_1-1}(1-x)^{p_2+p_3-1}}{\Gamma(p_1)\Gamma(p_2+p_3)}$$</p>

<p>Which is the probability density function for $\mathcal{Beta}(p_1,p_2+p_3)$</p>

<hr>

<p><strong>Note:</strong> that requires the denominator to be a product of gamma functions rather than a sum, as I suspect you should have.</p>
"
"2382011","2382171","<p>For $a\times b \equiv \bmod m$ they use a quadratic multiplication / reduction algorithm with a complexity of $O(\log(m)^2)$. Multiply this with the number of loops, i.e. $k=\log(n),$ and you get $O(\log(m)^2 \times \log(n)).$ </p>

<p>Note that the square <code>power*power</code>is computed $k$ times, but <code>x*power</code> only $k/2$ on average (depending on the bit count of $a$).</p>
"
"2382013","2382029","<p>Consider a sphere $S $ of radius $r$ where $K$ is inscribed in $S$.</p>

<p>Here inscribed implies that $K$ is in $r$-ball $B$ with a boundary
$S$ and there is a point $x\in K$ s.t. $x$ is in $S$.</p>

<p>Then $x$ is an extreme point of $K$. If not, there is $p,\ q\neq p
\in K$ s.t. $x$ is an interior point of a segment $[pq]$.</p>

<p>Note that $B$ can not contain a segment $[pq]$ so that $[pq]$ is not
in $K$. It is a contradiction.</p>
"
"2382014","2382138","<p>The claim is not true. There is a counterexample of order $32$ which is a semidirect product of $D_8\times C_2$ with $C_2$. I found this example by asking GAP to go through the groups and check. The group in question is the one with ID [32,49].</p>

<p>Note that what you have written up is the definition of the centralizer of that element, not the normalizer. Normalizers are usually defined for subgroups, but one can also define them for elements as the normalizer of the subgroup generated by them.</p>
"
"2382017","2382063","<p>Maybe this will make it clearer: put the forms together into one linear operator
$$
\Omega :TM\rightarrow \mathbb{R}^{n-k} \\
\Omega(X) := (\omega^1(X),\ldots,\omega^{n-k}(X)).
$$
Then pointwise
$$
\operatorname{Ker} \Omega = \bigcap_{i=1}^{n-k}\operatorname{Ker}\omega^i = D.
$$
So $\dim\operatorname{Ker}\Omega = \dim D = k$, and hence by rank-nullity
$$
\dim\operatorname{Im}\Omega = \dim TM - \dim\operatorname{Ker}\Omega = n-k.
$$
So $\Omega$ is surjective at each point of $U$, implying that the $\omega^i$ are linearly independent.</p>
"
"2382021","2382048","<p>If you add $u$ and $v$ together, you'll get
$$u+v=x^2+y^2+2xy=(x+y)^2.$$
Since $x+y\ge0$ due to the given constraint $y\ge-x$, we can take the square root, so
$$x+y=\sqrt{u+v}.$$
Similarly, if you subtract them, you'll get
$$u-v=x^2+y^2-2xy=(x-y)^2.$$
Again, since $x-y\ge0$ due to the given constraint $x\ge y$, we can take the square root, so
$$x-y=\sqrt{u-v}.$$</p>

<p>Now you can solve the system of equations
$$\left\{\begin{align} x+y&amp;=\sqrt{u+v} \\ x-y&amp;=\sqrt{u-v} \end{align}\right.$$</p>

<hr>

<p><strong>UPDATE:</strong> now that the change of variable issue has been resolved, and we need to finish integration of the new double integral.</p>

<p>Your limits of integration for $v$ are incorrect. If you graph the region, you'll see that in the $(u,v)$-plane it's not going to be a rectangular region, and the limits of integration for $v$ are not from $-2$ to $2$ (even though these actually are its least and greatest values). Here are the correct constraints and how we can find them.</p>

<ul>
<li><p>From $y\le x$, we have an ""upper"" boundary given by
$$y=x \; \Rightarrow \; x-y=0 \; \Rightarrow \; \sqrt{u-v}=0 \; \Rightarrow \; u-v=0 \; \Rightarrow \; v=u.$$</p></li>
<li><p>From $-x\le y$, we have a ""lower"" boundary given by
$$y=-x \; \Rightarrow \; x+y=0 \; \Rightarrow \; \sqrt{u+v}=0 \; \Rightarrow \; u+v=0 \; \Rightarrow \; v=-u.$$</p></li>
</ul>

<p>So $v$ ranges within $-u\le v\le u$ (reaching the values of $-2$ and $2$ only when $u=2$). And the correct double integral is
$$\frac{1}{4} \cdot \int_1^2 \int_{-u}^u \cos\left(\frac{\pi v}{2u}\right)\,dv\,du.$$</p>

<p>I hope you can take it from here. The inside integral is now with respect to $v$, which is actually doable.</p>
"
"2382026","2382036","<p><strong>Hint:</strong></p>

<p>By expanding we have 
\begin{align*}
(x+y)^2&amp;=(x+3)(y-3)\\
\iff x^2+2xy+y^2&amp;=xy-3x+3y-9\\
\iff x^2+xy+y^2+3x-3y+9&amp;=0\\
\iff \tfrac12(x+y)^2+\tfrac12(x+3)^2+\tfrac12(y-3)^2&amp;=0
\end{align*}
So we must have $$x+y=x+3=y-3=0$$</p>
"
"2382034","2382045","<p>Let $x\in X$ such that $x\in U$ implies $A\cap U\ne\emptyset$, for every open subset $U$ of $X$. To show $x\in A\cup A'$, we assume $x\not\in A$ and show $x\in A'$. Since $x\not\in A$, we know
$$x\in A' \iff x\in\overline{A\setminus\{x\}}=\overline{A}.$$
Thus, in order to conclude $x\in\overline{A}$, we just need to prove that every closed set containing $A$ also contains $x$. To this end, suppose by contradiction that $F$ is a closed set such that $A\subseteq F$ and $x\not\in F$. Then there is an open set $U$ such that $x\in U \subseteq F^c$. But this implies $U\cap A\subseteq U\cap F=\emptyset$, which contradicts our assumption on $x$. Therefore
$$ x\in \bigcap\{F\subseteq X\ \text{closed} \mid A\subseteq F\} = \overline{A}$$
and consequently $x\in A'$ as desired.</p>
"
"2382035","2382042","<p>To prove $Z(I)=X$ you need to show that if $x$, $y\in\Bbb C$ and $x^3=y^2$, there is
$t\in\Bbb C$ with $(x,y)=(t^2,t^3)$.</p>

<p>If $x=0$ then $y=0$ and you can take $t=0$.</p>

<p>If $x\ne0$, set $t=y/x$. Then $t^2=y^2/x^2=x^3/x^2=x$ and $t^3=xt=y$.</p>
"
"2382050","2382053","<p>The Zariski closure is just the $x$-axis. All you have to do is prove that
if a polynomial $f(x,y)$ has $f(n,0)=0$ for all $n\in\Bbb Z$ then $f(x,0)=0$ for all $x\in\Bbb C$.</p>

<p>Write $f(x,y)=g(x)+yh(x,y)$. Then $g(x)=0$ for all $n\in\Bbb Z$, so it's a polynomial with infinitely many zeroes...</p>
"
"2382051","2382480","<p>I think an <a href=""https://math.stackexchange.com/questions/1921534/operation-in-homotopy-group-agrees-with-h-space-multiplication"">Eckmann-Hilton argument</a> would show that the group structure on $\pi_k(O(n))$ coming from the fact that $O(n)$ is a group agrees with that coming from the fact that $S^k$ is a cogroup, at least when $k \geq 1$.  Moreover, this operation will be abelian.  So the conjugation action is trivial.  </p>

<p>So, we see that the choice of framing is not important.  This should help answer the rest of your questions.  </p>
"
"2382052","2382100","<p>Usually this means that you will have developed your skills in not only proof-writing, but also proof-reading$^\dagger$. Moreover, depending on the level of the course, this means that you will be able to fill in omitted bits of reasoning on your own. </p>

<p>$\dagger$ In particular, you should be able to actually extract the reasoning behind the construction of a well-written proof, rather than simply the steps.  </p>
"
"2382057","2382121","<p>For part b. </p>

<p>We have $$\alpha_1 + \alpha_{2011} = 0$$</p>

<p>$$\alpha_1 + \alpha_2 = 0$$</p>

<p>$$\vdots$$</p>

<p>$$\alpha_{2010}+\alpha_{2011}=0$$</p>

<p>Let's write this in matrix form.</p>

<p>$$\begin{bmatrix} 1 &amp; 0 &amp; \ldots &amp; \ldots &amp; 0 &amp; 1 \\ 1 &amp; 1 &amp; 0 &amp; \ldots &amp; \ldots  &amp; 0 \\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; \ldots &amp; 0 \\ \vdots &amp; \vdots &amp;  \vdots &amp; \vdots &amp; \vdots &amp;  \vdots \\ &amp; 0 &amp; 0  &amp; \ldots  &amp; 1 &amp; 1\end{bmatrix} \begin{bmatrix} \alpha_1 \\ \vdots \\ \alpha_{2011}\end{bmatrix}=0$$</p>

<p>We can compute the determinant of the coefficient matrix by expanding along the first row which is equal to </p>

<p>\begin{align}&amp;(-1)^{1+1}\det\left(\begin{bmatrix}   1 &amp; 0 &amp; \ldots &amp; \ldots  &amp; 0 \\   1 &amp; 1 &amp; 0 &amp; \ldots &amp; 0 \\   \vdots &amp;  \vdots &amp; \vdots &amp; \vdots &amp;  \vdots \\ 0 &amp; 0  &amp; \ldots  &amp; 1 &amp; 1\end{bmatrix}\right) + (-1)^{2011+1} \det\left(\begin{bmatrix}   1 &amp; 1 &amp; \ldots &amp; \ldots  &amp; 0 \\   0 &amp; 1 &amp; 1 &amp; \ldots &amp; 0 \\   \vdots &amp;  \vdots &amp; \vdots &amp; \vdots &amp;  \vdots \\ 0 &amp; 0  &amp; \ldots  &amp; 0 &amp; 1\end{bmatrix}\right) \\&amp;= 1 + 1=2 \neq 0\end{align}</p>

<p>Hence we can conclude that $\alpha_i = 0, \forall i \in \{ 1, \ldots, 2011\}$.</p>

<p>Note that the two determinants can be evaluated easily as they are both triangular matrices with $1$ on the diagonals.</p>
"
"2382061","2382066","<p>Claim:</p>

<p>We have $$(\mathbf{1}^Tx)^2 \leq (\mathbf{1}^TG\mathbf{1})(x^TG^{-1}x)$$</p>

<p>Proof:</p>

<p>\begin{align} (\mathbf{1}^Tx)^2&amp;=(\mathbf{1}^TIx)^2\\&amp;=((\mathbf{1}^TG^{1/2})(G^{-1/2}x))^2\\
&amp;=((G^{1/2}\mathbf{1})^T(G^{-1/2}x))^2\\
&amp;\leq (\mathbf{1}^TG\mathbf{1}) (x^TG^{-1}x)\end{align}</p>

<p>by Cauchy-Schwarz inequality.</p>

<p>Note that $(\mathbf{1}^TG\mathbf{1})&gt;0$.</p>
"
"2382067","2382096","<p>$\begin{array}{rlll}F(D)~&amp;=~\{(x^2+y^2, 2xy) &amp;: 1\leq x^2+y^2\leq 2,&amp; -x\leq y\leq x&amp;\}
\\ &amp;=~ \{(r^2, 2r^2\cos\theta\sin\theta) &amp;: 1\leq r^2\leq 2,&amp; -r\cos\theta\leq r\sin\theta\leq r\cos\theta, -\pi\leq 2\theta\leq \pi&amp;\}
\\ &amp;=~ \{(r^2, r^2\sin2\theta) &amp;: 1\leq r^2\leq 2,&amp; \sin^2\theta\leq \cos^2\theta, -\pi\leq 2\theta\leq \pi&amp;\}
\\ &amp;=~ \{(r^2, r^2\sin2\theta)&amp;: 1\leq r^2\leq 2,&amp; 2\sin^2\theta\leq 1, -\pi\leq 2\theta\leq \pi&amp;\}
\\ &amp;=~ \{(r^2, r^2\sin2\theta)&amp;: 1\leq r^2\leq 2,&amp; -\arcsin2^{-1/2}\leq\theta\leq \arcsin2^{-1/2}&amp;\}
\\ &amp;=~ \{(r^2, r^2\sin2\theta)&amp;: 1\leq r^2\leq 2,&amp; -\sin(2\arcsin2^{-1/2})\leq\sin 2\theta\leq \sin(2\arcsin2^{-1/2})&amp;\}
\\ &amp;=~ \{(u, uw)&amp;: 1\leq u\leq 2,&amp; -1\leq w\leq 1&amp;\}
\\ &amp;=~ \{(u, v)&amp;: 1\leq u\leq 2,&amp; -u\leq v\leq u&amp;\}
\end{array}$</p>
"
"2382072","2382091","<p>A function $f$ from a set $X$ (called <strong>domain</strong>) into a set $Y$ (called <strong>codomain</strong>) is a rule that assigns every $x$ in $X$ to a unique $y$ in $Y$, where $y$ is usually denoted by $f(x)$. The function $f$ has an inverse if and only if it makes sense to reverse the rule on $f$. That is, if and only if for every $y$ in $Y$ there is a unique $x$ in $X$ such that $f(x)=y$. With this, there are two ways that the function can fail to have an inverse.</p>

<p>1) The function $f$ may map multiple elements of the domain to the same point in the codomain. As an example, suppose $f:\{0,1\}\to\{a\}$ is defined by $f(0)=f(1)=a$. This function doesn't have an inverse because defining $g:\{a\}\to\{0,1\}$ by $g(a)=0$ and $g(a)=1$ is not a valid function. Indeed,
 this $g$ does not satisfy the uniqueness part of the definition (or, as you may be familiar with, the ""vertical line test""). This doesn't work precisely because $f$ is not <strong>injective</strong>.</p>

<p>2) The function $f$ may not map to all elements of the codomain, so there is no way to define an inverse on the entire codomain. As an example, suppose $f:\{0\}\to\{a,b\}$ is defined by $f(0)=a$. This function does not have an inverse because defining $g:\{a,b\}\to\{0\}$ by $g(a)=0$ is not a valid function. To see this, note that $b$ is an element of the domain of $g$ that is left unassigned. This doesn't work precisely because $f$ is not <strong>surjective</strong>.</p>

<p>In fact, these are exactly the only two reasons that $f$ can fail to have an inverse. One can prove that $f$ has an inverse if and only if it is both injective and surjective.</p>
"
"2382076","2383215","<p>If $X_1, X_2, \dots, X_{10}$ is a random sample from $\mathsf{Norm}(\mu, \sigma),$
then $\bar X \sim \mathsf{Norm}(\mu, \sigma/\sqrt{n}).$ </p>

<p>Here is a systematic way to obtain the probability you seek:
$$P(\mu - \sigma \le \bar X \le \mu + \sigma)
= P(-\sigma \le \bar X - \mu \le \sigma)\\
= P\left(-\frac{\sigma}{\sigma/\sqrt{n}} \le \frac{\bar X - \mu}{\sigma/\sqrt{n}} \le \frac{\sigma}{\sigma/\sqrt{n}}\right)\\
= P(-\sqrt{n} \le Z \le \sqrt{n}) = \Phi(\sqrt{n}) - \Phi(-\sqrt{n}) = \cdots,$$
where $Z \sim \mathsf{Norm}(0,1).$</p>

<p>Computation in R statistical software (or you could use printed standard normal CDF tables):</p>

<pre><code>pnorm(sqrt(10)) - pnorm(-sqrt(10))  # in R 'pnorm' is standard normal CDF
## 0.9984346
</code></pre>

<p>As @zhoraster says, you got the right answer (+1). But with this systematic approach, you won't
have to <em>wonder</em> if you got it right.</p>
"
"2382081","2382106","<p>Take $u=2x-1$ then 
$$\to \begin{cases}x=0 \to u=2(0)-1 &amp; u=-1\\x=2 \to u=2(2)-1&amp; u=3 \\u=2x-1 \to du=2dx &amp; dx=\frac12 du\end{cases} $$so 
$$=\int_{0}^{2} f(2x-1)dx-\int_{0}^{2} xdx \\ 
= \int_{-1}^{3} f(u) \frac12 du -\int_{0}^{2} x dx \\ = \frac12\int_{-1 }^{3}f(u)du - \int_{0}^{2} xdx \\ =\frac12\int_{-1}^{3} f(x)dx-\int_{0}^{2} xdx \\ =\frac12 \times 10- [\frac{x^2}{2}]_{0}^{2}  \\ =5-2 \\ =3$$</p>
"
"2382082","2382103","<p>If you are looking for a expression for $Q^{-1}$ in terms of all the $c_{ij}$'s, the only generic way you can do so is either</p>

<p>1) Compute $Q^{-1}$ directly or</p>

<p>2) Express each $u_i$'s as a linear combination of all the $v_i$'s and take it from there.</p>

<p>However if both $B$ and $B'$ are both orthonormal bases, one can show that the transition matrix from one basis to another is in fact orthogonal, whose inverse can be simply obtained by taking transpose.</p>
"
"2382089","2382111","<p>Imagine you have an implicit relation between $x$ and $y$: 
$$f(x,y) = 0$$
Then the total differential of $f$ is exactly $0$. For your case let $y$ depend on $x$, clearly:
$$\frac{df(x,y(x))}{dx}=\frac{\partial f}{\partial x} + \frac{\partial f}{\partial y}\frac{dy}{dx}=0$$
As a result, we can obtain the derivative of $y$ wrt. $x$ as follows:
$$\frac{dy}{dx} = -\frac{\partial f}{\partial x}\bigg/\frac{\partial f}{\partial y} \tag{*}$$</p>

<p>The point is that you know how to compute analytically or numerically the derivatives involved in $(*)$</p>

<p>You have to compute $\frac{\partial S}{\partial N}$ and you know by the chain rule that:
$$\frac{\partial S(V,T,\mu(V,T,N))}{\partial N}=\frac{\partial S}{\partial \mu}\frac{\partial \mu}{\partial N}$$
Since you do not have an explicit relation like $\mu = \mu(V,T,N)$ you have to apply $(*)$ to the function $f(T,V,\mu,N)=N-N(T,V,\mu)=0$ with $x=N$ and $y=\mu$. Therefore you will have that the sought derivative is simply:
$$ \frac{\partial \mu}{\partial N}=-\frac{\partial f}{\partial N}\bigg/\frac{\partial f}{\partial \mu}=-1\bigg/\frac{\partial N}{\partial\mu}$$ </p>
"
"2382090","2382098","<p>Do a few more steps:</p>

<p>$$ \left[
    \begin{array}{ccc|c}
      1&amp;-2&amp;-3&amp;-1\\
      0&amp;1&amp;k+1&amp;k\\
      0&amp;-5-2k&amp;3k-3k&amp;-9k
    \end{array}
\right] $$</p>

<p>$$ \left[
    \begin{array}{ccc|c}
      1&amp;-2&amp;-3&amp;-1\\
      0&amp;1&amp;k+1&amp;k\\
      0&amp; 0 &amp;(3k-3k)+(k+1)(5+2k)&amp;-9k+k(5+2k)
    \end{array}
\right] $$</p>

<p>It has not solution when $(3k-3k)+(k+1)(5+2k)=0$ but $-9k+k(5+2k) \neq 0$.</p>

<p>Solve the quadratic equation 
$(3k-3k)+(k+1)(5+2k)=0$</p>

<p>and verify if the condition $-9k+k(5+2k) \neq 0$ hold.</p>
"
"2382092","2382181","<p>There are several differences here.</p>

<p>Note that in the balls and bins problem the only input data are $m$ and $n$, whereas for each instance of the stair climbing problem we are given $m$, $n$, and in addition  a data vector $(a_1,a_2,\ldots, a_m)\in{\mathbb N}_{\geq1}^m$. This complicates matters and necessitates a recursive or generating functions approach whose complexity increases with $m$.</p>

<p>But even if all $a_i$ were $=1$ (think of parallel stairs of different colors) there remains a difference: We are not only interested in how many jumps of which sizes, but in the full climbing history. This means that $5+6+1+3+6+2$ is not considered the same as $2+6+6+1+5+3$.</p>
"
"2382112","2382226","<p>Since there are six regions and three ways to paint each region, we could form $3^6$ color patterns if there were no restrictions. From these, we must exclude those in which fewer than three colors are used.</p>

<p>There are three ways to exclude one of the colors and $2^6$ ways to paint the regions with the remaining two colors.  Hence, there are 
$$\binom{3}{1}2^6$$
color patterns if one color is excluded.</p>

<p>However, if we subtract $\binom{3}{1}2^6$ from the total, we will have subtracted those cases in which two of the colors twice, once for each of the ways we could designate one of the colors as the excluded color.  Therefore, we must add those cases back.</p>

<p>There are three ways to exclude two of the colors and one way to paint all six regions with the remaining color.  Hence, there are 
$$\binom{3}{2}1^6$$
color patterns in which two of the colors have been excluded.</p>

<p>By the <a href=""https://en.wikipedia.org/w/index.php?search=Inclusion-Exclusion+Principle&amp;title=Special%3ASearch&amp;fulltext=Search"" rel=""nofollow noreferrer"">Inclusion-Exclusion Principle</a>, the number of color patterns in which all three colors are used at least once is 
$$3^6 - \binom{3}{1}2^6 + \binom{3}{2}1^6$$</p>
"
"2382113","2382123","<p>$E(XY)=E(E(XY\mid X))=E(XE(Y\mid X))$</p>

<p>$E(Y\mid X=N)=\sum_{y=1}^{N}\frac{y}{N}=\frac{N+1}{2}$</p>

<p>Then $$E(XE(Y\mid X))=E(X\cdot \frac{X+1}{2})=\frac{1}{2}[E(X^2)+E(X)]\\=\frac{1}{2}\cdot \Big[\frac{(n+1)(2n+1)}{6}+ \frac{n+1}{2}\Big]\\=\frac{(n+1)(n+2)}{6}$$</p>

<p>(Check if my calculation is ok or not!)</p>
"
"2382118","2382135","<p>$\begin{array}{rlll}F_N(y)~&amp;=~
\frac{1}{N+1}\sum_{n=0}^ND_n(y)=\frac{1}{N+1}\sum_{n=0}^N\sum_{k=-n}^ne^{iky}
\\ &amp;=~ \frac{1}{N+1}\sum_{m=-N}^Ne^{imy}\#\{k \in [0,N]:m \in [-k,k]\}
\\ &amp;=~ \frac{1}{N+1}\sum_{m=-N}^Ne^{imy}(N+1-|m|)=\sum_{m=-N}^Ne^{imy}(1-\frac{|m|}{N+1})
\end{array}$</p>
"
"2382119","2382124","<p>Combine your first example on one interval with your second on another.</p>
"
"2382120","2382177","<p>Without attempting to copy the notation, I will try to give the idea of what is happening here. The representation of the Klein bottle is a square. It has four sides and an interior. The opposite sides are identified, one pair with and the other against the orientation. The image of $\delta_2$ is the boundary of the interior of the square, and this is the sum of all four sides. Taking the identifications into account, one pair cancels and the other pair of opposite sides is doubled. This gives the image you mentioned. </p>

<p>As for the kernel of $\delta_1$ this is simply the two pairs of identified sides so you have </p>

<p>$$\ker \delta_1=\mathbb{Z}\oplus\mathbb{Z}$$</p>

<p>And $$\operatorname{im} \delta_2=0\oplus 2\mathbb{Z}$$ giving $$H_1(K)=\mathbb{Z}\oplus\mathbb{Z}_2$$ Hope this helps.</p>
"
"2382131","2382134","<p>Your $b$ is correct, but you should get $a = 3/b^2$ which is not $\sqrt{12}/12$.</p>
"
"2382133","2382143","<p>Subtracting a function with two poles gives you something analytic on the extended complex plane.  In particular it is entire and bounded, so Liouville says it is constant.</p>
"
"2382136","2382197","<p>For totally ordered sets you are right, the definitions are equivalent. For partially ordered sets this is not true. The problem is that the first definition doesn't imply that the preimage of comparable elements $a,b \in B$ have to be comparable in $A$.</p>

<p>Therefore, take $A = \{a, b\}$ with $a \leq a$ and $b \leq b$. Moreover, on the set $B = \{a', b'\}$ we take the order $a' \leq a', b' \leq b', a' \leq b'$. The mapping $a \mapsto a', b \mapsto b'$ is then a bijection which fulfills $(1)$ but not $(2)$.</p>
"
"2382140","2382156","<p>Suppose you have a Family of $n$ Inputs $\{I_j \}_{j \in \{ 1, \dots, n\}}$. We define a function of this $n$ Inputs by a function $f$ mapping the $n$-fold cartesian product of all the Inputs to an Output $J$, formally written as:</p>

<p>$f: \times_{j=1}^n I_j \rightarrow J$.</p>

<p>Note that this n-fold cartesian product $\times_{j=1}^n$ is an Operation that forms These $n$ sets $I_j$ to one single new set denoted by $I_{1,\dots,n}$. We can see elements of this set as $n$-tuples $(i_1,\dots,i_n) \in I_{1, \dots, n}$, where $i_1 \in I_1, \dots i_n \in I_n$.</p>
"
"2382153","2386470","<p>I have found the following estimation:</p>

<p>$4^{-x} \le 1-x \le 2^{-x}$</p>

<p>where the first inequality hold for $0 \lt x \le \frac{1}{2}$ and the second for $0 \lt x \lt 1$.</p>

<p>With some basic arithmetic rules we can get the bounds asked for in the question.</p>

<p>(For others it might also be helpful if someone could provide a proof)</p>
"
"2382165","2382169","<p>Take $x\in\overline A$. If $x\notin A^\circ$, then $x\in\overline A\setminus A^\circ=\operatorname{fr}(A)$. Here, I used that fact that $X\setminus A^\circ=\overline{X\setminus A}$.</p>
"
"2382192","2382211","<p>Your strategy is correct, but I don't agree with the figures.</p>

<p>$|U|=5^7$ as you correctly second-guessed.</p>

<p>$|A|=|B|=|C|=4^7$ since you are forming 7 letters words with an alphabet of 4 signs.</p>

<p>$|AB|=|BC|=|AC|=3^7$ since you are forming 7 letters words with an alphabet of 3 signs.</p>

<p>$|ABC|=2^7$ since you are forming 7 letters words with an alphabet of only 2 signs.</p>

<p>Hence the result is $5^7-3*4^7+3*3^7-2^7=35406$</p>
"
"2382200","2382761","<p>I find that your volume is incorrect. In the first place, we can say that</p>

<p>$$
V=2\pi\int x\Delta y(x)~dx\\
\Delta y(x)= \sqrt{4ax}-mx\\
\text{or}\\
V=\pi\int \Delta(x^2(y))~dy\\
\Delta(x^2)=\left(\frac{y^2}{m^2}-\frac{y^4}{(4a)^2} \right)
$$</p>

<p>I used the integration over $y$ and found that for $y\in[0,4a/m]$</p>

<p>$$V=\frac{2}{15}\frac{(4a)^3}{m^5}\pi$$</p>

<p>Then following through with your suggestion for</p>

<p>$$Vy_c=\int_0^{4a/m} y~dV=\frac{1}{12}\frac{(4a)^4}{m^6}\pi$$</p>

<p>There follows</p>

<p>$$y_c=\frac{5a}{2m}$$</p>

<p>as required. I have verified these results for arbitrary values of $a$ and $m$.</p>

<p>Also, notice that all of the quantities have dimensions. It must be that $a$ has the dimensions of <em>length</em> and $m$ is dimensionless. Accordingly, $V$ has dimensions of length cubed, while the centroid has dimensions of length.</p>
"
"2382203","2396002","<p>In most cases, the problem that one wants to solve is 
$$
\underset{x}{\text{minimize}}\ ||y-Ax||_2^2+\lambda||x||_0
$$
However, when using the $0$-""norm"", the problem is not convex and we have to solve a combinatorial problem. To avoids this, the $0$-""norm"" is often approximated with the $1$-norm, resulting in the Lasso problem you describe. As the $1$-norm is the closest convex approximation to the $0$-""norm"", we can solve the Lasso very efficiently using the powerful tools from convex optimization. </p>

<p>Now one could ask if there is a better approximation. To find a ""better"" approximation (closer to the original $0$-""norm""), it seems that we have to sacrifice the convexity of the problem. However, if we use $\sum_{i=1}^N \log(1+\frac{|x_i|}{\xi'})$ we can actually get a ""better"" approximation but still utilize convexity. To solve the optimization problem with the $\log$ penalty, we need to implement an iterative approach. Let's start with solving the Lasso, yielding the solution $x^{(0)}$. Next, we linearize the $\log$-penalty around the latest value (using the first Taylor terms)
$$
\arg \min_{u_{i}}\ \sum_{i=1}^N\left(\log(1+\frac{u^{(0)}_i}{\xi'})+\frac{1}{\xi'+u_i^{(0)}}(u_i-u_i^{(0)})\right)
$$
which yields
$$
\arg \min\ \sum_{i=1}^N \frac{|x_i|}{\xi'+|x_i^{(0)}|}
$$
(here I used $u_i$ just to avoid the case when $x_i=0$ in $|x_i|$. One has to do some more work but this works as intuition). So now we have a iterative approach: Solve the weighted lasso using the latest solution, update the weights $\left(\frac{1}{\xi'+|x_i^{(k)}|}\right)$, solve again. Here $(\cdot)^{(k)}$ denotes the $k$th iteration. In each iteration, the problem is convex. Here, the constant $\xi'$ is added to avoid numerical problems when $x_i=0$. </p>

<p>An onther intuition can be found by realizing that when $k$ tends to infinity we get $|x_i^{k+1}-x_i^{k}|\rightarrow 0$ which yields
$$
\sum_{i=1}^N \frac{|x_i^{k+1}|}{\xi'+|x_i^{(k)}|}\approx ||x||_0
$$
Hope it helps!</p>
"
"2382204","2382222","<p>Careful. We need to distinguish between ""$X$ tells the truth"" and ""$X$ claims that $Y$ told the truth"". The former always happens with probability $1/3$, while the latter depends on whether or not $Y$ is true or false. Let's revisit part (a) and redefine our events. Define:</p>

<ul>
<li>$X_C$: $C$ makes a true statement.</li>
<li>$X_A$: $A$ claims that $C$ makes a true statement.</li>
</ul>

<p>We're asked to find the probability that $C$ made a true statement, given that $A$ claims that $C$ made a true statement:
\begin{align*}
\Pr[X_C \mid X_A]
&amp;= \frac{\Pr[X_CX_A]}{\Pr[X_A]} \\
&amp;= \frac{\Pr[X_CX_A]}{\Pr[X_CX_A] + \Pr[\overline{X_C}X_A]} \\
&amp;= \frac{\frac{1 \cdot 1}{9}}{\frac{1 \cdot 1}{9} + \frac{2 \cdot 2}{9}} \\
&amp;= \frac{1}{1 + 4} \\
&amp;= \frac{1}{5}
\end{align*}</p>

<p>Can you see how to generalize this for part (b)?</p>

<hr>

<p>EDIT: Here's part (b). Define:</p>

<ul>
<li>$X_C$: $C$ makes a true statement.</li>
<li>$X_B$: $B$ claims that $C$ makes a true statement.</li>
<li>$X_A$: $A$ claims that $B$ makes a true statement.</li>
</ul>

<p>We're asked to find the probability that $C$ made a true statement, given that $A$ claims that $B$ made a true statement:
\begin{align*}
\Pr[X_C \mid X_A]
&amp;= \frac{\Pr[X_CX_A]}{\Pr[X_A]} \\
&amp;= \frac{\Pr[X_CX_BX_A] + \Pr[X_C\overline{X_B}X_A]}{\Pr[X_CX_BX_A] + \Pr[X_C\overline{X_B}X_A] + \Pr[\overline{X_C}X_BX_A] + \Pr[\overline{X_C} \, \overline{X_B}X_A]} \\
&amp;= \frac{\frac{1 \cdot 1 \cdot 1}{27} + \frac{1 \cdot 2 \cdot 2}{27}}{\frac{1 \cdot 1 \cdot 1}{27} + \frac{1 \cdot 2 \cdot 2}{27} + \frac{2 \cdot 2 \cdot 1}{27} + \frac{2 \cdot 1 \cdot 2}{27}} \\
&amp;= \frac{1 + 4}{1 + 4 + 4 + 4} \\
&amp;= \frac{5}{13}
\end{align*}</p>

<p>This diagram might help:</p>

<p><a href=""https://i.stack.imgur.com/TiSei.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TiSei.jpg"" alt=""enter image description here""></a></p>
"
"2382205","2383822","<p>What you are trying to show is not correct. To see this, assume that your claim holds.</p>

<p>Let $(g_\ell)_{\ell \in \Bbb{N}}$ be an arbitrary sequence with $\| g_\ell \|_{L^1} = 1$ and $g_\ell \geq 0$. For $n \in \Bbb{N}$, define
$$
\phi_n := g_\ell \text{ for the unique } \ell \in \Bbb{N} \text{ with } 2^\ell -1 \leq n &lt; 2^{\ell+1} -1.
$$
By the assumed claim of yours, we get because of $\phi_n \geq 0$ for all $n$ that
\begin{align*}
\infty
 &gt; \sup_{n \in \Bbb{N}} \frac{s_n}{n}
&amp;\geq \sup_{\ell \in \Bbb{N}} \frac{\phi_{1} + \dots + \phi_{2^{\ell + 1} - 2}}{2^{\ell + 1} - 2} \\
&amp;\geq \sup_{\ell \in \Bbb{N}} \frac{\phi_{2^\ell - 1} + \dots + \phi_{2^{\ell + 1} - 2}}{2^{\ell + 1} - 2} \\
&amp;= g_\ell \cdot \frac{2^{\ell + 1} - 2 - (2^{\ell} - 1) + 1}{2^{\ell + 1} - 2} \\
&amp;\geq g_\ell \cdot \frac{2^\ell}{2^{\ell + 1} - 2}
\geq \frac{g_\ell}{2},
\end{align*}
for arbitrary $\ell \in \Bbb{N}$.</p>

<p>Thus, we have shown that if your claim was true, then the following holds:</p>

<blockquote>
  <p>For each sequence $(g_\ell)_\ell$ with $g_\ell \geq 0$ and $\| g_\ell \|_{L^1} = 1$, we have $\sup_{\ell \in \Bbb{N}} g_\ell &lt; \infty$ almost everywhere.</p>
</blockquote>

<p>But this is clearly false: As a counterexample, let $X_n \overset{iid}{\sim} U([0,1])$, i.e., the $X_n$ are (jointly) independent, and each $X_n$ is uniformly distributed in $[0,1]$. Next, set $Y_n := \sqrt{n} \cdot 1_{X_n \leq 1/\sqrt{n}}$. Note that $Y_n \geq 0$ and $\| Y_n \|_{L^1} = 1$. But we have
$$
\sum_{n=1}^\infty \Bbb{P} ( Y_n \geq \sqrt{n}) = \sum_n \Bbb{P} (X_n \leq 1/\sqrt{n}) = \sum_n \frac{1}{\sqrt{n}} = \infty,
$$
so that the <a href=""https://en.wikipedia.org/wiki/Borel%E2%80%93Cantelli_lemma#Converse_result"" rel=""nofollow noreferrer"" title=""Second part of the Borel-Cantelli Lemma"">second part of the Borel-Cantelli Lemma</a> shows
$$
\Bbb{P} ( Y_n \geq \sqrt{n} \text{ infinitely often}) = 1.
$$
Therefore, we actually have $\sup_n Y_n = \infty$ almost surely.</p>

<blockquote>
  <p>All in all, we have thus shown that the claim you wanted to prove is false.</p>
</blockquote>
"
"2382210","2382350","<p>For the purposes of this answer, let superscript denote compositions. You want to evaluate:
$$y=\lim_{n \to \infty} \cos^n{(x)}$$
$$y=\lim_{n \to \infty} \cos{\left(\cos^{n-1}{(x)}\right)}$$
$n-1$ goes to infinity the same as $n$
$$y=\lim_{n \to \infty} \cos^{n-1}{(x)}$$
$$y=\cos{y}$$
The unique solution to this equation is called the Dottie number $=D \approx 0.739085$. Now to prove convergence for all initial real $a_0$. After one composition, $-1 \leq a_n \leq 1$. Now that the sequence is bounded, we want to prove that it is either monotonically increasing or decreasing. If it is, it must converge to something within the interval. This cannot be done for the sequence as it is because it alternates about $D$. The operation must be changed from $\left (a_{n+1}=\cos{a_n}\right)$ to $\left(a_{n+1}=\cos{\left(\cos{a_n}\right)}\right)$
We want to prove that:
$$a_n \leq a_{n+1} \leq D \quad \text{if} \quad a_n \leq D$$
$$a_n \geq a_{n+1} \geq D \quad \text{if} \quad a_n \geq D$$
$$\text{Define } f(x)=\cos{\left(\cos{x}\right)}-x \quad\quad f(D)=0$$
Since $D$ is the only zero of $f(x)$, $f(x)$ is continuous, and the end behaviors $\infty, -\infty$:
$$f(x) \geq 0 \text{ when } x \leq D$$
$$f(x) \leq 0 \text{ when } x \geq D$$
The relationship between adjacent terms in the sequence is verified. Next, we must show that the sequence stays on the same side of $D$. Observe that after two compositions, $a_n$ is bounded: $0 \leq \cos{1} \leq a_n \leq 1 \quad$ since $\quad 1 \leq \frac{\pi}{2}$
$$\text{Define } f(x)=\cos{\left(\cos{x}\right)}-D \quad\quad f(D)=0$$
$$f'(x)=\sin{x}\sin{\left(\cos{x}\right)}$$
$$f'(D)=\sin^2{D}$$
Since the derivative is positive at $D$, $f(x)$ is continuous, and $D$ is the only zero of $f(x)$ in the interval $[0, 1]$:
$$f(x) \geq 0 \text{ when } D \leq x \leq 1$$
$$f(x) \leq 0 \text{ when } 0 \leq x \leq D$$
Therefore, the two original inequalities are proven, and the sequence converges to $D$.</p>
"
"2382213","2382561","<p>Suppose that $X$ is your random variable. Then the moment generating function is $M_X(t) = \mathbb{E}[e^{tX}]$, where $t$ is a real number, though not necessarily <em>any</em> real number (you might only be allowed to pick $t$ within a certain range).</p>

<p>There is no non-mathematical interpretation as to what a moment generating function is. It is purely a mathematical concept (it is related to Fourier transforms). That said, moment generating functions are useful since a random variable with some distribution has a unique moment generating function, and a moment generating function maps to some unique distribution. Thus, given a moment generating function, it is possible to identify the distribution it is associated with.</p>

<p>For example, suppose $X$ is a coin flip with probability $p$ of appearing heads, and equals 1 when heads, 0 when tails. The moment generating function can easily be calculated:</p>

<p>$$M_X(t) = \mathbb{E}[e^{tX}] = p e^{t \times 1} + (1 - p) e^{t \times 0} = p e^t + (1 - p)$$</p>

<p>I could even generalize this and say that when the coin lands heads-up, $X = A$ and when it lands tails-up, $X = B$ (instead of always 1 or 0) and the moment generating function would be:</p>

<p>$$M_X(t) = \mathbb{E}[e^{tX}] = p e^{At} + (1 - p) e^{Bt}$$</p>

<p>Suppose then I were told that the moment generating function for $Y$ is $M_Y(t) = \frac{1}{2} e^t + \frac{1}{2}$. A model of a fair coin that yields $Y = 1$ when the coin lands heads-up and $Y = 0$ when the coin lands tails-up would fit this moment generating function, so that <em>must</em> be the distribution of $Y$.</p>

<p>Moment generating functions are used extensively since they allow for easily computing the distribution of transformations of random variables. Suppose I wanted to identify the distribution of the sum of two independent random variables $X$ and $Y$; this could be a painful computation, but thanks to moment generating functions, I can say:</p>

<p>$$M_{X + Y}(t) = \mathbb{E}[e^{t(X + Y)}] = \mathbb{E}[e^{tX}e^{tY}] = \mathbb{E}[e^{tX}] \mathbb{E}[e^{tY}] = M_X(t) M_Y(t)$$</p>

<p>That is, just multiply the random variables moment generating functions together and you get the moment generating function for the distribution of their sum, which you can use to identify their distribution. (Exercise: Look up the moment generating function of Normally-distributed random variables with mean $\mu$ and variance $\sigma^2$, and use this technique to find the distribution of $X + Y$ when $X \sim N(1, 1)$ and $Y \sim N(2, 3)$, with $X$ and $Y$ independent.)</p>

<p>Moment generating functions get their name from the fact that they can be used to compute the expected values of moments of random variables (where the $n$th moment of a random variable $X$ is $\mathbb{E}[X^n]$). For example, to get the 1st moment, first take the derivative of the moment generating function:</p>

<p>$$M'_X(t) = \frac{d}{dt}\mathbb{E}[e^{tX}] = \mathbb{E}[\frac{d}{dt}e^{tX}] = \mathbb{E}[Xe^{tX}]$$</p>

<p>Set $t = 0$ to get $M'_X(0)=\mathbb{E}[X]$. Take more derivatives to get more moments. (Exercise: Use the moment generating function of a standard Normal random variable to compute $\mathbb{E}[X]$, $\mathbb{E}[X^2]$, $\mathbb{E}[X^3]$, and $\mathbb{E}[X^4]$.)</p>

<p>Moment generating functions have a lot of useful properties but I'll let you look those up. Often moment generating functions for common distributions are distributed in tables, already computed for you. The <a href=""https://en.wikipedia.org/wiki/Moment-generating_function#Examples"" rel=""nofollow noreferrer"">Wikipedia article</a> has one.</p>
"
"2382215","2382218","<p>If you have a non-singular square matrix, there are situations where the entry of your matrix are very small numbers. That can lead to eigen-values very close zero. if you write a condition number of those matrices, you would see that the number may grow unbounded.</p>
"
"2382223","2382238","<p>For a smooth map $\phi:(M,g)\to(N,h)$ between two Riemannian manifolds $(M,g)$ and $(N,h)$, its <em>energy</em> $E(\phi)$ is defined by
$$E(\phi)={1\over 2}\int_M |d\phi(x)|^2\;dM,$$
where $|d\phi(x)|^2$ is the square of the norm of the differential $d\phi(x)$ at a point $x\in M$.</p>

<p>A map $\phi:(M,g)\to(N,h)$ is called <em>harmonic</em> if $\phi$ is a critical point of the energy functional $E$.</p>

<p>In the literature, one can also find the notion of <em>quasi-harmonic</em> map. Suppose that $N$ is a smooth compact Riemannian manifold. We call $\phi:(M,g)\to(N,h)$ a quasi-harmonic map, if $\phi$ is a non-constant smooth map which is a critical point of the <em>quasi-energy</em> functional $E_q$ with respect to any smooth compactly supported variation, where
$$E_q(\phi)={1\over 2}\int_M |d\phi(x)|^2e^{-{|y|^2\over 4}}\;dy.$$</p>
"
"2382225","2382262","<p>There are $\binom{7}{k}$ ways to fill exactly $k$ of the seven positions with a $\beta$ and $2^{7 - k}$ ways of filling the remaining positions with one of the other two letters.  Of these $2^{7 - k}$ ways of filling the remaining positions with $\alpha$ or $\gamma$, only one way involves using only $\gamma$.  Hence, there are 
$$\binom{7}{k}(2^{7 - k} - 1)$$
ways to fill exactly $k$ of the positions with $\beta$s if at least one $\alpha$ is used.  Thus, the number of seven-letter words with at least two $\beta$s and at least one $\alpha$ is 
$$\sum_{k = 2}^{7} \binom{7}{k}(2^{7 - k} - 1) = \binom{7}{2}(2^5 - 1) + \binom{7}{3}(2^4 - 1) + \binom{7}{4}(2^3 - 1) + \binom{7}{5}(2^2 - 1) + \binom{7}{6}(2^6 - 1) + \binom{7}{7}(2^0 - 1) = 1491$$ </p>
"
"2382230","2382265","<p>I will assume that you are only interested in real values. That is,</p>

<blockquote>
  <p><strong>Assumption.</strong> $A$, $B$, $C$, $\alpha $ are all in $\mathbb{R}$.</p>
</blockquote>

<p>In particular,</p>

<p>$$f(x) = A\cosh x + B\sinh x + C$$</p>

<p>is a real-valued function on $\mathbb{R}$. Then this setting allows us to avoid some pesky issue regarding the branch cut of the complex logarithm. So let us compute the integral.</p>

<ul>
<li><p>Let $p, q$ be zeros of the quadratic polynomial $\frac{A+B}{2}t^2 + Ct + \frac{A-B}{2}$. In other words, they satisfy the identity  $ \frac{A+B}{2}t^2 + Ct + \frac{A-B}{2} = \frac{A+B}{2}(t - p)(t - q)$. Then plugging $t = e^{\alpha x}$, we find that</p>

<p>$$f(x) = \frac{A+B}{2} e^{\alpha x} (1-pe^{-\alpha x}) (1-qe^{-\alpha x}) $$</p>

<p>In particular, we have</p>

<p>\begin{align*}
\log |f(x)|
= \log\left|\frac{A+B}{2}\right| + \alpha x + \log |1-pe^{-\alpha x}| + \log |1-qe^{-\alpha x}|. \tag{1}
\end{align*}</p>

<p>(Here, I am assuming that you are only interested in the region where $f(x) &gt; 0$. Then taking absolute values causes no harm.)</p></li>
<li><p>In order to integrate the above equation, we need a special function called the <a href=""https://en.wikipedia.org/wiki/Polylogarithm#Dilogarithm"" rel=""nofollow noreferrer""><em>dilogarithm</em></a>. It is the function which is denoted by $\operatorname{Li}_2$ and is defined by</p>

<p>$$ \operatorname{Li}_2(z) = -\int_{0}^{z} \frac{\log(1-t)}{t} \, dt = -\int_{0}^{1} \frac{\log(1-zu)}{u} \, du, \quad z \in \mathbb{C}\setminus[1,\infty). $$</p>

<p>(Here, the first integral is taken along the line segment joining $0$ and $z$. The equivalence of two integrals can be shown by the substitution $t = zu$.) So $\operatorname{Li}_2(z)$ is not differentiable along the branch cut $[1,\infty)$. On the other hand, its real part behaves much better since</p>

<p>$$ \operatorname{Re}\operatorname{Li}_2(z)
= -\int_{0}^{1} \frac{\log|1-zu|}{u} \, du \tag{2}$$</p>

<p>A bit of complex analysis tells that both sides of $\text{(2)}$, understood as function of real variables $x$ and $y$ (with $z = x+iy$), extends to a smooth on all of $\mathbb{R}^2\setminus\{(1,0)\}$. As a useful consequences, for any $0 &lt; a &lt; b$ we have</p>

<p>\begin{align*}
-\int_{a}^{b} \frac{\log|1-zu|}{u} \, du
&amp;= -\int_{0}^{b} \frac{\log|1-zu|}{u} \, du - \left( -\int_{0}^{a} \frac{\log|1-zu|}{u} \, du \right) \\
&amp;= -\int_{0}^{1} \frac{\log|1-bzv|}{v} \, dv - \left( -\int_{0}^{1} \frac{\log|1-azw|}{w} \, dw \right) \\
&amp;= \operatorname{Re}\operatorname{Li}_2(bz) - \operatorname{Re}\operatorname{Li}_2(az).
\end{align*}</p>

<p>(Here, we utilized the substitution $u = bv$ and $u = aw$ for respective terms.) This is useful when computing some indefinite integrals to come.</p></li>
<li><p>Using the previous computation, we can integrate $\text{(1)}$. Indeed, let us substitute $u=e^{-\alpha x}$. Then</p>

<p>$$\int \log|1 - p e^{-\alpha x}| \, dx
= -\frac{1}{\alpha}\int \frac{\log|1 - pu|}{u} \, du
= \frac{1}{\alpha}\operatorname{Re}\operatorname{Li}_2(pu) + \text{constant} $$</p>

<p>This remains true if we replace $p$ by $q$. Therefore we get</p></li>
</ul>

<blockquote>
  <p>\begin{align*}
&amp;\int \log|f(x)| \, dx \\
&amp;\hspace{1em}= x\log\left|\frac{A+B}{2}\right| + \frac{\alpha x^2}{2} + \frac{1}{\alpha}\operatorname{Re}\operatorname{Li}_2(pe^{-\alpha x}) + \frac{1}{\alpha}\operatorname{Re}\operatorname{Li}_2(qe^{-\alpha x}) + \text{constant}.
\end{align*}</p>
</blockquote>

<hr>

<p><strong>Example.</strong> Let $A = -1$, $B = 2$, $C = 1$ and $\alpha = 3$. Then both computation shows that</p>

<p>$$ \int_{-2}^{3} \log |f(x)| \, dx \approx 17.88480462181253086\cdots. $$</p>

<p>The following is an actual computation using <em>Mathematica 11</em>:</p>

<p><a href=""https://i.stack.imgur.com/NQ78o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NQ78o.png"" alt=""numerical computation""></a></p>
"
"2382235","2382269","<p>Let $\sin{x}=a$ and $\cos{x}=b$.</p>

<p>Hence, $a^2+b^2=1$ and by AM-GM we obtain:
$$\sec x+\csc x+\sec^{2}x+\csc^{2}x=$$
$$=\frac{a+b}{ab}+\frac{1}{a^2b^2}\geq\frac{2\sqrt2}{\sqrt{a^2+b^2}}+\frac{4}{(a^2+b^2)^2}=4+2\sqrt2.$$
The equality occurs for $a=b=\frac{1}{\sqrt2}$, which says that we got a minimal value.</p>
"
"2382245","2383285","<h2>Hyperbolic case</h2>

<p>First off, the combinatorics of tilings in hyperbolic geometry will also affect the size of the cells. So I'd guess a hyperbolic bee might be more interested in having approximately bee-sized cells, even if it means using slightly more wax. But that's a question for hyperbolic biology, so back to the geometry.</p>

<p>The way you quoted it, you'd be minimizing line length per number, which has a length unit remaining. Thus the optimum is not invariant under scale. But the video mentions that the cells should have unit area. So if you had a honeycomb with cells of area 9 square length units, you'd scale it down by 3 length units and then start measuring edge lengths. Or in other words, you are minimizing perimeter divided by the square root of the enclosed area. You could as well square that and minimize squared perimeter divided by enclosed area. If you care about absolute numbers, particularly comparing them with the limit process described in the video, you might want to divide the perimeter by two since each wall is shared by two adjacent cells, so the contribution per cell is just half of that. But for the sake of finding the optimum it does not matter.</p>

<p>Let's consider a regular hyperbolic tiling of $n$-gons, $m$ of them meeting at each corner, with $\frac1m+\frac1n&lt;\frac12$. What's its edge length? The <a href=""https://en.wikipedia.org/wiki/Hyperbolic_law_of_cosines"" rel=""nofollow noreferrer"">hyperbolic law of cosines</a> for curvature $-1$ states</p>

<p>$$\cosh a=\frac{\cos\alpha+\cos\beta\cos\gamma}{\sin\beta\sin\gamma}$$</p>

<p>Now consider a right triangle covering $1/2n$ of your cell. It has $\alpha=\frac\pi n$ at the center, $\beta=\frac\pi m$ at the vertex and $\gamma=\frac\pi 2$ at the center of the edge of the cell. This leads to</p>

<p>$$a=\operatorname{arcosh}\frac{\cos\frac\pi n}{\sin\frac\pi m}$$</p>

<p>The area of that triangle is equal to the angle defect:</p>

<p>$$A=\pi-\frac\pi n-\frac\pi m-\frac\pi 2=\pi\left(\frac12-\frac1n-\frac1m\right)$$</p>

<p>Now the whole cell is composed of $2n$ such triangles, so the total perimeter is $2na$ and the total area is $2nA$. Thus the number to optimize is</p>

<p>$$\frac{(2na)^2}{2nA}=\frac{2n\left(\operatorname{arcosh}\frac{\cos\frac\pi n}{\sin\frac\pi m}\right)^2}{\pi\left(\frac12-\frac1n-\frac1m\right)}$$</p>

<p>Now you can try this out for some values of $m$ and $n$:</p>

<p>$$\begin{array}{c|ccccccc}
n\backslash m &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 \\\hline
3 &amp; &amp; &amp; &amp; &amp; 23.8496 &amp; 26.7747 &amp; 29.5759 \\
4 &amp; &amp; &amp; 20.0136 &amp; 23.7379 &amp; 27.2316 &amp; 30.5319 &amp; 33.6653 \\
5 &amp; &amp; 17.9257 &amp; 22.5929 &amp; 26.8885 &amp; 30.8947 &amp; 34.6616 &amp; 38.2245 \\
6 &amp; &amp; 19.8745 &amp; 25.2172 &amp; 30.1103 &amp; 34.6579 &amp; 38.9226 &amp; 42.9479 \\
7 &amp; \mathbf{15.0035} &amp; 21.8342 &amp; 27.8626 &amp; 33.3653 &amp; 38.4677 &amp; 43.2445 &amp; 47.7470 \\
8 &amp; 16.1524 &amp; 23.7997 &amp; 30.5195 &amp; 36.6382 &amp; 42.3027 &amp; 47.5994 &amp; 52.5871 \\
9 &amp; 17.3024 &amp; 25.7687 &amp; 33.1832 &amp; 39.9220 &amp; 46.1528 &amp; 51.9740 &amp; 57.4518
\end{array}$$</p>

<p>It turns out the optimum is the 7,3 tiling: regular heptagons, three of them meeting at each vertex. This is also the tiling with the smallest cells, so if the cells aren't too small for our hyperbolic bees, they should pick this one. For comparison, the Euclidean hexagon is at $8\sqrt3\approx13.8564$, the Euclidean square at $16$. So that hyperbolic heptagonal tiling is still better than the Euclidean 4,4.</p>

<p><a href=""https://i.stack.imgur.com/ADqyA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ADqyA.png"" alt=""Rendering of the regular 7,3 tiling""></a></p>

<h2>Elliptic case</h2>

<p>You also asked for the elliptic case, and there the approach is pretty much the same. Use the <a href=""https://en.wikipedia.org/wiki/Spherical_law_of_cosines"" rel=""nofollow noreferrer"">spherical law of cosines</a> and the surplus angle as a measure of area and you get</p>

<p>\begin{gather*}
a = \arccos\frac{\cos\alpha-\cos\beta\cos\gamma}{\sin\beta\sin\gamma}
= \arccos\frac{\cos\frac\pi n}{\sin\frac\pi m}
\\
A = \frac\pi m+\frac\pi n+\frac\pi 2-\pi = \pi\left(\frac1m+\frac1n-\frac12\right)
\\
\frac{(2na)^2}{2nA}=\frac{2n\left(\arccos\frac{\cos\frac\pi n}{\sin\frac\pi m}\right)^2}{\pi\left(\frac1n+\frac1m-\frac12\right)}
\end{gather*}</p>

<p>But here the degenerate situations win out. For $m=2$ you get two hemispheres no matter the value of $n$, which is already better than any $m&gt;2$. For $m=1$ you get a division by zero since $\sin\frac\pi m=0$ in that case. But you can well imagine that considering the whole sphere a single cell is optimal as it uses no walls at all.</p>
"
"2382254","2382277","<p>That's goes under the (more or less standard) name of ""Representation problem"", or ""Frobenius coin-exchange problem"".<br>
You can get some insight on it <a href=""http://www.math.udel.edu/~angell/Opt/farkas.pdf"" rel=""nofollow noreferrer"">in this paper</a> and <a href=""http://math.sfsu.edu/beck/papers/frobnote.pdf"" rel=""nofollow noreferrer"">in this other one</a>.</p>
"
"2382255","2382282","<p>Computers: they have limited precision. Once you put floating point numbers in that (ridiculously enormous) root solution, numerical errors get in and screw the result, especially in something like:</p>

<p><a href=""https://pastebin.com/YemafsAc"" rel=""nofollow noreferrer"">https://pastebin.com/YemafsAc</a> (I wanted to put it in here, but the SE doesnt allow me to put more than 30.000 chars in the post).</p>

<p>The solution is correct, but you have errors numerically evaluating the expression. </p>

<p>Is there anything impeding you from doing the following? </p>

<pre><code>syms x
a=1; b=0; c=0; d=0; e=-16; %x^4-16=0 (solutions should be [2,-2,2i,-2i]
coeffs = [a b c d e]; %eq = a*x^4 + b*x^3 + c*x^2 + d*x + e==0;
x_symbolic = roots(coeffs) %solving the equation
</code></pre>

<p>Numerically solving these equations is also ridiculously cheap in a computer. </p>

<pre><code>for ii=1:1000
   coeffs=rand(1,5)*20;
   tic;
   x_symbolic = roots(coeffs); %solving the equation
   t(ii)=toc;
end
mean(t)

ans
&gt;&gt; 4.5216e-05
% or a total of 0.045 in my PC.
</code></pre>

<p>Compared to the symbolic option:</p>

<pre><code>syms a b c d e
syms x

coeffs = [a b c d e]; %eq = a*x^4 + b*x^3 + c*x^2 + d*x + e==0;
tic
x_symbolic = roots(coeffs); %solving the equation
toc


&gt;&gt; Elapsed time is 1.350836 seconds.
</code></pre>

<p>Note that in my PC, you need approximately 100.000 numerical solutions to get to the same time consumption as the symbolic equation solution, and that is only without evaluating the result after. I'll leave as an exercise to the reader the evaluation of the numerical accuracy of roots, but I'll say: its quite accurate!</p>
"
"2382274","2382323","<p>Given a collection of objects $C$ (usually a set in this context), there is an ""obvious"" category that has objects $C$: it's the category whose only morphisms are the identity morphisms with obvious composition, which is often called the discrete category associated to $C$.</p>

<p>In the answer to your previous question, this category was denoted $DC$. It's obvious that any map $f: C\to E$ can be thought of as a functor $Df: DC \to DE$ that sends the object $c\in C$ to $f(c)$, and the arrow $id_c$ to $id_{f(c)}$. Since in both $DC, DE$ there are only trivial compositions, these things fill out to be a functor $D: Set \to \mathbf{Cat}$, where $\mathbf{Cat}$ is the category of small categories and functors between them. This is the ""discrete category"" functor, because it sends sets to the associated discrete category.</p>

<p>So in your previous question, the notation $Set^S$ can be equivalently seen as : the category of functors $DS \to Set$ where $DS$ is as above, or as the category of $S$-indexed families. Generally, for sets $A,B$, $B^A$ can be seen as the category of functors $DA\to DB$ because such a functor is precisely a map $f: A\to B$, i.e. the assignement $D: Hom_{Set}(A,B) \to Hom_{\mathbf{Cat}}(DA,DB)$ is full (and faithful): $D$ is a full (and faithful) functor. </p>

<p>What's interesting is that this last analogy between $B^A$ and $Hom_{\mathbf{Cat}}(DA,DB)$, is that $D(B^A) \cong [DA,DB]$ where $[DA,DB]$ is the functor category. That's because if you have $Df, Dg: DA\to DB$ two functors, and $\tau: Df\to Dg$ a natural transformation, then for any $a\in Ob(DA)=A$, $\tau_a: f(a) \to g(a)$ is a morphism in $DB$, which implies $g(a)= f(a)$. Therefore if there is a natural transformation (or a $[DA,DB]$-morphism) $Df \to Dg$, we get $f=g$ and this natural transformation is the identity, hence $Df= Dg$, that is, $[DA,DB]$ is the discrete category associated to $B^A$.</p>
"
"2382276","2382281","<p>First thing about exponential function is, its values/graph approximates zero, so the whole graph is lifted by two (that's the $+2$ there).</p>

<p>Once we subtract the 2, we see that the graph is going through one at point $x=-4$, but should go at zero, so the graph is moved by 4 to the left, hence the $+4$, (becuase if $x=-4$, then $x+4=0$). And we move it to the right.</p>

<p>The last thing (or two) we notice is the translated graph going through point $(-1,3)$ and $(-2,9)$, therefore we need to find base a, for which this is true, and that is $1/3$. </p>

<p>So, putting all of that together gives us $y=\frac 1 3^{(x+4)}+2 $</p>

<p>EDIT: The situation would be more complicated if there was a multiplicative constant, fortunately there is not. But in case there would be, you need to be looking for known exact values and determining individual components of the exponential function, the base for instance as ratio of two values. </p>
"
"2382278","2382377","<p>I think it is always convex in $\mathbb{C}$. To prove that, we may need that </p>

<p>\begin{align}
E &amp;= Ker(M) + Im(M)\\
Im(M) &amp;= (Ker(M))^{\perp}
\end{align}</p>

<p>Let also $P$ denote the orthogonal projection on $Im(M)$ </p>

<p>After that for every $x \in E$ one can write $x = y + z$ with $y = Px \in Im(M)$ and $z = x - Px \in Ker(M)$. Then</p>

<p>\begin{align}
||x||_M^2 &amp;= ||y||_M^2\\
\langle M T x, x \rangle &amp;= \langle M T y , y \rangle + \langle M T z, y\rangle
\end{align}</p>

<p>It is clear that : </p>

<p>\begin{align}
W_M^E(T) = \{\langle M T y , y \rangle + \langle M T z, y\rangle :  z \in Ker(M), y \in Im(M); ||y||_M = 1\}
\end{align}</p>

<p>I am using $W^E_M$ to denote that I am working in $E$</p>

<p><strong>1st case</strong> : $T(Ker(M)) \subset Ker(M)$ </p>

<p>In this case we have $MTz = 0$ for $z\in Ker(M)$:
\begin{align}
W_M^E(T) = \{\langle M T y , y \rangle : y \in Im(M); ||y||_M = 1\} 
\end{align}</p>

<p>Since $\langle M T y , y \rangle = \langle M P T y , y \rangle$, we have :
\begin{align}
W_M(T) = \{\langle M PT y , y \rangle : y \in Im(M); ||y||_M = 1\} = W^{Im(M)}_M (PT_{Im(M)}) 
\end{align}
which is convex since $M_{Im(M)}$ is injective and $PT_{Im(M)}$ is a bounded element of $\mathcal{L}(Im(M))$. </p>

<p><strong>2nd case</strong> $T(Ker(M)) \not\subset Ker(M)$ </p>

<p>Let $u \in Ker(M)$ such that $Tu \not \in Ker(M)$ and let $v = \frac{PTu}{||PTu||_M}$, then $v \in Im(M)$ and $||v||_M = 1$ then : </p>

<p>\begin{align}
W_M^E(T) &amp;= \{\langle M T y , y \rangle + \langle M T z, y\rangle :  z \in Ker(M), y \in Im(M); ||y||_M = 1\} 
\\&amp;\supset \{\langle M T v , v \rangle + \langle M T (\lambda u), v\rangle\ : \lambda \in \mathbb{K}\} \\
&amp; \supset \{\langle M T v , v \rangle + \lambda ||T u||_M \langle M v, v\rangle\ : \lambda \in \mathbb{K}\}\\
&amp; \supset \{\langle M T v , v \rangle + \lambda ||T u||_M \rangle\ : \lambda \in \mathbb{K}\} = \mathbb{K}
\end{align}</p>

<p>Finally $W_M^E(T) = \mathbb{K}$ which is convex.</p>
"
"2382284","2382298","<p>No.</p>

<p>This step:</p>

<p>$$\exists i \in I \quad x\in A_{i}^c\iff \forall i\in I \quad x\not \in A_{i}$$</p>

<p>isn't true.</p>

<p>You should have:</p>

<p>$$\exists i \in I \quad x\in A_{i}^c\iff \exists i\in I \quad x\not \in A_{i}$$</p>

<p>which leads to:</p>

<p>$$x\in\cup_{i\in I}A_{i}^c\iff \exists i \in I \quad x\in A_{i}^c\iff \exists i\in I \quad x\not \in A_{i}\iff  x\not \in \cap A_{i}\iff x\in(\cap_{i\in I} A_{i})^c$$</p>
"
"2382285","2382300","<p>Because of the estimate using AM-GM$$\left|\frac{x}{n(1+x^2n)}\right|=\frac1{n\left(\frac1{|x|}+|x|n\right)}\le\frac1{n\cdot2\sqrt{n}}=\frac1{2n^{3/2}},$$ which is valid even in the case $x=0,$ your series converges uniformly on the whole real line.</p>
"
"2382286","2382307","<p>Multiplying by $1$ doesn't change the value of anything. Thus if the left hand side and right hand side were equal before you multiplied one of them by $1$, then they're still equal after you've multiplied one of them by $1$ (and, just as important, but more subtle: If they are equal after you've multiplied one of them by $1$, then they were equal before you did so).</p>

<p>The same goes for adding $0$, or simplifying expressions; neither of those operations change the value of anything, so you're allowed to do it to only one side of an equation.</p>
"
"2382301","2382340","<p>You're right, somebody mixed up the order.</p>

<p>If $\mu \ll \nu$, then $\nu$-a.e. implies $\mu$-a.e. (since the exceptional set is $\nu$-null and thus by absolute continuity $\mu$-null), but not the other way round. Whether it was intended that the Lebesgue measure should be absolutely continuous with respect to $\mu$ or something else, I can't guess.</p>
"
"2382302","2382303","<p>$$\sum_{i=0}^0 a_i = a_0$$ by definition, because by definition</p>

<p>$$\sum_{i=0}^na_i = \begin{cases}a_0 &amp; n=0\\
a_n + \sum_{i=0}^{n-1}a_i &amp; n\geq 1 \end{cases}$$</p>
"
"2382308","2382472","<p>Let $r_i=\frac{1}{e}&gt;0$ and $r'_i=2&gt;0$.</p>

<p>Then, as $\log \frac{1}{e} = -1$, the first sum would always be negative and equal $-M(2-\frac{1}{e})&lt;0$. </p>

<p>On the other hand, the second sum would be</p>

<p>$$\sum^M \frac{2-\frac{1}{e}}{\frac{1}{e}}=Me \left ( 2-\frac{1}{e} \right ) &gt;0 $$</p>
"
"2382318","2382328","<p>we can assume that $$\gcd(m,n)=1$$ and $$2=\frac{m^2}{n^2}$$ then $$2n^2=m^2$$ thus the left-hand side is even and so $$m^2$$ this is a contradiction, both numbers $m,n$ can not be even</p>
"
"2382332","2382358","<p>From the Menelaus Theorem on $\triangle CBP$ and the line $A-D-E$ we have:</p>

<p>$$\frac{CA}{AB} \times \frac{BD}{DP} \times \frac{PE}{CE} = 1$$</p>

<p>So from this it's enough to prove that $\frac{CA}{AB} \times \frac{BD}{DP} = 2$. </p>

<p>Now we have that $AB = 2R$, while from the power of point $P$ we get: $DP = \frac{PA^2}{PB}$. Using some well-known formulas for <a href=""https://en.wikipedia.org/wiki/Right_triangle#Altitudes"" rel=""nofollow noreferrer"">altitudes in right-angled triangles</a> we have:</p>

<p>$$\frac{CA}{AB} \times \frac{BD}{DP} = \frac{CA \cdot BD \cdot PB}{2R \cdot AP^2} = \frac{BD \cdot PB}{2R^2} = \frac{AB^2}{2R^2} = \frac{4R^2}{2R^2} = 2$$</p>

<p>Hence the proof.</p>
"
"2382333","2382958","<p>As Nefertiti said, in an arbitrary finite-dimensional algebra, the bilinear map is a morphism of varieties. Indeed in a basis, the bracket takes the form
$$(x_1,\dots,x_n,y_1,\dots,y_n)\mapsto (z_1,\dots,z_n)$$
$$z_k=\sum_{(i,j)}a_{i,j,k}x_iy_j,$$
where $a_{i,j,k}$ are the structure constants. So each coordinate $z_k$ is indeed polynomial in the variables $(x_1,\dots,y_n)$.</p>
"
"2382337","2382537","<p><strong>Edit: The second figure actually shows the spectrum of $I+A$.</strong></p>

<p>Neumann series, Behind the Scenes (read; Behind the Proof);</p>

<p>Suppose you have some operator $A$ such that $\|A\|&lt;1$. It's spectrum is contained in the ball of radius $1$. It is actually a coroallry of the Neumann series theorem which we are discussing, but let's take that as a fact, for we try to realize what is happening <em>behind the proof</em>. When I picture the spectrum I would think about something like this:
<a href=""https://i.stack.imgur.com/sh6l3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sh6l3.png"" alt=""enter image description here""></a>
The spectrum of $A$ is that odd shape, it is contained in a circle of radius $r&lt;1$ about the origin (this is another not-so-easy fact, there are several theorems that show that, for instance Neumann series again. The formula for the spectral radius shows it too). Note I drew the spectrum $\sigma(A)$ as continuous shape, and that it contains $0$ (so $A$ is not invertible), but this has nothing to do with the rest of the intuitive geometric feeling i'm trying to communicate. You make take it to be discrete or whatever, but it must be in that ball of radius $r&lt;1$.</p>

<p>Now look at $I-A$, the spectrum then looks like <a href=""https://i.stack.imgur.com/PKqcA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PKqcA.png"" alt=""enter image description here""></a></p>

<p>so it is separated from $0$.</p>
"
"2382341","2382616","<p>Your equation for the volume is correct. However, actually plotting the volume is somewhat more complicated. Specifically, in 3D space you need a matrix of terms for each of $x, y$, and $z$. This is most easily visualized in a vertical arrangement, with rotation about the $z$-axis. So imagine the line $r=f(z)$. Then take a vector $\theta\in[0,2\pi]$ with however many points you wish and for each value of $z$ find $X=r\cos\theta$, $Y=r\sin\theta$. These are matrices of the size of $r$ by the size of $\theta$. The $Z$ matrix is just a uniformly spaced matrix for all the $z$ and $\theta$ values. Many computer languages have such functions built in. I used Matlab's <em>cylinder</em> function to create the figure below.</p>

<p>EDIT: At the request of the OP, I am adding the Matlab code. Note that function <em>cylinder</em> is a Matlab built-in function described <a href=""https://www.mathworks.com/help/matlab/ref/cylinder.html?searchHighlight=cylinder&amp;s_tid=doc_srchtitle"" rel=""nofollow noreferrer"">here</a>.</p>

<pre><code>x=16*linspace(0,1,4001)';
f=6.4./(x+12).*sin(2*pi*x/6.5)+3;

[X,Y,Z]=cylinder(f,50);

figure;surf(X,Y,16*Z)
axis equal
shading flat
xlabel('X');ylabel('Z');zlabel('Y','Rotation',0)
</code></pre>

<p><a href=""https://i.stack.imgur.com/grCyx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/grCyx.png"" alt=""3D volume for $f(x)$""></a></p>
"
"2382342","2382665","<p>with 1), why do you say $(p_n)_n$ has no  convergent subsequence? It <em>is</em> true but needs a small lemma: a convergent sequence is bounded, plus the fact that every subsequence of $(p_n)$ is also unbounded.</p>

<p>With 2: Let $x \in \overline{A}$. Then $x \in \overline{A}$ means there is a sequence $(a_n)_n$ from $A$ such that $a_n \to x$ (this holds in all metric spaces, and indeed it suffices to pick $a_n \in B(x,\frac{1}{n})$ for this).</p>

<p>So there is an $a \in A$ and a subsequence $a_{n_k} \to a$ by assumption on $A$.
But any subsequence of a convergent sequence converges to the same limit, so $a_{n_k} \to x$. As limits of (sub)sequences are unique: $x =a \in A$, so $A$ is closed.</p>

<p>No need to go with a contradiction.</p>
"
"2382346","2382366","<p>If $V$ is connected, so is $\overline{V}$, and also $U = \overset{\Large\circ}{\overline{V}}$, since $V \subset U \subset \overline{V}$. We thus may assume that $V = U$. Consequently, $\partial \overline{V} = \partial V$.</p>

<p>If $W \subset \mathbb{R}^n$ is open with $W \subset \overline{V}$, then it follows that $W \subset V$. If also $\partial W \subset \partial V$, then $W$ is open and closed in $V$. Since $V$ is connected, it follows that either $W = \varnothing$ or $W = V$.</p>

<p>Thus, unless one allows a sequence containing infinitely many terms $C_k = \varnothing$ and at most one term $C_m = V$, no such sequence exists.</p>
"
"2382363","2382398","<p><strong>Hint:</strong></p>

<p>$$T_{a,b}=\inf\{t \geq 0; W_t = a+bt\} \stackrel{d}{=} \inf\{t \geq 0; B_t = a+bt\}$$</p>

<p>for any two Brownian motions $(W_t)_{t \geq 0}$ and $(B_t)_{t \geq 0}$. How to choose $(B_t)_{t \geq 0}$ such that the right-hand side equals $T_{-a,-b}$?</p>
"
"2382365","2382367","<p>If $x$ is $t$-times greater than $y$, that usually means (and I would understand it as) $x=t\cdot y$.</p>
"
"2382369","2382391","<p>HINT:  try substituting $z = y - b/a^2$ into your original ODE and see what happens.</p>
"
"2382372","2382419","<p>You should pay more attention to your hints.</p>

<p>$\tan \alpha = 2-\sqrt{3}$</p>

<p>Since we are given that $\alpha$ is acute and $0 &lt; \tan \alpha &lt; 1$, then 
$0 &lt; \alpha &lt; \frac{\pi}{4}$ and $0 &lt; 2\alpha &lt; \frac{\pi}{2}$.</p>

<p>Since \begin{align}
   \tan 2\alpha
   &amp;= \dfrac{2\tan \alpha}{1 - \tan^2 \alpha} \\
   &amp;= \dfrac{4-2\sqrt 3}{1 - (7-4 \sqrt 3)} \\
   &amp;= \dfrac{4-2\sqrt 3}{-6 + 4 \sqrt 3} \\
   &amp;= \dfrac{2(2 - \sqrt 3)}{2\sqrt 3(2 - \sqrt 3)} \\
   &amp;= \dfrac{1}{\sqrt 3} \\
   &amp;= \tan \frac{\pi}{6}
\end{align}</p>

<p>Then $\alpha = \dfrac{\pi}{12}$</p>
"
"2382387","2382457","<p>Your starred equation is not correct.  You plugged in the terms from $y'_p$ instead of $y_p$.  If you plug into the original equation, you should get $$x(A_1\cos x + A_2 \sin x)-2A_1\sin x-A_1x\cos x+2A_2\cos x-A_2x\sin x=\cos x$$ from which the terms proportional to $x$ cancel, leaving 
$$-2A_1 \sin x+2A_2\cos x =\cos x$$ as desired.</p>
"
"2382394","2382451","<p>If $x\in\mathbb Q$, then $f(x)&gt;0$. But any interval $(x-\varepsilon,x+\varepsilon)$ contains irrational points and at those points the value of $f$ is $0$. Therefore, $f$ is not continuous at $x$.</p>

<p>On the other hand, is $x\in\mathbb{R}\setminus\mathbb{Q}$, then $f(x)=0$. Take $\varepsilon&gt;0$. The open interval of length $1$ centered at $x$ contains only finitely many numbers $y$ such that $f(y)\geqslant\varepsilon$. Pick $\delta&gt;0$ such that $(x-\delta,x+\delta)$ contains no such point. Then$$|y-x|&lt;\delta\Longrightarrow\bigl|f(y)-f(x)\bigr|&lt;\varepsilon.$$</p>
"
"2382397","2382482","<p>We know that $f \in L^1(\mathbb R)$, then $\hat{f}$ is bounded and continuous. If $\hat{f} \in L^1(\mathbb{R})$ then almost everywhere $$f(x)=\dfrac{1}{2\pi}\int_{\mathbb R}e^{i\xi x}\hat{f}(\xi)\ \mathrm{d}\xi$$
This means that $f$ is equal almost everywhere to a continuous function $g$. We find a contradiction around $x=0$ because we know that there exists $\delta &gt; 0$ such that for $x \in [-\delta, \delta], \ g(x) \in [g(0)-0.1, g(0)+0.1]$. Therefore for almost every $x \in [-\delta, \delta], \ f(x) \in [g(0)-0.1, g(0)+0.1]$, which is not the case.</p>
"
"2382401","2382450","<p>He simply calculated each ""fundamental solution"" by assigning 1 to one of the free variables and 0 to the others. Then by expressing x as the linear combination  of $f_1, f_2, f_3, f_4$, one arrives at the general solution.</p>

<p>Note that this is just to say that the vector space of solutions to the linear system is of dimension $4$ and ${f_1, f_2, f_3, f_4}$ is its basis.</p>
"
"2382402","2382484","<p>T3 to add generator $y$ and relation $y = b$: $\langle x,b,c,y | b^2, (bc)^2, y^{-1}b \rangle$</p>

<p>T1 to add relation $(yc)^2 = 1$ (derivable from $y = b$ and $(bc)^2 = 1$): $\langle x,b,c,y | b^2, (bc)^2, y^{-1}b, (yc)^2 \rangle$</p>

<p>T2 to remove relator $(bc)^2$: $\langle x,b,c, y | b^2, y^{-1}b, (yc)^2 \rangle$</p>

<p>T1 to add relator $y^2$: $\langle x,b,c,y | b^2, y^{-1}b, (yc)^2, y^2 \rangle$</p>

<p>T2 to remove relator $b^2$: $\langle x,b,c,y | y^{-1}b, (yc)^2, y^2 \rangle$</p>

<p>Now there is only one relator involving $b$ and it has the form $bw$ for a word $w$ not involving $b$, then, we may remove it.</p>

<p>T4 to remove generator $b$: $\langle x,c,y | (yc)^2, y^2 \rangle$</p>

<p>T3 to add generator $z$ with relation $z = yc$: $\langle x,c,y, z | (yc)^2, y^2, z^{-1}yc \rangle$</p>

<p>Now, $z = yc$ and $(yc)^2 = 1$ imply $z^2 = 1$, so, we may add this relation:</p>

<p>T1 to add relator $z^2$: $\langle x,c,y, z | (yc)^2, y^2, z^{-1}yc, z^2 \rangle$</p>

<p>Now, $(yc)^2 = 1$ is derivable from $z = yc$ and and $z^2 = 1$, so, we may remove it:</p>

<p>T2 to remove relator $(yc)^2$: $\langle x,c,y, z | y^2, z^{-1}yc, z^2 \rangle$</p>

<p>T4 to remove generator $c$: $\langle x,y, z | y^2, z^2 \rangle$</p>
"
"2382403","2382410","<p><strong>hint</strong></p>

<p>$$\ln (n+1)=\ln (n)(1+\frac {\ln (1+\frac {1}{n})}{\ln (n)})$$
$$\sim \ln (n) $$</p>

<p>use ratio test.</p>
"
"2382408","2382422","<p>Substitute $x=\frac{1}{\sqrt{m}}$ into the sum &amp; do partial fractions
\begin{eqnarray*}
\sum_{n=1}^{\infty} \frac{\frac{1}{\sqrt{M}}}{n(1+\frac{n}{M})}=\sqrt{M} \sum_{n=1}^{\infty} \frac{1}{n(n+M)} = \frac{1}{\sqrt{M}}\sum_{n=1}^{\infty} \left( \frac{1}{n} -\frac{1}{n+M} \right) =\color{red}{ \frac{H_M}{\sqrt{M}}}.
\end{eqnarray*}</p>
"
"2382414","2382452","<p>let $$a_n=\frac{x}{n(1+nx^2)}$$ then we have $$\frac{a_{n+1}}{a_n}=\left(\frac{n}{n+1}\right)^2\left(\frac{(\frac{1}{n}+x^2)}{\frac{1}{n+1}+x^2}\right)$$
and for the second series we get
let $$a_n=\frac{x^n}{\ln(n+1)}$$ and $$\frac{a_{n+1}}{a_n}=\frac{x\left(1+\frac{\ln(1+\frac{1}{n})}{\ln(n)}\right)}{1+\frac{\ln(1+\frac{2}{n})}{\ln(n)}}$$</p>
"
"2382417","2382438","<p>The function $x^{\frac{1}{x}}$ is decreasing for $x &gt; e$. So if $ e&lt;a&lt;b $ then $a^{\frac{1}{a} } &gt; b^{\frac{1}{b}}$ . In your case $ \color{red}{3^{\pi} &gt; \pi^{3}}$.</p>

<p>EDIT:
In order to solve the problem when $a &lt; e &lt; b$ we need to invert the equation $ \frac{ln(b')}{b'}= \frac{ln(b)}{b}$ where $b'&lt;e&lt;b$. This can be done using the Lambert $W$ function. Given $b$, $b'$ can be calculated using 
\begin{eqnarray*}
b'= \exp( -W_0(-\frac{ln(b)}{b})).
\end{eqnarray*}
You can use Wolfie to do this calculation Eg $b=4$ <a href=""https://www.wolframalpha.com/input/?i=e%5E(-productlog(-ln(b)%2Fb)),b%3D4"" rel=""nofollow noreferrer"">https://www.wolframalpha.com/input/?i=e%5E(-productlog(-ln(b)%2Fb)),b%3D4</a>
Then compare the returned value $b'$ with $a$. We are now in the increasing part of the function so if $a&lt;b'$ then $a^{b'} &lt; (b')^a$.</p>
"
"2382421","2382439","<p>The $n$ in the formula isn't the $n$ you're working with. What you are supposed to do is sum over all possible outcomes $x$ of the random variable $-P(x)\log_2 P(x)$. Here there are $62^{16}$ possibilities, all with probability $62^{-16}$, so you get $62^{16}62^{-16}\log_2(62^{16})=16\log_2 62\approx 95$.</p>

<p>In fact whenever all possibilities are equally likely the entropy is just $\log_2$ of the number of possibilities.</p>

<p>(The formula is for a random variable with $n$ possibilities $x_1,\ldots,x_n$, not a random variable $x$ with $n$ components.)</p>
"
"2382423","2382542","<p>As others have shown this isn't true in general. In their examples, the sequences $a_n$ and $b_n$ either both tend to $0$ or to $\infty$. What happens in other cases?</p>

<p>Suppose that $\lim b_n = L$ is finite and $\lim \frac{a_n}{b_n} = 1$. Then,
$$
\left| a_n - b_n \right|
= |b_n| \left| \frac{a_n - b_n}{b_n} \right|
= |b_n| \left| \frac{a_n}{b_n} - 1 \right|
\to |L| \cdot 0 = 0
$$
Thus, $\lim (a_n-b_n) = 0.$</p>

<p>In the same way, if $\lim b_n = L \neq 0$ and $\lim (a_n-b_n) = 0$ then
$$
\left| \frac{a_n}{b_n} - 1 \right|
= \left| \frac{a_n-b_n}{b_n} \right|
= \frac{1}{|b_n|} \left| a_n-b_n \right|
\to \frac{1}{|L|} \cdot 0 = 0
$$
Thus, $\lim \frac{a_n}{b_n} = 1.$</p>

<p>Therefore, if $\lim b_n$ exists and is neither zero or infinite, then we do have equivalence. In this case, of course $\lim a_n = \lim b_n = L.$</p>
"
"2382425","2382444","<p>Given the information you have provided, we may not derive such a bound because $f$ might be arbitrarily flat in an arbitrarily large neighbourhood of its unique minimiser. It would be useful if, for example, we knew that $f$ is strongly convex with a known strong convexity modulus so that we know that $f$ grows faster than some quadratic function close to its minimiser. </p>

<p>Additionally, even if we know that $L$ is the <em>best</em> Lipschitz constant of the function, it may be arbitrarily flat close to the minimizer and steeper aways from it.</p>
"
"2382453","2382488","<p>For $x&gt;e^5$,</p>

<p>$$\ln (x)+5\le 2\ln (x) $$
and
$$2+\sin (x)\ge 1$$
thus</p>

<p>$$\frac {1+\sin (x)}{\ln (x)+5}\ge \frac {1}{2\ln (x)} $$</p>

<p>but</p>

<p>$$\lim_{+\infty}\frac {\sqrt {x}}{\ln (x)}=+\infty$$
thus for $x $ large enough</p>

<p>$$\frac {\sqrt {x}}{\ln (x)}\ge 1$$
and</p>

<p>$$\frac {1}{\ln (x)}\ge \frac {1}{\sqrt {x}} $$</p>

<p>finally, as $\int^{+\infty}\frac {dx}{\sqrt {x}} $ diverges, we conclude that your integral is Divergent.</p>
"
"2382459","2382467","<p>No, consider the sequence of functions
$$f_n(x) = \begin{cases} 1/n &amp; \text{if $x=0$} \\ 0 &amp; \text{otherwise} \end{cases}$$
Then $f_n \to 0$ uniformly, and while $0$ is continuous, none of the $f_n$ are.</p>
"
"2382463","2382475","<p>The answer is <em>yes</em>. More precisely, if $n$ is a square-free integer, then the expansion of $\sqrt{n}$ in continued fraction is $$\sqrt{n} = [a_0; \, \overline{a_1, \, a_2, \, a_3, \ldots, a_2, \, a_1, \, 2a_0}],$$
where the repeating part (excluding the last term) is symmetric upon reversal.</p>

<p>For more details, see <a href=""http://mathworld.wolfram.com/PeriodicContinuedFraction.html"" rel=""nofollow noreferrer"">here</a>.</p>
"
"2382464","2382496","<p>The sum is a geometric sum &amp; is easy enough to calculte. Let $ x =e^{2 \pi i u}$
\begin{eqnarray*}
\sum_{k=-M}^{M} x^k= \frac{x^{-M}-x^{M+1}}{1-x} = \frac{x^{M+\frac{1}{2}}-x^{-M-\frac{1}{2}}}{x^{\frac{1}{2}}-x^{-\frac{1}{2}}}
\end{eqnarray*}
Now use $e^{i \pi u}-e^{-i \pi u}=2 \sin( \pi u)$.</p>
"
"2382474","2382664","<p>There are a lot of incorrect ideas here.  First, antiderivatives of $f$ aren't even in a cohomology class at all.  A cohomology class consists of a coset of the set of closed forms by the vector subspace of exact forms.  In particular, for a form (in this case, a $0$-form) to be in a cohomology class, it must be a closed form!  In order for a function $F$ to be a closed $0$-form, its derivative must be $0$.  So the antiderivatives $F$ and $G$ themselves are not elements of any cohomology class (unless $f=0$), though the differences $F-G$ are.</p>

<p>As for whether the differences $F-G$ are all elements of the same cohomology class, the answer is no.  Two closed $0$-forms are in the same cohomology class iff they differ by an exact $0$-form.  By definition, an exact $0$-form is the differential of a $(-1)$-form.  But there is no such thing as a $(-1)$-form (or rather, we formally say the vector space of $(-1)$-forms is the trivial vector space), so the only exact $0$-form is $0$.  So two closed $0$-forms are in the same cohomology class only if they are equal.  This renders your question (2) rather trivial, since any degree $0$ cohomology class has only one element.</p>

<p>Also, it is not correct that $F-G$ is constant: it is only <em>locally</em> constant.  If $A$ is disconnected, $F-G$ can take different values on different connected components.</p>

<p>Here, then, is the correct story.  The set of closed $0$-forms consists of the set of all smooth functions whose derivative vanishes.  These are exactly the locally constant functions.  Since the only exact $0$-form is $0$, the closed $0$-forms are the same thing as the $0$th de Rham cohomology.</p>

<p>On the other hand, given any smooth function $f$ which has an antiderivative $F$, you can find another antiderivative $G$ of $f$ by adding any locally constant function to $F$, and all other antiderivatives of $f$ can be found in this way.  That is, the ambiguity in defining ""the"" antiderivative of $f$ is determined by the set of all locally constant functions, aka the $0$th de Rham cohomology.</p>

<p>As for your third question, is this formulation in terms of de Rham cohomology enlightening?  Maybe, maybe not.  As I see it, its value is that it gives directions to generalize the phenomenon of non-uniqueness of antiderivatives.  For instance, if instead of an open subset $A\subseteq\mathbb{R}$ you had an arbitrary manifold, the correct generalization would be to talk about antiderivatives of $1$-forms on your manifold, not antiderivatives of functions (the differential of a function on a manifold is a $1$-form, not another function).</p>

<p>And of course, you can also consider de Rham cohomology in higher dimensions as another generalization.  Note though that there is an important difference in higher dimensions: exact forms no longer have to be trivial, so closed forms (which, as the forms with differential $0$, literally capture the non-uniqueness of ""antiderivatives"") are no longer the same as cohomology classes.  Instead, cohomology classes capture the non-uniqueness of antiderivatives modulo the ""trivial"" reasons that antiderivatives should not be unique (those coming from exact forms).  Another way to think of it is that they capture the part of the non-uniqueness of antiderivatives that comes from the global topology of your manifold, rather than just from the formal local properties of your manifold.  In the case of dimension $0$, the global topology in question is how many connected components your manifold has, which determines how many locally constant functions there are.</p>
"
"2382504","2382511","<p>There is not enough information.  It could be anywhere from $0$ to $100\%$.  Imagine for a hypothetical scenario that in the world there are a total of ten people named Joe overall and a total of ten people with blond hair, three of which with blond hair are named Joe.  The remaining seven Joe's all have black hair.  The remaining seven blond people are named Bob.</p>

<p>Now... here are two possible scenarios, each unable to be distinguished from the details you have given so far:</p>

<ul>
<li>The three blond Joe's are all rockstars</li>
</ul>

<p>This would lead to the probability that a blond Joe is a rockstar as being $100\%$</p>

<ul>
<li>None of the three blond Joes are rockstars.  That is, all of the rockstar Joe's are blackhaired and all of the rockstar blondes are named Bob.</li>
</ul>

<p>This would lead to a probability that a blonde Joe is a rockstar as being $0\%$.</p>

<p>Of course, the specific numbers could be literally anywhere between those.  To give any further information we would need to make several heavy assumptions as to the relationship between being a Joe, being blonde, and being a rockstar.  We know that one such assumption such as ""being a joe, being a blonde and being a rockstar are all independent events"" cannot be true otherwise the chance of being a rockstar should have been the same for Joes as it is for blondes.</p>

<hr>

<p>As per the edited question:</p>

<p>What you are talking about is <a href=""https://en.wikipedia.org/wiki/Conditional_probability"" rel=""nofollow noreferrer"">conditional probability</a>.  The notation $Pr(A\mid B)$ is the probability that $A$ occurs given that $B$ has occurred.</p>

<p>Let $J$ be the event that a randomly selected person is named Joe.  Let $R$ be the event that a randomly selected person is a rockstar.  Let $B$ be the event that a randomly selected person has blond hair.</p>

<p>Your givens are that $Pr(R\mid J)=0.7$ and that $Pr(R\mid B)=0.3$, that is to say the probability that a randomly selected Joe is a rockstar is $0.7$ and that a randomly selected person with blond hair is a rockstar is $0.3$.</p>

<p>You ask what the probability that a randomly selected person whose name is Joe and who has blond hair is a rock star.  I.e. you are asking for $Pr(R\mid J\cap B)$.  As mentioned previously, there is not enough given information to determine this and this number could have little to nothing to do with the values of $Pr(R\mid J)$ and $Pr(R\mid B)$.</p>
"
"2382507","2382528","<p>When the base of the logarithm is larger than $1$ they are increasing and they are negative when their argument is between $0$ and $1$</p>

<p>so you must solve $\dfrac{2x-8}{x-2} &gt;0 $ <em>AND</em> $\dfrac{2x-8}{x-2} &lt;1$</p>

<p>the first is verified for $x&lt;2;\;x&gt;4\quad(*)$</p>

<p>to solve the second remember to move RHS in the LHS and add them together</p>

<p>$\dfrac{2x-8}{x-2} -1&lt;0\to \dfrac{2x-8-x+2}{x-2}&lt;0\to \dfrac{x-6}{x-2}&lt;0$</p>

<p>which is verified for $2&lt;x&lt;6$</p>

<p>we need to find the intersection of the last solution and the solution $\quad(*)$</p>

<p>which is $4&lt;x&lt;6$</p>

<p>Hope this helps</p>
"
"2382512","2382516","<p>Regardless of what $X$ is, ""$P(x)$ holds for all but finitely many $x\in X$"" means that the set $\{x\in X:\neg P(x)\}$ is finite.  If $X$ is finite, then $\{x\in X:\neg P(x)\}$ is always finite, since it is a subset of $X$.  So if $X$ is finite, every property holds for all but finitely many $x\in X$.</p>
"
"2382526","2382764","<p>Let us say that we work over the complex numbers. Take the closed immersion $i:C\hookrightarrow Y$, where $C$  is the rational curve. Then I claim that the sheaf $E=i_*\mathcal{O}_C$ is supported on $C$ but $Rf_*E$ is not zero in the derived category of X. It's enough to show that $f_*E$ is not zero. Let us say that the curve is contracted to a point $p$ in $X$, then if we take an open $U\subset X$ containing $p$ we have that $f_*E(U)$ is not zero, and actually is  equal to $\mathbb{C}$, because $f^{-1}U$ contains $C$.</p>

<p>The other implication should be true: indeed the hypothesis implies that $f_*E=0$, and using the fact that $f:Y-C\rightarrow X-p$ is an isomorphism, then you see that the stalks of E at the points in $Y-C$ are all zero, thus the support of $E$ must be contained in $C$.</p>
"
"2382530","2382541","<p>Let $(e_1,\ldots,e_n)$ a basis of $E$ and define the linear forms $e_j^*$ by</p>

<p>$$e_j^*(e_i)=\delta_{i,j}$$
It's easy to prove that the $(e_j^*)_{1\le j\le n}$ are linearly independent. In fact let $\alpha_j\in\Bbb R$ such that
$$\sum_{j=1}^n \alpha_je_j^*=0$$
and we apply it to $e_i$ and we get $\alpha_i=0$. Now let $f\in E^*$ and let $f(e_i)=x_i$. Then $f=\sum_{j=1}^n x_j e_j^*$ because  they are equal on the basis $(e_i)$. Hence $(e_j^*)$ spans $E^*$ and so it's a basis of $E^*$. We conclude that $\dim E^*=n=\dim E$.</p>
"
"2382539","2382544","<p>Yes. Indeed $\cos ((n+1)x) = -\cos((n-1)x) + 2\cos(x)\cos(nx)$. Then it is enough to take the sequence of polynomials such that $T_{n+1}(x) = -T_{n-1}(x) + 2 x T_n(x)$. For the initialization $T_0(x) = 1$ and $T_1(x) = x$ satisfy the condition for $n=0$ and $n=1$</p>
"
"2382568","2382587","<p>Let $y=x+2$. Then define $f(y)$ by 
$$
|x+1|+|x+2|+|x+3| = |y-1|+|y|+|y+1| \equiv f(y) 
$$
$f(y) = |1-y|+|y|+|1+y|$ so $f(-y) = f(y)$.  </p>

<p>Above $y=1$, $f(y) = 3y$ which is monotonic increasing.  For $0\leq y \leq 1$,
$$
f(y) = (1-y)+y+(1+y) = 2+y
$$
which is also monotonic increasing and has a minimum at $f(0)=2$.</p>

<p>So $f(y)$ has a minimum value of $2$ at $y=0$, and is increasing for positive $y$ and decreasing for negative $y$.   Therefore, $f(y)=a$ (and therefore your original equation) has:</p>

<ul>
<li>No solutions for $a&lt;2$.</li>
<li>One solution for $a=2$.</li>
<li>Two solutions for $a&gt;2$.</li>
</ul>
"
"2382569","2382573","<p>$\newcommand{\Q}{\Bbb Q}|\Q(e^{\pi i/5}):\Q|=4$ and
$|\Q(2^{1/5}):\Q|=5$. What does that tell you about
$|\Q(2^{1/5},e^{\pi i/5}):\Q|$?</p>
"
"2382583","2382588","<p>As it's a simple pole, the residue is
$$\lim_{z\to\xi}(z-\xi)\frac{H(z)}{P(z)}
=\lim_{z\to\xi}H(z)\times
\left(\lim_{z\to\xi}\frac{P(z)-P(\xi)}{z-\xi}
\right)^{-1}=H(\xi)P'(\xi)^{-1}.$$</p>
"
"2382584","2382602","<p>For $f(u, v)$, $g(x)$, $h(x)$ sufficiently differentiable (what Thomas Andrews in his comment terms <em>nice</em>), taking</p>

<p>$u = g(x) \tag 1$</p>

<p>and</p>

<p>$v = h(x), \tag 2$</p>

<p>we have</p>

<p>$\dfrac{df}{dx} = \dfrac{\partial f(u, v)}{\partial u} \dfrac{dg(x)}{dx} + \dfrac{\partial f(u, v)}{\partial v} \dfrac{dh(x)}{dx}; \tag 3$</p>

<p>or, in terms of (1), (2),</p>

<p>$\dfrac{df}{dx} = \dfrac{\partial f(g(x), h(x))}{\partial u} \dfrac{dg(x)}{dx} + \dfrac{\partial f(g(x), h(x))}{\partial v} \dfrac{dh(x)}{dx}. \tag 4$</p>

<p>The above is simply an application of the multiple-variable <strong><em>chain rule</em></strong>.  See <a href=""https://en.wikipedia.org/wiki/Chain_rule"" rel=""nofollow noreferrer"">this widipedia page</a> for more.</p>
"
"2382586","2382592","<p>\begin{align}
 abab &amp;= e \\
 a^2bab &amp;= a \\
 bab &amp;= a \\
 bab^3 &amp;= ab^2 \\
 ba &amp;= ab^2.
\end{align}</p>
"
"2382589","2382756","<p>If you are vetting a random number generator (RNG), this chi-squared test 
is one useful criterion. However, there are some additional considerations:</p>

<p>(a) The chi-squared <em>statistic</em> is based on integer counts, and thus is
discrete. This means the P-values from the test will not be <em>exactly</em>
uniformly distributed in $(0,1).$ [The approximating chi-squared <em>distribution</em>
is continuous, but it is the statistic that matters.] Even though
the P-values are are not exactly $\mathsf{Unif}(0,1),$ they will take 
many values throughout $(0,1)$ if the RNG is working as advertised.</p>

<p>(b) Even if the RNG is is 'good', the P-value will be below 0.05 in about 5%
of your runs. Thus it is necessary to do many runs (as you have done) before
drawing a conclusion about the RNG. You should be suspicious of an RNG that
consistently gives P-values below 0.05.</p>

<p>(c) Perhaps surprisingly, you should also be suspicious of an RNG that
consistently gives P-values <em>above</em> 0.95 (corresponding to very <em>low</em> chi-squared values). A frequent flaw in 'bad' RNGs is that their behavior is 'too regular'.
A chi-squared statistic of 0 arises from <em>perfect fit</em> of observed counts to
expected counts. Analogously, you should be suspicious if someone claims
to have rolled a fair die 600 times, obtaining exactly 100 instances each
of faces 1 through 6. The result seems ""too good to be true.""</p>

<p><em>More generally,</em> thoroughly vetting an RNG requires subjecting it to a large
number of simulation tasks to see that it gets the same answers predicted
by theory. Some of the tests need to be multivariate, because it is possible
for an RNG to make a suitable almost uniform-looking histogram in one dimension
and yet put all its points on only a few hyperplanes of an $n$-dimensional
unit hypercube. [The ""Mersenne-Twister,"" the default RNG in R statistical software, has been vetted up to $n = 623$ dimensions. If you have access to R,
type <code>help .Random.seed</code> and <code>help runif</code> in the Session window for
explanatory pages.]</p>

<p>There are curated 'batteries' of test problems for RNGs. Problems are chosen because they are
notoriously difficult for RNGs that are only 'pretty good' to simulate correctly. One useful package
of such problems is Marsaglia's ""Die Hard,"" which you can read about on
several Internet pages.</p>
"
"2382591","2382595","<p>Read this pdf
<a href=""https://www.dpmms.cam.ac.uk/~dc340/EGT3.pdf"" rel=""nofollow noreferrer"">https://www.dpmms.cam.ac.uk/~dc340/EGT3.pdf</a></p>

<p>This exactly answers your question.</p>
"
"2382593","2382783","<p>Found <a href=""http://www.mathematicsgre.com/viewtopic.php?t=1498"" rel=""nofollow noreferrer"">here</a></p>

<p>If the symmetric group $S_n$ is split into $n$ disjoint subgroups of orders $a_1, a_2...a_n$, the <em>Cauchy</em> number is $\sum _{i=1}^n (a_i-1)$</p>

<p>In your case $(1354)(267)$ means two subgroups of order $4!$ and $3!$, then following the definition the Cauchy number should be $(4!-1)+(3!-1)=28$</p>

<p>$$\left(
\begin{array}{ccccccc}
 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 \\
 3 &amp; 6 &amp; 5 &amp; 1 &amp; 4 &amp; 7 &amp; 2 \\
\end{array}
\right)$$</p>

<p>Hope it's not too wrong :)</p>
"
"2382596","2382614","<p>The author is arguing as follows: for any <em>fixed</em> $n$-tuple $(x^{*1},\ldots,x^{*n})$ of covectors, the function $$(x_1,\ldots,x_n) \mapsto \Omega(x^{*1},\ldots,x^{*n};x_1,\ldots,x_n)$$
is multilinear and skew-symmetric (i.e., it is a determinant function). Therefore by the uniqueness theorem of determinant functions, there exists a scalar $\Phi(x^{*1},\ldots,x^{*n})$ (depending on our fixed $n$-tuple) such that our determinant function is just a multiple of the fixed non-trivial determinant function $\Delta$, i.e., $$\Omega(x^{*1},\ldots,x^{*n};x_1,\ldots,x_n) = \Phi(x^{*1},\ldots,x^{*n}) \Delta(x_1,\ldots,x_n).$$</p>

<hr>

<p>Stated differently: the set of all determinant functions on $E$ actually forms a vector space. (If you're familiar with exterior algebra, it is the space $(\Lambda ^n E)^*$.) The uniqueness theorem above amounts to the statement that this space is one-dimensional, spanned by $\Delta$. Therefore there is an isomorphism $\phi:(\Lambda ^n E)^* \to K$ characterized by $\phi(\Delta) = 1$, where $K$ denotes the base field. The author is implicitly defining a map which I will call $\psi:(E^*)^n  \to (\Lambda ^n E)^*$, by the rule $$\psi(x^{*1},\ldots,x^{*n}) = [(x_1,\ldots,x_n) \mapsto \Omega(x^{*1},\ldots,x^{*n};x_1,\ldots,x_n)].$$
Then his $\Phi$ is precisely the composition $\Phi = \phi \circ \psi$.</p>
"
"2382599","2382605","<p>The solution is wrong. 
Every set in the discrete topology is both open and closed. Closure of a closed set is itself. Hence, the answer should be $(a,b).$</p>
"
"2382604","2382620","<p>You show it's saturated by figuring out what all the Galois types are (there aren't many in this example!), and checking that they're all realized. </p>

<p>Let's say an element which satisfies all the $P_n$ is central. The point is that if a model is uncountable of size $\lambda$, then it must contain a central element (actually $\lambda$-many), so you know which $E$-class all the central elements are in. Over a countable model with no central elements, there are two central Galois types, which can't both be realized in the same model. This is the only obstruction to saturation. </p>
"
"2382611","2382718","<p>Hereâs another technique, which even though of limited utility, works beautifully and quickly when it works at all.</p>

<p>You are asking for roots of $X^4=-1$ over $\Bbb F_5$, and the degree of the extension that one of them (and hence all) generates. But these are the primitive eighth roots of unity, and you need to know the degree of the smallest extension of $\Bbb F_5$ that contains these. In other words, you are looking for the first $n$ such that $8|(5^n-1)$. Answer is $n=2$, of course, thus each such eighth root generates $\Bbb F_{25}$, and each is merely quadratic over $\Bbb F_5$, so that $X_4+1$ necessarily factors into two quadratics. (The method doesnât give the factorization.)</p>

<p>But it works equally well for $X^4+2$, since now youâre talking about fourth roots of $3$, which in turn is a primitive fourth root of unity. So roots of $X^4+2$ are sixteenth roots of unity, and the first $n$ such that $16|(5^n-1)$ is $n=4$. Irreducible.</p>
"
"2382617","2382639","<p>(a) is true.  Let's see why using the definition of lim inf.</p>

<p>Let $\epsilon &gt; 0$.  $\liminf \limits_{n \to \infty} x_{n} = a$ means $\lim \limits_{n \to \infty} g_{n} = a$, where $g_{n} := \inf \limits_{k \geq n} \{x_{k}\}$, right?  So, for some point $N$ in the sequence, we have all later points (i.e., for all $n \geq N$) satisfying $g_{n} \in (a - \epsilon, a + \epsilon)$.  That means $\inf \limits_{k \geq n}\{x_{k}\} \in (a - \epsilon, a + \epsilon)$ <strong>for all $n \geq N$</strong>.  Ok, so let's look at $N$ first.  $g_{N} \in (a - \epsilon, a + \epsilon)$ implies $\inf \limits_{k \geq N} \{x_{k}\} \in (a - \epsilon, a + \epsilon)$, and so by properties of infimum, we can find some point in the sequence $x_{n_{1}}$, with $n_{1} \geq N$.  Now, since $n_{1} \geq N$, that means $g_{n_{1}} \in (a - \epsilon, a + \epsilon)$.  By the same argument as above, we can find some $n_{2} &gt; n_{1}$ so that $x_{n_{2}} \in (a - \epsilon, a + \epsilon)$.  Then we can find $n_{3} &gt; n_{2}$, and so on.  In particular, we have a subsequence $x_{n_{k}}$ satisfying $x_{n_{k}} &lt; a + \epsilon$, which is what we wanted to find, so (a) is true.</p>

<hr>

<p>(b) is false, as Daniel Schepler noted in the comments.  </p>

<p>To show this using the definitions, note that $a - \epsilon &lt; a$, right?  Now, if $\liminf \limits_{n \to \infty} x_{n} = a$, that means $\lim \limits_{n \to \infty} g_{n} = a$, where $g_{n} := \inf \limits_{k \geq n} \{x_{k}\}$, right?  So, after some $N$, all of the $g_{n}$ are within $\frac{\epsilon}{2}$-distance of $a$ by definition of limit.  In particular, for some $N$, $a - \frac{\epsilon}{2} &lt; g_{N} = \inf \limits_{k \geq N} \{x_{k}\}$, implying that $a - \frac{\epsilon}{2} \leq x_{k}$ for all $k \geq N$.  This shows that you can't find a subsequence less than $a - \epsilon$.</p>
"
"2382618","2382640","<p>Note that
$$\frac{P'(z)}{P(z)}=\frac m{z-\xi}+Q(z)$$
where $Q$ is analytic near $\xi$, so that
$$\lim_{z\to\xi}(z-\xi)\frac{P'(z)}{P(z)}=m$$
etc.</p>
"
"2382621","2384999","<p>In ergodic theory it is common to define a <em>simple eigenvalue</em> as one for which the eigenspace has dimension $1$, although as you say this is not the usual definition in some areas. See for example the book <em>Ergodic Theory</em> by Peter Walters; it is not made precise anywhere, as far as I can tell, but that's what the proofs use, see for example the proof of Theorem $1.19$.</p>

<p>Actually, the common definition in dynamical systems is the same as in the particular case of ergodic theory. See for example page 87 of <em>Dynamical Systems: An International Symposium</em> (edited by L. Cesari, J. Hale and J. LaSalle).</p>
"
"2382626","2382641","<p>In contemporary usage, ""algebraic field"" does not have any precise meaning. As you say, ""a field $F$ <em>algebraic</em> <em>over</em> a field $E$"" does have a precise meaning, namely, that every element $x\in F$ is algebraic over the field $E$. Note that $F$ need not be of finite degree over $E$. Yes, an ""(algebraic) number field"" is of finite degree over $\mathbb Q$. A ""global field"" is either a number field or a ""function field"", the latter being a finite extension of $\mathbb F_q(x)$.</p>

<p>It may be that since the word ""field"" has other uses (e.g., ""vector field""), the authors wanted to emphasize that their current use was in this abstract algebra sense.</p>
"
"2382638","2382692","<p>The answer is no in general. You are assuming that $A\ll \lambda$, where $\lambda$ is the Lebesgue measure on the time axis. If the Lebesgue measure also dominates $\langle M,M\rangle$, so that 
$$\langle M,M\rangle\ll \lambda,\tag{1}\label{1}$$
then yes. </p>

<p>The following can be helpful. If there is an equivalent local martingale measure (ELMM) for $X$ then 
necessarily $A\ll \langle M,M\rangle$. If ELMM exists then necessarily 
$B \ll \langle M,M\rangle$. If additionally we know that \eqref{1} holds then you get what you wanted.</p>
"
"2382647","2382681","<p>$2n$ is the correct value. Start from a cycle and then add an extra edge from each vertex to the vertex two spaces clockwise. Can you show that this is $4$-connected?</p>
"
"2382654","2383514","<p>Not really. First, note that each functional $\psi $ on $C_b (X) $ restricts to a bounded functional on $C_0 (X) $, so that there is a (unique!) measure (regular, complex valued)  with $\psi (f) =\int f d\mu $ for $ f \in C_0$.</p>

<p>The problem is that even though both sides of the equality make sense for $f \in C_b $, it is not necessarily true for all such $f $. As an example, consider the (closed) subspace
$$
L := \{f : \Bbb {R} \to \Bbb {C} \,:\, \lim_{|x|\to \infty} f (x) \text { exists}\},
$$
and define a  bounded functional on $L $ by $\psi (f) =\lim_{|x|\to\infty} f (x) $. Since $\psi $ vanishes on $C_0$, the measure from above is $\mu =0$, although $\psi (x\mapsto 1) =1 \neq 0$.</p>

<p>Essentially, the problem is that $C_b / C_0$ is quite large in general.</p>

<p>Note though that the theorem is true if $X $ is compact, simply because in this case, $C_0 = C_b $.</p>

<p><strong>EDIT</strong>: Related to your additional question: One can identify the set of functionals on $C_b (X)$ with the set of regular complex measures on a suitable compactification of $X$. Indeed, if $\beta X$ denotes the <a href=""https://en.wikipedia.org/wiki/Stone%E2%80%93%C4%8Cech_compactification"" rel=""nofollow noreferrer"" title=""StoneâÄech compactification"">StoneâÄech compactification</a> of $X$, then each $f \in C_b (X)$ extends uniquely to a map $\tilde{f} \in C(\beta X) = C_0 (\beta X)$ (note that $f(X)$ is a bounded subset of $\Bbb{C}$, and thus contained in a compact set; in fact, this shows $\| \tilde{f} \|_\sup = \|f \|_\sup$).</p>

<p>Let $\iota : X \to \beta X$ be the embedding of $X$ into $\beta X$ and note that $\iota$ is a homeomorphism of $X$ onto its range in $\beta X$, since $X$ is locally compact, and thus Tychonoff. Now, if we set
$$
V := \{ \tilde{f} \, : \, f \in C_b (X)\},
$$
then $\varphi : V \to \Bbb{C}, \tilde{f} \mapsto \psi(f)$ is well-defined and bounded, and by the Hahn-Banach theorem, we can extend $\varphi$ to a bounded functional on all of $C(\beta X)$. Thus, there is a regular complex measure $\nu$ on $\beta X$ satisfying
$$
\psi(f)=\varphi (\tilde{f}) = \int \tilde{f} d \nu \quad \forall f \in C_b (X).
$$
Note though that we integrate $\tilde{f}$, and not $f$ itself.</p>

<p>Conversely, each complex measure $\nu$ on $\beta X$ also induces a linear functional on $C_b (X)$ by setting $\psi(f) := \int \tilde{f} d \nu$.</p>

<p>In fact, $\nu$ is uniquely determined by $\psi$: It is uniquely determined once one knows $\int g d\nu$ for all $g \in C(\beta X)$. But in fact, we have $V = C(\beta X)$, since if $g \in C(\beta X)$ is arbitrary, then $f := g \circ \iota \in C_b (X)$, and $g = \tilde{f}$ is the unique extension of $g$ to a continuous map on $\beta X$.</p>
"
"2382673","2382957","<p>By <a href=""http://mathworld.wolfram.com/GlassersMasterTheorem.html"" rel=""nofollow noreferrer"">Glasser's Master Theorem</a> for any $a&gt;0$ we have
$$\begin{eqnarray*} \int_{0}^{+\infty}\exp\left[-a\left(s^2+\frac{1}{s^2}\right)\right]\,ds&amp;\stackrel{\text{parity}}{=}&amp;\frac{e^{-2a}}{2}\int_{-\infty}^{+\infty}\exp\left[-a\left(s-\frac{1}{s}\right)^2\right]\\&amp;\stackrel{\text{GMT}}{=}&amp;\frac{e^{-2a}}{2}\int_{-\infty}^{+\infty}e^{-as^2}\,ds=\color{blue}{\frac{\sqrt{\pi}}{2e^{2a}\sqrt{a}}}.\end{eqnarray*} $$
As mentioned by the previous answer, the full generality of $\text{GMT}$ is not really needed, it is enough to prove Boole's statement</p>

<blockquote>
  <p>If $f(s)$ and $g(s)=f\left(s-\frac{1}{s}\right)$ are integrable
  function over the real line,<br> they have the same integral.</p>
</blockquote>

<p>Indeed,
$$ \int_{-\infty}^{0}f\left(s-\frac{1}{s}\right)\,ds\stackrel{s\mapsto\frac{t-\sqrt{4+t^2}}{2}}{=}\int_{-\infty}^{+\infty}f(t)\left(\frac{1}{2}-\frac{t}{2\sqrt{4+t^2}}\right)\,dt $$
$$ \int_{0}^{+\infty}f\left(s-\frac{1}{s}\right)\,ds\stackrel{s\mapsto\frac{t+\sqrt{4+t^2}}{2}}{=}\int_{-\infty}^{+\infty}f(t)\left(\frac{1}{2}+\frac{t}{2\sqrt{4+t^2}}\right)\,dt $$
and the claim simply follows by adding the left hand sides and the right hand sides of these identities.</p>
"
"2382697","2382731","<p>$[F(a) : F] = n&lt; \infty =&gt; \exists \{\lambda_0, ...,\lambda_n\}: \sum_0^n\lambda_ia^i = 0$ $(n+1$ elements can't be linearly independent). </p>

<p>So there exists a polynomial of degree $n$, with root $a$. Hence $a$ is algebraic.</p>
"
"2382705","2382717","<p>Assuming that $A,B$ are invertible, then the relation $ABC=B$ implies that $A^{-1}$ and $C$ are <a href=""https://en.wikipedia.org/wiki/Matrix_similarity"" rel=""nofollow noreferrer"">similar</a> matrices. Indeed, we have 
$$ C = (AB)^{-1}B = B^{-1}A^{-1}B$$</p>
"
"2382710","2385892","<p>$\mathbb{Z}^{(A)}$ is <em>not</em> the definition of a <a href=""https://en.wikipedia.org/wiki/Free_abelian_group"" rel=""nofollow noreferrer"">free abelian group</a>, but rather an <em>explicit construction</em>.</p>

<p>A <a href=""https://en.wikipedia.org/wiki/Free_abelian_group"" rel=""nofollow noreferrer"">free abelian group</a> is just an abelian group that has a basis. This means that every element in the group is a unique (finite) linear combination with integer coefficients of elements in the basis.</p>

<p>A basis for $\mathbb{Z}^{(A)}$ is given by $\{\phi_a : a \in A\}$. The representation of elements in that basis is explicit:
$$
f = \sum_{a \in A}{f(a)\phi_a}
$$
Since $f$ is zero at almost every point of $A$, this is a finite sum. So, the answer to your first question is that $n_a$ is just an integer: $n_a=f(a)$.</p>

<p>About your last point, the simplest example is the <a href=""https://en.wikipedia.org/wiki/Baer%E2%80%93Specker_group"" rel=""nofollow noreferrer"">BaerâSpecker group</a> $\mathbb Z ^{\mathbb N}$ of all integer sequences. This is the same as the set of all functions $\mathbb N \to \mathbb Z$ with componentwise addition.
Baer proved that the BaerâSpecker group is <em>not</em> free abelian, a nontrivial theorem.<a href=""https://en.wikipedia.org/wiki/Baer%E2%80%93Specker_group#Properties"" rel=""nofollow noreferrer"">*</a></p>
"
"2382719","2382894","<p>For you first bullet, you actually have $â«_E |f| \mathrm{d}ð &lt; ð$, not just $|â«_E f \mathrm{d}ð| &lt; ð$. You can see that from your previous result by considering the function $|fâ£$ instead of $f$ (as said in the comments).</p>

<p>For the second bullet, yes, it makes sense.</p>

<p>If you apply the result for each $nââ$, you get a $ð¿_n$ (depending on $n$) such that $â«_A |f_n| &lt; ð$ if $m(A) &lt; ð¿_n$. But these $ð¿_n$ can have an infimum of $0$, so you cannot take this infimum to get a $ð¿$ working for all $n$. But for every <em>finite</em> family of natural number, it works. So we have proved the result for all interval $\{0,1,â¦,N\}$ instead of all $â$ (we now have to treat what happens at infinity).</p>

<p>We have to find an $Nââ$ and a $ð¿&gt;0$ such that $â«_A |f_n| &lt; ð$ if $m(A)&lt;ð¿$ and $nâ¥N$. You have to use that $âf_n-fâ_1 â 0$ to make $f_n$ near enough to $f$. Then, apply the result (Folland 3.6) to $f$ and try to âtransferâ it to each measurable function near enough to $f$ (but you will have a bigger $ð$ depending on the distance at $f$ you allow). I write what I have in mind below if you want.</p>

<blockquote class=""spoiler"">
  <p> Suppose $âg-fâ &lt; ð$. It means that $â«_a^b |g-f| &lt; ð$. If we restrict to a subset $Aâ[a,b]$, the integral can only decrease since the integrand is positive: $â«_A |g-f| &lt; ð$. We get $â«_A |g| &lt; ð + â«_A |f|$. So by forcing $n$ to be big enough, we can control $â«_A |f_n|$ by $â«_A |f|$. But by the result (Folland 3.6) we can make $â«_A |f|$ small by making $A$ small.<br>
 More precisely, let $N$ be such that $âf_n-fâ&lt;ð/2$ for all $nâ¥N$. Let $ð¿_â$ be such that $â«_A |f| &lt; ð/2$ if $m(A) &lt; ð¿_â$. Then for all $nâ¥N$ and all $A$ with $m(A) &lt; ð¿$, we have $â«_A |f_n| &lt; ð/2 + ð/2 = ð$.<br>
 After that, for each $i&lt;N$, take $ð¿_i$ such that $â«_A |f_i| &lt; ð$ if $m(A) &lt; ð¿_i$. The final $ð¿$ is $\min(ð¿_0,ð¿_1,â¦,ð¿_{N-1},ð¿_â)$.</p>
</blockquote>

<p>I don't see the relation with parts (a) and (b) either.</p>

<hr>

<p>If you are interested, we can formulate that more abstractly using the notion of compactness ($ââª\{â\}$ behave like a finite set). The two say the same thing, we can make the proofs correspond.</p>

<blockquote class=""spoiler"">
  <p> Let $M$ be the set of measurable regions. For $A,BâM$, the distance between $A$ and $B$ is $m(Að¥B)$, where $ð¥$ is the symmetric difference. (It is like a metric space.) Then we have a continuous function $MÃL^1 â â_{â¥0}$ sending $(A,f)$ on $â«_A |f|$ (it is $1$-lipschitz in $f$ and continuous in $A$, this is the important thing to prove). The sequence $(f_n)_n$ converging to $f$ is a continuous function from $\overline â$ to $L^1$. This gives a continuous function $MÃ\overline{â} â â_{â¥0}$. The subspace $\{â\}Ã\overline{â}$ is sent on $0$. So by the tube lemma, for all $ð&gt;0$, there is $ð¿&gt;0$ such that $\{AâM\,|\,m(A)&lt;ð¿\}Ã\overline{â}$ is sent in $\left]-ð,ð\right[$.</p>
</blockquote>
"
"2382722","2384126","<p>Here's a solution where your first rule is prioritized over your second rule:</p>

<p><a href=""https://i.stack.imgur.com/zeF3P.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zeF3P.png"" alt=""enter image description here""></a></p>

<p>For each group, the column ""Group tables"" has the number of tables exclusively occupied by that group, ""Leftovers"" has the number of people from that group which are not at a group table, and the column ""Tables with leftovers"" gives the distribution of the leftovers among the remaining tables.</p>
"
"2382726","2382734","<p>The word you are looking for is ""<em>bounded</em>"" (<em>restricted</em> would typically apply to the domain of the function, and mean it is a proper subset of a larger, original domain).</p>

<p>And <strong>no</strong>, it need not be: consider $f\colon[0,1]\to\mathbb{R}$ defined by $f(x)=\sqrt{x}$.</p>
"
"2382738","2382787","<p>When $X=I+tvv^T$, $YX = Y + tYvv^T = Y + t\lambda vv^T$. Trace is linear, so
$$ \operatorname{tr}{(YX)} = \operatorname{tr}{(Y+t\lambda vv^T)} = \operatorname{tr}{Y} + t\lambda \operatorname{tr}{(vv^T)}. $$
Trace is also cyclic, in that $\operatorname{tr}{(AB)}=\operatorname{tr}{(BA)}$, so $\operatorname{tr}{(vv^T)} = \operatorname{tr}{(v^Tv)} = \operatorname{tr}{(\lVert v \rVert_2^2)} = 1$ since this is just a scalar, and the red expression follows.</p>

<p>For the determinant, $\det{(I+tvv^T)}$, since a determinant is invariant under change-of-basis, we can evaluate it using an orthonormal basis where $v$ is the first vector. Then the matrix of $I+tvv^T$ is diagonal with elements $ 1+t , 1,1, \dotsc $, which has determinant $(1+t) \cdot 1 \cdot 1 \cdot \dotsb = 1+t$, which is the blue expression.</p>
"
"2382739","2382741","<p>If you can express the extension as a series of simple extensions, then the basis is any multiplication of basis elements for those extensions.
(what is $\omega$?)</p>
"
"2382749","2382808","<p>We take $\Bbb{N}$ to include zero.</p>

<p>For $x\in \Bbb N $ let proposition $P_x$ be
$$
\forall y\in \Bbb N (x+x \neq y+y+1)
$$
Let $K$ be the set of all $x$ such that $P_x$ is true.  We will establish induction on $K$, to show that $K$ contains every natural number.</p>

<p>To show that zero is in $K$ we must prove 
$$
\forall y \in \Bbb N ( 0+0 \neq y+y+1)
$$
But by the additive identity property, $u+0 = u$ and in particular $0+0 = 0$. And (with $S(t)$ the successor function), by the definition of $1$  $v+1 = v+S(0)$.  Substituting $(y+y)$ for $v$ that reads 
$$
(y+y)+1 = S(y+y)
$$
And by the first ordering axiom, $0$ is not the successor of any natural number, so $0 \neq (y+y)+1$ so 
$$
\forall y\in \Bbb N ( 0+0 \neq y+y+1)
$$</p>

<p>To complete the conditions of the axiom of induction we must show that for every natural number $n$, $n\in K \implies S(n) \in K$.</p>

<p>By the induction hypothesis $P_n$ is true. So
$$
\forall y\in \Bbb N ( n+n \neq y+y+1)
$$
Now assume that for some $y \in \Bbb N$ it were true that 
$ (n+1)+(n+1) = y+y+1$. By the commutative and associative properties of additions, that would be the same as 
$$
(n+n+1) + 1 = y+y+1 
\\ S(n+n+1) =S(y+y)
$$
and by the second axiom defining the successor function if $S(n+n+1)=S(y+y)$ then
$$
y+y=n+n+1
$$ 
That forces $y \neq 0$ by the same ""$0$ is not a successor"" argument,
and also says $y+y = S(n+n)$. Now it is easy to show (again using induction) that $\forall p\in \Bbb N^+( \exists q : (p=S(q))$ so in particular $\exists
z : y=S(z)$.</p>

<p>Then 
$$
y+y = S(z) + S(z) = (z+1)+(z+1) = (z+z+1)+1 = S(z+z+1)
y+y = n+n+1 \implies S(z+z+1) = n+n+1 = S(n+n) \implies (z+z+1) =(n+n)
$$
which contradicts the induction hypothesis. </p>

<p>So for all $y\in \Bbb N$ we have shown  that 
$ (n+1)+(n+1) \neq y+y+1$. But replacing $x$ by $(n+1)$ in the definition of $K_x$,
this is just the statement of $K_{n+1}$:
$$
\forall y\in \Bbb N (  (n+1)+(n+1) \neq y+y+1
$$
Thus induction is established, $K$ includes all natural numbers, $P_x$ is true whenever $x$ is a natural number, and the theorem is proven.</p>
"
"2382754","2382790","<p>Thanks to the comments above, the problem can be solved easily using the Argument Principle, i.e., the solution is the number of zeros of $$f(z)=\frac{z^3(z-1)^2+1}{(z-1)^2}$$
minus number of poles of $f(z)$ inside the disk $|z|&lt;2$, which is clearly $5-2=3$. </p>

<p>Alternatively, observe that $f(z)$ has no zero or poles outside the disk $|z|&lt;2$, so we can choose a large enough disk $|z|&lt;R$ to compute the winding number. More specifically, when $R$ is large enough such that $ \dfrac{f'(z)}{f(z)} \sim \dfrac{3}{z}$, one easily observes that the winding number is 3. </p>
"
"2382757","2382796","<p>The answer is yes and the reason is quite simple. Indeed if $X =\left( \begin{array}{c} x_1 \\ x_2 \\ . \\ .\\ .\\ x_p\end{array}\right)$ </p>

<p>since for all $y\in \mathcal{N}(X)$, $y'x_i = 0$, then $\mbox{Vect}(x_1, x_2,\cdots, x_p) \subset \mathcal{N}(X)^{\perp}$.</p>

<p>On other hand the Gram matrix $X'X$ is symmetric (hermitian in complex) so </p>

<p>$\mathcal{N}(X'X)^{\perp} = \mathcal{I}(X'X)$. Then $\mbox{Vect}(x_1, x_2,\cdots, x_p) \subset \mathcal{I}(X'X)$. Or the two spaces have the same dimension if becomes then $\mbox{Vect}(x_1, x_2,\cdots, x_p) = \mathcal{I}(X'X)$</p>
"
"2382763","2382769","<p><strong>No.</strong> Not all of the elements in $A$ are in $P(A)$. Your error here is that you are assuming that $\{1\}$ means the same thing as $1$ and likewise for $2$, but that isn't true. $\{1\}$ represents the set with the number $1$ as its only element, and $1$ is the first natural number. It is true that
$$1\in \{1\}$$
But not
$$1\in \{\{1\}\}$$</p>

<p>However, you could say that
$$A\in P(A)$$</p>
"
"2382771","2382793","<p>The answer is no. Consider the example</p>

<p>$$\begin{pmatrix}1&amp;1\end{pmatrix}\begin{pmatrix}1&amp;0\\-4&amp;1\end{pmatrix}\begin{pmatrix}1\\1\end{pmatrix}=-2&lt;0.$$</p>
"
"2382774","2382778","<p>Note that $\log n&lt;\sqrt{n}$, so your terms are smaller than $\frac{\sqrt{n}}{n^2-n}$. </p>

<p>Can you take it from here?</p>
"
"2382775","2382809","<p>I think you want the largest circle with center inside the convex hull that does not contain any of the given points in its interior.</p>

<p>Circles that pass through the given points but contain no other given  points in its interior are called Delaunay circles; they are the key to Delaunay diagrams (triangulations in the generic case).</p>

<p>So, the answer to your problem is: find the Delaunay diagram of the given points. Each face determines an empty circle. Take the largest such circle whose center is in the convex hull; this is the same as being in the corresponding Delaunay face.</p>

<p>It takes $\Theta(n \log n)$ time to find the Delaunay diagram. The other steps run in time $O(n)$.</p>
"
"2382781","2382815","<p>To compute this probability, you need to know two things:</p>

<ol>
<li>How many different ways are there of drawing 9 numbers that add to 20?</li>
<li>How many different ways are there of choosing 9 numbers?</li>
</ol>

<p>The probability of drawing numbers that sum to 20 is then the ratio of these two, so the probability that you want is 1 minus this ratio.  The second question is pretty straightforward:  you have 10 choices for the first number, 10 choices for the second number, etc.  Thus there are a total of $10^9$ ways of choosing 9 numbers.</p>

<p>The first question is a little harder, but not impossible if you think about it the right way.  I would suggest that the ""right way"" to think about it is a <a href=""https://en.wikipedia.org/wiki/Stars_and_bars_(combinatorics)"" rel=""nofollow noreferrer"">Stars and Bars</a> diagram.  To get 9 numbers that add to 20, you can draw 20 stars, then put 8 bars between them (dividing 20 into 9 pieces---count the number of stars in each piece).  There are a total of 19 places in which you could put bars, of which you need to <em>choose</em> 8.  Hence there are
$$ \binom{19}{8} = \frac{19!}{8!(19-8)!} = \frac{19!}{8!11!} $$
different ways of choosing 9 numbers that sum to 20.  Note that this overcounts a bit---we would need to eliminate the choices that include the selection of 11 or 12.  But this should be doable by hand (there aren't very many ways of including 11, and only one way (modulo permutation) that I can see of including 12).</p>
"
"2382785","2382803","<p>Here's an application of projections, called a <a href=""https://en.wikipedia.org/wiki/Stereographic_projection"" rel=""nofollow noreferrer"">stereographic projection</a>, that should seem a bit more ""concrete"" in a way.</p>

<p><strong>This below is the called the <em>inverse stereographic projection</em>, mapping a lower dimension to a higher dimension.</strong></p>

<p>Let's begin like this, I want you to look at the image below first. All it contains is a circle of radius $1$ and a line is in the form $y=mx+1$ with $m$ ranging from $-10$ to $10$.</p>

<p><a href=""https://i.stack.imgur.com/VmmSr.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VmmSr.gif"" alt=""enter image description here""></a></p>

<ol>
<li><p>Consider the very top point of the circle, the point $(0,1)$, as undefined.</p></li>
<li><p>Note that as the slope changes, where the line intersects the circle and the $x$-axis changes as well. In a single snapshot of the gif, let's call the point it intersects on the $x$-axis as $(x_0, 0)$ or just $x_0$ for short. Call the point it intersects on the circle $(x_c,y_c)$</p></li>
<li><p>Now try to imagine this: in that random snapshot of the gif (where the line is held still) map (""assign"") the point $x_0$ toÂ $(x_c,y_c)$. Now go to the next snapshot and do it again, and again, ...</p></li>
</ol>

<p>Now let the slope instead ofÂ being anything in the rangeÂ $[-10, 10]$, be anything in $( - \infty, \infty)$. What follows isÂ <em>every single pointÂ on the real number line ($x$-axis)Â gets assigned to a point on the circle!</em> This is the <em>projection</em> aspect. With room to spare even, that point on the top of the circle never got assigned anywhere after all.</p>

<p><strong>Can we find the function of this projection, you say?</strong></p>

<p>Yes! Let's find the function $f$ that tells us where our point $x_0$ goes on the circle. It'll be a function mapping one number to two, or more explicitly, $f: \mathbb{R} \to \mathbb{S} \backslash \{ (0,1) \}$, where $\mathbb{S} \backslash \{ (0,1) \} = \{ Â x \in \mathbb{R}^2 | x^2 + y^2 = 1, \ \ (x,y) \neq (0,1) \}$.</p>

<p>To get started we need to know what we have to work with. Without loss of generality let's let the radius be $1$ (that is, you can make the radius whatever you please, I just wanted to make it look pretty in the end). What we have is</p>

<p>$$\begin{cases} x^2+y^2=1 \\ y=mx+1 \text{ where } m = -\frac{1}{x_0} \end{cases} $$</p>

<p>which is actually all we need. Let's leave the $m$ in there for now though to avoid having to deal with messy equations, and just remember that our circle doesn't include the point $(0,1)$.</p>

<p>From here it's a matter of some plug and chug.</p>

<p>$x^2+(mx+1)^2=1$</p>

<p>$x^2+m^2x^2+2mx=0$</p>

<p>$(1+m^2)x^2+2mx=0$</p>

<p>$\displaystyle{ x=\frac{-2m \pm \sqrt{4m^2}}{2(1+m^2)}}$</p>

<p>$\implies x=\displaystyle{\frac{-2m}{1+m^2}}$</p>

<p>Great! Now we complete the same procedure for the other coordinate.</p>

<p>$\displaystyle{\left(\frac{y-1}{m} \right) ^2 + y^2 = 1}$</p>

<p>$y^2-2y+r^2+m^2y^2=m^2$</p>

<p>$(1+m^2)y^2-2y+(1-m^2)=0$</p>

<p>$y=\displaystyle{\frac{2 \pm \sqrt{4-4(1+m^2)(1-m^2)}}{2(1+m^2)}}$</p>

<p>$ y= \displaystyle{\frac{1 \pm m^2}{1+m^2}}$</p>

<p>Here we must choose $1-m^2$ for the numerator because otherwise things would simplify to $y=1$, and remember, (0,1)Â wasn't defined on our circle.</p>

<p>$\displaystyle{ \implies y=\frac{1-m^2}{1+m^2}}$</p>

<p>All this, in summary, provides the following function:
$$f(x_0)=\displaystyle{ \left&lt; \frac{-2m}{1+m^2}, \frac{1-m^2}{1+m^2} \right&gt;}$$
which, after plugging in the fact that $m=-\frac{1}{x_0}$Â provides...</p>

<p>$$f(x_0)=\displaystyle{ \left&lt; \frac{2x_0}{{x_0}^2+1}, \frac{{x_0}^2-1}{{x_0}^2+1} \right&gt;}$$</p>

<p>And there we have it, any point $x_0$ on the real number line can be mapped to the point $f(x_0)$ on our circle of radius $1$! :)</p>

<p><strong>Now the inverse of our above function, which goes from $\mathbb{S} \backslash \{ (0,1) \} \to \mathbb{R}$ is the stereographic projection, projecting the circle to the real number line.</strong></p>

<p>Try to find the function, which we'll call $g: \mathbb{S} \backslash \{ (0,1) \} \to \mathbb{R}$ yourself. </p>

<p>I'll put the answer below in a spoiler. </p>

<blockquote class=""spoiler"">
  <p>It's found as follows: the slope, since we don't have $x_0$, is now given by $\displaystyle{m=\frac{1-y_c}{-x_c}}$. Solving $y=mx+1$ for $x_0$ is done by substituting $y=0$ then some algebra, giving $\displaystyle{x_0 = f^{-1}(x_0) = g(x_c, y_c) = \frac{x_c}{y_c -1}}$.</p>
</blockquote>

<p>Also note, by finding the inverse of our above function you've also shown that a circle of radius $1$ has ""as many elements"" as the real number line ($\mathbb{S} \backslash \{ (0,1) \} \cong \mathbb{R}$) :)</p>
"
"2382800","2382853","<p>This property (isomorphisms between submodules of a module extend to an automorphism of the module) is possessed by any finitely generated projective module over a <a href=""https://en.wikipedia.org/wiki/Quasi-Frobenius_ring"" rel=""nofollow noreferrer"">quasi-Frobenius ring</a>. (See <em>Lectures on modules and rings</em> by T. Y. Lam pg 415).</p>

<p>$R^n$ is finitely generated and projective, of course, and quasi-Frobenius rings are always Artinian, which relaxes the finiteness condition a lot.</p>

<p>I find myself wondering now if the condition characterizes QF rings or not... I don't know offhand.</p>
"
"2382820","2382891","<p>You are correct, and Godement's proof is incorrect.  His proof proceeds by letting $G$ and $G'$ be two different $\sigma$-compact locally compact groups with the same underlying group, and considers the diagonal $D\subseteq G\times G'$.  He then applies a theorem about $\sigma$-compact locally compact groups to the projection maps $D\to G$ and $D\to G'$.  The problem is that the theorem does not apply, since $D$ may not be $\sigma$-compact or locally compact, since it may not be closed in $G\times G'$.</p>

<p>This is exactly what happens in the case of $\mathbb{R}$ and $\mathbb{R}^2$: the ""diagonal"" in $\mathbb{R}\times\mathbb{R}^2$ is the graph of a group-isomorphism $\mathbb{R}\to\mathbb{R}^2$.  A group-isomorphism $\mathbb{R}\to\mathbb{R}^2$ is a horrendously discontinuous map, so its graph is a horrendous non-closed subgroup of $\mathbb{R}\times\mathbb{R}^2$.  (Quick proof that the graph of an isomorphism $f:\mathbb{R}\to\mathbb{R}^2$ cannot be closed: since $f$ cannot be $\mathbb{R}$-linear, there exists $x\in \mathbb{R}$ such that $f(x)\neq xf(1)$.  But $f(q)=qf(1)$ for all $q\in\mathbb{Q}$, so by approximating $x$ by rationals, we find that $(x,xf(1))$ is in the closure of the graph of $f$.)</p>
"
"2382832","2382848","<p>Your argument might fail because it implicitly assumes that there exists $N$ such that $S_n=S$ for $n\geq N$. For a counter example, we could take $S_n = 1/n^2$, 
and $t_n = 1/n$, then both $(S_n)$ and $(t_n)$ converge towards $0$ and $|S_n| \leq t_n$ for every $n$, however $S_n \neq 0$ for every $n$.</p>

<p>So, let us assume that there exists $M&gt;0$ such that $|S_n-S|\leq t_n$ for every $n\geq M$ and $\lim_{n\to \infty}t_n =0$. </p>

<p><em>Proof of $\lim_{n\to\infty}S_n=S$:</em><br>
Let $\epsilon &gt;0$, then there exists $N$ such that $t_n&lt;\epsilon$ for all $n\geq N$, it follows that for every $n\geq \max\{N,M\}$ we have $|S_n-S|\leq t_n &lt;\epsilon$. Since this is true for every $\epsilon&gt;0$, it follows that $\lim_{n\to \infty} S_n = S$.</p>
"
"2382834","2382844","<p>For fixed $x \in (0,1]$,  the maximum of $f(n,x) = n^2 x/(1+n^3 x^2)$ occurs at $n = 2^{1/3}/x^{2/3}$, with $f(2^{1/3}/x^{2/3}, x) = 2^{2/3}/(3 x^{1/3})$.  This is integrable on $[0,1]$.  Thus you can use Dominated Convergence.</p>
"
"2382836","2382893","<p>$r=1$ in all cases. If $r=2$ (similar arguments for $r&gt;2$) then take two points on some tangent line to the curve. Then any hyperplane containing these will also contain the tangent line and thus tangential to the curve, giving smaller number of points of intersection.</p>
"
"2382842","2382864","<p>This is equivalent to prove that $\mathbb{E} \left[ \eta \zeta (M_t^\tau - M_t^\sigma)(M_t^\rho - M_t^\tau)\right] = 0$. Since $\sigma \le \tau$ then $\mathcal{F}_\sigma \subset \mathcal{F}_\tau$, and $\mathbb{E} \left[ \eta \zeta (M_t^\tau - M_t^\sigma)(M_t^\rho - M_t^\tau)\Bigg | \mathcal{F}_\tau\right] = \eta \zeta (M_t^\tau - M_t^\sigma) \mathbb{E} \left[(M_t^\rho - M_t^\tau) \Bigg | \mathcal{F}_\tau\right]$. Using the Optional sampling theorem we have $\mathbb{E} \left[M_{\rho\wedge t}\Bigg | \mathcal{F}_\tau\right] = M_{\tau \wedge t}$. Can you conclude from here ?</p>
"
"2382851","2382860","<p>First, a moderate simplification:
\begin{equation}0 = z^{14} + z^{15} + z^{16} + z^{17} = z^{14}(1+z+z^2+z^3)\end{equation}
For this to be true, either $z^{14} = 0$ (in which case $z=0$), or
\begin{equation}1+z+z^2+z^3=0.\end{equation}
Since
\begin{equation}
1+z+z^2+z^3 = \frac{z^4-1}{z-1},
\end{equation}
you must have $z^4-1 = 0$.  Hence $z$ must be a fourth root of unity, i.e. $z = \pm 1$, or $z=\pm i$.  But we can't have $z=1$ (this would ""break"" the fraction), and leaving the three roots $z=-1,i,-i$ (you can check these by hand easily enough, but writing
\begin{equation}
1+z+z^2+z^3 = \frac{z^4-1}{z-1} = \frac{(z+1)(z-1)(z+i)(z-i)}{z-1} = (z+1)(z+i)(z-i)
\end{equation}
should be convincing enough.)</p>
"
"2382854","2382875","<p>Suppose $z\in\nu^{-1}(\nu (C))$. This is equivalent to saying that there exists $c\in C$ such that $\nu(z)=\nu(c)$. We have four possibilities.</p>

<p>If $z\in Y$ and $c\in Y$, then $z=c$, and $z\in C\cap Y$.</p>

<p>If $z\in Y$ and $c\in X$, then $f(c)=z$ and $z\in f(C)=f(C\cap A)$.</p>

<p>If $z\in X$ and $c\in Y$, then $f(z)=c$ and $z\in f^{-1}(C)=f^{-1}(C\cap Y)$.</p>

<p>Now, if $z\in X$ and $c\in X$, we have one of the following: either $f(z)=f(c)$ and $z\in f^{-1}(f(C))=f^{-1}(f(C\cap A))$, OR $z=c$ and $z\in C\cap X$.</p>

<p>Since this exhausts all possibilities, we have proved the hint.</p>
"
"2382867","2382877","<p>I usually tell my students not to memorize a bunch of identities, then very quickly tell them that there are two very useful and important trigonometric identities that let you derive all the rest:</p>

<ol>
<li>The Pythagorean Identity: $\cos(\theta)^2 + \sin(\theta)^2 = 1$ for all $\theta$, and</li>
<li>The Angle Addition Formula for Cosine: $\cos(\theta+\varphi) = \cos(\theta)\cos(\varphi) - \sin(\theta)\sin(\varphi)$</li>
</ol>

<p>Writing $2x = x+x$ and applying the angle addition formula, we get
\begin{equation}
\cos(2x)
= \cos(x+x)
= \cos(x)\cos(x) - \sin(x)\sin(x)
= \cos(x)^2 - \sin(x)^2.\end{equation}
Then the Pythagorean identity implies that $\sin(x)^2 = 1-\cos(x)^2$, and so we have
\begin{equation}
\cos(2x)
= \cos(x)^2 - \sin(x)^2
= \cos(x)^2 - (1-\cos(x)^2)
= 2\cos(x)^2 - 1,\end{equation}
which is what you wanted.</p>

<hr>

<p>Geometrically, the Pythagorean identity is just the distance formula, which is really just the Pythagorean theorem in disguise.  Suppose that $p = (x_1,y_1)$ and $q = (x_2,y_2)$ are points in the plane.  How do we measure the distance between them?  This is the same as asking how long the segment joining the to points is, and this segment is the hypotenuse of a right triangle with legs of length $|x_1-x_2|$ (the horizontal distance between the two points) and $|y_1-y_2|$ (the vertical distance between the two points).  By the Pythagorean theorem
\begin{equation}
\text{hypotenuse}^2 = \text{leg}_1^2 + \text{leg}_2^2.
\end{equation}
That is
\begin{equation}
d(p,q)^2 = |x_1-x_2|^2 + |y_1-y_2|^2,
\end{equation}
where $d(p,q)$ denotes the distance between the two points.  Now, remember that if $\theta$ is some angle, then $\cos(\theta)$ and $\sin(\theta)$ are the coordinates of a point on the unit circle (specifically, the point where the angle $\theta$ crosses the unit circle).  But if $(x,y)$ is any point on the unit circle, then
\begin{equation}
1 = 1^2
= d((0,0),(x,y))^2
= |0-x|^2 + |0-y|^2
= x^2 + y^2.
\end{equation}
Setting $x=\cos(\theta)$ and $y=\sin(\theta)$, we get the Pythagorean identity.</p>

<p>The angle addition formula also encapsulates some geometric meaning.  There is a nice visualization <a href=""https://www.geogebra.org/m/cnVdAC6N"" rel=""nofollow noreferrer"">here</a>.</p>
"
"2382879","2382946","<p>A group action $*: X \times G \to X,\, (x,g) \mapsto x*g$ is <strong>transitive</strong> if for any $x,y\in X$ there is a $g \in G$ such that $x*g=y.$</p>

<p>A group action $*: X \times G \to X,\, (x,g) \mapsto x*g$ is <strong>simply transitive</strong> if for any $x,y\in X$ there is a <strong>unique</strong> $g \in G$ such that $x*g=y.$</p>

<p>In your particular example $G=E$ as an additive abelian group $(E,+)$. Let's show first that the action is transitive.</p>

<p>Let $(x', y'),(x,y) \in \mathbb{A}.$ We want to show that there is a $(g,h) \in E$ such that $(x',y')*(g,h)=(x' + g,e^h y')=(x,y).$</p>

<p>So, choose $g=(x-x')$ and $h=\ln(\frac{y}{y'})$. Since $y,y' &gt;0, \ln(\frac{y}{y'})\in \mathbb{R}$ is well defined and $(g,h)\in E$. Then</p>

<p>$$(x',y')*(g,h)=(x' + (x-x'), e^{\ln(\frac{y}{y'})}y')=(x,\frac{y}{y'}y')=(x,y).$$</p>

<p>Hence, the action is transitive. To show that $(g,h)\in E$ is unique, suppose $(g',h')\in E$ also satisfies $(x',y')*(g',h')=(x,y).$ Then</p>

<p>$$(x'+ g, e^hy')=(x,y)=(x' +g',e^{h'}y'),$$</p>

<p>so</p>

<p>$$x'+g=x'+g' \, \text{ and } \, e^hy' = e^{h'}y', $$</p>

<p>so</p>

<p>$$g =g' \, \text{ and } \, y'&gt;0 \implies e^h = e^{h'} \implies h=\ln(e^h)=\ln(e^{h'})=h'.$$</p>

<p>Therefore $(g,h)=(g',h')$ and so the action is simply transitive.</p>
"
"2382881","2382954","<p>When all identity maps satisfy your property, then this is, at heart, a sub-category, where the objects are the same and we take a subset of the morphisms. </p>

<p>You probably want to confine yourself to cases where all identity maps satisfies your property, because the cases where some identity functions are not in the set are ""odder.""</p>

<p>For example:</p>

<ol>
<li>Functions which are not onto.</li>
<li>Functions which are not 1-1.</li>
<li>Functions which are contractible to a point.</li>
<li>Functions that factor through a class of sets - $f:A\to B$ factoring though the class if there is a $C$ in the class such that $f$ factors as $A\to C\to B$ for some functions.</li>
<li>The case of constant functions is a special case of 4.</li>
</ol>

<p>These cases also have a side-property that give $f,g$, for $g\circ f$ to have the property, you need only one of them to have the property. For (1), you only need $g$ in the class. For (2), if $f$ is in the class. For (3-5), If either $f$ or $g$ satisfies our property, then $g\circ f$ is.</p>

<p>So, these cases are sort of like ""ideals"" in the parent category, rather than sub-categories.</p>

<p>The cases (1) and (2) act like prime ideals - if $g\circ f$ is not one-to-one, then one of $f$ or $g$ is not one-to-one.</p>

<hr>

<p>If you have a collection of these properties (either with the identities or not,) then the intersection of them is also a property like this. So differentiable one-to-one functions are closed under composition. </p>
"
"2382895","2388279","<p>Here is the story of my quest for the bounds. I put $\varepsilon=\|Q\|_\infty$ and  split the quest into two parts.</p>

<p>The first of them was for upper bounds for $\det(I+Q)-1$. Clearly, they follow from  upper bounds for $|\det(I+Q)|$. We have  $\|I+Q\|_\infty\le 1+\varepsilon$, so $|\det(I+Q)|\le (1+2\varepsilon+n\varepsilon^2)^{n/2}$, by <a href=""https://en.wikipedia.org/wiki/Hadamard%27s_inequality#Alternate_forms_and_corollaries"" rel=""nofollow noreferrer"">Hadamardâs inequality</a>. Compare it with the bound $2^nn!\varepsilon$ which you have (as I guess, for $\varepsilon\le 1$). By Stirlingâs formula, there exists a number $0&lt;\theta&lt;1$ such that $n!=\sqrt{2\pi n}\left(\frac ne\right)^n e^{\frac \theta{12n}}$. Thus $2^nn!\varepsilon\simeq n^n(2/e)^n\varepsilon$, so Hadamardâs inequality based bound is better. On the other hand, it is almost tight for big $\varepsilon$ (and at least some $n$), because $|\det (1+\varepsilon H_n)|\simeq |\varepsilon H_n|=\varepsilon^nn^{n/2}$, where $H_n$ is a <a href=""https://en.wikipedia.org/wiki/Hadamard_matrix"" rel=""nofollow noreferrer"">Hadamard matrix</a> of order $n$ (provided it exists for such $n$). In case when you are interested only in small $\varepsilon\le 1$, I conjectured that this lower bound can be improved. From the other hand, I looked for inequalities applicable to bounds for $|\det(I+Q)-1|$ in âInequalitiesâ by Edwin F. Beckenbach and Richard Bellman and âIntroduction to matrix analysisâ by the latter, and found none, besides already used  Hadamardâs inequality. </p>

<p>So I started the quest for lower bounds for $\det(I+Q)-1$. Of course, it is at least $-|\det(I+Q)|-1$, so we can apply here the upper bounds for  $|\det(I+Q)|$ from the first part of the quest. For big $\varepsilon$ the summand $-1$ is not essential, so we stop here. </p>

<p>Cleary, if $\det(I+Q)=0$ then we are done. So we may assume the converse. Then provided  $\det(I+Q)&gt;0$ (so we put $\varepsilon&lt;1$) we can bound it using the equlity $\det(I+Q)=1/\det (I+Q)^{-1}$, represent the matrix $(I+Q)^{-1}$ as $I+P$ with $\|P\|_\infty$ small and then apply use for $\det(I+P)$ the upper bounds from the first part of the quest. </p>

<p>For instance, for $\varepsilon&lt;1/n$ the series $-Q+Q^2-Q^3\dots$ converges, and I guess we can put as $P$ its limit. In this case $$\|P\|_\infty\le \varepsilon+n\varepsilon^2+n^2\varepsilon^3+\dots=\frac\varepsilon{1-n\varepsilon} .$$</p>

<p>An other, straightforward way to bound $\det (I+Q)^{-1}$ using the <a href=""https://en.wikipedia.org/wiki/Adjugate_matrix"" rel=""nofollow noreferrer"">adjugate matrix</a> of $I+Q$, but this way is complicate, uses the inductive bounds for minors, and I guess that finally itâll give a weak bound. </p>

<p>But I came to idea to use bounds following from Gauss elimination method for solving systems of linear equations, which looked much more promising. </p>

<p>But at this point I decided to google and found relevant results, which already were partially overlapping with mine. Namely, I found a paper âNote  on  best  possible  bounds  for  determinants of matrices  close  to  the  identity matrixâ [BOS2] by Richard P. Brent, Judy-anne H. Osborn, and Warren D. Smith. </p>

<p>My prize was that bound $\det(I+Q)\ge 1ân\varepsilon$ for $\varepsilon&lt;1/n$ is known from Ostrowskiâs paper from 1938.  Bounds based on Gauss elimination method are worse. This is not so surprising,  because Ostrowskiâs bound is best-possible, as it is attained if $Q =-\varepsilon J$, where $J$ is the $n\times n$ matrix of all ones.</p>

<p>On the other hand, as it sometimes happens in a life of a professional mathematician, the upper bound $|\det(I+Q)|\le (1+2\varepsilon+n\varepsilon^2)^{n/2}$ was already proven (two years ago by the same way) in Theorem 2 of [BOS2]. The authors also remarked that this bound is  best-possible  if  a  <a href=""https://en.wikipedia.org/wiki/Hadamard_matrix#Skew_Hadamard_matrices"" rel=""nofollow noreferrer"">skew-Hadamard matrix</a> $H$ of order $n$ exists. To  see  this,  consider $I+Q=(1 + \varepsilon)I + \varepsilon(H â I)$. Such a matrix exists for $n =1, 2$, all multiples of four up to and including $4\times Ã 68$, as well as infinitely many larger $n$, such as all powers of two, see [CD]. Sharp bounds for small orders for which a skew-Hadamard matrix does not exist (e.g. $n=3$) are considered in [BOS1, Â§4.1].</p>

<p><em>References</em></p>

<p>[BOS1] Richard P. Brent, Judy-anne H. Osborn, Warren D. Smith, <a href=""https://arxiv.org/abs/1401.7084"" rel=""nofollow noreferrer"">Bounds on determinants of perturbed diagonal matrices</a>,  arXiv:1401.7048v7, 2014.</p>

<p>[BOS2] Richard P. Brent, Judy-anne H. Osborn, Warren D. Smith, <em><a href=""https://maths-people.anu.edu.au/~brent/pd/rpb258-LAA.pdf"" rel=""nofollow noreferrer"">Note  on  best  possible  bounds  for  determinants of matrices  close  to  the  identity matrix</a></em>, Linear Algebra and its Applications, <strong>466</strong> (2015), 21â26.</p>

<p>[CD] C.J. Colbourn, J.H. Dinitz, <em>Handbook of Combinatorial Designs</em>, 2nd edition, CRC Press, New York, 2006.</p>

<p>[O] A.M. Ostrowski, <em>Sur lâapproximation du dÃ©terminant de Fredholm par  les dÃ©terminants des systÃ¨mes dâequations linÃ©aires</em>, Ark. Math. Stockholm Ser. A, <strong>26</strong> (1938), 1â15.</p>
"
"2382897","2382916","<p>From the inequality it follows that $f$ is never zero. </p>

<p>Therefore $1/f$ is entire and $|1/f|\leq e^{-|z|}\leq1$. Therefore $f$ ought to be constant.</p>

<p>But $e^{|z|}\rightarrow\infty$ as $z\rightarrow \infty$.</p>

<p>...</p>

<p>You probably don't want that, but in a sense $f(z)=\infty\in S^2$ is constant, therefore analytic everywhere, and $|f(z)|\geq e^{|z|}$.</p>
"
"2382899","2382907","<p>$$ (a-1)^2+(b-1)^2 - 2(\sqrt{ab}-1)^2 = a^2+b^2-2ab-2(a+b)+4\sqrt{ab} = (\sqrt{a}-\sqrt{b})^2((\sqrt{a}+\sqrt{b})^2-2), $$
which is nonnegative if and only if $\sqrt{a}+\sqrt{b} \geq \sqrt{2}$ or $a=b$.</p>
"
"2382912","2383796","<p>Your problem is known as EXACT-TSP and lives in the complexity class DP. EXACT-TSP was proven DP-complete over thirty years ago in the paper ""The complexity of facets (and some facets of complexity)"" by C. H. Papadimitriou and M. Yannakakis (see Theorem 2).  As such, there is no polynomial-time witness for this problem unless the polynomial hierarchy collapses to the first level.</p>
"
"2382920","2382935","<p>The shared side $WY$ is not necessarily the altitude of either triangle, because we don't know if it is perpendicular to the base $XZ$.</p>

<p>However, the triangles <em>do</em> have a shared altitude -- it just might not be shown in the diagram.  Consider the sketch below:
<a href=""https://i.stack.imgur.com/2TS1Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2TS1Y.png"" alt=""enter image description here""></a></p>

<p>In this diagram $XY$=$YZ$ and the dotted line is the altitude of both triangles.  The altitude lies inside one triangle and outside the other, but is perpendicular to both of the two equal bases.</p>
"
"2382923","2382932","<p>Let us compute the expectation of you reward $R$:</p>

<p>$$\mathbb E(R)=\dfrac{11}{38}(-19)+\dfrac{9}{38}(35-18)+\dfrac{18}{38}(10-9)=\dfrac{-11 \cdot 19+9\cdot 17+18}{38}=-1$$</p>

<p>On average you will lose 1$ per game... Even if you have the sentiment to win in 70% of the cases. This is very pernicious! Actually when you lose, you lose quite a lot.</p>
"
"2382925","2382933","<p>Let $N \geq \frac{1}{\varepsilon}$. Then we want to show that for any $n \geq N$, it is true that $|S_n - 2| \leq \varepsilon$. Indeed,
$$|S_n - 2| = \left|2+\frac{1}{n+1} - 2\right| = \left|\frac{1}{n+1}\right|$$
But $n \geq N$, so $n+1 &gt; N$ and $\frac{1}{n+1} &lt; \frac{1}{N} = \frac{1}{\frac{1}{\varepsilon}} = \varepsilon$, as desired.</p>
"
"2382927","2382931","<p>Looks like your integral is $$ \frac 1 {k_j+1} \int_0^{\bar x} d F^{k_j+1}(x) = \frac 1 {k_j+1} (F(\bar x)^{k_j+1}- F(0)^{k_j+1}).$$</p>
"
"2382928","2382930","<p>The first term is zero.
$$ \sum_{m=0}^{\infty} \frac{mx^m}{m!}
 = \frac{0 x^0}{0!} + \sum_{m=1}^{\infty} \frac{m x^m}{m!}
 = \sum_{m=1}^{\infty} \frac{x^m}{(m-1)!}
 = x\sum_{m=1}^{\infty} \frac{x^{m-1}}{(m-1)!}
 = x\sum_{m=0}^{\infty} \frac{x^m}{m!}. $$</p>
"
"2382934","2382959","<p>I consistently find that video lessons/courses are too slow for me to feel like I'm learning at the rate I'd like to be. I prefer to find books or especially online PDFs on the exact topic I'd like to learn about and tackle those, and leave the videos to supplement my understanding.</p>

<p>Reading Pros:  </p>

<ul>
<li>Learn at your own pace</li>
<li>Feels like there exists a wider range of topics as well as more specific texts on a single topic</li>
<li>Tends to have more technical language</li>
<li>Less intrusive than video (for reading at a coffee shop for instance)</li>
<li>Offers more references/further readings than videos</li>
</ul>

<p>Reading Cons:</p>

<ul>
<li>Requires more work keeping yourself focused</li>
<li>Can start to feel dry after too long of staring at a page</li>
<li>Tends to be more difficult to visualize a concept (a large part of my understanding)</li>
</ul>

<hr>

<p>Video Pros:</p>

<ul>
<li>Requires less work on your end; hit play and sit back</li>
<li>Have real people going through each step with you</li>
<li>More options for visualization</li>
</ul>

<p>Video Cons:</p>

<ul>
<li>Usually much slower than books</li>
<li>Often includes wasted time (taking attendance, recap of previous video, stopping to ask questions (usually never good ones), end of the video asks to like/subscribe, etc.)</li>
</ul>

<p>That said, sometimes I find videos that really are worth watching, for example I remember an old series on complex analysis that is on youtube and I really appreciated, also anything by Tadashi Tokieda like the lectures he gave in South Africa. Other than that though, if I really want to use a video to stipend my book learning, I'll set it to 1.25/1.5x speed which helps to make up for how slow it goes, the biggest drawback to videos in my opinion.</p>
"
"2382950","2382962","<p>If $y = \frac{x^2 \ln(x)}{2}-\frac{x^2}{4}$, then $y'=\frac12 (2x\ln x+\frac{ x^2}x)-\frac{2x}4=x\ln x$ since the polynomial terms cancel.</p>

<p>Horizontal lines have gradient $0$, and therefore any tangent line to the curve that is horizontal will have gradient $0$, the same gradient as the curve at that point. Hence we look for them when we set $y'=0$.</p>

<p>Ie: $x\cdot \ln x=0$. Note that both functions ($x$ and $\ln x$) are zero exactly once, so it is enough to check where these functions are zero. For the first one, the obvious one is $x=0$, but note that the logarithm is not defined at $0$, so this answer is discarded. Next, $\ln x=0 \iff x=1$, so this is our only zero.</p>
"
"2382955","2382960","<p>Consider that for $n\geq 1$, $$\sqrt{n+1}\leq 2\sqrt{n}$$ Therefore, $$\frac{\sqrt{n+1}}{4n+\sqrt{n+1}}\leq \frac{\sqrt{n+1}}{4n}\leq \frac{\sqrt{n}}{2n} = \frac{1}{2\sqrt{n}}$$ Therefore, for any $\epsilon &gt; 0$, if $n\geq N &gt; \frac{1}{4\epsilon^2}$ (which we get by solving $\frac{1}{2\sqrt{N}} &lt; \epsilon$), then $$\frac{\sqrt{n+1}}{4n+\sqrt{n+1}} &lt; \epsilon$$</p>
"
"2382965","2382967","<p>Hint: </p>

<ol>
<li><p>$|\cos(1)| &lt; 1$. </p></li>
<li><p>Geometric series.</p></li>
</ol>
"
"2382972","2382990","<p>Let's work out the first example (we'll use common letters for convenience):</p>

<p>$aabcd$</p>

<p>First letter $a$ needs to go to position $3,4$ or $5$ so $3$ choices</p>

<p>Second letter $a$ now should have the same $3$ choices but the first letter $a$ already took a spot, so $2$ choices.</p>

<p>Those first two $a$'s don't generate $3*2$ combinations but only $3$, of the form $\cdot \cdot aa\cdot ,\cdot\cdot a\cdot a,\cdot\cdot\cdot aa$</p>

<p>Now for the last three letters we can work them all in one pack : if the first two $a$ didn't take their place, then they have only $2$ choices, if not, they have $3$ choices. There can only be one of the three initial letters $cde$ so that none of the two $a$ moved to their spot, so one letter out of $cde$ has $2$ choices, and the two left have each $3$ choices. Thanks to Platty you also need to take into consideration that each time you place one of those three letters you take away a spot for the remaining letters so they generate $2*2*1$ combinations.</p>

<p>So finally we got the number of possible derrangement : $3*2*2*1=12$</p>

<p>For the second string $aabbcc$ the number of derangement is $10$.</p>
"
"2382979","2388762","<p>Your analysis is correct. The only reason to exclude $x=0$ would be that the requested regions have to be open sets by the definition of the term ""region"" or ""domain of the ODE"".</p>

<p>This could be also be implied in </p>

<blockquote>
  <p>""<em>graph passing through the point $(x_0,y_0)$ in these regions</em>""</p>
</blockquote>

<p>because to ""pass through"" there needs to be a left and right of $x_0$, resp. $x_0$ needs to be an inner point of the domain of the solution, which is not given if $x_0=0$</p>
"
"2382982","2383361","<p>Your approach is quite  ok.</p>

<blockquote>
  <p>We can enhance readability a bit by denoting the sets $A_1,A_2,A_3,A_4$    with
  \begin{align*}
A_{123},A_{321},A_{456},A_{654}
\end{align*} 
  instead. We  apply the <em><a href=""https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle"" rel=""nofollow noreferrer"">IEP</a></em>
  and obtain
  \begin{align*}
|A|&amp;=6!-\left(|A_{123}|+|A_{321}|+|A_{456}|+|A_{654}|\right)\\
&amp;\qquad+\left(|A_{123}\cap A_{456}|+|A_{123}\cap A_{654}|+|A_{321}\cap A_{456}|+|A_{321}\cap A_{654}|\right)
\end{align*}</p>
  
  <p>We  do not  have to respect more terms, since $$A_{123}\cap A_{321}=A_{456}\cap A_{654}=\emptyset$$</p>
  
  <p>Since $|A_{123}|=4!$ and $|A_{123}\cap A_{456}|=2!$ we get due to symmetry
  \begin{align*}
\color{blue}{|A|}&amp;=6!-4\cdot4!+4\cdot 2!\\
&amp;=720-96+8\\
&amp;=\color{blue}{632}
\end{align*}</p>
</blockquote>
"
"2382992","2382994","<p>$$1+\log_2(5)=\log_2(2) + \log_2(5)=\log_2(2 \times5)=\log_2(10)$$</p>
"
"2382993","2383001","<p>Consider the difference between terms, 
$$a_{n+1} - a_n = (a_n+1/4)^2 - a_n  = (a_n - 1/4)^2 \ge 0$$</p>

<p>For the starting point $a_1 = 1$, all term differences are positive:</p>

<p>$$a_{n+1} &gt; a_n &gt; \cdots &gt; a_2 &gt; a_1$$</p>

<p>And for larger $a_n$, the difference is larger. For any $n$,</p>

<p>$$\begin{align*}
a_{n+1} - a_n &amp;= (a_n-1/4)^2\\
&amp;\ge (a_1-1/4)^2\\
&amp;= (3/4)^2
\end{align*}$$</p>

<p>So by telescoping,</p>

<p>$$a_{n+1} -a_1 \ge n(3/4)^2$$</p>

<p>So the sequence is monotonically increasing and unbounded.</p>
"
"2383014","2383099","<p>Let's go about it piece by piece. This will be an informal explanation but I hope it helps...</p>

<p>You are trying to prove that </p>

<p>$$X= \{\,f\in \mathbb{C}[x]: f(x)=\frac{1}{x-\alpha}, \text{ for some } \alpha \in \mathbb{C}\}$$</p>

<p>is linearly independent. That is, for any finite number of distinct $\, f_1,\ldots,f_k \in X,$
$$F(x)=\sum_{\nu=1}^k \frac{c_\nu}{x-\alpha_{\nu}}=0 \iff \forall \nu \,(c_{\nu}=0).$$</p>

<p>The author of your proof argues by contradiction. If not all of the $c_{\nu}$'s are zero, could $F(x)=0$ for all $x$? The answer is no. Why?</p>

<p>Assume that you already know that $(x-\alpha_1)^{-1}$ is unbounded ""near"" $\alpha_1, c_1 \neq 0$ and the $(x-\alpha_{\nu})^{-1}, \nu \neq 1$ are  bounded ""near"" $\alpha_1.$ Let $N_2,\ldots,N_k$ be their respective bounds.
In that case</p>

<p>$$\bigg\| \sum_{\nu\neq1}\frac{c_{\nu}}{x-\alpha_{\nu}}\bigg\|= C \leq \|c_2\|N_2 +\ldots+\|c_k\|N_k= K$$
for all $x$ ""near"" $\alpha_1$. Now, since $(x-\alpha_1)^{-1}$ is unbounded ""near"" $\alpha_1$ we can find some $x$ ""near"" $\alpha_1$ such that $$\bigg\|\frac{c_1}{x-\alpha_1}\bigg\|&gt;K.$$</p>

<p>For that particular $x$ we have that $F(x)\neq 0$ since the additive inverse of a complex number has the same norm and this 2 numbers do not. Hence $F(x)$ cannot be the zero function. <strong>QED</strong></p>

<p><strong>Why is $(x-\alpha_1)^{-1}$ unbounded ""near"" $\alpha_1$?</strong></p>

<p>Recall that for $x \in \mathbb{R}^+$,$$\lim_{x\to0}\frac{1}{x}= \infty $$</p>

<p>Well, 
$$\lim_{x\to\alpha_1}\|x-\alpha_1\| = 0 \implies \lim_{x\to\alpha_1}\frac{1}{\|x-\alpha_1\|}= \infty,$$</p>

<p>so the norm of the complex number $(x-\alpha_1)^{-1}$ tends to infinity as $x$ tends to $\alpha_1,$ hence $(x-\alpha_1)^{-1}$ is unbounded ""near"" $\alpha_1.$</p>

<p><strong>Why is $(x-\alpha_{\nu})^{-1}$ bounded ""near"" $\alpha_1, \nu \neq 1$?</strong></p>

<p>Take any $\nu \neq 1,$ then $\alpha_{\nu} \neq \alpha_1$ and hence $d_{\nu} = \|\alpha_{\nu} - \alpha_1 \| &gt; 0.$ </p>

<p>Now, assume $x$ is ""near"" $\alpha_1$ if $\|x - \alpha_1\| &lt; \frac{d}{2}$, where $d=\min\{d_2, \ldots, d_k\}.$ So none of the $\alpha_{\nu}$'s are ""near""  $\alpha_1.$</p>

<p>Then if $x$ is ""near"" $\alpha_1$, the distance between say $\alpha_\nu$ and $x,$ $\|x-\alpha_{\nu}\|&gt;d_{\nu} - \frac{d}{2},$ hence $\|(x-\alpha_{\nu})^{-1}\| &lt; \frac{1}{d_{\nu}- \, \frac{d}{2}}=N_{\nu},$ hence $(x-\alpha_{\nu})^{-1}$ is bounded ""near"" $\alpha_1, \nu \neq 1.$</p>
"
"2383019","2383027","<p>The strict inequality should be $\le$, as pointed out in the comments.</p>

<p>One way is to look at the derivative of $f(x) = e^x - x - 1$, which is $f'(x) = e^x - 1$, and note that it is zero only at $x=0$. The second derivative is $f''(x)=e^x &gt; 0$ for all $x$, so $x=0$ is a global minimizer. Finally, note $f(0) = 0$, which yields $e^x-x-1 = f(x) \ge 0$ for all $x$.</p>
"
"2383020","2383028","<p>Note that in your definition, you implicitly quantify your variables $x$ and $y$ over your domain. That is, your definition is:</p>

<p>$$
\forall x,y \in \emptyset , x \neq y \implies g(x) \neq g(y).
$$</p>

<p>Since there are no elements in the null set, this statement holds for every element in the null set (none), and so the function is indeed injective.</p>
"
"2383021","2383056","<p>First, for any $x\in \mathbb{R}_{[0,1)}$ consider these steps. Let's call it <strong><em>Structure</em></strong>:</p>

<blockquote>
  <ol>
  <li><p>Calculate $2x$, if $2x\geq 1$, then  fix first number $1$, and if $2x&lt;1$, fix first number $0$. Now calculate $\{2x\}$, the fraction part of $2x$. </p></li>
  <li><p>Calculate $2\{2x\}$, if $2\{2x\}\geq 1$, then  fix second number $1$, and if $2\{2x\}&lt;1$, fix second number $0$. Now calculate $\{2\{2x\}\}$, the fraction part of $2\{2x\}$.</p></li>
  </ol>
  
  <p>Continue this process. Finally you will get an element of $2^{\mathbb{N}}$.</p>
</blockquote>

<p><strong>The construction of a bijective function:</strong></p>

<p>Let $f:\mathbb{R}_{[0,1)}\rightarrow 2^{\mathbb{N}}$</p>

<p>Suppose $x\in \mathbb{R}_{[0,1)}$ is not of the form $\frac{n}{2^m}$, where $n&lt;2^m$ is odd natural number and $m\in \mathbb{N}$, then $f(x)=$ the final sequence you get from the <strong><em>Structure</em></strong> for $x$.</p>

<p>Now if $x\in \mathbb{R}_{[0,1)}$ is of the form $\frac{n}{2^m}$, where $n&lt;2^m$ is odd natural number and $m\in \mathbb{N}$, then values looks like this:
$$\frac{1}{2}\\\frac{1}{4}\space\space\frac{3}{4}\\\frac{1}{8}\space\space\frac{3}{8}\space\space\frac{5}{8}\space\space\frac{7}{8}\\\frac{1}{16}\space\space\frac{3}{16}\space\space\frac{5}{16}\space\space\frac{7}{16}\space\space\frac{9}{16}\space\space\frac{11}{16}\space\space\frac{13}{16}\space\space\frac{15}{16}\\\dots\\\dots\\\dots$$</p>

<p>First fix $f(\frac{1}{2})=(1,1,1,\dots)$. Now calculate the sequence you get from <strong><em>Structure</em></strong> for taking $x=1/2$. Surely you get $(1,0,0,0,\dots)$. Now fix $f(\frac{1}{4})=(1,0,0,0,\dots)$ and $f(\frac{3}{4})=(0,1,1,1,\dots)$. Again calculate the sequence you get from <strong><em>Structure</em></strong> for taking $x=1/4$ and $x=3/4$. Sequence you get are $(0,1,0,0,0,\dots)$ and $(1,1,0,0,0,\dots)$ respectively. You immediately fix $f(\frac{1}{8})=(0,1,0,0,0,\dots)$, $f(\frac{3}{8})=(0,0,1,1,1,\dots)$, $f(\frac{5}{8})=(1,1,0,0,0,\dots)$ and $f(\frac{7}{8})=(1,0,1,1,1,\dots)$.Continue in this way and finally you have $f$ is a bijective mapping.</p>
"
"2383022","2383094","<p>Since the x-series of this plot has two elements, the y-series needs two elements as well:</p>

<pre><code>plot([0 max(k)], [G G], 'Color', 'red', 'Displayname', 'Infinite Sum');
</code></pre>

<p>The same goes for the second figure:</p>

<pre><code>plot([0 max(n)], [P P], 'Color', 'red', 'Displayname', 'Infinite Sum');
</code></pre>
"
"2383032","2383038","<p>I think you are asking how to interpret the statement of the lemma, as opposed to asking for intuition as to why it is true. Here is a way of interpreting the statement of the lemma:</p>

<ol>
<li>The number of vertices in $G$ is odd or even, as determined by $|V(G)|\pmod 2$</li>
<li>The statement is: for any set of vertices $S\subseteq G$, the difference between the number of odd components of $G-S$, and the number of elements in $S$ is always even (if $G$ has an even number of vertices), or always odd (if $G$ has an odd number of vertices).</li>
<li>The difference between any two numbers is even if they have the same parity, or odd if they don't. So another version of this statement is that $o(G-S)$ and $|S|$ always have the same parity (if $G$ has an even number of vertices), or always have different parity (if $G$ has an odd number of vertices).</li>
</ol>
"
"2383034","2383051","<p>Intuitively, the sum of digits is like a logarithm, so $\delta (x^3)$, the sum of digits of $x^3$, is something like $3 \log x$ and clearly there will be a largest $x$ that satisfies your equation.  We want to find an upper limit for $x$, then we can just search up to that and be done.  </p>

<p>To make this precise, suppose $10^{n-1} \lt x \lt 10^n$, so $x$ has $n$ digits.  Then $x^3 \lt 10^{3n}$ so $x=\delta(x^3) \lt 27n=27 \lceil \log_{10}x\rceil$  We can <a href=""http://www.wolframalpha.com/input/?i=x-27%20log(10,x)%3D0"" rel=""nofollow noreferrer"">ask Alpha to find that</a> for $x \ge 45$, there is no hope because $x \gt 27 \log_{10}x$.  Then we can just search up to $45$ and discover that $27$ is the largest solution.</p>
"
"2383035","2389378","<p>Suppose $Y_i\in\{0,1\}$. Then we have completely characterized $Y_i$ by specifying $$P(Y_i=0)=p_i$$ because it forces $P(Y_i=1)=1-p_i$. The probability mass function completely specifies the random variable, so there is only one parameter for a specific $i$, which is $p_i$.
Furthermore, you said the variables are IID, so $p_i=p\;\forall\;i$. But this is exactly a Bermoulli distribution. So that's really the only option.</p>

<p>However, for logistic regression, I tend to think of it as trying to construct a function $f : \mathbb{R}^n\rightarrow \{0,1\}$. You feed it vectors $X_i\in\mathbb{R}^n$ (with components $x_{ij}$), and try to predict the correct label $\mathfrak{y}_i=\ell(X_i)$. Here are there are many assumptions, beyond the normal IID assumption on the data. For instance, logistic regression is a <em>linear</em> model: it does not assume that the features and labels are linearly related, but it does assume that the relation between the features and the log-odds of the labels <em>are</em> linearly related. 
It assumes that $P(\mathfrak{y}|X)$ is Bermoulli (whereas linear regression tends to assume $P(\mathfrak{y}|X)$ is Gaussian).
It also assumes that the error terms are independent  and that there is no <a href=""https://en.wikipedia.org/wiki/Multicollinearity"" rel=""nofollow noreferrer"">multicollinearity</a>.
Note that these <em>assumptions</em> do not mean that logistic regression <em>won't work</em> without it; it just won't work <em>as well</em> as theory would predict.</p>
"
"2383041","2383266","<p>The volume of this shape is equal to the volume of a whole prism divided by two.
<a href=""https://i.stack.imgur.com/dExoG.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dExoG.jpg"" alt=""enter image description here""></a></p>
"
"2383045","2383076","<p>Your answer is almost correct, though as you point out you can't easily integrate until the different variables $(p,t)$ are on different sides of the equation. The reason is that the proportion $p$ is actually a function of $t$, and so when we integrate</p>

<p>\begin{align*}
\int r[1- p(t)]dt &amp;= r\int (1-p(t))dt &amp; \neq r(1-p)t
\end{align*}</p>

<p>So the only adjustment I would make is in that step, grouping different variables on different sides of the equation to make it clearer. Here's one way to do it. </p>

<p>Let $H$ represent the volume of helium. </p>

<p>The volume of helium at one particular instant is $pV$. You correctly determine that the volume in the next instant will be $pV + r\,dt - pr\,dt$. Hence the change in volume of helium is</p>

<p>$$\begin{align*}
d H &amp;= (pV +r\,dt - pr\,dt)-pV &amp;= r(1-p)\,dt 
\end{align*}$$</p>

<p>so the change in proportion of helium is </p>

<p>$$\begin{align*}
d p = \frac{d H}{V} &amp;= \frac{r(1-p)\,dt}{V} = (1-p)R\,dt 
\end{align*}$$</p>

<p>which we can rearrange as</p>

<p>\begin{align*}
\frac{1}{1-p}dp &amp;= R \cdot dt
\end{align*}</p>

<p>This is called a separable differential equation, because we can put the different variables $(p, t)$ on separate sides of the equation. To solve, we integrate each side separately:</p>

<p>\begin{align*}
\int \frac{1}{1-p}dp &amp;=  \int R \cdot dt\\
-\log(1-p) &amp;= Rt + C\\
1 - p &amp;= \exp{(-C)} \exp{(-Rt)}\\
(p - 1) &amp;= -A\exp(-Rt) &amp; \{A \equiv \exp{(-C)}\}\\
p &amp;= 1 - A\cdot \exp{(-Rt)}
\end{align*}</p>

<p>And from our initial condition that $p(t=0)=0$, we find that $A=1$ so</p>

<p>$$p(t) = 1-\exp{(-Rt)}$$</p>
"
"2383050","2383055","<p>Hint: use the so-to-speak ""multiply and divide by the conjugate"" trick &mdash; it often helps to rationalize. In this case, since you're given a difference $\sqrt{n^2+n}-n$, multiply and divide by the sum of the same two terms $\sqrt{n^2+n}+n$:</p>

<p>$$\lim_{n\to\infty} \left(\sqrt{n^2+n}-n\right)=\lim_{n\to\infty} \frac{\left(\sqrt{n^2+n}-n\right)\left(\sqrt{n^2+n}+n\right)}{\sqrt{n^2+n}+n}=\cdots$$</p>
"
"2383054","2383098","<p><em>What would be the meaning of the value G($\omega$)?</em></p>

<p>Using the notation you gave, a function $f(t)$ can be expressed by its Fourier transform (I excluded the constant parameters) as:
$$f(t) = \int_{-\infty}^{\infty} G(\omega)e^{i\omega t}d\omega$$
In this form, the original signal is expressed as a 'sum' over all frequencies $\omega$ of the functions $e^{i\omega t}$. The function $G(\omega)$ gives the amplitude and the phase (this is a complex value) for the specific function $e^{i\omega t}$ that corresponds to $\omega$.</p>

<p><em>Mathematically why we get that so close relationship between the domains?</em></p>

<p>Can you rephrase this one? What I guess you ask here is why there is a change from time to frequency. If that is the case, I'll try cover it. </p>

<p>Your signal is a function that depends of $t$. By applying a Fourier transform on it, you basically break that function in a 'sum' of $e^{i\omega t}$ functions that depend on the frequency $\omega$, each function having a given amplitude and a phase. As mentioned above, $G(\omega)$ gives you just that, for each frequency needed, in order to reconstruct your signal. That is why the Fourier Transform of the signal aka $G(\omega)$ is a function of frequency, and not time.</p>

<p>I would suggest, you look at this video: <a href=""https://www.youtube.com/watch?v=r18Gi8lSkfM"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=r18Gi8lSkfM</a></p>

<p>It should help with the idea behind Fourier transform, and what it does, although the video focuses on periodic functions. Once you get that part, going to your specific signals should be OK.</p>

<p><strong><em>Edit:</em></strong> </p>

<p>From what I understand from your comment, you asked if one can use the FT to 'encode' some data, such that whenever is needed it can be decoded back and analysed. Yes, this can be done, not sure if it is worth it in terms of data storage, but it is possible.</p>

<p>I'll also add this because it might help get a qualitative understanding of the FT and why it is used most of the time. You can think of this frequency in the musical sense. Say you have a sound generator, that can make one perfect sine wave sound of a given frequency. Now, you wish to generate a 'sound' that results from combining more sine wave frequencies generated individually using one generator each. So at the end, let's assume that you have 1000 generators, each generating a different a sound with a given frequency, and for each generator/frequency you have the ability to control the volume. If you record this using a microphone, you'll get a signal. If you take the FT of that signal, surprise, surprise, what you get are the frequencies and the volume settings you used to generate it in the first place (also with some phase, but again, it isn't important here). You will see if you plot $|G(\omega)|$, that for each frequency you used, say 1kHz, 2kHz, 3kHz and 4kHz, you'll get a very narrow peak around the values $\omega =$ 1kHz, 2kHz, 3kHz and 4kHz and that the value $|G(\omega)|$ at those peaks will be proportional with the volume that you used.</p>

<p>From this, consider that you have a generator for every real number and that you can adjust the volume on all of them as before. Start all of them after they are set up, record the signal with a microphone, do the FT of the signal and you will be in your scenario with functions that are not periodic.</p>

<p>Now, the reason I mentioned all of that is to link it with what I assume the geophysics people use it for and that is to check what frequencies make up an earthquake (or any other earth vibration) and what amplitude (aka volume) does it have. It is the same reasoning as above, with the only difference that now you don't know the frequencies and the volumes on the generators, you only have the resulting signal. What do you do in this case if you want the set up parameters? You take the FT. Why does it matter? Because at some frequencies, different structures might resonate and they can collapse even for small intensity of vibrations. This would be an example.</p>
"
"2383060","2383065","<p>It is $A_5$ of order $60$. Note that $(123)(345)=(12345)$ and since $(12345)$ and $(123)$ generates $A_5$, it must be $A_5$. This is because they are even permutations and so no odd permutations are generated.</p>
"
"2383067","2383089","<p>If it fails then there is a minimal counterexample $n = ab$ with $\,p\mid ab,\ p\nmid a,b.\ $ Necessarily $\, b&lt; p\,$ else $\,p\mid a(b\bmod p) = a(b-jp)\,$ would be a smaller counterexample. Then, similarly we deduce that  $\,p\mid a(p\bmod b) = a(p-kb)\,$ is a smaller counterexample, contradiction.</p>

<p><strong>Remark</strong> $ $ The integers $n$ such that $\,p\mid an\,$ are closed under subtraction and contain $\,b,p\,$ so they contain $\,\gcd(b,p).\,$ The descent in the above proof corresponds to using the Euclidean algorithm to compute the gcd.</p>
"
"2383069","2383179","<p>Let $y_n=x_1+\ldots+x_n$ for $n\ge1$. Then, the recursion reads $y^2_n\,(y_{n+1}-y_n)=1.$ We see that $y_{n+1}-y_n&gt;0$ for $n\ge1.$ Since $y^2$ is monotone, it follows that
$$n=\sum^n_{k=1}y^2_k\,(y_{k+1}-y_k)\le\int^{y_{n+1}}_{y_1}y^2\,dy=\frac13(y^3_{n+1}-y^3_1),$$
i.e. $$y_{n+1}\ge3^{1/3}n^{1/3}.\tag1$$ Consequently $$\frac1{y^2_n}\le3^{-2/3}\frac1{(n-1)^{2/3}},$$ and $$y_{n+1}=y_2+\sum^n_{k=2}\frac1{y^2_k}\le y_2+3^{-2/3}\sum^n_{k=2}\frac1{(k-1)^{2/3}}=y_2+3^{-2/3}\sum^{n-1}_{k=1}\frac1{k^{2/3}}.$$ Now we can estimate the RHS with a telescope: $$k^{1/3}-(k-1)^{1/3}=\frac1{k^{2/3}+k^{1/3}(k-1)^{1/3}+(k-1)^{2/3}}\ge\frac1{3k^{2/3}},$$ that means $$y_{n+1}\le y_2+3^{1/3}(n-1)^{1/3}.\tag2$$
(1) and (2) together give $$\lim_{n\rightarrow\infty}y_n\,n^{-1/3}=3^{1/3}.$$
Since $x_{n+1}=y_{n+1}-y_n=1/y^2_n,$ we have $$\lim_{n\rightarrow\infty}x_n\,n^{2/3}=3^{-2/3},$$ and thus $$\lim_{n\rightarrow\infty}x^3_n\,n^2=3^{-2}=\frac19.$$</p>
"
"2383075","2383091","<p><strong>P.S. The poster forgot to add the condition ""V is bounded"" at first.</strong></p>

<p>I don't think so and the counter example is simple.</p>

<p>Assume $V=\mathbb{R}$ and $F_n=(-\infty,n]$</p>

<p>$$\mathbb{R}=\bigcup_{n\geq1}(-\infty,n)$$
and
$$\partial V=\emptyset$$</p>

<p><strong>And I don't think so even though $V$ is bounded.</strong></p>

<p>Assume $V=(0,1)$ and $F_n=\{0\}\cup[\frac{1}{n+2},1-\frac{1}{n+2}]\cup\{1\}$. 
$$\overset{\circ}F_n=(\frac{1}{n+2},1-\frac{1}{n+2})\quad \Rightarrow\quad V\subset \bigcup_{n\geq1}\overset{\circ}F_n $$
$$\partial V=\{0,1\}\quad \&amp;\quad \partial F_n=\{0,1,\frac{1}{n+2},1-\frac{1}{n+2}\}$$
$$\Rightarrow  \partial V\subset\bigcup_{k\geq1}\bigcap_{n\geq k}\partial F_n $$</p>
"
"2383077","2383088","<blockquote>
  <p>Let $\;P(x+1)-P(x)= Q(x)\;$ so $\;Q(x)-Q(x-1)=6x$</p>
  
  <p>by induction,  $Q(x+n)-Q(x)= 6((x+n)+(x+n-1)+...+(x+1))$</p>
</blockquote>

<p>For $\,x=0\,$: $\displaystyle\;\;Q(n)=Q(0) + 6\big(n+(n-1)+\cdots+1\big)=Q(0)+3n(n+1)\,$, then:</p>

<p>$$\,P(n)=Q(n-1)+Q(n-2)+\cdots+Q(0)+P(0)= P(0)+n\,Q(0)+ 3\sum_{k=0}^{n-1} k(k+1)=\cdots$$</p>
"
"2383092","2383111","<p>The common notation for this is </p>

<p>$\arg \min_{x} f(x)$</p>
"
"2383093","2383100","<p>Seat Albert anywhere. Then for the other chairs, you know the gender of the person that will sit in it.</p>

<p>There are $5!$ ways to place the remaining men in the ""man"" seats.</p>

<p>Then there are $6!$ ways to seat the women in the ""woman"" seats.</p>

<p>So there are $6!5!$ total arrangements.</p>
"
"2383096","2383374","<p>Intersection point is $A$ is where $\sin 0.5x=\cos(2x+2)$</p>

<p>That is $x_A\approx 1.80826$</p>

<p>Derivative of $g(x)=\cos(2x+2)$ is $g'(x)=-2 \sin (2 x+2)$</p>

<p>Slope of tangent is $m=g'(x_A)=1.23674$</p>

<p>Derivative of $f(x)=\sin 0.5x$ is $f'(x)=\dfrac{1}{2} \cos \dfrac{x}{2}$</p>

<p>slope of tangent is $m'=f'(x_A)=0.309185$</p>

<p>The angle formed by the two tangent line is $\alpha$  and</p>

<p>$\tan \alpha=\dfrac{m-m'}{1+mm'}\approx 0.75$</p>

<p>$\alpha=\arctan 0.75\approx 36.86Â°$</p>

<p><a href=""https://i.stack.imgur.com/q7xeo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q7xeo.png"" alt=""enter image description here""></a></p>
"
"2383110","2383138","<p>Which page of G&amp;H did you see $\overline{\nabla}$ without definition ?</p>

<p>p.97 of G&amp;H defines it.</p>

<p>$\overline{\nabla} = \overline{\partial}+A^0$</p>
"
"2383136","2383146","<p>Just as you did $$\frac {dy}{dx}= \frac {a\sqrt{x^2+y^2}+by}{bx}$$ Let $$y=x v\implies y'=v+x v'$$ making the equation to be $$x v'=\frac ab \sqrt{1+v^2}$$ which is separable $$\int \frac{dx} x=\frac b a \int\frac{dv}{\sqrt{1+v^2}}$$ making $$\log(x)+C=\frac b a \sinh ^{-1}(v)$$ So $$v=\sinh\left(\frac ab \log(x)+C\right)\implies y=x \,\sinh\left(\frac ab \log(x)+C\right)$$</p>
"
"2383139","2383168","<p>You are asking for the dimension of a fiber of the projection map $\pi_1$, that is, for $x \in \mathbb{P}^n$, what is the dimension of the set
$$
  \{ f \in P(n,d) \mid f(x) = 0 \}.
$$
This turns out to be a hyperplane in $P(n,d)$, so its dimension is $\dim P(n,d) - 1 = \binom{n+d}{n} - 1$, and that is the rank of the vector bundle.</p>

<p>One simple way to see that the indicated set is indeed a hyperplane in $P(n,d)$ is to take the coordinates on $P(n,d)$ to be the coefficients of monomials appearing in polynomials $f \in P(n,d)$. That is, write each $f = a_1 x_0^d + a_2 x_0^{d-1} x_1 + \dotsb$, and take the $a_i$ to be the coordinates on $P(n,d)$. Now for a fixed $x \in \mathbb{P}^n$, the condition $f(x) = 0$ is a single linear condition on the $a_i$. It is a nonzero condition because $x \in \mathbb{P}^n$ has at least one non-vanishing coordinate, so at least one of the $x_i^d$ is nonzero.</p>

<p>Finally a note of caution. Just because the fibers are vector spaces doesn't mean that $Z$ is a vector bundle. You have to have local trivializations. </p>
"
"2383142","2384082","<p>I will answer your original question. By density you can assume that $u$ is regular and that $\Omega$ is a rectangle, say, $(a,b)\times (c,d)$. By the fundamental theorem of calculus and Holder's inequality
$$|u(x,c)|^r\le |u(x,y)|^r+\int_c^y r|u(x,s)|^{r-1}|\partial_yu(x,s)|\,ds.$$
Integrating in $x$ you get
$$\int_a^b|u(x,c)|^rdx\le \int_a^b|u(x,y)|^rdx+\int_a^b\int_c^y r|u(x,s)|^{r-1}|\partial_yu(x,s)|\,dsdx.$$
Integrating in $y$ over $(c,c+\varepsilon)$ gives 
$$\varepsilon\int_a^b|u(x,c)|^rdx\le \int_c^{c+\varepsilon}\int_a^b|u(x,y)|^rdxdy+\varepsilon\int_a^b\int_c^{c+\varepsilon} r|u(x,s)|^{r-1}|\partial_yu(x,s)|\,dsdx.$$
By Holder's inequality you get
$$\varepsilon\int_a^b|u(x,c)|^rdx\le \int_c^{c+\varepsilon}\int_a^b|u(x,y)|^rdxdy+r\varepsilon\left(\int_a^b\int_c^{c+\varepsilon} |u(x,s)|^{2(r-1)}dsdx\right)^{1/2}\left(\int_a^b\int_c^{c+\varepsilon} |\partial_y u(x,s)|^2dsdx\right)^{1/2}
$$
If $r=1$ you get 
$$\int_a^b|u(x,c)|\,dx\le\frac1\varepsilon \int_c^{c+\varepsilon}\int_a^b|u(x,y)|\,dxdy+((b-a)\varepsilon )^{1/2}\left(\int_a^b\int_c^{c+\varepsilon} |\partial_y u(x,s)|^2dsdx\right)^{1/2}.
$$
This inequality gives you compactness of $W^{1,2}(\Omega) \hookrightarrow L^{1}(\partial \Omega)$. Indeed, if you have a bounded sequence in $W^{1,2}(\Omega)$, then by Rellich-Kondrachov compactness, a subsequence $\{u_n\}$ will converge to some function $u$ in $L^2(\Omega)$ and so 
$$\int_a^b|(u_n-u)(x,c)|\,dx\le\frac1\varepsilon \int_c^{c+\varepsilon}\int_a^b|(u_n-u)(x,y)|\,dxdy+((b-a)\varepsilon )^{1/2}C.
$$
Letting $n\to \infty$ gives
$$\limsup_n\int_a^b|(u_n-u)(x,c)|\,dx\le ((b-a)\varepsilon )^{1/2}C.
$$
and now you let $\varepsilon\to 0$ to conclude that 
$$\lim_n\int_a^b|(u_n-u)(x,c)|\,dx=0.$$</p>

<p>If $r&gt;1$ you get 
$$ \varepsilon\int_a^b|u(x,c)|^rdx\le\int_c^{c+\varepsilon}\int_a^b|u(x,y)|^rdxdy+r\varepsilon\left(\int_a^b\int_c^{c+\varepsilon} |u(x,s)|^{2(r-1)}dsdx\right)^{1/2}\left(\int_a^b\int_c^{c+\varepsilon} |\partial_y u(x,s)|^2dsdx\right)^{1/2}
\\\le \int_c^{c+\varepsilon}\int_a^b|u(x,y)|^rdxdy+r\varepsilon \int_a^b\int_c^{c+\varepsilon} |u(x,s)|^{2(r-1)}dsdx+r\varepsilon \int_a^b\int_c^{c+\varepsilon} |\partial_y u(x,s)|^2dsdx.
$$
Since $N=2$ you have that $W^{1,2}(\Omega)$ is contained in $L^q(\Omega)$ for every $q&lt;\infty$. Hence, the previous inequality says that  $W^{1,2}(\Omega) \hookrightarrow L^{r}(\partial \Omega)$ for every $r$. Since for $r=1$ you have compactness, you get compactness for every $r$ by interpolation.
The general case follows by flattening the boundary locally and using a partition of unity.</p>
"
"2383145","2383149","<p>Let area of $\triangle AED = x$. Then area of $\triangle ADC = 7x$ (same height, consider  the ratio of bases).</p>

<p>By the same reasoning (considering that the ratio of areas of $\triangle BDC$ to $\triangle ADC$ is $5:9$), area of $\triangle BDC = (\frac 59) \cdot (7x) = \frac{35x}{9}$</p>

<p>So the ratio between areas of $\triangle BDC$ and $\triangle AED = \frac{35}{9}x: 8x = 35:72$. Since it's in the lowest terms, the required answer is $35 + 72 = 107$.</p>
"
"2383155","2383172","<p>No. Let $E$ be $\ell^2$ with the norm topology, and 
$$U = \{x\in \ell^2 : \sum |x_n|^2/n^2 \le 1\}$$
Then $U$ is a convex closed neighborhood of $0$. Also, $F=\{0\}$ and $U$ induces a norm on $E/F = E$, namely
$$
\|x\| = \sqrt{\sum |x_n|^2/n^2}
$$
But this topology on $E$ is not the topology we started with. For example, the sequence of standard basis elements $\{e_n\}$ converges to $0$ with respect to $\|\cdot\|$, whereas it did not converge in the original topology of $E$.</p>

<p>Also, $U$ is an unbounded convex neighborhood of $0$ which does not contain any subspace. It can be described as an infinite-dimensional ellipsoid with semi-axes $1,2,3,4,\dots$</p>
"
"2383163","2383164","<p>The integral computes the <em>signed</em> area, which is $0$. Essentially, the ""negative"" when $x \in [\pi, 2\pi]$ cancels out the ""positive"" when $x \in [0, \pi]$. </p>

<p>If you want the area in a purely geometric sense, you want to integrate $\int_{0}^{2\pi} |\sin x| dx = 4$. </p>
"
"2383165","2383188","<p>See <a href=""https://math.stackexchange.com/questions/95537/does-the-set-of-matrix-commutators-form-a-subspace"">Does the set of matrix commutators form a subspace?</a> and <a href=""https://math.stackexchange.com/questions/125219/traceless-matrices-and-commutators"">Traceless matrices and commutators</a>.</p>

<p>Here is an answer using the idea from the first of those links. Let $A$ be a diagonal matrix with distinct, nonzero diagonal entries $a_1,\dotsc,a_n$. Let $B = A + E_{ij}/(a_i-a_j)$. Then $A$ is invertible because it is diagonal with nonzero diagonal entries, and $B$ is invertible because it is upper or lower triangular with nonzero diagonal entries; and we have that
$$
\begin{split}
  AB-BA &amp;= A(A+E_{ij}/(a_i-a_j))-(A+E_{ij}/(a_i-a_j))A \\
   &amp;= (AE_{ij}-E_{ij}A)/(a_i-a_j) \\
   &amp;= (a_i E_{ij} - E_{ij} a_j)/(a_i-a_j) \\
   &amp;= E_{ij} ,
\end{split}
$$
as desired.</p>
"
"2383166","2383198","<p>On adding the two equations we get
$$a(x^2+y^2)+b(x+y)+2c =0.$$
Which can be written as
$$(ax^2+bx+c)+(ay^2+by+c)=0.$$
Since $b^2-4ac=0$ (equal roots), therefore $ax^2+bx+c=a(x-\alpha)^2$, likewise $ay^2+by+c=a(y-\alpha)^2$.
Thus the above equation can be written as:
$$a[(x-\alpha)^2+(y-\alpha)^2]=0.$$
Since $a\neq 0$, therefore $x=y=\alpha$. Thus $x/y=1$.</p>
"
"2383169","2383192","<p>$$\tan^2\dfrac{\pi}{16}+\tan^2\dfrac{2\pi}{16}+\tan^2\dfrac{3\pi}{16}+\tan^2\dfrac{4\pi}{16}+\tan^2\dfrac{5\pi}{16}+\tan^2\dfrac{6\pi}{16}+\tan^2\dfrac{7\pi}{16}=$$
$$=\tan^2\dfrac{\pi}{16}+\cot^2\dfrac{\pi}{16}+\tan^2\dfrac{3\pi}{16}+\cot^2\dfrac{3\pi}{16}+\tan^2\dfrac{\pi}{8}+\cot^2\dfrac{\pi}{8}+1=$$
$$=\left(\tan\frac{\pi}{16}+\cot\frac{\pi}{16}\right)^2+\left(\tan\frac{3\pi}{16}+\cot\frac{3\pi}{16}\right)^2+\left(\tan\frac{\pi}{8}+\cot\frac{\pi}{8}\right)^2-5=$$
$$=\frac{1}{\sin^2\frac{\pi}{16}\cos^2\frac{\pi}{16}}+\frac{1}{\sin^2\frac{3\pi}{16}\cos^2\frac{3\pi}{16}}+\frac{1}{\sin^2\frac{\pi}{8}\cos^2\frac{\pi}{8}}-5=$$
$$=\frac{4}{\sin^2\frac{\pi}{8}}+\frac{4}{\sin^2\frac{3\pi}{8}}+\frac{4}{\sin^2\frac{\pi}{4}}-5=$$</p>

<p>$$=\frac{4}{\sin^2\frac{\pi}{8}}+\frac{4}{\cos^2\frac{\pi}{8}}+3=\frac{4}{\sin^2\frac{\pi}{8}\cos^2\frac{\pi}{8}}+3=\frac{16}{\sin^2\frac{\pi}{4}}+3=35$$</p>
"
"2383170","2383173","<p>No, it is not asymptotic to $x$</p>

<p>$$\lim_{x \to \infty} \frac{x^2(\log(1 + \sqrt{x} + x) + C)}{x} = \infty$$</p>
"
"2383178","2383194","<p>Your argument is wrong because $\lim_n a_n = 1/2$, which binary digits are $0.1$ The $k$-th binary digit of number $x$ and the digits after it are <em>discontinuous</em> at $x$ if $x$ is a fraction $n/2^k$ where $n$ is an odd number.</p>

<p>Your algorithm simply computes the sequence of binary digits of real numbers, and as such, it cannot generate sequences that end up with $111...$.</p>

<p>Edit <strong>after your last algorithm update with ""the structure""</strong></p>

<p>I think the argument is now fundamentally <strong>correct</strong>. The function $f$ now maps every number that is not of the form $\frac{m}{2^k}$ to the sequence of its binary digits. You are left with countably many numbers without an image, namely the $\frac{m}{2^k}$ (with $m$ odd) and also countably many missing elements of $2^\mathbb{N}$, namely the sequences that end with an constant sequence of $0$ or a constant sequence of $1$. You can easily obtain a bijection between these two countable sets by enumerating their elements.</p>

<p>Good job!</p>
"
"2383184","2383190","<p>The general description of a sphere with center $(a,b,c)$ and radius $r$ is
$$
\{(x,y,z)\in \Bbb R^3\mid (x-a)^2+(y-b)^2+(z-c)^2=r^2\}
$$
The description comes directly from the Pythagorean theorem, stating that ""the distance from $(a,b,c)$ to $(x,y,z)$ is $r$"". Now you just need to figure out the center and radius of your sphere and insert that into the above description, and you're done.</p>
"
"2383185","2383200","<p>For part A take the contrapositive i.e to show$ P  \implies Q $ we show ~Q$\implies $~P .Let $Tr(A^*A)=0$ this means that $\sum_{i}^n \sum_{j}^n |a_{ij}|^2=0$ .This means that each $a_{ij}=0\implies  A=0$ and hence not invertible.with this you automatically prove (d) as well.hence A and D are true. To prove C again take contapositive
 Let $|a_{ij}|$ be greater than $1$ for each $i,j $ then $|a_{ij}|^2$ is geater than $1$ for each $i,j $ and since you have $n^2 $ elements hence the sum is greater than $n^2$.Hence c is also true</p>
"
"2383202","2383204","<p>Your approach is correct. However a few steps can be shortened.</p>

<p>Let $d=\gcd(a+b,a-b)$, then $d | a+b$ and $d | a-b$, thus $d$  divides all linear combinations of $a+b$ and $a-b$, in particular $d|(a+b)-(a-b)=2b$ and $d|(a+b)+(a-b)=2a$. Thus $d$ divides both $2a$ and $2b$. </p>

<p>Now you can take it from here.</p>
"
"2383203","2383226","<p>If you want to formalize the idea that any $E \in \mathbb{B}$ can be obtained by applying countably many set operations on sets of $\mathbb{B}$, here is a useful lemma:</p>

<blockquote>
  <p><strong>Lemma.</strong> Let $\mathbb{B}_0 = \mathbb{B}$, and we define $\mathbb{B}_{\alpha}$ for countable ordinals $\alpha &gt; 0$ recursively by</p>
  
  <p>$$ \mathbb{B}_{\alpha} = \{ E^c : E \in \cup_{\beta &lt; \alpha} \mathbb{B}_{\beta} \} \cup \{ \cup_{n=1}^{\infty} E_n : E_n \in \cup_{\beta &lt; \alpha} \mathbb{B}_{\beta} \}. $$</p>
  
  <p>Then the union $\cup_{\alpha} \mathbb{B}_{\alpha}$ over all countable ordinals $\alpha$ is exactly $\sigma(\mathbb{B})$.</p>
</blockquote>

<p>Indeed, we check that</p>

<ul>
<li><p>If $E \in \mathbb{B}_{\alpha}$, then $E^c \in \mathbb{B}_{\alpha+1}$. This shows that $\cup_{\alpha} \mathbb{B}_{\alpha}$ is closed under complement.</p></li>
<li><p>By the transfinite induction, $\mathbb{B}_{\alpha} \subseteq \sigma(\mathbb{B})$ for all countable ordinals $\alpha$. So $\cup_{\alpha} \mathbb{B}_{\alpha} \subseteq \sigma(\mathbb{B})$.</p></li>
<li><p>To show that $\cup_{\alpha} \mathbb{B}_{\alpha}$ is closed under countable union, it suffices to notice that for any sequence $(\beta_n)$ of countable ordinals there exists a countable ordinal $\alpha$ such that $\beta_n &lt; \alpha$ for all $n$.</p></li>
</ul>

<p>So it follows that $\cup_{\alpha} \mathbb{B}_{\alpha}$ is a $\sigma$-algebra satisfying $\mathbb{B} \subseteq \cup_{\alpha} \mathbb{B}_{\alpha} \subseteq \sigma(\mathbb{B})$. By the minimality of $\sigma(\mathbb{B})$, the desired conclusion follows.</p>

<hr>

<p>As an alternative argument,</p>

<ol>
<li><p>It is easy to check that $f^{-1}(\sigma(\mathbb{B}))$ is a $\sigma$-algebra. Since $f^{-1}(\mathbb{B})$ is a subset of this $\sigma$-algebra, we have $\sigma(f^{-1}(\mathbb{B})) \subseteq f^{-1}(\sigma(\mathbb{B}))$.</p></li>
<li><p>To show the other direction, we define $\mathcal{F}$ by</p>

<p>$$ \mathcal{F} = \{ E \in \sigma(\mathbb{B}) : f^{-1}(E) \in \sigma(f^{-1}(\mathbb{B})) \}. $$</p>

<p>Then $\mathbb{B} \subseteq \mathcal{F}$ and it is routine to check that $\mathcal{F}$ is closed under both complement and countable union. So $\mathcal{F}$ is a $\sigma$-algebra and by the minimality we have $\mathcal{F} = \sigma(\mathbb{B})$. This proves the reverse direction $f^{-1}(\sigma(\mathbb{B})) \subseteq \sigma(f^{-1}(\mathbb{B}))$ .</p></li>
</ol>
"
"2383207","2383209","<p>Let $V\subset\mathbb{R}$ be a vector subspace over $\mathbb{Q}$ such that $\mathbb{R}=V\oplus\mathbb{Q}$ (e.g., extend $\{1\}$ to a basis of $\mathbb{R}$ over $\mathbb{Q}$ and let $V$ be the span of all the basis elements except $1$).  We identify $\mathbb{R}$ with the set of ordered pairs $(v,q)$ where $v\in V$ and $q\in \mathbb{Q}$.  Let $A$ be the set of $(v,q)$ such that $v&gt;0$ and $B$ be the set of $(v,q)$ such that $v&lt;0$.  Then $A$ and $B$ are a partition of the irrational numbers, since $(v,q)$ is rational iff $v=0$.  Furthermore, $A$ and $B$ are clearly closed under addition and nonempty.</p>

<p>More generally, if $\preceq$ is any total order on $V$ compatible with the group structure, we could let $A$ be the set of $(v,q)$ such that $v\succ 0$ and $B$ be the set of $(v,q)$ such that $v\prec 0$.  Conversely, every partition $A\cup B$ of the irrationals into two sets closed under addition arises from a total order on $V$ in this way.  Indeed, given such a partition, we get an ordering $\preceq$ on $V$ by saying $V\cap A$ is the set of positive elements of $V$.  For any $v\succ 0$ and $q\in \mathbb{Q}$, we then have $(-v,0)\in B$ and therefore $(v,q)\in A$ since $(-v,0)+(v,q)=(0,q)\not\in B$.  Thus $A$ contains all $(v,q)$ such that $v\succ 0$, and similarly $B$ contains all $(v,q)$ such that $v\prec 0$.</p>
"
"2383208","2383223","<p>Note that we have $$\tan^2 (\theta) + 1 = \sec^2 (\theta)$$
From
$$a \tan \theta + b \sec \theta = c$$
we have $$b\sec \theta = c-a \tan \theta$$</p>

<p>squaring both sides,</p>

<p>$$b^2\sec^2 (\theta)=(c-a\tan\theta)^2$$</p>

<p>$$b^2(\tan^2(\theta)+1)=c^2-2ac\tan\theta+a^2 \tan^2\theta$$</p>

<p>$$(a^2-b^2)\tan^2 (\theta)-2ac\tan \theta + (c^2-b^2) = 0$$</p>

<p>View this as a quadratic equation in $\tan \theta$.</p>

<p>We have $$\tan \alpha + \tan \beta = \frac{2ac}{a^2-b^2}$$</p>

<p>and </p>

<p>$$\tan \alpha \tan \beta = \frac{c^2-b^2}{a^2-b^2}.$$
 Note that 
$$\tan( \alpha + \beta) = \frac{\tan \alpha + \tan \beta}{1 - \tan \alpha \tan \beta}$$</p>

<p>Hence $$\tan (\alpha + \beta) = \frac{\frac{2ac}{a^2-b^2}}{1-\frac{c^2-b^2}{a^2-b^2}} =\frac{2ac}{a^2-c^2}$$</p>
"
"2383210","2383225","<p>It follows from partition norm $\lt \delta$. That means $|x_{i-1} - x_i| \lt \delta$ which implies $x_{i} - x_{i-1} \lt \delta$</p>
"
"2383214","2383230","<p>Seems like you are doing a lot of extra work. How about recalling that the integral of a simple function $\psi(x) = \sum \limits_{i=1}^n a_i \chi_{A_i}Â \left( x \right)$ is $\sum \limits_{i=1}^n a_i \mu(A_i) $. In this case the proof is trivial. Then you can wrap up by e.g. recalling that any measurable $f$ is a pointwise limit of a monotone sequence of simple functions and then applying monotone convergence theorem.</p>
"
"2383218","2383236","<p>It is just a rotation in the plane. $X(t)=Uz$, $z=(x(t),y(t))$;
$U$ is the rotation matrix.</p>
"
"2383229","2384748","<p>This is not true. With a computer search you soon find counterexamples. For example, the claim is false with $G=A_4$, $S= \{1, (1,3,2),(1,4,3)\}$, $T=\{(1,2,3),(1,4,2),(1,3)(2,4)\}$.</p>
"
"2383231","2383468","<p>In a concise way, the Dickman function can be defined as the continuous function $\rho:\mathbb{R}^+\to (0,1)$ which equals $1$ on the interval $(0,1)$ and fulfills the differential equation $\rho'(u) = -\frac{1}{u}\rho(u-1)$. In particular $I&lt;\int_{0}^{+\infty}e^{-u^2}\,du=\tfrac{1}{2}\sqrt{\pi}$ is trivial and</p>

<p>$$\begin{eqnarray*} I &amp;=&amp; \int_{0}^{1}e^{-u^2}\,du +\int_{1}^{+\infty}\rho(u)e^{-u^2}\,du\\&amp;=&amp;\sqrt{\pi}\,\text{Erf}(1)+\tfrac{\sqrt{\pi}}{2}\int_{1}^{+\infty}\rho(u-1)\frac{\text{Erf}(u)}{u}\,du\end{eqnarray*} $$
can be deduced from integration parts and leads to better bounds.<br>
$J$ can be managed in a similar way,
$$ J=\int_{1}^{+\infty}\rho'(u)e^{-u^2}\,du = -\int_{0}^{+\infty}\rho(u)\frac{e^{-(u+1)^2}}{(u+1)}\,du $$
leads to $J\in\left(-\frac{1}{9},0\right)$ and better bounds can be deduced by applying integration by parts and recalling the differential equation fulfilled by $\rho$.</p>
"
"2383233","2383251","<p>Direct calculation!
$$(kABC)\left(\frac{1}{k}C^{-1}B^{-1}A{-1}\right) = \left(\frac{k}{k}\right)ABCC^{-1}B^{-1}A^{-1} = ABB^{-1}A^{-1} = AA^{-1} = I $$</p>
"
"2383238","2383331","<p>In discussing orientation of linear transformations, one (naturally and customarily) <em>fixes an orientation</em> for each vector space under consideration. Consequently, if $E = F$, then $\Delta_{E} = \Delta_{F}$.</p>
"
"2383252","2383414","<p>The equality should be for $(a,1,\sqrt[3]2)||(\sqrt[3]2,b,\sqrt[4]4)||(\sqrt[3]4,2,c),$ which is impossible.</p>

<p>Let $a_1+a_2=3$, $b_1+b_2=6$, $c_1+c_2=12$, $b_1c_1=a_1c_2=a_2b_2$, where $a_i\geq0$, $b_i\geq0$ and $c_i\geq0$.</p>

<p>Thus, for the equality occurring after using Holder we need 
 $$\left(a,\sqrt[3]{a_1},\sqrt[3]{a_2}\right)||\left(\sqrt[3]{b_1},b,\sqrt[3]{b_2}\right)||\left(\sqrt[3]{c_1},\sqrt[3]{c_2},c\right),$$
which is very ugly:
$k_{max}=8.093...$ for $(a,b,c)=(0.904...,1.368...,2.297...)$,</p>

<p>but it says $8$ is a maximum possible integer value, for which our inequality is true </p>

<p>and $(0.904,1.368,2.297)$ is a counterexample for all integer $k&gt;8$. </p>

<p>By the way, there is a counterexample, which is a bit of better: $$(a,b,c)=(1,2,2).$$</p>

<p>In the exam you apply your proof for $k=8$ and for $k\geq9$ you write:</p>

<p>The equality $$(a^3+3)(b^3+6)(c^3+12)\geq k(a+b+c)^3$$
is wrong. $(1,2,2)$ is the counterexample.</p>
"
"2383261","2383264","<p>$I$ is an indexing set. For example, if your family of sets is countable, one can use $I$ to be the set of natural numbers. Since $I$ is not necessarily countable, the general indexing set is expressed as a set $I$.</p>
"
"2383265","2383284","<p>0) Case $\beta = \alpha$ $\rightarrow$ $S_n = 1$ , convergent.</p>

<p>Let $ \beta \ne \alpha$ :</p>

<p>Consider:</p>

<p>$f(x) = \frac{\alpha + x}{\beta +x}, x\in \mathbb{R^+}, \beta \gt 0$.</p>

<p>$f'(x) = $</p>

<p>$\frac{(\beta +x) - (\alpha +x)}{(\beta +x)^2} = \frac{(\beta - \alpha)}{(\beta +x)^2}$ ;</p>

<p>1) $\beta \gt \alpha \rightarrow$  $f'(x) \gt 0$.</p>

<p>2) $\beta \lt \alpha \rightarrow$  $f'(x) \lt 0$.</p>

<p>Choose $x = x_n = n$.</p>

<p>1) $x_{n+1} \gt x_n$  $\rightarrow$  $S_{n+1} \gt S_n $, increasing.</p>

<p>2) $x_{n+1} \gt x_n$  $\rightarrow$  $S_{n+1} \lt S_n $, decreasing.</p>

<p>Now find for 1) an upper bound, and for 2) a lower bound.</p>
"
"2383267","2383315","<p>Your argument is valid. You didn't need the WLOG though; what you wanted to prove was that $u$ is in $(0,1)$. You know that $|u-x|&lt;\epsilon$ therefore, $-\epsilon&lt;u-x&lt;\epsilon$. Adding $x$ yields $$x-\epsilon&lt;u&lt;x+\epsilon\quad (*)$$. Now, $\epsilon=\min\{x,1-x\}$ means that $x+\epsilon\leq x+1-x=1$ and $x-\epsilon\geq x-x=0$. Replacing this to $*$ brings you to
$$0&lt;u&lt;1$$
as desired.</p>
"
"2383279","2383290","<p>Thanks to Aretino i got my answer. </p>

<p>The equation for a line paralel to the y axis is $ x = constant $</p>

<p>The constant is were the line crosses the Y axis, which in (2,4) and (2,1) is 2.</p>

<p>$ x = constant $
$ x= 2$</p>
"
"2383281","2383351","<p>So you have an Artin-Schreier extension $L=K(b)$ where $b^p-b=a$
where $v(a)=-m$, $m&gt;0$ and $p\nmid m$. Then $v_K(b)=-m/p$, or
$V_L(b)=-m$. The generator $\tau$ of the Galois group takes $b$ to $b+1$. To find the ramification groups under the
lower numbering you want to find the largest $n$ such that $\tau$
acts trivially on the ring $O_L/P_L^n$.</p>

<p>Let's identify a generator of $P_L$. There are integers $r$, $s$ with $1=-rm+sp$. Choose $r$ positive with $0&lt;r&lt;p$. We can take $\pi_L=b^r\pi_K^s$. Then
$$\frac{\tau(\pi_L)}{\pi_L}=\frac{(b+1)^r}{b^r}\equiv 1+r/b\pmod{b^{-2}}.$$
So
$$\tau(\pi_L)-\pi_L\equiv r \pi_L/b\pmod{\pi_L/b^2}.$$
So $\tau$ acts trivially on $O_L/P_L^n$ iff $n\le m$
(Serre, <em>Local Fields</em>, Lemma IV.1). This
determines the ramification jump for the lower numbering, and then
one can determine it for the upper numbering too.</p>
"
"2383283","2386662","<p>In your <a href=""http://www.math.harvard.edu/~gross/preprints/Mersenne.pdf"" rel=""nofollow noreferrer"">paper</a> p.1-6 formulates the <a href=""https://fr.wikipedia.org/wiki/Test_de_primalit%C3%A9_de_Lucas-Lehmer_pour_les_nombres_de_Mersenne#Preuve"" rel=""nofollow noreferrer"">Lucas-Lehmer test</a> in the words of Franz's answer. The elliptic curve part starts p.7. In short :</p>

<ul>
<li><p>Look at the elliptic curve $E : y^2=x^3-12x, (x,y) \in \mathbb{Q}$. With its group law define $[n](x,y)$, and also $[i](x,y)= (-x,iy)$ so $E$ has complex multiplication by $\mathbb{Z}[i]$. Also (with the Schoof algorithm ?) one can show $E$ is generated by two points $P= (-2,4)$ of infinite order and $Q = (0,0)$ of order $2$.</p></li>
<li><p>Let $p = 2^l-1$, assume it is prime, and set $E(p) : y^2 = x^3-12x,  (x,y) \in \mathbb{F}_p$, still having CM by $\mathbb{Z}[i]$. The CM theory says $E(p) \simeq \mathbb{Z}[i]/(F-1)$ where $F = a+ib$ is equal to the Frobenius $(x,y) \mapsto (x^p,y^p)$, and $F^2 =-p$.</p></li>
<li><p>Thus $-p = (F-1)^2-1+2(F-1)$ and $F-1$ is coprime with every prime $\ne 2$ and $E(p)$ is cyclic with $2^l$ elements with $P$ as generator.</p></li>
<li><p>If $p$ is not prime and $ord(P) = 2^l$, then take $q$ the least prime factor of $2^l-1$, we still have $ord(P) = 2^l$ in $E(q)$. But (by the Hasse bound) this group has at most $q+1+2\sqrt{q} &lt; 2^l$ points, a contradiction.</p></li>
</ul>

<p>I'm not sure why you need to study it if you don't know the (CM) theory of elliptic curves, you should better start with Silverman's, the arithmetic of elliptic curves.</p>
"
"2383288","2383345","<p>This seems to follow just from the definition of a minimal non-$P_1$-group.</p>

<p>The definition given in the paper is that $G$ is a minimal non-$P_1$-group, if $G$ is not a $P_1$-group and all proper sections of $G$ are $P_1$-groups.</p>

<p>So being a proper section, the quotient $G/\langle z \rangle$ is a $P_1$-group.</p>
"
"2383292","2383320","<p>When you take partial derivatives:
$$\begin{cases}f_x=y-3x^2=0 \\ f_y=x-3y^2=0\end{cases} \stackrel{-}\Rightarrow (y-x)(3y+3x+1)=0 \Rightarrow$$
$$1) \ y=x \Rightarrow x-3x^2=0 \Rightarrow x_1=0=y_1, x_2=\frac13=y_2.$$
$$2) \ y=-\frac{3x+1}{3} \Rightarrow 3x^2+\frac{3x+1}{3}=0 \Rightarrow \emptyset.$$
Second derivatives:
$$f_{xx}=-6x,f_{yy}=-6y,f_{xy}=1,\Delta=f_{xx}f_{yy}-f_{xy}^2=36xy-1.$$
$$1)\ (0,0): \ \Delta&lt;0 \ (saddle).$$
$$2) \ (\frac13,\frac13): \ \Delta=3&gt;0,f_{xx}=-2&lt;0,f_{yy}=-2&lt;0 \ (max).$$</p>

<p>The answer is $E$.</p>
"
"2383295","2383648","<p>Since we aim to apply the closed graph theorem, there is no need to get a bound on $\|A^{-1}\|$. We just apply the theorem.</p>

<p>Suppose that $A:X\to Y$ is a bounded linear bijection between Banach spaces.
Due to the closed graph theorem, it suffices to prove that the graph $\Gamma(A^{-1})$ of $A^{-1}$ is a closed subset of $Y\times X$. To this end, suppose $(y_n)$ is a sequence in $Y$ such that $y_n\to y$ in $Y$ and $A^{-1}y_n\to x$ in $X$ for some $y\in Y$ and $x\in X$. We need to show that $A^{-1}y=x$.
The continuity of $A$ yields that $y_n=AA^{-1}y_n\to Ax$, and thus by uniqueness of limits, $y=Ax$. Therefore $A^{-1}y=x$. This completes the proof that $\Gamma(A^{-1})$ is closed, and consequently that $A^{-1}$ is bounded.</p>
"
"2383310","2383322","<p>If $x$ isn't a unit, then $x\in\mathfrak{m}$ for some maximal ideal.
Could then $x/1\in R_{\mathfrak m}$ be a unit?</p>
"
"2383323","2383330","<p>You have found a candidate for the inverse of $A$ to be $B(AB)^{-1}$.</p>

<p>This is, in fact, a right inverse since $A (B(AB)^{-1})=(AB)(AB)^{-1}=I$.</p>

<p>Now we need to use that the space is finite dimensional.</p>

<p>Since $A$ has a right inverse, its range is the whole space. Therefore its kernel is zero since the dimension of the space is finite.</p>

<p>This means that multiplication by $A$ is surjective and injective. Therefore it has an inverse.</p>

<p>For $B$ the argument is similar. $(AB)^{-1}A$ is a left inverse of $B$. Therefore the kernel of $B$ is zero. Using that the space is finite dimensional we get that the range is the whole space. Therefore it has an inverse too.</p>
"
"2383338","2383359","<p>In simple words, the Central Limit Theorem says that the sum of a <em>sufficiently large</em> number of weakly dependent quantities has a distribution close to normal, <em>regardless of their initial distribution</em>.</p>

<p>So</p>

<blockquote>
  <p>But as I understand, this theorem should only be used if I can't
  assume a normal distribution (if the plotted observations differed
  from a straight line).</p>
</blockquote>

<p>you don't need any assumptions on prior distributions, and it is really the main key point of CLT.</p>

<p><strong>UPDATE</strong> (Thanks to comment by Karl): You don't need any special assumptions on prior distribution while it has well-defined finite mean $\mu$ and variance $\sigma^2$. There exist examples of such distributions which don't satisfy this requirement.</p>
"
"2383341","2383405","<p>Denote $$\text{Si}(x) = \int_0^x \frac{\sin t}{t} dt$$
You can show that, using the Taylor series of $\sin t$ at the origin, $$\text{Si}(x) = x + O(x^3) $$as $x\to 0$.</p>

<p>Thus $$\sum_{n=1}^{\infty} \text{Si}\left(\frac{\sin n}{n}\right) = \sum_{n=1}^{\infty} \left[\frac{\sin n}{n} + O(\frac{\sin^3 n}{n^3}) \right]$$
Since $\sum \sin n / n$ is conditionally convergent, and the big-$O$ term is absolutely convergent, the desired series converges conditionally.</p>
"
"2383342","2383347","<p>This is not quite correct. In particular, $P(a^n)=0\Leftrightarrow P(a)=0$ does not necessarily hold : only one implication does, namely $P(a)=0\Rightarrow P(a^n)=0$.</p>

<p>Indeed, $P(x)|P(x^n)$ means that there exists a polynomial $Q$ such that $P(x^n)=P(x)\cdot Q(x)$; and then of course if $P(a)=0$, then $P(a^n)=P(a)\cdot Q(a)=0\cdot Q(a)=0$.</p>

<p>Here's a hint on how to finish the proof : since the implication $P(a)=0\Rightarrow P(a^n)=0$ holds for all $a\in \Bbb R$, you can iterate this result, and thus any root $a$ gives a sequence of roots $a,a^n,(a^n)^n=a^{n^2},\dots$ (assuming $n\geq 2$ as Henning Makholm mentioned in his comment and his answer) But a polynomial cannot have infinitely many roots, so this sequence must repeat itself at some points. What does that imply?</p>
"
"2383352","2383360","<p>Well, you are given that:
$$f^{(2n)}(0)=0\mbox{ and }f^{(2n+1)}(0)=\frac{(2n)!}{n!}\ \forall n\in\mathbb{N}$$
So, shall we write it - let $k=n$:
$$f^{(2k)}(0)=0\mbox{ and }f^{(2k+1)}(0)=\frac{(2k)!}{k!}\ \forall k\in\mathbb{N}$$
We have just ""renamed"" $n$ to $k$. Now, we have, since all even terms of the MacLaurin series expansion of $f$ are zero:
$$\sum_{k=0}^\infty\frac{f^{k}(0)}{k!}x^k\overset{\text{all even terms}}{\underset{\text{are zero}}{=}}\sum_{k\text{ is odd}}\frac{f^{k}(0)}{k!}x^k\overset{k=2m+1}{=}\sum_{m=1}^\infty\frac{f^{2m+1}(0)}{(2m+1)!}x^{2m+1}\overset{(2)}{=}\sum_{m=0}^\infty\frac{1}{(2m+1)m!}x^{2m+1}$$</p>

<p>So, to sum up, the key is that that given equalities hold for every $n\in\mathbb{N}$.</p>

<p>Moreover, $k,n,m$ are just indices that ""run"" throughout the natural numbers or the odd natural numbers etc. I could write ""whatever"" instead of $m$, which would make no difference to what I would mean.</p>
"
"2383353","2383408","<p>Split the interval $[0,1[\&gt;$ into half-open subintervals of length ${1\over36}$. Allocate the first of these subintervals to the sum (dice outcome) $2$, the next two to the sum $3$, the nect three to the sum $4$, and so on, until the last subinterval which is allocated to the sum $12$. In this way we have obtained a partition of $[0,1[\&gt;$ into eleven <em>fractions</em>  whose lengths correspond to the probabilities of the various sums.</p>

<p>Now draw bits $b_k\in\{0,1\}$ from your random generator and observe the sums $$s_n:=\sum_{k=1}^n b_k 2^{-k}\qquad(n\geq1)\ .$$</p>

<p><strong>Stop</strong> when $s_n+2^{-n}$ is in the same fraction as $s_n$.</p>

<p>The random variable $S$ representing the sum of two independent dice throws has entropy
$$H=\sum_{k=2}^{12} p_k \log_2\left({1\over p_k}\right)=3.2744\quad  {\rm [bits]}\ .$$
This means that in the ""Shannon limit"" $3.2744$ throws of a coin would be sufficient to simulate one sample of $S$. I made $3.6\cdot 10^6$ trials with the proposed algorithm, and came to $5.062$ drawings of a random bit per sample. This is better than the $6$ bits  required in other proposed setups.</p>
"
"2383354","2383363","<p>There's really no trick to this. It is
$$\left(\sum_j(v_j-w_j)\phi_j\right)^2
=\left(\sum_j(v_j-w_j)\phi_j\right)\left(\sum_k(v_k-w_k)\phi_k\right)
=\sum_{j,k}(v_j-w_j)(v_k-w_k)\phi_j\phi_k$$
plus a few integral signs.</p>
"
"2383355","2383460","<p>The formula should read $$T(n,k)={n \choose k}\;_2F_1(1,-k;n+1-k;-1)$$ but an equivalent formula is $$T(n,k)=\sum_{i=0}^k{n \choose i}$$
where $k&lt;=n$ and zero if $k&gt;n$.</p>
"
"2383365","2383377","<p>No, unfortunately you aren't right.</p>

<p>Remind that $\frac{d}{dx} x^n = n\cdot x^{n-1}$ and $(u(v))' = u'(v)\cdot v'$.</p>

<p>These rules lead us to</p>

<p>$$\frac{d}{dx} (x+2)^{-1} = -1\cdot (x+2)^{-2} = -\frac{1}{(x+2)^2}$$
and
$$\frac{d^2}{dx^2} (x+2)^{-1} = \frac{d}{dx} \left(-\frac{1}{(x+2)^2}\right) = -2\cdot -(x+2)^{-3} = \frac{2}{(x+2)^3}$$</p>
"
"2383366","2383383","<p>You can do euclidean algorithm...
I'm starting off with bigger $n$...</p>

<p>So</p>

<p>$21n+4=1(14n+3)+(7n+1) $ since only one 14 in 21.</p>

<p>$14n+3=2(7n+1)+1 $  since two 7's in 14.</p>

<p>So now we work backwards...</p>

<p>$(14n+3)-2(7n+1)=1$ 
Then replace $(7n+1)$ with $(21n+4)-1(14n+3)$</p>

<p>$(14n+3)-2[(21n+4)-(14n+3)]=1$</p>

<p>$3(14n+3)-2(21n+4)=1$
Then you could multipy both sides by -1 giving you
$2(21n+4)-3(14n+3)=-1$</p>

<p>But this might be overkill. </p>
"
"2383367","2383372","<p>Since $x^3+1=(x+1)(x^2-x+1)$, $x^6+1=(x^2+1)(x^4-x^2+1)$. Therefore, your final sum is equal to$$\int\frac1{x^4-x^2+1}\,\mathrm dx-\int\frac{x^2}{x^6+1}\,\mathrm dx$$</p>
"
"2383380","2383452","<p>As mentioned in the comment. The three given points form a $V$ of bottom $C$. The three cases are : 1) placing $D$ so $CA$ is diagonal  $CB$ a side.
2) versa 1)</p>

<p>3) $CA$ and $CB$ are two sides.</p>
"
"2383387","2383546","<p>Write $I_{m,i} = [\frac{i}{2^m},\frac{i+1}{2^m}]$ and introduce the averaging operator $A_m : L^2([0,1]) \to L^2([0,1])$ given by</p>

<p>$$ (A_m f)(t) = \sum_{i=0}^{2^m-1} \left( 2^m \int_{I_{m,i}} f(s) \, \mathrm{d}s \right) \mathbf{1}_{I_{m,i}}(t). $$</p>

<p>We claim that $A_m$ is a bounded operator on $L^2([0,1])$. Indeed, this follows from the following computation</p>

<p>\begin{align*}
\|A_m f(t)\|_{L^2}^2
&amp;= \int_{0}^{1} |A_m f(t)|^2 \, \mathrm{d}t \\
&amp;\leq \int_{0}^{1} \sum_{i=0}^{2^m-1} \left( 2^m \int_{I_{m,i}} |f(s)| \, \mathrm{d}s \right)^2 \mathbf{1}_{I_{m,i}}(t) \, \mathrm{d}t \\
&amp;= \sum_{i=0}^{2^m-1} 2^m \left( \int_{I_{m,i}} |f(s)| \, \mathrm{d}s \right)^2  \\
&amp;\stackrel{\text{C-S}}{\leq} \sum_{i=0}^{2^m-1} \int_{I_{m,i}} |f(s)|^2 \, \mathrm{d}s
= \|f\|_{L^2}^2.
\end{align*}</p>

<p>Notice that this operator is related to the quantity of interest by the following identity:</p>

<p>$$ \int_{[0,1]}\int_{[0,1]} f(t)g(s) 2^m \sum_{i=1}^{2^m-1}\mathbf{1}_{I_{m,i}}(t)\mathbf{1}_{I_{m,i}}(s) \, \mathrm{d}t\mathrm{d}s
= ( A_m f, g )_{L^2}
= ( f, A_m g )_{L^2} $$</p>

<p>Next, if $f$ is continuous then it is easy to prove that $A_m f \to f$ in $L^{\infty}$ and hence in $L^2$. Thus for any $f, g \in L^2([0,1])$ and for any $\tilde{g} \in C([0,1])$, we have</p>

<p>\begin{align*}
|(f, A_m g)_{L^2} - (f, g)_{L^2}|
&amp;\leq \|f\|_{L^2} \| A_m g - g\|_{L^2} \\
&amp;\leq \|f\|_{L^2} \left( \| A_m(g - \tilde{g}) \|_{L^2} + \| A_m \tilde{g} - \tilde{g} \|_{L^2} + \| \tilde{g} - g \|_{L^2} \right) \\
&amp;\leq \|f\|_{L^2} \left( 2 \| g - \tilde{g} \|_{L^2} + \| A_m \tilde{g} - \tilde{g} \|_{L^2} \right).
\end{align*}</p>

<p>Taking limsup as $m\to\infty$, since $A_m \tilde{g} \to \tilde{g}$ in $L^2$, we have</p>

<p>$$ \limsup_{m\to\infty} |(f, A_m g)_{L^2} - (f, g)_{L^2}| \leq 2 \|f\|_{L^2} \| g - \tilde{g} \|_{L^2}. $$</p>

<p>Then taking $\tilde{g} \to g$ in $L^2$ proves the desired conclusion:</p>

<p>$$ \lim_{m\to\infty} (f, A_m g)_{L^2} = (f, g)_{L^2}. $$</p>

<hr>

<p><em>Remark.</em> This result itself tells that $A_m \to I$ in the <a href=""https://en.wikipedia.org/wiki/Weak_operator_topology"" rel=""nofollow noreferrer"">weak operator topology</a>. On the other hand, a slight modification of the proof actually tells that $A_m \to I$ in the <a href=""https://en.wikipedia.org/wiki/Strong_operator_topology"" rel=""nofollow noreferrer"">strong operator topology</a>:</p>

<p>$$ \forall g \in L^2([0,1]) \ : \qquad A_m g \to g \quad \text{in } L^2. $$</p>

<p>I am not sure if $A_m \to I$ in the operator-norm topology.</p>
"
"2383389","2383425","<p>As mentioned in the comments in this particular case an automorphism of the field $\Bbb{Q}[\alpha]$ is determined by its action on $\sqrt{3}$ and $\sqrt{5}$.  To answer your question as to what is going on more generally one can look at the discriminant $\Delta = \prod_{i&lt;j}(\alpha_i-\alpha_j)^2$.  $\Delta$ is fixed by the action of the Galois group, but if we consider the action of the Galois group on $\sqrt{\Delta}$, we find for $\sigma$ in the Galois group, $\sigma(\sqrt{\Delta}) = \text{sign}(\sigma)\sqrt{\Delta}$.  It's obvious that $\Delta$ lies in the ground field (since it's fixed by the Galois group), but if $\Delta$ is a square, then $\sqrt{\Delta}$ also lies in the ground field from which we can conclude that all the elements of the Galois group have even signature.  Finally getting back to your question about $\Bbb{Q}[\alpha]$.  Since the discriminant of $f(x)$ equals $2^{14}\,3^2\,5^2$, we can conclude the discriminant of $\Bbb{Q}[\alpha]$ is a square (in fact, $2^4\,3^2\,5^2$).  The reason there can be no isomorphism $\alpha_1\rightarrow \alpha_2,\alpha_3\rightarrow \alpha_1$ is that such an isomorphism would necessarily have signature $-1$ contradicting $\sqrt{\Delta}$ is fixed by the Galois group.</p>
"
"2383397","2383412","<p>We can see that $f$ isn't differentiable at $0$ [and generally at $2m\pi$ by periodicity] by estimating the difference quotient</p>

<p>$$\frac{f(x_k) - f(0)}{x_k}$$</p>

<p>for a suitable sequence $(x_k)$ converging to $0$. Let's choose $x_k = \frac{1}{k}$. Then we have</p>

<p>$$f(x_k) \geqslant \sum_{n = 1}^k \frac{1}{n^2}\sin \frac{n}{k} - \sum_{n = k+1}^{\infty} \frac{1}{n^2} &gt; \frac{2}{\pi k} \sum_{n = 1}^k \frac{1}{n} - \frac{1}{k} &gt; \biggl(\frac{2}{\pi}\log k - 1\biggr)x_k$$</p>

<p>since $\sin x &gt; \frac{2}{\pi} x$ for $0 &lt; x &lt; \pi/2$, $\sum_{n = 1}^k \frac{1}{n} &gt; \log k$, and $\sum_{n = k+1}^{\infty} \frac{1}{n^2} &lt; \sum_{n = k+1}^{\infty} \frac{1}{n(n-1)} = \frac{1}{k}$. So we directly see that</p>

<p>$$\lim_{k\to\infty} \frac{f(x_k)}{x_k} = +\infty.$$</p>
"
"2383399","2384010","<p>You are not really using Evan's results since it considers bounded open sets, which has boundary $\Gamma_T$. Indeed just prove directly: </p>

<blockquote>
  <p>Weak maximum principle: Let $f : M\times [0,T]\to \mathbb R$ satisfies 
  $$ (\partial_t - \Delta) f \ge 0.$$ 
  If $f\ge 0$ at $t=0$, then $f\ge 0$ on $[0,T]$. </p>
</blockquote>

<p>Proof: Let $\epsilon &gt;0$ and consider $f_\epsilon = f+ \epsilon t$. Then 
$$ (\partial_t - \Delta ) f_\epsilon \ge \epsilon, \ \ \ f_\epsilon \ge 0.$$
If $f_\epsilon &lt;0$ at some points, since $M\times [0,T]$ is compact, let $X=(x, t)$ be where $f_\epsilon$ is minimized. Then $t&gt;0$ and so at this point, 
$$ \partial_t f_\epsilon \le 0, \Delta f_\epsilon \ge 0$$
(We use also that $M$ has no boundary to conclude the second inequality). Thus $(\partial_t -\Delta )f_\epsilon \le 0$, which is impossible. Thus $f_\epsilon \ge 0$ and so $f\ge -\epsilon T$. Then $f\ge 0$ by taking $\epsilon \to 0$. </p>

<p>Now note that both $H, \phi\ge H_{min}(0)&gt;0$. Thus $g = H-\phi$ satisfies</p>

<p>$$(\partial_t -\Delta )g \ge \frac{1}{n}g (H^2 + H\phi + \phi^2) \ge Cg$$</p>

<p>for $C= \frac{3}{n}H^2_{min}(0)$. Then $f = e^{-Ct}g$ satisfies 
$$(\partial_t -\Delta)f \ge 0$$
and $f\ge 0$ at $t=0$. So the weak maximum principle implies $f\ge 0$ for all time $t\in [0,T]$ and so $H\ge \phi$. </p>

<p>I assume this theorem can be found in any books on Ricci flows. </p>
"
"2383400","2383404","<p>Same as the reals. Every real number can be written in base $2$, thus using only $0,1$.  So that's the same cardinality as the reals.  Now take those expressions and change all the $0's$ to $2's$ and all the $1's$ to $3's$.  That the same set but reading them as base $10$ expansions we now have a subset of the reals with the same cardinality as the reals.  Your set is between the two. </p>

<p>Note:  if you want to see your exact set in this manner, look at the base $9$ representations of the reals.  Now adding one to each digit gives us your set.</p>
"
"2383433","2384035","<p>Since $|A|^2 = \sum_i^n \kappa_i^2$ and $H = \sum_i^n \kappa_i$ we have $H^2 \le n |A|^2$. Thus we have $$|A|^2H - |A|^2 = |A|^2 (H-1) \ge \frac 1 n \left(H^3 - H^2\right)$$ whenever $H-1 \ge 0.$ </p>

<p>Thus the comparison principle for parabolic PDE gives $H \ge \varphi$ for $\varphi$ any solution of $\varphi'(t) = \frac 1 n \left( \varphi^3 - \varphi^2 \right)$ with $\varphi(0) \le \inf H(\cdot,0)$, <em>so long as $\varphi - 1 \ge 0.$</em> </p>

<p>Since this inequality is true by assumption at the initial time, we now just need to show that it is preserved by the ODE. As $F(\varphi) = \frac 1 n \left( \varphi^3 - \varphi^2 \right)$ is locally Lipschitz near $1$ and $\varphi(t) = 1$ is a solution, the uniqueness theorem tells us that a solution that is initially $\ge 1$ cannot fail to be so at a later time.</p>
"
"2383437","2384154","<p>Both the Lagrangian and Newtonian method should result in the same equations of motion. The only difference is how you derive them. I find that Newtonian mechanics can be a bit more intuitive, however Lagrangian mechanics is a more systematic approach and therefore less prone for errors. Especially when dealing with systems with rotational motion, like your robot arm, Newtonian mechanics can be a real pain, so I would suggest Lagrangian mechanics. But you could always do both and compare the two to be more confident about the correctness of your model.</p>
"
"2383442","2383516","<p>Since $T(1,0)=(1,0)$ and $T(0,1)=(0,-1)$, the matrix of $T$ with respect to the canonical basis is $\left(\begin{smallmatrix}1&amp;0\\0&amp;-1\end{smallmatrix}\right)$. Therefore, the trace is $0$ and the determinant is $-1$.</p>
"
"2383448","2383661","<p>A function $f: (X, \mathcal{U}_d) \to (Y, \mathcal{U})$ (supposing you use entourages) is uniformly continuous iff for every $U \in \mathcal{U}$, $(f \times f)^{-1}[U]\in \mathcal{U}_d$, which is clear as $\Delta_X \subseteq (f \times f)^{-1}[U]$ as $\Delta_Y \subseteq U$. So $f$ is uniformly continuous.</p>
"
"2383454","2383565","<p>Let $\theta$ denote a polar angle in the $(x, y)$-plane. If you mean a standing wave with amplitude $A$, frequency $\omega$ with respect to time $t$, and $n$ waves in one turn, you want a formula of the type
$$
z(\theta) = A\sin(n\theta)\sin(2\pi\omega t).
$$
(The overall phase of either trig function may be changed harmlessly.)</p>

<p><a href=""https://i.stack.imgur.com/xD5lz.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xD5lz.gif"" alt=""A standing wave over a circle""></a></p>
"
"2383456","2383977","<p>Trying some extended mind-reading skills. I hope I understood the question correctly.</p>

<p>Given the sum of n independent and identically distributed U(0, 1) random variables: $ X=\sum _{k=1}^{n}U_{k}$.</p>

<p>Now $P(X&lt;1)=\frac{1}{n!}$.</p>

<p>To show: the expected number of distributions, n, such that $P(X&gt;1)$ is $\sum_{n=0}^{\infty}(\frac{1}{n!}) =e$</p>

<p>How to show this:</p>

<p>the probability that the sum of n variates is bigger than 1 while the sum of n-1 variates is smaller than one is the same as the chance that the sum of n  variates is bigger than 1 minus the chance that n-1 variates are bigger than 1: </p>

<p>$(1-\frac{1}{n!})-(1-\frac{1}{(n-1)!})=\frac{1}{n(n-2)!}$</p>

<p>So the desired expected value is $\sum_{n=1}^{\infty} n\cdot \frac{1}{n(n-2)!}=\sum_{n=0}^\infty \frac{1}{n!}=e$</p>
"
"2383457","2383652","<p>I honestly think it makes no difference.</p>

<p>Although ""let"" and ""Suppose"" have different meanings, their logical conclusions are the same.</p>

<p>""let"" means assign an concrete instance.  However, this instance must be <em>general</em> and have no specific known characteristics.  Therefore anything we can conclude from the specific instance with no known characteristics, would have to be a conclusion that applies to any other hypothetical instance.  So ""let"" $\implies$ ""suppose"".</p>

<p>And likewise ""suppose"" means to take a hypothetical instance and then reach a general conclusion that applies to all instances.  So it applies to all specific instances.  So ""suppose"" $\implies$ ""let"".</p>

<p>For style I'd say ""Suppose $x_1$ and $x_2$ are linearly dependent and let $c_1x_1 + c_2x_2 = 0$"".  We are trying to prove something in all hypothetical cases, but we are using a equation to get a specific result which we apply to all cases.</p>

<p>But honestly, <em>no-one</em> will notice and even if they did, they could not deny that the logic of any choice would work.</p>

<p>=====</p>

<p>On the other hand, it could be argued ""let"" and ""suppose"" are  not  significantly different.</p>

<p>If we are walking through a forest full of vectors and we trip over a pair, $x_1$ and $x_2$, and it just so happens they are linearly independent.  That is ""suppose"".</p>

<p>If instead we get on our hands and knees and pick a pair, $w_3$ and $y_7$ and they are not linearly independent so we throw them away, and we keep searching until we find $x_1$ and $x_2$ that are linearly independent and we say ""you'll do"".  That's ""let"".</p>

<p>In both cases, though, we end up with the same thing; a pair of linearly independent vectors.</p>

<p>=========</p>

<p>By the way, this isn't pertinent to you question, if your definition of linear independence is: $\{x_i\}$ are linearly independent if--- $\sum c_i x_i = 0 \iff c_i = 0$, then you do not need the second ""let/suppose $c_1zx_1 + c_2zx_2 =0$"".</p>

<p>We'd simply say: Let/suppose $x_1$ and $x_2$ are linearly independent.  Then $c_1zx_1 + c_2zx_2 = 0 \iff c_iz = 0 \iff c_i= 0$.   So $zx_1$ and $zx_2$ are linearly independent.</p>
"
"2383461","2383464","<p>If $\csc x=\pm\frac2{\sqrt3}$, then $\sin x=\pm\frac{\sqrt3}2$, and so $x=\pm\frac{\pi}3+2k\pi$ or $x=\pm\frac{2\pi}3+2k\pi$, for some $k\in\mathbb Z$.</p>
"
"2383467","2383471","<p>If $y_j\in S$ is such that $y_jx_j$ is integral over $S$ then
you can take $y=y_1\cdots y_t$.</p>

<p>You can take a minimal polynomial for $x_j$ over $k(x_{t+1},\ldots,x_n)$
and multiply it by a common denominator to give a nonzero polynomial
$\phi_j$ over $S$ with $\phi_j(x_j)=0$. Take $y_j$ to be the leading
coefficient of $\phi_j$.</p>
"
"2383470","2383558","<p>Any reasonable coding mechanism suffices. If $X$ and $Y$ are proper classes, you might define the ordered pair $(X, Y)$ as $X \times \{0\} \cup Y \times \{1\}.$ In fact you could code a transfinite sequence of proper classes $\langle X_{\alpha}: \alpha&lt;\beta \rangle$ as $\bigcup X_{\alpha} \times \{\alpha\},$ where $\beta$ is an ordinal or $Ord.$ This kind of situation arises, for example, when iterating a proper class model by an ultrafilter. The coding scheme isn't usually specified since the specific encoding doesn't really matter.</p>
"
"2383472","2383482","<p>If $K$ is a subset in a metric space and $\delta &gt; 0$, then
$$
K \subset \bigcup_{x \in K} B(x, \delta)
$$
where $B(x,\delta)$ is the open ball centered at $x$ with radius $\delta$. This union is an open cover of $K$. If $K$ is compact, there exists a finite subcover
$$
K \subset \bigcup_{i = 1,\cdots,n} B(p_i, \delta)
$$</p>
"
"2383478","2383487","<p>You just follow the same procedure as for the real case:
$$\int_1^N x^s\,dx=\frac{N^{s+1}-1}{s+1}$$
(for $s\ne -1$). This converges iff $N^{s+1}$ tends to a limit. But $|N^{s+1}|=N^{\Re(s)+1}$ so this can't happen when $\Re(s)&gt;-1$ and
certainly happens if $\Re(s)&lt;-1$. This leaves $s=-1+it$ where $t\ne0$.
Then
$$N^{s+1}=\exp(it\log N)$$
which passes through every point on the unit circle infinitely often.</p>
"
"2383493","2383507","<p>Yes. </p>

<p>Taking $t_k=\frac kb$, $k\in\Bbb Z$, we obtain a set of points $f(t_k)$ dense in $S^1\times \{1\}$. With $u_k=\frac ka$, $k\in\Bbb Z$, we obtain a set of points $f(u_k)$ dense in $\{1\}\times S^1$. The combinations $f(t_k+u_m)$ are readily seen to be dense in $S^1\times S^1$.</p>
"
"2383495","2383513","<p>Your potential function is not defined on all of $\Bbb R^3\setminus\{z\textrm{-axis}\}$: $\tan^{-1}\left(\frac{x}{y}\right)$ does not exist if $y = 0$. So, you've shown that if $D\subseteq\Bbb R^3$ is a domain whose intersection with the plane $y = 0$ is empty, $\left.\mathbf{F}\right|_D$ (the vector field restricted to $D$) is conservative. Similarly, you could construct a potential function away from the $x = 0$ plane.</p>

<p>The vector field above is closed (i.e. $\nabla\times\mathbf{F} = \mathbf{0}$), but a closed vector field is not necessarily conservative. The converse is true in two dimensions if the domain of definition of the vector field is <a href=""https://en.wikipedia.org/wiki/Simply_connected_space"" rel=""nofollow noreferrer"">simply connected</a> and in three dimensions if the domain of definition is <a href=""https://en.wikipedia.org/wiki/Contractible_space"" rel=""nofollow noreferrer"">contractible</a>, but $\Bbb R^3\setminus\{z\textrm{-axis}\}$ (the domain of $\mathbf{F}$) is not simply connected (hence not contractible), so even though the vector field is closed, it need not be conservative. And indeed, it isn't: if you compute the line integral over the unit circle in the $z = C$ plane centered at $(0,0)$, you will obtain a nonzero answer, which shows that $\mathbf{F}$ cannot be conservative on any domain containing $(0,0,C)$ for any $C$.</p>

<p>However, there is good news. If you restrict your vector field to <em>any</em> simply connected domain which avoids the $z$-axis, your restricted vector field will be conservative. You can construct a potential function on this domain much like you did with $\tan^{-1}(x/y)$, although you will need to be a little careful in constructing the potential function if the domain intersects both the $x = 0$ plane and the $y = 0$ plane.</p>
"
"2383499","2383604","<h3>Version 1: incorrect form of Lipschitz continuity</h3>

<blockquote>
  <p>for all $x_1,x_2\in K$, $\|F_1-F_2\|\le L\|x_1-x_2\|$ holds for all $F_1\in F(x_1)$ and $F_2\in F(x_2)$</p>
</blockquote>

<p>This implies $\|F_1-F_2\| = 0$ for all $F_1,F_2\in F(x_1)$. So the map $F$ is actually single-valued. The set-valued upper semicontinuity then becomes regular continuity of $F$.  </p>

<p>The latter still takes a bit of effort to prove since we only have the Lipschitz property on compact subsets. It goes like this: pick any convergent sequence $x_n\to x$. The set $K=\{x_n\}\cup \{x\}$ is compact, so $f$ is Lipschitz continuous on it. Hence $f(x_n)\to f(x)$. Since we are in a metric space,  sequential continuity is equivalent to continuity.</p>

<h3>Version 2: Lipschitz continuity with respect to Hausdorff distance</h3>

<blockquote>
  <p>For any $x\in X$ and for any compact subset $K\subset X$ containing $x$, there exists $L&gt;0$ such that $\forall x_1,x_2\in K$, $F(x_1)\subset F(x_2) + B_{L\|x_1-x_2\|}$</p>
</blockquote>

<p>This does not imply upper semicontinuity. For example, let $F(x) = (x,x+1)\subset \mathbb R$ for $x\in \mathbb{R}$. The <a href=""https://mathoverflow.net/a/131209"">definition of upper semicontinuity</a> says that for every open set $W\subset Y$, the set $\{x\in X: F(x)\subset W\}$ must be open in $X$. But this fails with $W=(0,1)$ for which the latter set is $\{0\}$. </p>

<p>Aside: The definition of upper semicontinuity makes more sense for maps such that $F(x)$ is compact for every $x$, or at least closed. </p>

<h3>Version 3: Lipschitz continuity with respect to Hausdorff distance, compact-valued map</h3>

<p>Suppose $F(x)$ is compact for every $x$, and is globally Lipschitz in the Hausdorff metric. (Not just Lipschitz on each compact set). Then upper semicontinuity holds. Indeed, take an open set $W\subset Y$ and a point $x_0$ such that $F(x_0)\subset W$. Let $\rho = \operatorname{dist}(F(x_0), Y\setminus W) $, which is a positive number. If $d(x,x_0)&lt;\rho/L$, then the Lipschitz condition on $F$ implies $F(x)\subset W$, as required for upper semicontinuity.</p>
"
"2383503","2384057","<p>This would be very unusual, for two important reasons.</p>

<p>First, <em>models in sets are not enough</em>. It is important to consider <em>large</em> categories, such as <strong>Set</strong> or <strong>AbGrp</strong> or <strong>Top</strong> or <strong>Cat</strong>.</p>

<p>Secondly, category theory develops its own take on first-order logic &mdash; it would be a wasted effort (and somewhat counter-philosophical) to study the subject in the traditional set-oriented version of logic.</p>

<p>However, one <em>does</em> study categories as models &mdash; we call such a thing an <em>internal categories</em>.</p>

<hr>

<p>Regarding your specific note on natural transformations, there are several paths that might lead you there.</p>

<p>The first is that you can show the product has a right adjoint, so that there is a natural bijection</p>

<p>$$ \hom(\mathcal{C} \times \mathcal{D}, \mathcal{E}) \cong \hom(\mathcal{C}, \mathcal{E}^{\mathcal{D}}) $$</p>

<p>This means that you can treat $\mathcal{D}^\mathcal{C}$ as the <em>category</em> of morphisms from $\mathcal{C}$ to $\mathcal{D}$. A natural transformation, then, is an arrow in this category.</p>

<p>A similar phenomenon happens, for example, in abelian groups, which allows you to construct the <em>abelian group</em> of morphisms from one group to another.</p>

<p>Now, assuming you didn't think to show that, one can still draw <em>inspiration</em> from it; since arrows can be viewed as morphisms $\hom(\uparrow, \mathcal{C})$, where $\uparrow$ means the arrow category, even if you didn't know that $\mathcal{E}^\mathcal{D}$ existed, one can still draw inspiration from the idea of the adjunction above and define a natural transformation to be a morphism $\uparrow \times \mathcal{C} \to \mathcal{D}$.</p>

<p>And this whole thing is similar to topology, and one might be tempted to mimic the definition of a homotopy.</p>
"
"2383508","2383531","<p>As already noted we want $$\frac{3x-1}{x+2}&gt;0$$  Now a rational function can only change sign at a zero or vertical asymptote, so we see any sign changes can only occur at the zero of the numerator $x=\frac{1}{3}$ or the zero of the denominator $x=-2$.  Since both of these zeros have multiplicity 1 which is odd, the function will change sign.  We need to look at the intervals $(-\infty, -2)$, $(-2,\frac{1}{3})$, and $(\frac{1}{3}, \infty)$.  If we choose a test value such as $x=0$ we find $$\frac{3\cdot 0-1}{0+2}=\frac{-1}{2}&lt;0$$  Using the observation from above that the function will change sign at $x=\frac{1}{3}$ and $x=-2$ tells us that the domain is $(-\infty, -2)\cup (\frac{1}{3}, \infty)$.  If you are unfamiliar with the concept of multiplicity of a zero, you could test additional points such as $x=-3$ and $x=1$ to determine the sign of the function. </p>
"
"2383509","2383515","<p>Note that
$$\sum_{\{x,y\}\in E} (d(x)+d(y))=\sum_{x\in V}d(x)^2.$$</p>
"
"2383526","2383528","<p>$\left(1-\frac{x}{n}\right)^n \le e^{-x}$, for $x \le n$</p>
"
"2383530","2383539","<p>Notice that $(\forall n\in\mathbb{N}):f_n\left(\frac1n\right)=1$ and that $(\forall n\in\mathbb{N}):f_n\left(0\right)=0$. Therefore, given $\varepsilon\in(0,1)$, you will not be able to find a $\delta&gt;0$ such that$$(\forall x\in\mathbb{R})(\forall n\in\mathbb{N}):-\delta&lt;x&lt;\delta\Longrightarrow\bigl|f_n(x)-f_n(0)\bigr|&lt;\varepsilon.$$</p>
"
"2383534","2383580","<p>$$H:(x,t)\mapsto (1-t)f(x)+tg(x)$$
is a homotopy between $f$ and $g$, but within $\Bbb R^3$ alas!
But the no-fixed point condition means that $H$ avoids the origin.
So
$$(x,t)\mapsto\frac1{|H(x,t)|}H(x,t)$$
is a homotopy inside $S^2$.</p>
"
"2383543","2383553","<p>HINT: $$2(v^2-1)=(-2)(1-v^2)$$ it works aslo here
$$\sqrt{(2(v^2-1))^2}=\sqrt{((-2)(1-v^2))^2}$$</p>
"
"2383554","2383598","<p>Let $v_i^*$ be dual basis for $v_i$ and suppose that $\sum_i a_i\otimes v_i = 0$ and $a_j \not = 0$. Than apply $a_j^*\otimes v_j^*$ for both sides: $1 = 0$. Contradiction.</p>
"
"2383560","2383613","<p>Here's one way to prove that $W_1 \cap W_2 = \{\vec{0}\}$.  You can prove that if $\vec{v}$ is in the intersection, then $\vec{v}=\vec{0}$. </p>

<p>Suppose $\vec{v} \in W_1 \cap W_2$. Then $\vec{v}\in W_1$ and $\vec{v}\in W_2$. Because $\vec{v}\in W_2$, it is perpendicular to every vector in $W_1$. Because $\vec{v}\in W_1$, $\vec{v}$ is in particular perpendicular to <em>itself</em>:  $\vec{v}\cdot \vec{v} = 0$.</p>

<p>By definition of the dot product, this implies that $\vec{v} = \vec{0}$.</p>

<hr/>

<p>This shows that <em>no other vector</em> can be in $W_1 \cap W_2$. The last detail is that $\vec{0}$ does belong to $W_1\cap W_2$ because $W_1$ and $W_2$ are subspaces and so must each contain $\vec{0}$.</p>
"
"2383571","2383593","<p>Hint:</p>

<p>As $\dfrac{d(x^4+x^2)}{dx}=4x^3+2x$</p>

<p>$$\int e^{x^4+x^2}(2x+2x^3+4x^5)dx=\int[e^{x^4+x^2}x^2(2x+4x^3)+e^{x^4+x^2}(2x)dx$$</p>

<p>which is clearly of the from  $$\int e^{f(x)}[g'(x)+g(x)f'(x)]dx=e^{f(x)}g(x)+K$$</p>
"
"2383577","2383586","<p>Let $\sqrt[6]2=x\implies\sqrt[3]2=x^2$</p>

<p>We have $$\dfrac1{2+2x+2x^2}=\dfrac{1-x}{2(1-x^3)}=\dfrac{(1-x)(1+x^3)}{2(1-x^6)}$$</p>
"
"2383584","2383704","<p>Yes -- you're partially correct! Each edge represents <em>multiplying by the same generator (element)</em> but each vertex represents a <strong><em>different</strong> element of the group</em>. So this particular diagram represents the group $C_6$ (some people write it as $(\mathbb{Z}/6\mathbb{Z})$ or $\mathbb{Z}_6$). This group consists of six elements:</p>

<p>$$C_6=\{ [0], [1], [2], [3], [4], [5] \}$$</p>

<p>and has the binary operation of <em>addition modulus 6</em>, that is, $[1]+[3]=[4]$ but $[2]+[4]=[0]$, since $1+3 \equiv 4 \ (\text{mod} \ 6)$ but $2+4 \equiv 0 \ (\text{mod} \ 6)$.</p>

<p>Anyways, this group has a special property concerning the element $[1]$. Particularly, we can represent every element in the group with $[1]$! Let's let $[1]$ be $x$. Then:</p>

<p>$x^0 = [1]^0 = [0] = e$</p>

<p>$x^1 = [1]$</p>

<p>$x^2 = [1] + [1] = [2]$</p>

<p>$x^3 = [1] + [1] + [1] = [3]$</p>

<p>...</p>

<p>$x^5 = [1] + [1] + [1] + [1] + [1] = [5]$</p>

<p>$x^6 = [1] + [1] + [1] + [1] + [1] + [1] = [6] = [0] = e = x^0$.</p>

<p>Now let's get back to the diagram. In the diagram, let one vertex be $e$. Then, traveling (lets say ""counterclockwise"" here) along a path is the same as multiplying by $x$ (that is, by $[1]$). As a result, here is what our diagram looks like:</p>

<p><a href=""https://i.stack.imgur.com/99VkU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/99VkU.png"" alt=""xxxx""></a></p>

<p>So going picking any point in this diagram and going counterclockwise represents multiplying by $x$ whenever you go over an edge, that is, starting at $x^3$ for example and moving counterclockwise over two edges represents the element $x^3 \cdot x \cdot x = x^5 = [5] \in C_6$. Similarly, moving <em>clockwise</em> is the same as multiplying by $x^{-1}$. </p>
"
"2383587","2383591","<p>Let $V=W$ be two dimensional, with basis $\{e,f\}$. Consider
$$e\otimes e+f\otimes f.$$</p>
"
"2383589","2383597","<p>Any linear combination of linear functions is still a linear function. Since the function $q\colon[0,1]\longrightarrow\mathbb R$ defined by $q(x)=x^2$ is not linear, it does not belong to the span of the linear functions.</p>

<p>Besides, the set of all linear functions is not linearly independent.</p>
"
"2383607","2383887","<p>Let $u = PV\left(\frac1x\right)$. Then $xu = 1$. Now $\hat 1 = 2\pi \, \delta$ so we have
$$
\langle 2\pi \, \delta, \phi \rangle
= \langle \hat 1, \phi \rangle
= \langle \widehat{xu}, \phi \rangle
= \langle xu, \hat\phi \rangle
= \langle u, x \hat\phi \rangle
= \langle u, -i \widehat{\phi'} \rangle
= \langle -i \hat u, \phi' \rangle
= \langle i (\hat u)', \phi \rangle
$$</p>

<p>Thus, $i(\hat u)' = 2\pi \, \delta$ which gives $\hat u(\xi) = -i\pi \operatorname{sign}(\xi) + C$. But since $u$ is odd so is also $\hat u$ which forces $C = 0$.</p>
"
"2383614","2383621","<p>(1) In matrix terms, and working
over the reals, we have the hypothesis $v^t A v=0$ for all vectors
$v$. This does <strong>not</strong> imply $v^t Aw=0$ for all $v$ and $w$. Consider
$\pmatrix{0&amp;1\\-1&amp;0}$. To get this implication we need symmetry. Here the
analogue of symmetry is the Hermitian condition.</p>

<p>(2)
$$2\left&lt;Av,w\right&gt;
=(\left&lt;Aw,v\right&gt;+\left&lt;Av,w\right&gt;)
+(-\left&lt;Aw,v\right&gt;+\left&lt;Av,w\right&gt;).$$</p>
"
"2383622","2383630","<p>It is <em>not</em> empty, since $\left(\frac1{\sqrt2},\frac i{\sqrt2},0\right)\in M$.</p>
"
"2383634","2383673","<p>Operation $+:\mathbb R^2\to\mathbb R$ prescribed by $\langle x,y\rangle\mapsto x+y$ is continuous if $\mathbb R^2$ and $\mathbb R$ are both equipped with their usual topology. </p>

<p>That implies that the operation is measurable if $\mathbb R^2$ and $\mathbb R$ are both equipped with the Borel $\sigma$-algebra.</p>

<p>The measurability of $f,g:\mathbb R\to\mathbb R$ implies that the function $h:\mathbb R^2\to\mathbb R^2$ prescribed by $\langle x,y\rangle\mapsto\langle f(x),g(y)\rangle$ is measurable.</p>

<p>Then composition $+\circ h$ prescribed by $\langle x,y\rangle\mapsto f(x)+g(y)$ is measurable.</p>

<p>Also read my comment, and check whether you have interpreted the question posed unto you correctly.</p>
"
"2383640","2383692","<p>Let $A \subseteq \mathbb{R}$ be closed, and suppose that $-\infty &lt; \sup(A) = \alpha &lt; \infty$.  By definition of the supremum, for each $n\in\mathbb{N}$, there is some $x_n \in A$ such that
$$ \alpha - x_n = |\alpha - x_n| &lt; \frac{1}{n}.$$
But then $x_n$ is a sequence in $\mathbb{R}$ that converges to $\alpha$.  This implies that $\alpha$ is a limit point of $A$.  As $A$ is closed, it contains all of its limit points, therefore $\alpha \in A$.  But then we have
$$ \alpha \in A \qquad\text{and}\qquad \alpha \ge x\, \forall x\in A. $$
Therefore $\alpha = \max(A)$.</p>
"
"2383646","2383682","<p>we have $$a^3b^3+a^3c^3+b^3c^3\geq 3a^2b^2c^2$$ by $AM-GM$
then we have to Show that $$a^4c^2+a^2b^4+b^2c^4\geq a^3bc^2+a^2b^3c+ab^2c^3$$
let $$a\geq b\geq c$$ then we have $$c^2(a-b)(a^3-b^2c)+(b-c)b^2(a^2b-c^3)\geq 0$$</p>
"
"2383647","2383659","<p>We aim to apply the dominated convergence theorem to the sequence $(|f_n-f|^p)_n$. Clearly $|f_n-f|^p\to 0$, so that it remains to find the dominating function. Then $|f_n|\leq g$ implies $|f|\leq g$ a.e., so that 
$$
|f_n-f|^p \leq (|f_n|+|f|)^p \leq (2g)^p = 2^pg^p \in L^1
$$
holds a.e.
Now the dominated convergence theorem yields $\int |f_n-f|^p \to 0$ and consequently $\|f_n-f\|_p\to 0$.</p>
"
"2383651","2383713","<p>You have $F=f\circ{\rm rect}$ where ${\rm rect}$ is the map $${\rm rect}:\quad (r,\phi)\mapsto(r\cos\phi,r\sin\phi)\ .$$
The chain rule says that $dF=df\circ d{\rm rect}$, which means that matrixwise $[dF]$ is equal to the matrix  product  $[df]\cdot [d{\rm rect}]$. This can be typographically represented in various ways, one of them being
$$\nabla F(r,\phi)=\nabla f(r\cos\phi,r\sin\phi)\left[\matrix{\cos\phi&amp;-r\sin/phi \cr \sin\phi &amp;r\cos\phi\cr}\right]\ .$$
Now ${\partial F\over\partial r}$ is the first component of the vector $\nabla F$, hence is given by 
$$f_x(r\cos\phi,r\sin\phi)\cos\phi+f_y(r\cos\phi,r\sin\phi)\sin\phi\ .$$</p>
"
"2383655","2383663","<p>The intersection point is wrong</p>

<p>Indeed $\log x=\log(1-x)$ means $x=1-x$ that is $x=\frac12$</p>

<p>$m_1=2;\;m_2=-2$</p>

<p>$\tan\theta=\dfrac{-2-2}{1+2(-2)}=\dfrac{4}{3}$</p>

<p>$\theta\approx 53.13Â°$</p>

<p>Hope this helps</p>
"
"2383667","2383679","<p>$$\log\frac{n+1}{n-1} = \log\frac{1+\frac{1}{n}}{1-\frac{1}{n}} = 2\,\text{arctanh}\frac{1}{n} = \frac{2}{n}+O\left(\frac{1}{n^3}\right) \tag{1}$$
leads to
$$ n\log^2\frac{n+1}{n-1} = \frac{4}{n}+O\left(\frac{1}{n^3}\right) \tag{2} $$
so the given series is conditionally convergent but not absolutely convergent, i.e. has exactly the same convergence behaviour of $4\sum_{n\geq 2}\frac{(-1)^n}{n}$, since $\sum_{n\geq 2}\frac{1}{n^3}$ is clearly absolutely convergent.</p>

<p>Are you interested in a exact evaluation, too?</p>
"
"2383680","2383700","<p>I'll expand on what <em>yanko</em> mentioned above. Let $\mathcal{T}$ be any topology on $\mathbb{R}$ where $+ : \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ is continuous, and has continuous inverse </p>

<p>Fix $x \in \mathbb{R}$, and pick $y \neq x \in \mathbb{R}$ . Let $U$ be a neighbourhood of $x$.</p>

<p>If $y=x$, $U$ is also a neighbourhood of $x$ and the identity map is the desired homeomorphism.</p>

<p>If $x &lt; y$, then let $f : U \to \mathbb{R}$ be defined by $f(t) = t + |x-y|$, $f$ is clearly continuous, and bijective onto its image $f[U]$, and it has continuous inverse given by  $f^{-1}(t) = t - |x+y|$, hence $V = f[U]$, since $f$ is a homeomorphism onto its image and since $f(x) = x+ |x-y| = y$, we can see that $V$ is a neighbourhood of $y$.</p>

<p>A similar argument proves the existence of a neighbourhood $V$ of $y$, in the case of $y &lt; x$.</p>

<p>So your property is correct, and your intuition justified.</p>
"
"2383685","2383687","<p>The number of integers needed to select a prime, $X$, is geometrically distributed with $p = 1/2302$. The probability of the event $\{X = k\}$ is equal to $\mathbb{P}(X = k) = (1-p)^{k-1}p$ for $k \geq 0$. Its mean is
\begin{align}
\mathbb{E}(X) = \sum_{k=0}^\infty k\mathbb{P}(X=k) = \frac{1}{p}
\end{align}
So for finding a prime number when drawing integers with 1000 digits we would expectedly need $2302$ draws.</p>
"
"2383695","2383701","<p>Notice that $\cos^{2}(x):=(\cos(x))^{2}$ is not the same thing as $\cos(2x)$. It is indeed true that $\sin^{2}(x)=1-\cos^{2}(x)$ and that $\sin^{2}(x)=\frac{1-\cos(2x)}{2}$.</p>
"
"2383696","2383846","<p>Let $G$ be a group. </p>

<ul>
<li>$G$ is abelian if and only if the mapping $g\mapsto g^{-1}$ is an isomorphism on the group $G$.</li>
<li>If $G$ is finite and every irreducible character is linear then $G$ is abelian.</li>
<li>If $Aut(G)$ acts on the set $G-\{e\}$ transitively then $G$ is abelian.</li>
<li>If $\mathbb Z_2$ acts by automorphism on a finite group $G$ fixed point freely then $G$ is abelian.</li>
<li>Let $A$ act on $G$ by automorphism and the action of $A$ is faithful. Assume that $[G,A,A]=1$ then both $A$ and $[G,A]$ is abelian.</li>
</ul>

<p>If $G$ is a finite group in which every proper subgroup is abelian then $G$ must be a solvable group. Its proof is not so easy with elementary tools. Thus such groups must be meta-abelian groups. As far as I remember it is not true for infinite groups. (<strong>Paul Plummer</strong> supplied an example in comments.)</p>

<p>If $G$ is a finite solvable group in which every nontrvial quatient is abelian then $G'$ is a minimal normal subgroup. Thus, $G'$ is isomorphic to $\mathbb Z_p \times\mathbb Z_p \ldots \times \mathbb Z_p $. Hence $G$ is meta-abelian group. In that case,  One can also observe that $Q\in Syl_q(G)$ for $p\neq q$, $Q$ is abelian. $S_3,S_4$ are examples of such groups. </p>

<p>But such groups need not to be solvable in genaral. For example consider the group $G=(A_5\times A_5)\rtimes \mathbb Z_2$ where the action of the invouliton is the map $(a,b)\mapsto (b,a)$ on $A_5\times A_5$.</p>

<p>I think you should ask a more specific question though it seems that your question is too general.</p>
"
"2383705","2383772","<p>The PDFs and CDFs of Student's t and chi-squared distributions are known
and are displayed in the corresponding Wikipedia articles. The PDFs are
shown in appendixes of many intermediate probability and mathematical
statistics courses. Your specific questions involve use of the quantile
functions (inverse CDFs). In principle, methods of numerical integration
can be used to get probabilities from PDFs. Also, mathematical methods
can be used to invert CDFs for the same purpose. </p>

<p>An important computational difficulty is that the PDFs and CDFs of Student's
t and chi-squared distributions are expressed in terms of gamma functions,
which must also be evaluated by numerical methods.</p>

<p>You are correct that printed tables give results for Student's t distributions
that arise in most practical applications--and for chi-squared distributions
in many practical applications. Statistical software packages (SAS, Minitab,
SPSS, R, and so on) have functions that give a wider range of quantiles
than you will find in tables. Statistical calculators and Excel give useful approximations.</p>

<p>Chapter 26 of Abramowitz and Stegen (PDF file available online) catalogues some rational approximations that
give reasonable accuracy over various ranges of parameters. However, these
are less frequently used nowadays because it is easy to get more accurate
results from software. (Some software functions use carefully vetted rational approximations,
but generally more intricate and accurate ones than you will find in A&amp;S.)</p>
"
"2383714","2384277","<p>Yes, they are all similar. Because the transpose of $A_1$ is $A_3$, these two matrices are similar, because $A$ and $A^T$ are always similar (see <a href=""https://math.stackexchange.com/questions/94599/a-matrix-is-similar-to-its-transpose"">this duplicate</a>). To see that $A_1$ and $A_2$ are similar, we just can find an invertible matrix $S$ such that $A_1S=SA_2$, which means to solve a system of linear equations in the coefficients of $S$. This may not be the most elegant way, but it certainly helps to find the answer. We obtain as one of many possible solutions
$$
S=\begin{pmatrix} -1 &amp; 1 &amp; 0 &amp; 0 \cr 1 &amp; 0 &amp; -1 &amp; 0 \cr 0 &amp; 0 &amp; 0 &amp; -1\cr
1 &amp; 0 &amp; 0 &amp; 0\end{pmatrix}.
$$
Indeed, $A_2=S^{-1}A_1S$.</p>
"
"2383716","2383942","<p>It's trivial that $R = I_1 + I_2 + \cdots + I_n$, why? because a sequence with $R$ runs must have $R$ positions where each run starts, if there are 3 runs, there must exist 3 positions in the sequence where each of them starts. For example the runs at S,S,S,F,S,S,F,F,S start at positions 1,5, and 8, meaning $I_1 = I_5 = I_8 = 1$, the rest of the $I_j$'s are zero. Obviously $P(R=n) = 0$ because for there to exist at least 2 runs, there must be failures in between, which means at least one $I_j = 0$.
Now you can calculate the expectation of $R$
$$ E(R) = E(I_1 + I_2 + \cdots + I_n) = E(I_1) + E(I_2) + \cdots + E(I_n) $$
The problem has been reduced to finding the expected value of each $I_j$ and summing over all of them. You have two cases:</p>

<ul>
<li>$P(I_1=1) = p$, where $p$ is the probability of success of the Bernoulli trials, and</li>
<li>$P(I_j=1) = (1-p)p$ where $j \neq 1$, because if $j$ is not the first position, there must exist a failure in the previous position $(1-p)$, and a success at position j $(p)$. Therefore
\begin{align*}
E(R) &amp; = E(I_1 + I_2 + \cdots + I_n)\\
&amp; = E(I_1) + E(I_2) + \cdots + E(I_n)\\
&amp; = p + (1-p)p + \cdots + (1-p)p\\
&amp; = p + (n-1)(1-p)p  
\end{align*}
For example, if $n = 3$ and $p = 0.5$ you have $8$ possible outcomes with equal probability each:
$$
FFF \to 0 \text{ runs}\\
FFS \to 1 \text{ runs}\\
FSF \to 1 \text{ runs}\\
FSS \to 1 \text{ runs}\\
SFF \to 1 \text{ runs}\\
SFS \to 2 \text{ runs}\\
SSF \to 1 \text{ runs}\\
SSS \to 1 \text{ runs}
$$
Therefore the expected number of runs is:
$$ \frac{(0+1+1+1+1+2+1+1)}{8} = \frac{8}{8} = 1$$
Using the previous formula yields the same result (but quicker):
$$ \operatorname{E}(R) = p + (n-1)(1-p)p = 0.5 + (3-1)\times 0.5 \times 0.5 = 1$$</li>
</ul>
"
"2383718","2383723","<p>Consider the set $V$ of polynomials in $\Bbb R[X]$ with degree $\le 3$.</p>

<p>Then $\{1,x,x^2,x^3\}$ is a basis for $V$. Let $U$ be the subspace generated by $\{1,x,x^2+x^3\}$.</p>
"
"2383721","2383916","<p>Rudin's theorem stops one step short of constructing the coordinates that I construct. To get my result from his, the first thing to do is to choose bases in $\mathbb R^n$ and $\mathbb R^m$ such that $A$ is given by 
$$
A\big(x^1,\dots,x^n\big) = \big(x^1,\dots,x^r,0,\dots,0\big).
$$
(Theorem B.20 in my Appendix B shows that this can always be done for a linear map of rank $r$.) Then you can choose the complementary subspace $Y_2$ to
be the set of points of the form $\big(0,\dots,0,x^{r+1},\dots,x^n\big)$,
and $P$ is just the coordinate projection 
$$\big(x^1,\dots,x^m\big)\mapsto 
\big(x^1,\dots,x^r,0,\dots,0\big).
$$</p>

<p>With these choices, Rudin's theorem shows that $F\circ H$
can be written as
$$
F\circ H(x) = \big( x^1,\dots, x^r,\varphi^{r+1}\big( x^1,\dots, x^r\big),\dots,
\varphi^m\big( x^1,\dots, x^r\big)\big),
$$
where $\big(\varphi^{r+1},\dots,\varphi^m\big)$ are the coordinate functions of the map $\varphi$.
(The construction up to this point corresponds to formula (4.6) in my proof of the rank theorem.)</p>

<p>To get the coordinates in my version of the theorem, define new coordinates on a neighborhood of $0$ in $\mathbb R^m$ by $y=\Psi(x)$, where
$$
\Psi\big(x^1,\dots, x^m\big) = \big( x^1,\dots, x^r,x^{r+1}-\varphi^{r+1}\big( x^1,\dots, x^r\big),\dots,
x^m-\varphi^m\big( x^1,\dots, x^r\big)\big).
$$
Then a straightforward computation shows that the composite function $\Psi\circ F \circ H$ is given by my formula 4.1.</p>
"
"2383729","2383993","<p>There exists a set $S$ with the properties you want for every value of $l$. Here are a few examples for the smaller values of $l$:</p>

<ul>
<li>For $l=2$:  $|S \setminus \bar{S}|=1 &lt; |\bar{S}|=2$ 
$$       S= \{1,2,4\}$$
$$ \bar{S}= \{2,4\}$$</li>
<li>For $l=3$:  $|S \setminus \bar{S}|=3  &lt; |\bar{S}|=4$ 
$$       S= \{1, 2, 3, 5, 6, 8, 9\}$$
$$ \bar{S}= \{3, 5, 8, 9\}$$</li>
<li>For $l=4$:  $|S \setminus \bar{S}|=6  &lt; |\bar{S}|=7$ 
$$       S= \{1, 2, 3, 4, 6, 7, 8, 9, 11, 12, 13, 14, 16\}$$
$$ \bar{S}= \{4, 8, 9, 12, 13, 14, 16\}$$</li>
<li>For $l=5$:  $|S \setminus \bar{S}|=16 &lt; |\bar{S}|=17$ 
$$       S= \{1, 2, 3, 4, 5, 7, 8, 9, 10, 12, 13, 14, 15, 17, 18, 19, 20, 22, 23, 24, 25, 26, 33, 34, 35, 36, 37, 39, 43, 44, 45, 46, 47\}$$
$$ \bar{S}= \{5, 9, 13, 17, 22, 23, 24, 25, 26, 33, 34, 37, 43, 44, 45, 46, 47\}$$</li>
<li>For $l=6$:  $|S \setminus \bar{S}|=15 &lt; |\bar{S}|=16$ 
$$       S= \{1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 36\}$$
$$ \bar{S}= \{6, 12, 13, 18, 19, 20, 24, 25, 26, 27, 30, 31, 32, 33, 34, 36\}$$</li>
<li>For $l=7$:  $|S \setminus \bar{S}|=78 &lt; |\bar{S}|=79$ , $\max(S)=239$</li>
<li>For $l=8$:  $|S \setminus \bar{S}|=109 &lt; |\bar{S}|=110$ , $\max(S)=335$</li>
<li>For $l=9$:  $|S \setminus \bar{S}|=213 &lt; |\bar{S}|=214$ , $\max(S)=633$</li>
<li>For $l=10$:  $|S \setminus \bar{S}|=45 &lt; |\bar{S}|=46$ , $\max(S)=100$</li>
</ul>

<p>As the sets become rather large I left out the lasts few examples. These are the first sets I encountered in the following procedure.</p>

<p>We start for the chosen value of $l$ with the set $S=\{1,2,\dots,l-1\}$ which does not contain an arithmetic sequence of length $l$. (Optional $\bar{S}=\{l-1\}$). We also use an index $n=l$. Now we can use the following algorithm:</p>

<ol>
<li><p>Add $n$ to the set $S$</p></li>
<li><p>Check wether there is any arithmetic sequence of length $l$ in $S$ that ends in $n$. If that is the case we remove $n$ from $S$, if not we have a new set that does not contain any arithmetic sequences of length $l$ or longer.</p></li>
<li><p>Optional: If $n$ was accepted we can check whether there is any arithmetic sequence of length $l-1$ in $S$ that ends in $n$. If that is so, we add $n$ to $\bar{S}$.</p></li>
<li><p>Increase $n$ by one and go back to step 1.</p></li>
</ol>

<p>After that it is only repeating until the number of elements in $\bar{S}$ exceeds half the size of $S$.</p>

<p>I have not tried to prove the existence of such a set for any value of $l$, but I see no obvious reason why they would seize to exist for larger values of $l$. It should also be noted that there might very well be sets with the required properties that are smaller in size and of which the largest element in $S$ is less than the algorithm above gives. </p>

<p>It is, however, possible to construct a set for any value of $l$ in a systematic fashion. Hereto we start with the following sets:
$$S_0 = {1}$$
$$\bar{S}_0 = \emptyset$$
and apply the following iterative scheme:
$$S_{n+1} = \cup_{k=0}^{l-1} \left[ S_n + k \Delta_n \right]$$
$$S_{n+1} = \cup_{k=0}^{l-2} \left[ \bar{S}_n + k \Delta_n \right] \cup \left[S_n + (l-1) \Delta_n\right]$$
where $\Delta_n$ is not unique but should be large enough to avoid any problems with generating additional sequences. It is sufficient to choose $\Delta_n \geq 2 \max(S_{n-1})-1$. </p>

<p>We find that
$$|S_n| = l^n$$
$$|\bar{S}_n| = l^n - (l-1)^n$$
and if we choose
$$\Delta_n = (2l-1)^{n-1}$$
the largest element in $S_n$ is given by
$$\max(S_n) = \frac{(2 l-1)^n +1}{2}$$.</p>

<p>From this it follows that for $|S_n\setminus \bar{S}_n| &lt; |\bar{S}_n|$ we require $(l-1)^n &lt; l^n - (l-1)^n$, which means
$$n&gt;- \frac{\ln 2}{\ln(1- \frac{1}{l})}$$.</p>

<p>For the case $l=3$ we would obtain the set 
$$S=\{1,2,3,~~6,7,8,~~11,12,13\}$$
and for $l=4$ we get
$$S=\left\{\begin{array}{llll}
1,2,3,4, &amp; 8,9,10,11, &amp; 15,16,17,18, &amp; 22,23,24,25,\\
50,51,52,53, &amp; 57,58,59,60,&amp; 64,65,66,67, &amp; 71,72,73,74,\\
99,100,101,102,&amp;106,107,108,109,&amp;113,114,115,116,&amp; 120,121,122,123,\\
148,149,150,151,&amp;155,156,157,158,&amp;162,163,164,165,&amp; 169,170,171,172
\end{array} \right\}$$.</p>

<p>And finally, we get that
$$\frac{|\bar{S}_n|}{|S_n|} = 1 - \left( 1 - \frac{1}{l}\right)^n$$
and hence 
$$\lim_{n\rightarrow \infty} \frac{|\bar{S}_n|}{|S_n|} = 1.$$</p>
"
"2383731","2384733","<p>You'll need to make the part about having mitochondria part a predicate.  You can either follow LoMaPh's suggestion and use a $Have(x,y)$ ('$x$ has $y$') predicate together with a $M(x)$ ('$x$ is a mitochondria') predicate, or you can simply use a $M(x)$ predicate that stands for '$x$ has mitochondria'.</p>

<p>You'll also need to use an $\land$ somwhere, since you are basically dealing with two sentences with a 'but' in between, and 'but' in logic simply means 'and'.</p>
"
"2383732","2383740","<p>Your original method works only because $10$ has an inverse modulo $11$. You wrote $20 \equiv 10x \pmod{11}$ as
$$ 10(2) \equiv 10x \pmod{11}. $$
Now, using the fact that $10^{-1}$ exists, we can multiply by it to conclude $2 \equiv x \pmod{11}$. If $10$ wasn't invertible, we wouldn't be able to conclude this. For instance, $2(2) \equiv 2(3) \pmod{2}$, but $2\not\equiv 3 \pmod{2}$.</p>
"
"2383735","2383749","<p>Let $b_i=\frac{1}{2n}$ and $a_i=1$.</p>

<p>Hence, we need $$2n^2\leq c\cdot\frac{n}{\frac{1}{2}}$$ or
$$c\geq n,$$
which says that needed $c$ does not exist.</p>
"
"2383760","2383876","<p>I don't know how to express this as a product of categories, but your construction looks a lot like a <a href=""https://en.wikipedia.org/wiki/Comma_category#Definition"" rel=""nofollow noreferrer"">comma category</a> (so it is actually a <em>fibered</em> product of categories!). In fact, it is (up to the order of your terms) the category $\mathbf{Ab}\times (Id_{\mathbf{CHaus}^{op}}\downarrow S)$, where $\mathbf{Ab}$ denotes the category of abelian groups, $\mathbf{CHaus}$ denotes the category of compact Hausdorff spaces, and $S=Hom(\_ ,(\Bbb R,\Bbb R^+,1))^{op}:\mathbf{PrAb}\to \mathbf{CHaus}^{op}$ is (the dual of) the contravariant functor represented by $(\Bbb R,\Bbb R^+,1)$.</p>
"
"2383762","2383790","<p>If $\mathbf x=(x_1,x_2),\mathbf y=(y_1,y_2)$ then $t\mathbf y+(1-t)\mathbf x=\left(x_1+(y_1-x_1)t,x_2+(y_2-x_2)t\right)$.</p>

<p>So $$g(t)=(x_1+(y_1-x_1)t)^2 + (x_2+(y_2-x_2)t)^2$$</p>

<p>So $$\begin{align}g'(t)&amp;=2(x_1+(y_1-x_1)t)(y_1-x_1)+2(x_2+(y_2-x_2)t)(y_2-x_2)\\
&amp;=2(x_1(y_1-x_1)+x_2(y_2-x_2))+2t\left((y_1-x_1)^2+(y_2-x_2)^2\right)\\
&amp;=2\langle \mathbf x,\mathbf y-\mathbf x\rangle +2t\langle \mathbf y-\mathbf x,\mathbf y-\mathbf x\rangle\\
&amp;=2\langle t\mathbf y+(1-t)\mathbf x,\mathbf y-\mathbf x\rangle
\end{align} $$</p>
"
"2383763","2383787","<p>For fixed index $j_o$ and $k_o$, note that 
$$
\frac{\text{d}}{\text{d} w_{j_ok_o}} 
 w_{jk} Â·o_j 
=
\left\lbrace
\begin{array}{rcl}
1\cdot o_{j_o} &amp; \mbox{if} &amp; j_o=j \mbox{ and } k_o=k\\
0 &amp; \mbox{if} &amp; j_o\neq j \mbox{ or } k_o\neq k \\
\end{array}
\right.
$$
Then we have 
\begin{align}
\frac{\text{d}}{\text{d} w_{j_ok_o}} 
%\left\lgroup 
\sum_j w_{jk} Â·o_j
%\right\rgroup
&amp;=
\sum_j
\frac{\text{d}}{\text{d} w_{j_ok_o}} 
 w_{jk} Â·o_{j}
=
o_{j_o} 
\end{align}</p>
"
"2383773","2383861","<p>I've seen used in papers: ""$X$ has a non-trivial convergent sequence"" which is the obvious and self-explaining name. </p>

<p>It's obeyed by any infinite first countable $T_1$ space with a non-isolated point. So most spaces will have it. Not discrete spaces or weird ones like $\beta \omega\setminus \omega = \omega^\ast$. </p>
"
"2383774","2383776","<p>Note that $x^2\geq 0$, where equality holds if and only if $x=0$. Therefore
$$ x^2+y^2 \geq 0 + y^2 = y^2 $$
and equality holds if and only if $x=0$.</p>
"
"2383775","2383779","<p>The expression
$$ \frac{x^2}{|x|}$$
is not defined when $x=0$.  However, we can find another expression that ""does the same job"" as $x^2/|x|$ and is defined at zero.  To do this, define the function
$$ f(x) := \frac{x^2}{|x|}, $$
which is not defined at zero.  On the other hand, the function
$$ g(x) := |x| $$
is defined for all real values of $x$, and $f(x) = g(x)$ whenever both functions are defined.  Thus it is reasonable to ""extend"" $f$ to all real numbers by setting $f(0) = 0$.  In this way, we can reasonably say that
$$ \frac{x^2}{|x|} = |x|, $$
which is zero when $x=0$.</p>

<hr>

<p>At the suggestion of Henning Makholm, let us note that this $g$ is in no way unique.  I could just as easily have remarked that
$$ h(x) := \begin{cases} |x| &amp; \text{if $x\ne 0$, and} \\
47 &amp; \text{if $x = 0$} \\
\end{cases} $$
is equal to $f$ whenever both $h$ and $f$ are defined, therefore we could extend $f$ to all real numbers by setting $f(0) = 47$.  This approach is entirely reasonable, but somewhat unsatisfying.  We would like to extend $f$ to a function on $\mathbb{R}$ in the ""nicest possible way.""  What we mean by nicest possible way is nebulous, but in this context that probably means <em>continuity</em> (in other contexts it might mean smoothly, or up to a set of measure zero, or some other nonsensical mathematical gobbledygook).</p>

<p>That is, we would like to extend $f$ to a function on $\mathbb{R}$ so that the extended function is continuous.  The only way to to that is to set $f(0) = 0$.</p>
"
"2383782","2383821","<p>I would be tempted to try to solve directly $\varphi(t)=0$.</p>

<p>Let $\zeta$ such that $\phi(\zeta)=0$. Then $\cos(\zeta \sqrt{2})=-\sin(\zeta)=\cos(\zeta+\pi/2)$</p>

<p>This gives $\pm \zeta \sqrt{2} = \zeta+\pi/2 + 2n\pi$ for $n \in \mathbb{Z}$</p>

<p>Therefore you have two types of solutions:
$\alpha_n=\dfrac{\sqrt{2}+1}{2}\pi(1+4n)$ and $\beta_n=-\dfrac{\sqrt{2}-1}{2}\pi(1+4n)$ for $n \in \mathbb Z$.</p>

<p>You can see that $\beta_{n}-\beta_{n+1}=2(\sqrt{2}-1)\pi&lt;\pi$.</p>

<p>Of course since you can have $\alpha$ solutions between the $\beta$ ones you might be able to improve this bound.</p>
"
"2383805","2383857","<p>This indeed is a bit laboured. Suppose $L_1\cap L_2\ne K$.
Then $|L_1\cap L_2:K|=d&gt;1$. Let $n_i=|L_i:K|$. Then $|L_i:L_1\cap L_2|
=n_i/d$. Also $|L_1L_2:L_1|\le|L_2:L_1\cap L_2|=n_2/d$, so
$$|L_1L_2:L_1\cap L_2|=|L_1L_2:L_1||L_1:L_1\cap L_2|\le n_1n_2/d^2$$
and then
$$|L_1L_2:K|=|L_1L_2:L_1\cap L_2||L_1\cap L_2:K|\le n_1n_2/d&lt;n_1n_2.$$</p>

<p>Here I used the principle $|LM:L|\le|M:K|$ where $L$ and $M$ are
extensions of $K$ and $LM$ is a compositum of $L$ and $M$ over $K$.
This boils down to noting that if $M=K(\alpha)$ then $LM=L(\alpha)$.</p>
"
"2383806","2383900","<p>Note that it converges absolutely on $\Re(s)&gt;0$.  Let $s=a+bi$.</p>

<p>$$|\Gamma(s)|\le\int_0^\infty|x^{s-1}e^{-x}|~\mathrm dx=\int_0^\infty x^{a-1}e^{-x}~\mathrm dx=\Gamma(a)$$</p>

<p>For $-1&lt;\Re(s)\le0$, we can see by integration by parts that</p>

<p>$$\int_0^\infty x^{s-1}e^{-x}~\mathrm dx=\lim_{(a,b)\to(0^+,\infty)}\frac1sx^se^{-x}\bigg]_a^b+\frac1s\int_a^bx^se^{-x}~\mathrm dx$$</p>

<p>The integral converges absolutely while the $u\cdot v$ part diverges.  In general, for $-n&lt;\Re(s)\le1-n$, integrate by parts $n$ times, observing the final remaining integral converges whilst the rest diverges (mainly as $x\to0^+$)</p>
"
"2383834","2384086","<p>(b) If countably many values are allowed, the tail of the Fourier transform can be heavier than $|\omega|^{-1}$. Indeed, if $f(x) = |x|^{-\alpha}$ with $0&lt;\alpha&lt;1/2$ near zero (and is truncated at infinity), the Fourier transform will behave like $|\omega|^{\alpha-1}$ at infinity. (Why? The decay of Fourier transform is controlled by $\|\tau_{1/\omega} f - f\|_1$ where $\tau$ is the translation operator. Translating $|x|^{-\alpha}$ by $1/\omega$ and subtracting cancels off some of it, but it leaves a neighborhood of size $1/|\omega|$ around $0$ without much cancellation, and this neighborhood contributes $|1/\omega|^{-\alpha+1}$ to the $L^1$ norm.)</p>

<p>A countably-valued function can have this type of singularity at $0$ too, just discretize  $|x|^{-\alpha}$  at the scales $2^{-k} &lt; |x| &lt; 2^{1-k}$. </p>

<p>(a) If a function takes on finitely many values on interval, that is, $f(x) = \sum a_k \chi_{I_k}$ where each $I_k$ is an interval, then the <a href=""https://math.stackexchange.com/q/115927"">explicit form of the Fourier transform</a> can be found, and it indeed decays like $|\omega|^{-1}$.  </p>

<p>But if you allow general measurable sets, i.e, $f(x) = \sum a_k \chi_{E_k}$ for some disjoint measurable sets $E_k$, then again there's a problem because the decay of $\|\tau_{1/\omega}\chi_{E_k} - \chi_{E_k}\|_1$ can be slower than $1/|\omega|$; I think it can actually be arbitrarily slow.</p>
"
"2383835","2383847","<p>As a general rule, GRE Math Subject Exam questions do not require much nitty-gritty computation.  There is almost always a ""trick,"" or some clever way of getting around computation.  In this case, it is to use the linearity of matrix multiplication:</p>

<p>Using the fact that matrix multiplication is linear (i.e. $A(a\vec{u} + b\vec{v}) = aA\vec{u} + bA\vec{v}$ for any scalars $a$, $b$ and any vectors $\vec{u}$, $\vec{v}$), we can rewrite the last vector given as a linear combination of the other two, and obtain
\begin{align*}
&amp;\begin{pmatrix}6\\ 7\\ 8\end{pmatrix}
= -\begin{pmatrix}0\\1\\2\end{pmatrix} + 2\begin{pmatrix}3\\4\\5\end{pmatrix} \\
&amp;\quad\implies A\begin{pmatrix}6\\ 7\\ 8\end{pmatrix}
= A\left(-\begin{pmatrix}0\\1\\2\end{pmatrix} + 2\begin{pmatrix}3\\4\\5\end{pmatrix}\right)
= -A\begin{pmatrix}0\\1\\2\end{pmatrix} + 2A\begin{pmatrix}3\\4\\5\end{pmatrix}
= -\begin{pmatrix}1\\0\\0\end{pmatrix} + 2\begin{pmatrix}0\\1\\0\end{pmatrix}
= \begin{pmatrix}-1\\2\\0\end{pmatrix},
\end{align*}
which is option (B).</p>
"
"2383843","2383858","<p>$f^{-1}[[0,1)] = (-1,1)$ which <strong>is</strong> open as required. No problem.</p>

<p>But a base for $[0,\infty)$ is given by open sets of the form $[0,a) ,a&gt;0$ and $(a,b)$ with $0 &lt; a &lt; b$. The former has inverse image $(-\sqrt{a}, \sqrt{a})$ and the latter $(\sqrt{a}, \sqrt{b}) \cup (-\sqrt{b}, -\sqrt{a})$ and both are open. It suffices to check the inverse images of basic or subbasic elements for continuity.</p>
"
"2383849","2383895","<p>$\def\fitch#1#2{\begin{array}{|l}#1 \\ \hline #2\end{array}}$ </p>

<p>$\fitch{
1. A \quad Premise\\
2. (A \lor B) \rightarrow N \quad Premise}{
\fitch{
3. A \quad \ Assume}
{4. A \lor B \quad \lor \ Intro \ 3\\
5. N \quad \rightarrow \ Elim \ 2,4}\\
6. A \rightarrow N \quad \rightarrow \ Intro \ 3-5}$</p>

<p>Note that this proof never used premise 1, as indeed $(A \lor B) \rightarrow N$ follows from $A \rightarrow n$ alone; it is an example of the valid inference pattern called Strengthening the Antecedent.  Are you sure $A$ was an actual premise?</p>
"
"2383853","2383856","<p>Lemma: Given a non-constant polynomial
$$ p(x) = x^m+a_{m-1}x^{m-1}+\ldots+a_0,\qquad a_0\neq 0$$
then $\xi\neq 0$ is a root of $p(x)$ iff $\frac{1}{\xi}$ is a root of the reciprocal polynomial
$$ q(x) = a_0 x^m + a_1 x^{m-1} +\ldots + 1. $$</p>

<p>Proof: $p(\xi)=0$ implies $\frac{p(\xi)}{\xi^m} = q\left(\frac{1}{\xi}\right)=0$ and $\xi\to\frac{1}{\xi}$ is an involution.</p>

<p>If $T$ is an invertible linear operator and $p,q$ are the minimal polynomials of $T,T^{-1}$ is is not difficult to show that $\text{deg } p = \text{deg } q$ since both $\text{deg } p \leq \text{deg } q$ and $\text{deg } p \geq \text{deg } q$ have to hold. Then the claim follows from the above Lemma.</p>
"
"2383854","2384744","<p>A typo is the most reasonable explanation here, whether it's actually in your textbook (been known to happen) or you misread it (also likely).</p>

<p>Now, $$a - b = a + (-b),$$ right?</p>

<p>So if $a = 1$ and $b = -4$, we have $$1 - (-4) = 1 + 4$$ $$5 = 5,$$ which confirms your calculation.</p>

<p>My only problem with how you've posed this question is your notation. Technically, two consecutive minus signs make perfectly valid notation, and lots of calculators and computer programming languages will play along. But things are much clearer when you use parentheses to break up consecutive operators.</p>

<p>Often in computer programming, <code>-</code> is considered both a unary and a binary operator, depending on context. But you can also see the unary version as a shorthand for the binary operator when the first operand is 0, that is, $$(-b) = (0 - b).$$</p>

<p>Then we have $$1 - (-4) = 1 - (0 - 4)$$ $$ = 1 + (0 + 4)$$ $$ = 5.$$</p>
"
"2383863","2383874","<p>Suppose $y\ne 0$. We want that
$$\forall x\in \Bbb R \;\; f (x)=x^2+axy+y^2&gt;0.$$</p>

<p>with $f (0)=y^2&gt;0$.</p>

<p>the discriminant must be $&lt;0$.</p>

<p>$$\Delta=y^2 (a^2-4)&lt;0$$
which gives</p>

<p>$$-2 &lt;a &lt;2$$</p>
"
"2383872","2383881","<p>$$\sum_{n\geq 0}\frac{(-1)^n H_{2n+2}}{(n+1)(2n+1)}=2\int_{0}^{1}\sum_{n\geq 0}(-1)^n H_{2n+2}\left(x^{2n}-x^{2n+1}\right)\,dx \tag{1}$$
provides a solid starting point. We may notice that
$$ f(x)=\sum_{n\geq 0}H_{n+1} x^{n} = -\frac{\log(1-x)}{x(1-x)}\tag{2}$$
for any $x\in(-1,1)$, hence the RHS of $(1)$ equals
$$ \int_{0}^{1}\frac{1-x}{x^2+x^4}\left(2x\arctan(x)+\log(1+x^2)\right)\,dx \tag{3}$$
that is a manageable integral through the <a href=""http://mathworld.wolfram.com/Dilogarithm.html"" rel=""nofollow noreferrer"">dilogarithms</a> machinery.<br>
With the help of Mathematica I got:
$$ \sum_{n\geq 0}\frac{(-1)^n H_{2n+2}}{(n+1)(2n+1)}=\color{blue}{2 G+\frac{\pi }{2}-\frac{5 \pi^2}{48}-\log(2)-\frac{\pi}{4}\log(2)+\frac{\log^2(2)}{4}}\tag{4}$$
with $G$ being Catalan's constant. Numerically, the RHS of $(4)$ is $\approx 1.25721327371289764$.</p>
"
"2383875","2384081","<p><em>Generally</em>, for continuous real-valued random variables, $X$ and $Y$ with a joint probability density function $f_{X,Y}$, Fubini's theorem allows that: $$\begin{align}\mathsf P(X+Y\leqslant c) ~&amp;= \int_{\{(x,y):x+y\leqslant c\}} f_{X,Y}(x,y)~\mathrm d(x,y) \\[1ex] &amp;= \int_{-\infty}^{\infty}\int_{-\infty}^{c-x} f_{X,Y}(x,y)~\mathrm d y~\mathrm d x \\[1ex] &amp;= \int_{-\infty}^{\infty}\int_{-\infty}^{c-y} f_{X,Y}(x,y)~\mathrm d x~\mathrm d y\end{align}$$</p>

<p>(Notice that the constraint is placed on the inner integral and is a function of the bound variable for the outer integral.)</p>

<hr>

<p>In the <em>specific</em> case when the supports for the probability density functions of $X,Y$ are strictly non-negative, and $c$ is too, this may be expressed as:</p>

<p>$$\begin{align}\mathsf P(X+Y\leqslant c) ~&amp;= \int_{\{(x,y):0\leqslant x+y\leqslant c\}} f_{X,Y}(x,y)~\mathrm d(x,y) \\[1ex] &amp;= \int_{0}^{c}\int_{0}^{c-x} f_{X,Y}(x,y)~\mathrm d y~\mathrm d x \\[1ex] &amp;= \int_{0}^{c}\int_{0}^{c-y} f_{X,Y}(x,y)~\mathrm d x~\mathrm d y\end{align}$$</p>

<p>(Notice the constraint on the outer integral is <em>not</em> a function of either bound variables.)</p>

<hr>

<p>In the <em>particular</em> case when  $f_{X,Y}(x,y) = \tfrac 67(x+y)^2~\mathbf 1_{0\leqslant x\leqslant 1, 0\leqslant y\leqslant 1}$, we have:</p>

<p>$$\begin{align}\mathsf P(X+Y\leqslant c) ~&amp;=~ \int_{\{(x,y):0\leqslant x\leqslant 1,0\leqslant y\leqslant 1,0\leqslant x+y\leqslant\min\{2,c\}\}}\tfrac 67(x+y)^2~\mathrm d (x,y) \\[2ex] &amp;=~ \int_0^{\min\{1,c\}}\int_0^{\min\{1,c-x\}} \tfrac 67 (x+y)^2~\mathrm d y~\mathrm d x \\[2ex]&amp;=~ \begin{cases}0&amp;:&amp; c&lt;0 \\ \int_{0}^{c}\int_{0}^{c-x} \tfrac 67(x+y)^2~\mathrm d y~\mathrm d x &amp;:&amp; 0\leqslant c &lt; 1\\ \int_{0}^{c-1}\int_{0}^{1} \tfrac 67(x+y)^2~\mathrm d y~\mathrm d x+\int_{c-1}^{1}\int_{0}^{c-x} \tfrac 67(x+y)^2~\mathrm d y~\mathrm d x &amp;:&amp; 1\leqslant c &lt; 2\\ 1 &amp;:&amp; 2\leqslant c \end{cases}\\[2ex] &amp;= \begin{cases}0&amp;:&amp; c&lt;0 \\ \int_{0}^{c}\int_{0}^{c-y} \tfrac 67(x+y)^2~\mathrm d x~\mathrm d y &amp;:&amp; 0\leqslant c &lt; 1\\ \int_{0}^{c-1}\int_{0}^{1} \tfrac 67(x+y)^2~\mathrm d x~\mathrm d y + \int_{c-1}^{1}\int_{0}^{c-y} \tfrac 67(x+y)^2~\mathrm d x~\mathrm d y&amp;:&amp; 1\leqslant c &lt; 2\\ 1 &amp;:&amp; 2\leqslant c \end{cases}\end{align}$$</p>

<p>Evaluation is left to you.</p>
"
"2383880","2384028","<p>We define the characteristic function of a set $A$ as</p>

<p>$
\chi_A(x)=
\begin{cases}
1, &amp; \text{if $x\in A$}\\
0, &amp; \text{if $x\notin A$}.
\end{cases}$</p>

<p>Now</p>

<p>$x\in A\Delta(B\Delta C)\iff \chi_{A\Delta(B\Delta C)}=1\iff \chi_A(x)+\chi_{B\Delta C}(x)-2\chi_{A\cap(B\Delta C)}(x)=1\iff \chi_A(x)+\chi_{B}(x)+\chi_ C(x)-2\chi_A(x)\chi_B(x)-2\chi_A(x)\chi_{(B\Delta C)}(x)=1\iff \chi_A(x)+\chi_B(x)+\chi_C(x)\equiv1\mod 2.$</p>

<p>Hence $x\in A\Delta(B\Delta C)\iff x\  \text {belongs to an odd number of sets}A,B,C.$</p>
"
"2383886","2383905","<p>You need to add in the other terms so that $f$ takes the correct value at <em>all</em> points $x_j$.</p>

<p>Intuitively, the idea is that each polynomial of degree $&lt; n$ is uniquely determine by its values at $\,n\,$ distinct points $\,x_i,\,$ since if $f$ and $g$ agree on these points then $\,f-g$ is divisible by all $\,x-x_i$ so also by their lcm = product. But this has degree $n$ and $f-g$ has smaller degree, so $f-g = 0.\,$ </p>

<p>Thus we can represent $f$ by a vector of values $[f(x_1),\ldots f(x_n)]$ at the $x_i.\,$ Lagrange interpolation amounts to writing $f$ in terms of the unit vectors in this basis, i.e.</p>

<p>$$\begin{align}
f_1 &amp;= [1,0,0,\ldots]\\
f_2 &amp;= [0,1,0,\ldots]\\
&amp;\ \ \vdots\\
f_n &amp;= [0,\ldots,0,1]\\
\Rightarrow\ f:= \sum a_i f_i &amp;= [a_1,\ldots,a_n]
\end{align}$$</p>

<p>Therefore the above polynomial satisfies $\,f(x_i) = a_i,\,$ as desired.</p>

<p>This will become clearer when one learns about product rings and CRT = Chinese Remainder Theorem. Lagrange interpolation is a special case: solving $\,f\equiv a_i\pmod{x-x_i}.$</p>
"
"2383893","2383906","<p>Assume that $$2a^2 = u^2+v^2+w^2$$
holds for some $(a,u,v,w)\in\mathbb{N}^4$ and the system
$$ \left\{\begin{array}{rcl}y+z&amp;=&amp; u^2 \\ x+z&amp;=&amp;v^2 \\ x+y&amp;=&amp;w^2\end{array}\right. $$
has integer solutions. Then we are done, since $x+y+z=a^2$.<br>
If $u,v,w$ are even numbers the system clearly has integer solutions ($x=\frac{v^2+w^2-u^2}{2}$ and so on), so every triple $(\alpha,\beta,\gamma)\in\mathbb{N}^3$ such that $2(\alpha^2+\beta^2+\gamma^2)$ is a square leads to a solution of the original problem. But, wait. If $2(\alpha^2+\beta^2+\gamma^2)$ is a square it is an even square, i.e. a number of the form $4n^2$. In particular we get a solution of the original problem for every solution of the Diophantine equation $\alpha^2+\beta^2+\gamma^2 = 2n^2$.</p>

<p>For instance, $(\alpha,\beta,\gamma,n)=(0,1,7,5)$ leads to $2(10)^2 = 0^2+2^2+14^2$ and to the solution
$$ (x,y,z) = (-96,96,100). $$</p>

<p>The solution found by the OP, $(x,y,z)=(41,80,320)$, is associated with $11^2+19^2+20^2=2\cdot 21^2$. Another solution is $(x,y,z)=(-111,120,280)$, which is associated with $3^2+13^2+20^2=2\cdot 17^2$.</p>
"
"2383894","2383911","<p>Such manipulations are often left implicit in more advanced textbooks. You can choose a subsequence $x_{n_k}$ for the operator $A$ such that $Ax_{n_k}$ converges. Then you can choose a subsequence of $x_{n_k}$ (a bounded sequence, being a subsequence of a bounded sequence) $x_{n_{k_l}}$ such that $Bx_{n_{k_l}}$ converges. Since $Ax_{n_{k_l}}$ is a subsequence of the convergent sequence $Ax_{n_k}$, it also converges so we have a sequence $x_{n_{k_l}}$ such that both $Ax_{n_{k_l}}$ and $Bx_{n_{k_l}}$ converge and this is your required sequence (renamed as $x_{n_k}$). </p>
"
"2383902","2383913","<p><strong>hint for another approach</strong></p>

<p>Put $$x=\frac {\sinh (t)}{\sqrt {5}} $$</p>

<p>then it becomes</p>

<p>$$\frac {1}{\sqrt {5}}\int \frac { \frac {3\sinh (t)}{\sqrt {5}}+1}{\cosh (t)}\cosh (t)dt $$</p>

<p>$$=\frac {3}{5}\cosh (t)+\frac {t}{\sqrt {5}}+C $$</p>
"
"2383908","2383915","<ol>
<li>Suppose that such a function exists and let $g(z)=z^2$. Then the equality $f\left(\frac1n\right)=\frac{(-1)^n}{n^2}=g\left(\frac1n\right)$ holds for all even natural numbers $n$. Therefore, by the identity theorem, $f=g$, which is impossible, since $f\left(\frac13\right)\neq g\left(\frac13\right)$.</li>
<li>Suppose that such a function exists and let $g(z)=\frac{2z+1}{3z+1}$ ($z\in D(0,1)\setminus\left\{-\frac13\right\}$). Then, by the identity theorem, $f=g$. But this is impossible, because the limit $\lim_{z\to-\frac13}f(z)$ exists, whereas the limit $\lim_{z\to-\frac13}g(z)$ doesn't.</li>
</ol>
"
"2383934","2384055","<p>If $X,Y\overset{iid}\sim\mathcal B(p)$ and  $Z=\mathbf 1_{X+Y=0}$, then:</p>

<p>$$\mathsf E(X\mid \sigma(Z)) {~=~ \mathsf P(X{=}1\mid Z{=}1)\cdot\mathbf 1_{Z=1} + \mathsf P(X{=}1\mid Z{=}0)\cdot\mathbf 1_{Z=0}\\~=~ \mathsf P(X{=}1\mid X{+}Y{=}0)\cdot\mathbf 1_{Z=1} + \mathsf P(X{=}1\mid X{+}Y{\neq}0)\cdot\mathbf 1_{Z=0} \\ ~\ddots}$$</p>
"
"2383944","2383979","<p>Click <a href=""https://www.desmos.com/calculator/ujhnwdbfb3"" rel=""nofollow noreferrer"">HERE</a> to see a graph of what's going on.</p>
"
"2383965","2384206","<p>For a locally compact group $G$, the convolution algebra $L^1(G)$ is commutative if and only if G is Abelian.</p>

<p>This is Theorem 1.6.4 in <em>Principles of Harmonic Analysis</em> by Deitmar and Echterhoff.</p>
"
"2383969","2383986","<p>From what I understand you're comparing two lists of numbers.</p>

<p>The Euclidian distance seems like a good method (which is equivalent to the normalized Euclidian distance, you just scale it down to $[0,1]$).</p>

<p>The Chi square method is quite similar to the Euclidian distance but it punishes relative difference instead of absolute :</p>

<p>For example, let's have : $h_1=[1,10]$ and $h_2=[2,11]$.</p>

<p>In this example the Euclian method will treat the difference between ($1$ and $2$) and ($10$ and $11$) the same, whereas the chi method will consider the difference between $1$ and $2$ to be more significant than the one between $10$ and $11$. </p>

<p>That's up to you to decide which one you prefer. </p>

<p><del>You can discard the intersections as they really have no value, as is, in your case.</del></p>

<p>Another simple metric you could use is this one :</p>

<p>$$d = \sum_n|h1_i-h2_i|$$</p>

<p>This one, and the euclidian metric, are classic metrics in mathematic. The main difference between those two is the euclidian method will punish a lot more a large difference.</p>

<p>As for the code, are you sure you can't fix those square by any methods ? Also, for the normalized euclidian, you might want to assign a variable to <code>len(h1)</code> outside the loop instead of searching for it each iteration.</p>

<p>EDIT : As I read deeper into your references, I think I might have overlooked the intersection method. Can you explain a little bit more what you're trying to achieve by comparing the two histograms ?</p>
"
"2383970","2383982","<p>No, not necessarily. The issue is that $(A_1 \times B_1) \cup (A_2\times B_2)$ does not need to be of the form $A_3 \times B_3$. </p>

<p>For example, $([0,2]\times [0,2]) \cup ([1,3]\times [1,3])$ will look like:</p>

<p><a href=""https://i.stack.imgur.com/4Gx38.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4Gx38.png"" alt=""enter image description here""></a></p>

<p>However, these elements will form a <em>basis</em> for a topology on $X\times Y$, which is the product topology.</p>
"
"2383978","2383985","<p>Take the expression you are happy with, $x^a = \underbrace{x \cdot x \cdot x \ldots x}_a$ and think about what $x^{1/a}$ might mean.  Let's call it $y$ and note that $y^a = \underbrace{y \cdot y \cdot y \ldots y}_a=\underbrace{x^{1/a} \cdot x^{1/a} \cdot x^{1/a} \ldots x^{1/a}}_a=x^1$ where the last comes from the exponent law that a product of terms is the same as adding the exponents.  This gives us a good way to define fractional powers and shows the connection to roots.  </p>
"
"2383987","2384026","<p>well this is a simple equation and you can solve it by isolating the x, so yes, this is correct!</p>
"
"2383990","2383992","<p>$$\int_{0}^{+\infty}x^2 e^{-(1-i\alpha)x}\,dx=\frac{1}{(1-i\alpha)^3}\int_{0}^{+\infty}x^2 e^{-x}\,dx =\frac{2}{(1-i\alpha)^3}=\frac{2}{(1+\alpha^2)^3}(1+i\alpha)^3 $$
then you may just consider the real or imaginary parts of both sides to get:
$$ \int_{0}^{+\infty}x^2 e^{-x}\cos(\alpha x)\,dx = \frac{2-6\alpha^2}{(1+\alpha^2)^3},\qquad \int_{0}^{+\infty}x^2 e^{-x}\sin(\alpha x)\,dx = \frac{6\alpha-2\alpha^3}{(1+\alpha^2)^3}.$$</p>
"
"2383991","2384007","<p>Of course you can.  The real and imaginary parts of a complex number uniquely determine it.  You can identify $\mathbb{C}$ with the plane $\mathbb{R}^2$ via $a + ib \mapsto (a,b)$.  </p>

<p>Let $X$ be a set, and let $f$ be a function from $X$ to $\mathbb{C} = \mathbb{R}^2$.  Let $\pi_1: \mathbb{R}^2 \rightarrow \mathbb{R}$ be the function $\pi(a,b) = a$, and let $\pi_2: \mathbb{R}^2 \rightarrow \mathbb{R}$ be the function $\pi_2(a,b) = b$.</p>

<p>Let $u: X \rightarrow \mathbb{R}$ be the composition $\pi_1 \circ f$, and let $v: X \rightarrow \mathbb{R}$ be the composition $\pi_2 \circ f$.  Then obviously </p>

<p>$$f(x) = (u(x),v(x))$$</p>

<p>for any $x \in X$.  Or in the usual notation of complex numbers,</p>

<p>$$f(x) = u(x) + i v(x)$$</p>

<p>for all $x \in X$.  This is written more succinctly as $f = u + iv$.</p>
"
"2383994","2383997","<p>It comes from the concept of <a href=""http://mathworld.wolfram.com/TaylorSeries.html"" rel=""nofollow noreferrer"">Taylor/Maclaurin series</a> and is based on the formula</p>

<p>$$f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(a) \, (x-a)^n}{n!}$$</p>

<p>where $f^{(n)}(a)$ is the $n$th derivative of $f$ at $a$. Evaluate this for $f(x) = \frac{1}{1-x}$ and $a=0$ and you will have your answer.</p>

<p>Itâs worth noticing that this sum looks like it would diverge because each term is of a higher degree than the previous. However, for fractional $x$, each term actually gets <em>smaller</em>, so the series <em>converges</em>. </p>
"
"2384000","2384014","<p>Assuming you mean to give $\mathscr{C}(\mathbb{R}^n)$ the topology of uniform convergence, then $\mathcal{P}(\mathbb{R}^n)$ is very far from being dense.  For instance, if $f\in\mathscr{C}(\mathbb{R}^n)$ is any bounded continuous function, then it is infinitely far away from any nonconstant polynomial in the supremum ""norm"", since any nonconstant polynomial is unbounded.  Since any limit of constant functions is constant, this means a bounded function $f$ cannot be uniformly approximated by polynomials unless $f$ is constant.</p>
"
"2384013","2384021","<p>Assume $k$ standard decks.
<p>
<b>Probability of a pair:</b>
$$\large{\frac
{
\binom{13}{1}\binom{4k}{2}\binom{48k}{1}
}
{
\binom{52k}{3}
}}
$$
Explanation:</p>

<ul>
<li>Choose the rank for the pair: $\binom{13}{1}$ choices.
<li>Choose the two cards for that rank: $\binom{4k}{2}$ choices.
<li>Choose the non-pair card: $\binom{48k}{1}$ choices.
</ul>

<p><b>Probability of three-of-a-kind:</b>
$$\large{\frac
{
\binom{13}{1}\binom{4k}{3}
}
{
\binom{52k}{3}
}}
$$
Explanation:</p>

<ul>
<li>Choose the rank for the triple: $\binom{13}{1}$ choices.
<li>Choose the cards for that rank: $\binom{4k}{3}$ choices.
</ul>

<p><b>Update:</b>
<p>
Answering the additional questions in the OP's edit . . .
<p>
<b>Probability of a royal flush:</b>
$$\large{\frac
{\binom{4}{1}\binom{k}{1}^3}
{\binom{52k}{3}}
}
$$
Explanation:</p>

<ul>
<li>Choose the suit: $\binom{4}{1}$ choices.
<li>Choose the cards for that suit: $\binom{k}{1}^3$ choices.
</ul>

<p><b>Probability of a straight flush (but not royal):</b>
$$\large{\frac
{\binom{4}{1}\binom{11}{1}\binom{k}{1}^3}
{\binom{52k}{3}}
}
$$
Explanation:</p>

<ul>
<li>Choose the suit: $\binom{4}{1}$ choices.
<li>Choose the rank for the high card: $\binom{11}{1}$ choices.
<li>Choose $3$ cards, one for each rank: $\binom{k}{1}^3$ choices.
</ul>

<p><b>Probability of a straight (but not a flush):</b>
$$\large
{
\frac
{\binom{12}{1}\left(\binom{4k}{1}^3-\binom{4}{1}\binom{k}{1}^3\right)}
{\binom{52k}{3}}
}
$$
Explanation:</p>

<ul>
<li>Choose the rank for the high card: $\binom{12}{1}$ choices.
<li>Choose $3$ cards, one for each rank: $\binom{4k}{1}^3$ choices.
<li>Subtract the count for the flushes:  $\binom{4}{1}\binom{k}{1}^3$ choices.
</ul>

<p><b>Probability of a flush (but not a straight):</b>
$$\large
{
\frac
{\binom{4}{1}\left(\binom{13k}{3}-\binom{12}{1}\binom{k}{1}^3\right)}
{\binom{52k}{3}}
}
$$
Explanation:</p>

<ul>
<li>Choose the suit: $\binom{4}{1}$ choices.
<li>Choose $3$ cards from that suit: $\binom{13k}{3}$ choices.
<li>Subtract the count for the straights:  $\binom{12}{1}\binom{k}{1}^3$ choices.
</ul>
"
"2384042","2384044","<p>it means that $a$ is between $-10$ and $0$</p>

<p>so $a&lt;0$ and $a&gt;-10$</p>
"
"2384048","2384077","<p>No, the perimeter is not part of the model.</p>

<p>The lines that approach a mutual point in the perimeter are considered parallel.</p>

<p>In hyperbolic geometry, two distinct parallel lines are further classified as one of two types of parallel</p>

<ul>
<li>limit parallel: if the two lines approach a common point on the perimeter of the circle</li>
<li>ultraparallel: if they are not limit parallel.</li>
</ul>

<p>In your picture, the line through $AB$ and the line through $KL$ are ultraparallel, while $EF$ is limit parallel to $AB$.</p>
"
"2384049","2384058","<p>Write:</p>

<p>$$T = \frac{1}{2}(T + T^*) + \frac{i}{2i}(T - T^*)$$</p>

<p>Prove that $$K=\frac{1}{2}(T + T^*)$$ $$\Lambda=\frac{1}{-2i}(T^* - T)$$ are self adjoint operators and go on...</p>
"
"2384056","2387545","<p>Iâll assume that $O_2$ denotes the zero $2\times 2$ matrix.</p>

<blockquote>
  <p>$a^2 \ne b^2$</p>
</blockquote>

<p>We shall need only an assumption $a\ne b$.</p>

<p>Let $J$ be a Jordan canonical form of the matrix $A$. There exists a non-singular matrix $T$ such that $J=T^{-1}AT$. Put $H=T^{-1}BT$. Applying conjugation by $T$ to the given expressions we obtain </p>

<p>$$O_2=T^{-1}O_2T=T^{-1}(A(A-aB)+B(B-bA))T=$$ 
$$ = T^{-1}A(A-aB)T + T^{-1}B(B-bA)T=$$
$$= T^{-1}ATT^{-1}(A-aB)T + T^{-1}BTT^{-1}(B-bA)T=$$
$$= T^{-1}AT(T^{-1}AT-aT^{-1}BT) + T^{-1}BT(T^{-1}BT-bT^{-1}AT)=$$
$$J(J-aH)+H(H-bJ)=$$ $$J^2-aJH-bHJ+H^2.$$</p>

<p>and </p>

<p>$$T^{-1}(AB-BA)^2T=(JH-HJ)^2.$$</p>

<p>So it suffices to show that $$(JH-HJ)^2=O_2.$$</p>

<p>Let $$H=\begin{pmatrix} h_{11} &amp; h_{12} \\ h_{21} &amp; h_{22}\end{pmatrix}.$$ Then</p>

<p>$$H^2=\begin{pmatrix} h_{11}h_{11}+h_{12}h_{21} &amp; 
h_{11}h_{12}+h_{12}h_{22} \\ h_{11}h_{21}+h_{21}h_{22} &amp; h_{12}h_{21}+h_{22}h_{22}\end{pmatrix}.$$</p>

<p>The following cases are possible for $J$. </p>

<p><strong>Case 1.</strong> 
$$J=\begin{pmatrix} \lambda_{1} &amp; 0 \\ 0 &amp; \lambda_{2}\end{pmatrix}.$$ Then </p>

<p>$$J^2=\begin{pmatrix} \lambda_{1}^2 &amp; 0 \\ 0 &amp; \lambda_{2}^2\end{pmatrix},\,
JH=\begin{pmatrix} \lambda_1 h_{11} &amp; \lambda_1 h_{12} \\ \lambda_2 h_{21} &amp; \lambda_2 h_{22}\end{pmatrix},\mbox{ and }  
HJ=\begin{pmatrix} \lambda_1 h_{11} &amp; \lambda_2 h_{12} \\ \lambda_1 h_{21} &amp; \lambda_2 h_{22}\end{pmatrix}.$$</p>

<p>Thus $$JH-HJ=(\lambda_1-\lambda_2)\begin{pmatrix} 0 &amp; h_{12} \\ 
- h_{21}&amp; 0\end{pmatrix}.$$</p>

<p>and $$O_2=J^2-aJH-bHJ+H^2=$$
$$\begin{pmatrix}\lambda_1^2-a\lambda_1h_{11}-b\lambda_1h_{11}+ h_{11}h_{11}+h_{12}h_{21} &amp;
-a\lambda_1h_{12}-b\lambda_2h_{12}+ h_{11}h_{12}+h_{12}h_{22} \\
-a\lambda_2h_{21}-b\lambda_1h_{21}+ h_{11}h_{21}+h_{21}h_{22} &amp;
\lambda_2^2-a\lambda_2h_{22}-b\lambda_2h_{22}+ h_{12}h_{21}+h_{22}h_{22}
\end{pmatrix}.$$ </p>

<p>In particular $h_{12}(-a\lambda_1-b\lambda_2+ h_{11}+h_{22})$ and
$h_{21}(-a\lambda_2-b\lambda_1+ h_{11}+h_{22})$
are zeros. On the other hand, 
$(JH-HJ)^2=O_2$ provided one of the numbers $\lambda_1-\lambda_2$, $h_{12}$, and $h_{21}$ is zero. But if none of them is zero then 
$$-a\lambda_1-b\lambda_2+ h_{11}+h_{22}=0$$ and 
$$-a\lambda_2-b\lambda_1+ h_{11}+h_{22}=0.$$ Thus 
$$-a\lambda_1-b\lambda_2=-a\lambda_2-b\lambda_1,$$ or  </p>

<p>$$b(\lambda_1-\lambda_2)=a(\lambda_1-\lambda_2),$$
a contradiction.</p>

<p><strong>Case 2.</strong> 
$$J=\begin{pmatrix} \lambda &amp; 1 \\ 0 &amp; \lambda\end{pmatrix}.$$ Then </p>

<p>$$J^2=\begin{pmatrix} \lambda^2 &amp; 2\lambda \\ 0  &amp;\lambda^2\end{pmatrix},\,
JH=\lambda H+ \begin{pmatrix} h_{21} &amp; h_{22} \\ 0 &amp; 0\end{pmatrix},\mbox{ and }  
HJ=\lambda H+ \begin{pmatrix} 0 &amp; h_{12} \\ 0 &amp; h_{21}\end{pmatrix}.$$
Thus $$JH-HJ=\begin{pmatrix} h_{21} &amp; h_{22}-h_{12} \\ 
0&amp; -h_{21}\end{pmatrix}.$$</p>

<p>and $$O_2=J^2-aJH-bHJ+H^2=$$
$$\begin{pmatrix}\lambda^2-(a+b)\lambda h_{11}-ah_{21}+ h_{11}h_{11}+h_{12}h_{21} &amp; 2\lambda-(a+b)\lambda h_{12}-ah_{22}-bh_{12}+ h_{11}h_{12}+h_{12}h_{22} \\
-(a+b)\lambda h_{21}+ h_{11}h_{21}+h_{21}h_{22} &amp;
\lambda^2-(a+b)\lambda h_{22}-bh_{21}+ h_{12}h_{21}+h_{22}h_{22}
\end{pmatrix}.$$</p>

<p>In particular $h_{21}(-(a+b)\lambda + h_{11}+h_{22})$ is zero. On the other hand, it is easy to check that $(JH-HJ)^2=O_2$ iff $h_{21}$ is zero. But $h_{21}$ is not zero then 
$$(a+b)\lambda=h_{11}+h_{22}.$$</p>

<p>Thus $$O_2=\begin{pmatrix}\lambda^2-ah_{21}+ h_{12}h_{21}-h_{11}h_{22} &amp; * \\
0 &amp;
\lambda^2-bh_{21}+ h_{12}h_{21}-h_{11}h_{22}
\end{pmatrix},$$</p>

<p>and hence $a=b$, a contradiction.</p>
"
"2384060","2384066","<p>Your solution is correct and also indicates why your first approach didn't work.  Since $\Bbb{F}_2$ has characteristic 2 there are no $2^{n}$th roots of $1$ for $n&gt;0$ in any extension.  The $(x-1)^4$ factor of $f(x)$ reflects that.</p>
"
"2384061","2384067","<p>\begin{align}
\int_0^{\infty} x^n e^{-\frac{x}{a}}\,dx
&amp;\overset{u=x/a}{=} \int_0^{\infty} (au)^ne^{-u}\,(a\,du)\\
&amp;= a^{n+1}\int_0^{\infty}u^{(n+1)-1}e^{-u}\,du \\
&amp;= a^{n+1}\Gamma(n+1) = a^{n+1}n!
\end{align}
if $n$ is an integer.</p>
"
"2384070","2384119","<p>The following assumes that $\,M\,$ is the midpoint of $\,P_1P_2\,$, which was not explicitly stated in the question, but is implied by the context. In that case, $\displaystyle\,\overrightarrow{P_1M}=\frac{1}{2}\,\overrightarrow{P_1P_2}\,$.</p>

<blockquote>
  <p>$\displaystyle\overrightarrow{OM}=\overrightarrow{OP_1}+\frac{1}{2}\,\overrightarrow{P_1P_2}=\dots$</p>
</blockquote>

<p>Since $\displaystyle\,\frac{1}{2}\,\overrightarrow{P_1P_2}=\overrightarrow{P_1M}\,$, the above follows from $\,\overrightarrow{OM}=\overrightarrow{OP_1}+\overrightarrow{P_1M}\,$ by the <a href=""https://en.wikipedia.org/wiki/Euclidean_vector#Addition_and_subtraction"" rel=""nofollow noreferrer"">vector addition definition</a>, sometimes referred to in this context as the <a href=""http://mathworld.wolfram.com/VectorAddition.html"" rel=""nofollow noreferrer"">head-to-tail rule</a>.</p>

<blockquote>
  <p>$\displaystyle\dots=\overrightarrow{OP_1}+\frac{1}{2}\left(\overrightarrow{OP_2} - \overrightarrow{OP_1}\right)\,$</p>
</blockquote>

<p>This follows from the previous step because $\,\overrightarrow{P_1P_2}=\overrightarrow{OP_2}-\overrightarrow{OP_1} \iff \overrightarrow{OP_1}+\overrightarrow{P_1P_2}=\overrightarrow{OP_2}\,$, which in turn follows from the same vector addition property as above.</p>
"
"2384073","2384078","<p>We have that if $y \in S$ then $(y+a)-a \in S$</p>

<p>Thus $$(y+a) \in T \Rightarrow y+a=t \in T \Rightarrow y=t-a \leqslant \sup T -a$$</p>

<p>So $\sup S \leqslant \sup T-a \Rightarrow \sup S+a \leqslant \sup T$</p>
"
"2384085","2384100","<p>Let $f_n \in A=\{ f \in C_
\infty(U) : f(x) = O(\| x \|) \}$ such that $f_n \rightarrow f$ uniformly.</p>

<p>Let $\epsilon&gt;0$.</p>

<p>Then if $ x \neq 0$,
we have that exists $n_0 \in \mathbb{N}$ such that  $||f_n-f||_{\infty}\leqslant \epsilon||x|| ,\forall n \geqslant n_0,\forall x \in U$</p>

<p>Also $f_n=O(||x||), \forall n \in \mathbb{N} $</p>

<p>Thus $$ \frac{|f(x)|}{||x||} \leqslant \frac{|f_{n_0}(x)-f(x)|}{||x||}+\frac{|f_{n_0}(x)|}{||x||} \leqslant \epsilon + \frac{|f_{n_0}(x)|}{||x||}$$</p>

<p>Then $\limsup_{x \rightarrow 0}\frac{|f(x)|}{||x||} \leqslant \epsilon +0=\epsilon$ thus $f=O(||x||)$</p>
"
"2384093","2384109","<p>Note </p>

<p>$$\lim_{h \to 0^{+}} \left(\frac{f(h) - f(0)}{h}\right) = \lim_{h \to 0^{+}} f'(\alpha) = L$$ </p>

<p>where $\alpha \in (0,h)$. To be precise, the hypotheses of the mean value theorem are satisfied since $f$ is assumed continuous on $[0,h]$ and differentiable on $(0,h)$. </p>

<p>The above limit is necessarily $L$, since the $\alpha$ is forced to $0$ as $h \to 0^{+}$, and we know $\lim_{h \to 0} f'(h) = L$. This can easily be formalized with $\epsilon-\delta$ if so desired. </p>

<p>Similarly, $$\lim_{h \to 0^{-}} \left(\frac{f(h) - f(0)}{h}\right) = \lim_{h \to 0^{-}} f'(\alpha) = L$$ </p>

<p>where $\alpha \in (h, 0)$. </p>

<p>This implies </p>

<p>$$\lim_{h \to 0} \left(\frac{f(h) - f(0)}{h}\right) = L$$</p>

<p>and so $f'(0)$ exists. </p>
"
"2384099","2384724","<p>If there were an integrable minorant, say $M$, for the $S_n$, then Fatou's lemma, applied to the non-negative $S_n - M$ would yield</p>

<p>\begin{align}
\int S\,dP &amp;= \int M\,dP + \int (S -M)\,dP \\
&amp;= \int M\,dP + \int \liminf_{n\to\infty} (S_n - M)\,dP \\
&amp;\leqslant \int M\,dP + \liminf_{n\to\infty} \int (S_n - M)\,dP \\
&amp;= \liminf_{n\to\infty} \Biggl(\int M\,dP + \int (S_n - M)\,dP\Biggr) \\
&amp;= \liminf_{n\to\infty} \int S_n\,dP.
\end{align}</p>
"
"2384106","2384124","<p>You have, as you wrote, $$f(m)=m \log (m)+(n-m) \log (n-m)$$ $$f'(m)=\log (m)-\log (n-m)$$ $$f''(m)=\frac{1}{n-m}+\frac{1}{m}$$ So, the serivative cancels when $$\log (m)=\log (n-m)\implies m=n-m\implies 2m=n\implies m=\frac n 2$$ Now, using the second derivative test $$f''\left(\frac n 2\right)=\frac{4}{n}&gt;0$$ So, the point is the minimum and $$f\left(\frac n 2\right)=n \log \left(\frac{n}{2}\right)$$</p>
"
"2384110","2384149","<p>Let $2+2^2+...+2^n=x$. Hence, we obtain:
$$2x=2^2+2^3+...+2^n+2^{n+1}$$ or
$$2x=x-2+2^{n+1}$$ or
$$x=2^{n+1}-2.$$</p>
"
"2384112","2384250","<p>The correct definition of $A$ being a disconnected subset of $X$ is that $A$ is disconnected as a space in its own right, i.e. having the subspace topology.</p>

<p>If $A \subseteq X$ is a disconnected subset, we can translate this to a statement about open sets of $X$ if we like:</p>

<blockquote>
  <p>$A \subseteq X$ is disconnected iff there exist $U,V$, open sets in $X$, such that </p>
  
  <ul>
  <li>$U \cap A \neq \emptyset$</li>
  <li>$V \cap A \neq \emptyset$</li>
  <li>$A \subseteq U \cup V$</li>
  <li>$A \cap U \cap V = \emptyset$</li>
  </ul>
</blockquote>

<p>This corresponds to $\{U \cap A$, $V \cap A\}$ being a disconnection of $A$ in its subspace topology.</p>
"
"2384113","2384269","<p>Good work! Your calculation is correct.</p>

<blockquote>
  <p>The $\color{blue}{45}$ admissible strings are
  \begin{array}{ccccccccc}
13254&amp;13524&amp;13542&amp;14253&amp;14325&amp;14352&amp;15243&amp;15324&amp;15432\\
21354&amp;21435&amp;21543&amp;24135&amp;24153&amp;24315&amp;25314&amp;25413&amp;25431\\
31425&amp;31524&amp;31542&amp;32154&amp;32415&amp;32541&amp;35214&amp;35241&amp;35421\\
41325&amp;41352&amp;41532&amp;42135&amp;42153&amp;42531&amp;43152&amp;43215&amp;43521\\
52143&amp;52413&amp;52431&amp;53142&amp;53214&amp;53241&amp;54132&amp;54213&amp;54321\\
\end{array}</p>
</blockquote>
"
"2384116","2384233","<p>This question is simple after I find that we can use lemma 2 iteratively. Set $X=C_1$ and use the lemma 2 again , we get $C_2$, and set $X=C_2$ again. This gives the sequence clearly.</p>
"
"2384121","2384129","<p>It's Riemann integrable on compact intervals, hence continuous at some $x_0 \in \mathbb{R}$. </p>

<p>Then consider $f(y + x_0) - f(x_0)= f(y)$. Taking $y \to 0$ shows continuity at $0$ (since $f(0) = 0$). </p>

<p>For <em>arbitrary</em> $s$, we have</p>

<p>$$\lim_{t \to 0}\left(f(t+s) - f(s)\right) = \lim_{t \to 0}f(t) = 0$$</p>

<p>This establishes global continuity. You say you are able to prove it when $f$ is continuous, so you can fill in the rest. </p>

<p><em>Remark</em>: It remains true if we weaken the condition to Lebesgue integrable. </p>
"
"2384123","2384141","<p>Assume $K = k(\alpha)$. Then $k(\alpha)(X) = k(X)(\alpha)$. So $1,\alpha,\ldots,\alpha^{n-1}$ is a basis of the $k(X)$-vector space $K(X)$, with $n = [k(\alpha):k]$.</p>

<hr>

<p>In your example $K = k(\alpha), [k(\alpha):k] = 2$ so
$K(X)=\{\frac{u_1(X)}{v_1(X)}+\alpha\frac{u_2(X)}{v_2(X)}, u_i,v_i \in k[X]\}$.</p>

<p>If you have $\frac{a(X)+\alpha b(X)}{c(X)+\alpha d(X)}, a,b,c,d \in k[X]$ then 
$$\frac{a(X)+\alpha b(X)}{c(X)+\alpha d(X)} =\frac{(a(X)+\alpha b(X))(c(X)+\overline{\alpha} d(X))}{(c(X)+\alpha d(X))(c(X)+\overline{\alpha} d(X))}= \frac{w_1(X)+\alpha w_2(X)}{z(X)}$$
where $\overline{\alpha} = e+\alpha f$ is the other root of the minimal polynomial of $\alpha$ over $k$</p>
"
"2384125","2384143","<p>Let us tanslate the data $$F=20000\times (1.05)^{y}$$ $$C=25000+y\times500$$ You can graph the two functions and notice when $C&gt;F$.</p>

<p>Otherwise, you need a numerical method to first solve the equation $$f(y)=20000\times (1.05)^{y}-(25000+500y)$$ and Newton method is probably the simplest. Being lazy, let us start with $y_0=0$. The successive iterates will then be 
$$\left(
\begin{array}{cc}
 n &amp; y_n \\
  0 &amp; 0 \\
 1 &amp; 10.5 \\
 2 &amp; 7.73 \\
 3 &amp; 7.41 \\
 4 &amp; 7.40
\end{array}
\right)$$</p>

<p>Let us check : after $7$ years, $F=28142$ and $C=28500$;  after $8$ years, $F=29549$ and $C=29000$.</p>
"
"2384133","2384981","<p>Hint:</p>

<p>Each Jordan block corresponds to an eigenvector. You need $4$ Jordan blocks.</p>

<p>For example, $\begin{bmatrix} 2 &amp; 1 \\ 0 &amp; 2 \end{bmatrix}$ has $2$ as an eigenvalue and it has one independent eigenvector. Think of the other suitable Jordan blocks and put them together.</p>
"
"2384135","2384140","<p>Note that $(a,b)$ and $(b,c)$ are the only two pairs that have matching middle terms ($b$ in this case). Thus, in order for the relation to be transitive, it must contain $(a,c)$ - which it does. Therefore it is transitive.</p>

<hr>

<p><strong>EDIT:</strong></p>

<p>In general, to test if a relation is transitive, we need to grab all possible pairs in the relation (meaning pairs of pairs) that have matching middle terms. Then, one by one, look to see if the term required by transitivity is present. If any such terms are not, then the relation is not transitive. For a more complicated example, consider the relation</p>

<p>$$ R = \{(a,a),(a,b),(a,c),(a,e),(b,c),(b,e),(c,d) \}.
$$
We find all pairs of elements with matching middle terms. They are:
$$ (a,a)\ \text{and}\ (a,b) \\
(a,a)\ \text{and}\ (a,c) \\
(a,a)\ \text{and}\ (a,e) \\
(a,b)\ \text{and}\ (b,c) \\
(a,b)\ \text{and}\ (b,e) \\
(a,c)\ \text{and}\ (c,d) \\
(b,c)\ \text{and}\ (c,d)
$$
Now, test them one by one until we either find a failure or terminate the whole list.
$$ (a,a)\ \text{and}\ (a,b) \implies \text{need}\ (a,b)\ \checkmark \\
(a,a)\ \text{and}\ (a,c) \implies \text{need}\ (a,c)\ \checkmark \\
(a,a)\ \text{and}\ (a,e) \implies \text{need}\ (a,e)\ \checkmark \\
(a,b)\ \text{and}\ (b,c) \implies \text{need}\ (a,c)\ \checkmark \\
(a,b)\ \text{and}\ (b,e) \implies \text{need}\ (a,e)\ \checkmark \\
(a,c)\ \text{and}\ (c,d) \implies \text{need}\ (a,d)\times \\
$$
Thus we find that $R$ is not transitive and there is no need to check the pair. Note that we also could have made our list shorter by not considering the reflexive term $(a,a)$.</p>

<p>If things get even more complicated and our relation is no longer finite (or small), we have to go about things differently to prove transitivity or find a counter example.</p>
"
"2384136","2384144","<p>Using the integrating factor method $P(x)=3x^2$ and therefore the integrating factor is $e^{x^3}$, multiplying both sides by this we get $e^{x^3}y'+3x^2e^{x^3}y=x^2e^{x^3}\implies \left(e^{x^3}y\right)'=x^2e^{x^3}$ integrating both sides we get $e^{x^3}y=\frac{1}{3}e^{x^3}+C$ finally, $\boxed{y=\frac{1}{3}+Ce^{-x^3}}$</p>
"
"2384148","2384194","<p>Let $M\in AB$ such that $B$ placed between $M$ and $A$ and $MB=BE$ and </p>

<p>$K\in AC$ such that $C$ be a midpoint of $KD$.</p>

<p>Hence, since $\measuredangle CDE=60^{\circ}$, we have $AK=AD+DK=AD+DE$.</p>

<p>Also, $AM=AB+BM=AB+BE$.</p>

<p>Thus, it remains to prove that $MK=BD$ and $\measuredangle MKA=90^{\circ}$.</p>

<p>From here I used trigonometry:</p>

<p>We can show that a projection of $MB$ on the line $AC$ is equale to $KC$, </p>

<p>which gives that $\measuredangle MKA=90^{\circ}$.  </p>

<p>Now, we can show that $MK=BD$ and it ends the proof by the Pythagoras theorem.</p>

<p>Maybe it will help and you'll  find something nicer. </p>

<p>I'll prove that $MK=BD$ if you proved that $MK\perp AK$.</p>

<p>Let $AB=a$.</p>

<p>Hence, by law of sines for $\Delta ABD$ we obtain:
$$\frac{BD}{\sin10^{\circ}}=\frac{a}{\sin70^{\circ}}$$ or
$$BD=\frac{a\sin10^{\circ}}{\sin70^{\circ}}.$$
Now, by law of sines for $\Delta BED$ we obtain:
$$\frac{BE}{\sin10^{\circ}}=\frac{\frac{a\sin10^{\circ}}{\sin70^{\circ}}}{\sin150^{\circ}},$$
which gives
$$MB=BE=\frac{2a\sin^210^{\circ}}{\sin70^{\circ}}.$$
Thus, 
$$AM=AB+BM=a+\frac{2a\sin^210^{\circ}}{\sin70^{\circ}}=\frac{a(\sin70^{\circ}+1-\cos20^{\circ})}{\sin70^{\circ}}=\frac{a}{\sin70^{\circ}},$$
which says
$$MK=AM\sin10^{\circ}=\frac{a\sin10^{\circ}}{\sin70^{\circ}}=BD.$$</p>
"
"2384152","2384158","<p>Notice that the columns of $A_n$ obey this property:
$$\vec v_n=2\vec v_{n-1} - \vec v_{n-2}$$</p>

<p>This means that only the first two columns are linearly independent, and everything after that can be represented as a linear combination of the the first two columns. Thus, $\rm Col(A)=2=Rank(A)$</p>

<p>Another solution using row reduction:</p>

<p>$R_2-R_1$ of every matrix is a row of $4$'s. </p>

<p>We can replace $R_2$ with $\frac{1}{4}(R_2-R_1)$ to get a row of $1's$. We can then replace every $R_n$ with $n \ge 3$ with $R_n-R_1-kR_2$ where $k$ is the number of $1$'s that need to be subtracted from $R_2-R_1$ to get a row of $0$'s. </p>

<p>Thus, we get the first row of $1,2,3 \ldots$ and the second row of just ones, rows that can't be turned into zeroes, meaning that $\rm Rank(A)=2$.</p>

<p>I do not know what these matrices are called. </p>
"
"2384155","2384208","<p>You can show by induction on the complexity of the formula $\phi \equiv \phi_1 \vee \phi_2$ or $\psi \equiv \psi_1 \rightarrow \psi_2$ that the formula does not imply $\neg P$.</p>

<p>$\bf Base\ case.$ No atomic formula $Q$ implies $\neg P$.</p>

<p>$\bf Induction\ step\ 1.$ If either $\phi_1$ or $\phi_2$ fails to imply another formula $\theta$ (such as $\neg P$) then $\phi_1 \vee \phi_2$ also fails to imply $\theta$. (This may be easier to see from the counterfactual: if $\phi_1 \vee \phi_2$ implies $\theta$, then both $\phi_1$ and $\phi_2$ do as well.)</p>

<p>$\bf Induction\ step\ 2.$ If $\psi_2$ fails to imply $\theta$ then $\psi_1 \rightarrow \psi_2$ fails to imply $\theta$. (If you like, this can be because $\psi_1 \rightarrow \psi_2$ is equivalent to $\neg \psi_1 \vee \psi_2 $, which lets us reduce the $\rightarrow$ case to the $\vee$ case.)</p>

<hr>

<p>Question for you: does this strategy also work for (ii) $P \wedge Q$ and (iii) $P \leftrightarrow Q$? Or can you solve these cases by some other means, e.g. the sort of argument  John Griffin has used in the comments to show (ii) implies (iii)?</p>

<hr>

<p>Your reasoning that binary connectives be can't say anything about unary formulas is not correct in general. A famous and popular example of this behaviour is that $(P \rightarrow Q) \rightarrow P$ actually implies $P$.</p>
"
"2384156","2384164","<p>I'll sketch a solution for the first part and leave the second to you. Assume that $T$ is onto. Let us show that $T^t$ is one-to-one. Let $g \in W'$ such that $T^t(g) = g \circ T = 0$. Then $g(T(v)) = 0$ for all $v \in V$. Given $w \in W$, choose $v \in V$ such that $Tv = w$ and deduce that $g(T(v)) = g(w) = 0$. Since $w$ was arbitrary, $g = 0$ which shows that $T^t$ is one-to-one.</p>

<p>Assume now that $T^t$ is one-to-one. Let us show that $T$ is onto. Assume by contradiction that $\operatorname{Im}(T) \subsetneq W$ and choose a complement $U$ such that $\operatorname{Im}(T) \oplus U = W$. Since $U \neq \{ 0 \}$, there exists a linear functional $g \in W'$ such that $g|_{\operatorname{Im}(T)} = 0$. But then $T^t(g) = 0$ while $g \neq 0$, a contradiction. </p>
"
"2384166","2384215","<p>The Euclidean algorithm yields the GCD as the last non-zero remainder. (I assume you are familiar with this)
I'll continue where you got stuck: $$7^n = 4q + r, 0\leq r&lt;4 $$
If $r=0$, then $7^n$ would be divisible by $4$, which is nonsense. So $r\in\{1,2,3\}$  </p>

<p>Suppose $r=3$, then $7^n = 4q + 3$, for some $q\in\mathbb Z$ and $\mbox{gcd}(4,3)=1$, so $\mbox{gcd}(7^n+4,7^n)=1$.<br>
Apparently, $r=2$ is impossible. In any event, the greatest common divisor must be $1$.</p>
"
"2384168","2384196","<p>Everything looks correct to me but could be condensed and reworded a bit. Here is how I would do it:    </p>

<hr>

<p>Let $S_n = \frac{r^n}{1+r^n}=\frac{1}{1+r^{-n}}$ so that $S_n$ is clearly non-decreasing for $r\ge 1$ since $r^{-n}$ is non-increasing<br>
Then $\sup(S_n)=\lim\left( \frac{1}{1+r^{-n}}\right) =1$<br>
We now wish to prove that $S_n$ converges to $1$ when $r&gt;1$; to this end,  let $\epsilon &gt; 0$ be given, and let $N\in\mathbb{N}$ such that $N&gt; \frac{\log(1/\epsilon)}{\log(r)} \implies \frac{1}{r^N} &lt; \epsilon$<br>
Then, for all natural numbers $n \ge N$,
$$|S_n - 1| = \left|\frac{1}{1+r^{-n}} - 1\right| = \left|\frac{-r^{-n}}{1+r^{-n}}\right| = \frac{r^{-n}}{1+r^{-n}}=\frac{1}{1+r^n}&lt;\frac{1}{r^n} &lt;\frac{1}{r^N}&lt;\epsilon$$
And so we conclude that $S_n$ converges to $1$ .</p>

<hr>

<blockquote>
  <p>Is my method correct?     </p>
</blockquote>

<p>As far as I can tell, yes it is.  </p>

<p><strong>Edit:</strong> as <code>@1524</code> notes,this argument only works for $r &gt; 1$ and <strong>not</strong> for $r \ge 1$. At the point $r=1$ we just have a sequence with all terms $\frac{1}{2}$ which trivially converges to $\frac{1}{2}$</p>

<blockquote>
  <p>can this be negative?   </p>
</blockquote>

<p>Note that $\frac{-\log(\epsilon)}{\log(r)} = \frac{\log(1/\epsilon)}{\log(r)}$ is only negative $\epsilon &gt; 1$ in which case <strong><em>any</em></strong> positive value for $N$ will suffice.  </p>

<blockquote>
  <p>Does $N$ have to be a value or is a bound enough?  </p>
</blockquote>

<p>$N$ is definitely a value, since $N \in \mathbb{N}$. It is also a lower bound on $n$, since $n \ge N$.</p>
"
"2384171","2384173","<p>This representation comes from the <a href=""https://en.wikipedia.org/wiki/Fundamental_theorem_of_arithmetic"" rel=""nofollow noreferrer"">Fundamental Theorem of Arithmetic</a>, and changes a nonzero integer into a unique pair of integers, one nonnegative and the other odd.  We can do more:</p>

<p>$$a_j=2^{k_j}3^{i_j}q_j$$
Now, $k_j$ and $i_j$ are nonnegative integers, and $q_j$ is a nonzero integer having no divisors in common with $6$.  This representation is also unique, now as a triple.</p>

<p>There's nothing special about $2$, or $2$ and $3$; any primes can be used in this manner.  All that matters is that the leftover bit, the $q_j$, has none of those primes dividing it.</p>
"
"2384176","2384201","<p>Short answer: You were assuming that some ""x"" symbols were the multiplication symbol $\times$. In fact, each is the variable $x$.</p>

<p>Long answer: The symbol ""x"" is confusing because it can stand for two things: multiplication $\times$ and the variable $x$. </p>

<p>In more advanced math courses, people tend not to* use the $\times$ symbol for multiplication. Instead, they use one of two methods. First, the dot $\cdot$. So, you'd write $2\cdot 3 = 6$ instead of $ 2 \times 3 = 6$. Second, using parentheses. So, you'd write $2(3) = 6$, $(2)3 = 6$ or $(2)(3) = 6$ (all are equivalent) instead of $2 \times 3 = 6$.</p>

<p>This notation is useful when you work with the variable $x$. Note that $2x \cdot 3 = 6x$ and $(2x)3$ are much more legible than $2x \times 3 = 6x$, especially when you're writing these expressions by hand. Personally, I recommend using $\cdot$ or parentheses for multiplication whenever you're doing algebra with the variable $x$. It will help you avoid confusing $\times$ and $x$. </p>

<p>Now, back to your question. Let's look at example 1. You want to expand the expression <code>8x(x+3)</code> using the distributive property. You were interpreting this as $8\times (x + 3)$, so the first ""x"" was multiplication $\times$ and the second ""x"" was the variable $x$. But actually, the question is asking about $8x(x+3) = 8x \cdot (x+3)$. <strong>In these given examples, none of the ""x"" symbols stand for multiplication. They're all variables.</strong></p>

<p>To solve the problem, you just split up the left and right side, exactly as you said. Here's the solution you supplied in your question, rewritten for clarity using the $\cdot$ notation:</p>

<p>\begin{align*}
8x \cdot (x+3) &amp;= 8x \cdot x + 8x \cdot 3 \\
&amp;= 8x^2 + 24x
\end{align*}</p>

<p>Does this help?</p>

<p>*Footnote so nobody yells at me: The $\times$ symbol actually is used in advanced math for special purposes. For example, the cross product (product of two vectors) and Cartesian product (product of two sets) use the symbol. Since vectors and sets rarely use the symbol $x$, using the $\times$ symbol is ok here.</p>
"
"2384179","2385054","<p>Note that the 3 equations are equivalent to </p>

<p>$$7^y =3, 7^z=2, \mbox{ and } 3^x=2.$$</p>

<p>Replace the $3$ in the last equation with $7^y$, by dint of the first equation.  And replace the $2$ in the last equation with $7^z$ per the second equation,
and you have</p>

<p>$$(7^y)^x = 7^z$$</p>

<p>or </p>

<p>$$7^{xy} = 7^z.$$</p>

<p>Therefore $xy = z$ and so $x=z/y.$</p>
"
"2384188","2384743","<p>Assume that a given looped sequence $\{A_n\}$ can be described by a pair of integers $(p,b)$ and the relation
$$
A_n = A_0 p^n + n \sum_{k=0}^{n-1} p^k \mod M.
$$
The first thing to note is that we have a recurrence relation:
$$
A_{n+1} = p A_n + b \mod M
$$
and it follows that the same $(p,b)$ combination would be found wherever we start in the loop. The first three elements of the loop are:
$$A_0$$
$$A_1 = p A_0 + b$$
$$A_2 = p A_1 + b$$
and we can combine them to obtain
$$A_2 - 2 A_1 + A_0 = (A_2-A_1) - (A_1-A_0) = (p-1) (A_1 - A_0) $$
$$A_1^2 - A_2 A_0 = A_1(p A_0 +b) - (p A_1 + b) A_0 = b (A_1-A_0)$$
This gives us the two congruence equations:
$$A_2 - 2 A_1 + A_0  \equiv (p-1) (A_1 - A_0) \mod M$$
$$A_1^2 -A_1 A_0  \equiv b (A_1 - A_0) \mod M$$
to be solved for $p-1$ and $b$. These can be solved if and only if 
$$A_2 - 2 A_1 + A_0 \equiv A_1^2 - A_2 A_0 \equiv 0 \mod \gcd(M,A_1-A_0)$$
This double condition is easy to check and when satisfied, the values for $p$ and $b$ follow by solving the congruence equations with the standard algorithm, otherwise no such $(p,b)$ combinations exists that allows the loop to be described via the imposed formula.</p>
"
"2384190","2384264","<p>(Answer modified since the definitions of $a,$ $b_1,$ and $b_2$ have been clarified by providing them in the question itself. I confess I did not recognize them in the question's comments, partly due to the formatting there.)</p>

<p>This is indeed a summation over four variables, which would be computed much the way the question describes (although I think the algorithm could be described a little simpler).</p>

<p>Perhaps you can use properties of the gamma function to simplify the calculation of the last couple of lines of the formula.
You can at least rearrange the arguments to the gamma function so that each argument is merely a constant plus one linear term for each of the iteration variables used in that argument.</p>
"
"2384195","2384254","<p>Yes, 3 is <em>both</em> minimal and maximal. 2 is minimal;  4 is maximal.<br>
Exercise.  What are the maximal chains and maximal antichains?</p>
"
"2384200","2384212","<p>$TC = .4x^2-3x+40$</p>

<p>Marginal Cost $= .8x-3$</p>

<p>$TR = 22.2x - 1.2x^2$</p>

<p>Marginal Revenue =$ 22.2-2.4x$</p>

<p>Max Profit occurs when MC = MR, find x by equating both.</p>

<p>Althernatively, set $(\frac{d}{dx} (TC-TR) = 0$ and find x</p>

<p>Plug x in $(TR-TC)$ and find maximum profit</p>

<p>Similarly, set $(\frac{d}{dx} (TC) = 0$ and find x to find the quantity that will minimize the cost</p>

<p>Plug x to find the minimum cost</p>

<p>Similarly, find Average cost $= \frac{TC}{x}$</p>

<p>Set the derivative of this to 0 and find x that will the quantity that will minimize the Average cost.</p>
"
"2384202","2384216","<p>The number of ways for $n$ persons each to have a different birthday is
$^{365}P_n,$ the number of permutations of $n$ objects selected without replacement from $365$ objects:
$$
^{365}P_n = \frac{365!}{n!} \neq \binom{365}{n}.
$$</p>

<p>What you did not take into account was that in your $365^n$ different equally-likely outcomes of the birthdays of $n$ persons, the order in which those persons have those birthdays matters.</p>

<p>The case where Eva's birthday is January 1 and Ahmed's birthday is January 2
is a different subset of the $365^n$ possible outcomes than the
case where Eva's birthday is January 2 and Ahmed's birthday is January 1.
You don't merely choose $n$ birthdays for $n$ persons, you choose them in a particular order. So you must count <em>permutations</em> of $n$ objects out of $365$ rather than <em>combinations.</em> (When counting combinations, it only matters which $n$ objects you select, not the order in which you select them.)</p>

<p>If you work it out a little farther, the probability of at least one matching pair of birthdays among $n$ persons is
$$
1 - \frac{365!/n!}{365^n}
 = 1 - \frac{365}{365}\cdot\frac{364}{365}\cdot\frac{363}{365}\cdots\frac{(366-n)}{365},
$$
which is the usual formula.
The way to solve the problem is algorithmic: you multiply
$\frac{365}{365}$ by $\frac{364}{365},$ then $\frac{363}{365},$ then $\frac{362}{365},$ and so forth until the product is less than or equal to $\frac12,$ at which point the desired probability ($1$ minus the product) is at least $\frac12.$</p>
"
"2384204","2384211","<p>No.</p>

<p>Consider $f(x) = x$. In polar form, this has the equation $\theta = \pi/4$. There's no way to write this in the form of a polar function $r = g(\theta)$.</p>
"
"2384219","2384289","<p>Let's denote the typical matrix in $H(\mathbb{R})$</p>

<p>$$\begin{pmatrix}
1 &amp; x &amp; z \\ 0 &amp; 1 &amp; y \\ 0 &amp; 0 &amp; 1 \end{pmatrix}$$</p>

<p>by $M(x,y,z)$.</p>

<p>Observe that </p>

<ul>
<li>acting by $M(\pm1,0,0)$ successively (this is just a row operation) on $M(x,y,z)$, we get $M(x,y,z) \sim M(x',y,z')$, where $x \in[0,1]$.</li>
<li>Similarly using the action by $M(0,\pm1,0)$ we get $M(x',y,z') \sim M(x',y',z')$ , where $y' \in [0,1]$ </li>
<li>Finally, acting by $M(0,0,\pm)$, we obtain $M(x',y',z') \sim M(x',y',z'')$, where $z'' \in [0,1]$</li>
</ul>

<p>So, the continuous map $H([0,1]) \hookrightarrow H(\mathbb{R}) \rightarrow H(\mathbb{R}) \big/ \sim$, is surjective. Hence $H(\mathbb{R}) \big/ \sim$ is compact.</p>

<hr>

<p>Note that this shows only that the given space is compact (it doesn't show it is a manifold). Proving the latter is not difficult. As $H(\mathbb{Z})$ is discrete, we need only to prove:</p>

<ul>
<li><strong>The the action is free</strong>, indeed $ \ AX = X \implies A = I$</li>
<li><strong>The action is properly discontinuous</strong>. </li>
</ul>

<p>Clearly, $M(n,m,l) \cdot M(x,y,z) = M(x+n,z+ny+l+m,y+l)$. Now by taking a small enough neighborhood about $(a,b,c)$, say $$U = (a-1/4,a+1/4) \times (b-1/4,b+1/4) \times (c-1/4,c+1/4) $$</p>

<p>we see easily $H(\mathbb{Z})\cdot U \cap U = \emptyset$. Easily one can show that if $X,Y \in H(\mathbb{R})$ are not in the same orbit, then we can find neighborhoods $U$ and $V$ of $X$ and $Y$ respectively, such that $H(\mathbb{Z})\cdot U \cap V = \emptyset$.</p>
"
"2384222","2384231","<p>Just find the slope $m$ for the tamgent. Use this slope value to get the slope of the normal $-\frac{1}{m}$. Now use the formula,
$$y-y(t_0)=(\text{slope})(x-x(t_0))$$
to find the equations of the tangent and the normal at any time $t_0$.</p>
"
"2384234","2384240","<p><strong>Hint:</strong> You know that $a^3 &lt; a^3 + b^2 + c \leq 50$. If $a \geq 4$, this doesn't hold, so $a=2$.</p>

<p>Now you have to count solutions to $b^2 + c \leq 42$. Do this in a similiar way: Find an upper bound for $b$ and then, for each possible $b$, count the possibilities for $c$.</p>
"
"2384242","2384404","<p>We know $$\frac1{x+h}=\frac1x\frac1{1+h/x}=\sum^\infty_{n=0}\frac{(-1)^n}{x^{n+1}}\,h^n=\sum^\infty_{n=0}a_n(x)\,h^n,$$ and by integration
$$\log(x+h)=\log x+\int^h_0\frac1{x+h}\,dh=\log x+\sum^\infty_{n=1}\frac{(-1)^{n-1}}{n\,x^n}\,h^n=\sum^\infty_{n=0}b_n(x)\,h^n.$$ Then (Cauchy product of power series) $$\frac{\log(x+h)}{x+h}=\sum^\infty_{n=0}c_n(x)\,h^n,$$ where
$$c_n(x)=\sum^n_{k=0}a_k(x)\,b_{n-k}(x)=\sum^{n-1}_{k=0}\frac{(-1)^k}{x^{k+1}}\frac{(-1)^{n-k-1}}{(n-k)\,x^{n-k}}+\frac{(-1)^n}{x^{n+1}}\,\log x,$$
meaning $$c_n(x)=\frac{(-1)^n}{x^{n+1}}\,\left(\log x-1-\frac12-\ldots-\frac1n\right).$$ But the coefficient at $h^n$ of this Taylor series is $$\frac1{n!}\frac{d^n}{dx^n}\frac{\log x}x.$$</p>
"
"2384243","2384246","<p>Your proof is correct, and you are also correct that there was no use of the fact that you were within a power set. However, the question did need to mention some set on which to define the ordering $\subseteq$. The power set of some arbitrary set is a perfectly good choice.</p>
"
"2384247","2384248","<p><strong>Hint:</strong> set $v=x+2y\implies\frac{dv}{dx}=1+2\frac{dy}{dx}$</p>
"
"2384251","2384253","<p>Pick a finite subcover of $K=\bigcup_{x\in K} V(x,\delta/2)$ and pick points $x_i\in E$ therein.</p>
"
"2384252","2384294","<p>Assuming only that $f$ is bounded, let $|f(x)| \leqslant M$ for all $x \in [a,b]$ and let $A = \inf_P U(f,P)$.</p>

<p>By definition of the infimum, for any $\epsilon &gt; 0$ there is a partition $P'$ such that</p>

<p>$$A \leqslant U(f,P') &lt; A + \epsilon/2$$</p>

<p>Let $Î´=Ïµ/4mM\,$  where $m$  is the number of points in the partition $P'$, and  let $P$  be <em>any</em> partition with $||P|| &lt; \delta$ . Form the common refinement $Q=PâªP'$  .</p>

<p>You will see that the upper sums $U(f,P)$  and $U(f,Q)$  differ in at most $m$  subintervals and in each the deviation is bounded by $2M\delta$.</p>

<p>Thus, </p>

<p>$$U(f,P) - U(f,Q) \leqslant|U(P,f)-U(Q,f)| &lt; m2M\delta =m2M\frac{\epsilon}{4mM}=\epsilon/2,$$</p>

<p>and, since $Q$ is a refinement of $P'$ implying $U(f,Q) \leqslant U(f,P')$, we have </p>

<p>$$A \leqslant U(f,P) &lt; U(f,Q) + \epsilon/2 \leqslant U(f,P') + \epsilon/2 &lt; A + \epsilon.$$</p>

<p>Choosing $N$ such that $\|P_n\| &lt; \delta $ for all $n \geqslant N$, we have </p>

<p>$$A \leqslant U(f,P_n) &lt; A + \epsilon$$</p>

<p>Therefore, </p>

<p>$$\lim_{n \to \infty} U(f,P_n) = A = \inf_P U(f,P).$$</p>
"
"2384257","2384278","<p>This is not true. Take $X=(0,1)$ with the usual topology. It is locally compact. But $X$ is a closed and bounded subset of itself and it is <em>not</em> complete.</p>
"
"2384259","2384274","<p>Equation of the circle C1 is $|z-3|=1$ and the equation of C2 is $|z-3|=8/3$. If $z$ satisfies the given inequality then $1&lt;|z-3|&lt;8/3$ (you found it). This means that $z$ is outside C1 and inside C2 because the distance from $z$ to $(3,0)$ is between $1$ and $8/3$.</p>
"
"2384276","2384288","<p>The constraints of the principle of equivalence don't apply to notation. That is, it is a constraint on what formulas we can write within the ""object language"" of category theory, not on meta-logical statements <em>about</em> formulas in that object language. This is especially the case as you are not actually adding any constraints! You are just pointing out that distinct variables in the generalized algebraic theory of categories are in no way required to be instantiated to different objects.</p>

<p>Personally, for a mathematical audience I probably wouldn't say anything in most cases. Mathematicians are used to not assuming that different variables imply different values. If I did want to emphasize that they could refer to the same objects, I'd probably say something like ""... the $o_i$ not necessarily distinct"".</p>
"
"2384281","2384992","<p>What you have read is misrepresenting Boole. See page 57 of Boole's <a href=""http://www.ccapitalia.net/descarga/docs/1847-boole-laws-of-thought.pdf"" rel=""noreferrer"">Investigation of the Laws of Thought</a>. Boole used $x + y(1-x)\,$ for inclusive disjunction and $x(1-y) + y(1-x)\,$ for exclusive disjunction. So Boole's definitions give $1 \lor 1 = 1 + 1 \cdot (1 - 1) = 1\,$ and $1 \oplus 1 = 1\cdot 0 + 1\cdot 0 = 0\,$ just as we would expect today.</p>
"
"2384285","2384302","<p><strong>Hint:</strong>  The leading digit must be a $1$, for otherwise the number would have fewer than $12$ digits.  That leaves you with eleven positions to fill with five $1$s and six $0$s.  The number is completely determined by choosing which five of the remaining $11$ positions will be filled with $1$s.  The number of ways a subset of $k$ elements can be selected from a set with $n$ elements is 
$$\binom{n}{k} = \frac{n!}{k!(n - k)!}$$
where $n!$ is the product of the first $n$ positive integers if $n$ is positive and $0! = 1$.</p>
"
"2384297","2384574","<p>The Fourier series of the function $f(x)$ is given by
$$
f(x) = \sum_{n=-\infty}^\infty c_n e^{i n x}
$$
with 
$$
c_n = \frac{1}{2 \pi} \int_{-\pi}^\pi d x~f(x) e^{-\imath n x}
$$
In the case $f(x) = |\sin x |$, this results in
$$
c_n = 2 \frac{1}{2 \pi} \int_{0}^\pi d x~\sin x e^{-\imath n x} = \left\{ \begin{array}{ll} -\frac{2}{\pi (n^2-1)} &amp; \text{for $n$ even}\\ 0 &amp; \text{for $n$ odd} \end{array} \right.
$$
Using Parsifal's identity we have
$$
\sum_{n=-\infty}^\infty |c_n|^2 = \frac{1}{2 \pi} \int_{-\pi}^\pi d x ~ |\sin x|^2 = \frac{1}{2}
$$
Since $c_{n} = c_{-n}$ the sum on the left hand side can be rewritten as
$$
c_0^2 + 2 \sum_{n=1}^{\infty} c_n^2 = \frac{1}{2}
$$
From which it follows that with $c_0=\frac{2}{\pi}$
$$
\sum_{n=1}^{\infty} c_n^2 = \frac{1}{2}\left(\frac{1}{2} - c_0^2\right) = \frac{1}{4} - \frac{2}{\pi^2}
$$
Using the result we found for $c_n$ we get
$$
\sum_{n=1}^{\infty} c_n^2 = \sum_{n=1}^{\infty} c_{2 n - 1}^2 + \sum_{n=1}^{\infty} c_{2 n}^2 = \sum_{n=1}^{\infty} c_{2 n}^2 = \frac{4}{\pi^2} \sum_{n=1}^{\infty} \frac{1}{(4 n^2 -1)^2}
$$
and therefore we finally obtain
$$
\sum_{n=1}^{\infty} \frac{1}{(4 n^2 -1)^2} = \frac{\pi^2}{4} \left( \frac{1}{4} - \frac{2}{\pi^2} \right) = \frac{\pi^2-8}{16}
$$</p>
"
"2384298","2384311","<p><strong>Hint</strong> You can simplify a) by applying <a href=""https://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means"" rel=""nofollow noreferrer"">AM-GM</a>, obviously showing that each $S_n&gt;0$ which is easy to show
$$\frac{1}{2}\left(S_n+\frac{A}{S_n}\right)\geq \sqrt{S_n \cdot \frac{A}{S_n}}=\sqrt{A}$$
For c) more details <a href=""https://en.wikipedia.org/wiki/Monotone_convergence_theorem#Lemma_2"" rel=""nofollow noreferrer"">here</a> and you already proved the sequence is decreasing and bounded below in b) and a).</p>

<p>For d) as Eugen suggested, with a), b) and c) you are confident that the sequence is converging so:
$$\lim_{n \rightarrow \infty} S_{n+1}= \lim_{n \rightarrow \infty} \frac{1}{2}\left(S_n+\frac{A}{S_n}\right) \Rightarrow s=\frac{1}{2}\left(\lim_{n \rightarrow \infty}S_n+\frac{A}{\lim\limits_{n \rightarrow \infty} S_n}\right)\Rightarrow \\
s=\frac{1}{2}\left(s+\frac{A}{s}\right)$$
obviously $s \geq \sqrt{A}&gt;0$, thus
$$2s^2=s^2+A \Rightarrow s=\sqrt{A}$$</p>
"
"2384304","2384316","<p><strong>Hint:</strong>  Since $\tan^2x + 1 = \sec^2x$, 
\begin{align*}
\tan^2x - \sec^{10}x + 1 &amp; = 0\\
\sec^2x - \sec^{10}x &amp; = 0\\
\sec^2(1 - \sec^8x) &amp; = 0
\end{align*}
Since $\sec^2x \neq 0$, you need to find the solutions of $1 - \sec^8x = 0$ in the interval $(0, 10)$.  </p>
"
"2384308","2384431","<p>The language above defines the set of Turing machines which never halt on any input. Note, this is the same thing as visiting a state an infinite number of times, assuming that the number of states are finite- apply an infinite version of the pigeon hole principle.</p>

<p>So, if this language was recusively enumerable, there'd be a Turing machine to decide whether each of its members belonged or not. So we'd be able to compute whether a machine never halted on any input. But this is impossible, because then we'd be able to solve the halting problem. I'll explain why in the next paragraph.</p>

<p>The assumption is we have a machine that can decide when another machine never halts on any input. Now we want to build a machine that solves the halting problem: for any $M$ and $x$, we want to decided whether $M(x)$ halts. Well, we can easily define a computer $C$ which ignores its input and just runs $M(x)$ every time $C(y) = M(x)$. Now we know that $C(y)$, because it ignores its input, either always halts for all inputs $y$ or never halts for any of them. But by our assumption we have a machine that decides whether $C$ never halts. The answer we get from this will give us the answer to $M(x)$ and solve the halting problem.</p>
"
"2384312","2384318","<p>Because it divided by $n^2-n$, by $n^3-n$, by $n^5-n$, by $n^7-n$, by $n^{13}-n$ and by $n^{17}-n$ and use the Fermat's little theorem,</p>

<p>which gives that $n^2-n$ divided by $2$,  $n^3-n$ divided by $3$,$n^5-n$ divided by $5$,</p>

<p>$n^7-n$ divided by $7$, $n^{13}-n$ divided by $13$ and $n^{17}-n$ divided by $17$,</p>

<p>which says that our number divided by $2\cdot3\cdot5\cdot7\cdot13\cdot17=46410$</p>
"
"2384313","2384319","<p>A vector subspace $W$ is $\phi$-invariant if and only if $\phi(W)\subseteq W$ (this is the definition). </p>

<p>For $a\ne 0$, $$\phi(\ker(a\phi))=\phi(\ker\phi)=\{0\}\subseteq \ker (a\phi)$$
For $a=0$, $$\phi(\ker 0)=\phi(V)\subseteq V=\ker 0$$</p>
"
"2384323","2384372","<p>Recall that $A$ is measurable iff for every $T \subseteq \mathbb{R}$</p>

<p>$$m^*( T ) = m^*( A \cap T ) + m^*( A^c \cap T ).$$</p>

<p>Letting $T = A \cup B$, we get $m^*( A \cup B ) = m^*( A ) + m^*( B )$.</p>
"
"2384325","2384346","<p>The expression
$$f(x)=1-0^{|x-1|}=0^{0^{|x-1|}}$$
does the trick, provided you're O.K. with $0^0=1.$</p>

<p>Alternatively, using the ceiling function:
$$f(x)=\left\lceil\frac{(x-1)^2}{(x-1)^2+1}\right\rceil$$</p>
"
"2384337","2384341","<p>Hints:
$$\frac{a}{b} \le a$$
$$\frac{b}{a} \le b$$
If either inequality is strict, you get an immediate contradiction.
<p>
If both inequalities are equalities, then . . .</p>
"
"2384344","2384397","<p>Assume the roots are $r_1,r_2$. Then:
$$a(x-r_1)^2(x-r_2)^2=ax^2+(-ar_1-ar_2)x+ar_1r_2=0.$$
Hence the second equation:
$$f(x)=ax^4-ar_1x^3-ar_2x^2+(ar_1r_2-e)x+e=0.$$
Note:
$$f(r_1)=-er_1+e$$
$$f(0)=e$$</p>

<p>Now IVT is applicable.</p>
"
"2384348","2388010","<p>Found the error in my derivation. My first derivation is not correct. The heaviside cannot be delayed by simply adding a delay term.</p>

<p>SInce $y(t)$ only has a value for $t&gt;t_d$ the fourier transform yields $\int_{t_d}^{\infty}Ae^{-i\omega t}dt$.</p>

<p>The integral $\int_{t_d}^{\infty}Ae^{-i\omega t}dt = A\int_{t_d}^{\infty}cos(\omega t)dt + Ai\int_{t_d}^{\infty}sin(\omega t)dt $ is not defined. This also required for the derivation for the heaviside fourier transform where $\int_{0}^{\infty}Ae^{-i\omega t}dt$ is not defined</p>

<p>Using a briefer notation: Since $\int_{t_d}^{\infty}$ does not hold because of the infinity in the integral, it is possible to rewrite this to:  </p>

<p>$\int_{t_d}^{\infty}Ae^{-i\omega t}dt = \int_{0}^{\infty}Ae^{-i\omega t}dt - \int_{0}^{t_d}Ae^{-i\omega t}dt$ </p>

<p>The second term ($\int_{0}^{t_d}$) is finite, and therefore defined. The first term is can derived analogous to the fourier transform of the 'normal' (without delay) heaviside function. The first integral on the rhs is the correction term, if you will, which is a pulse in time domain and a sinc in the frequency domain.</p>
"
"2384351","2384377","<p>Your statement is false. <em>Every</em> set $W$ (closed or not) has the property that you mentioned. If $x\in W$, just define $(\forall n\in\mathbb{N}):x_n=x$. Then $\lim_{n\in\mathbb N}x_n=x$.</p>
"
"2384366","2384376","<p>It isn't necessary to plug in $t=0$ and $t=1$. It is easier to compare the coefficients. You suppose
$$
0=at-bt+\frac{c}2t^2+d=\frac{c}2t^2+(a-b)t+d
$$
for all $t$. You can see the LHS as a polymial too:
$$
0t^2+0t+0=\frac{c}2t^2+(a-b)t+d.
$$
LHS=RHS implies that the coefficients have to be the same and that yields
$$
\frac{c}2=0\\a-b=0\\d=0.
$$
Finally you get the plane $ax+ay=0$ which is the same as $x+y=0$.<p>
This is also a plane. Consider
$$
x+y=0\Leftrightarrow 1\cdot x+1\cdot y+0\cdot z=0
$$
and you can check that $r$ is in the plane $x+y=0$ because
$$
r_1(t)+r_2(t)=t+(-t)=0.
$$</p>
"
"2384367","2384389","<p>If you take $(x_1,x_2)=(1,0)$, then you get that $(1,0)\in M$ and if you take $(x_1,x_2)=(0,1)$, then you get that $(0,0)\in M$. Is it true that $\left(\frac12,0\right)\in M$? No, because if $\left(\frac12,0\right)=\bigl(|x_1|^2,x_1\overline{x_2}\bigr)$ then $|x_1|^2=\frac12$ and therefore $|x_2|^2=\frac12$. But then $\bigl|x_1\overline{x_2}\bigr|=\frac12$, whereas $\left|0\right|=0$. Therefore, $M$ is not convex.</p>
"
"2384369","2384394","<p>In a simply connected region $D$, an integral of the form
$$a+\int_{z_0}^z\frac{\psi'(z)}{\psi(z)}\,dz$$
is well-defined, and defines a branch of $\ln\psi(z)$ on $D$.
Can you use that to define a branch of $\sqrt{\psi(z)}$ on $D$?</p>
"
"2384386","2384419","<p>Say $q\geq 3$. Then $p$ is odd and we have:
$$ 2\underbrace{(2^{p-1}-1)}_a = q(q^{q-1}+1)$$
Since $3|a$ we have $3|q(q^{q-1}+1)$. If $3|q^{q-1}+1=z^2+1^2$ we
get $3|1$. So $3|q$ and thus $q=3$.</p>
"
"2384387","2384403","<p>I suppose that you meant to write$$f(z)=\frac1{2\pi i}\int_S\frac{f(\phi)\,\mathrm d\phi}{\phi-z}.$$The double use of $f$ here is not a good idea. Anyway, let us see what happens when $f(z)=\overline z=\frac1z$. In this case, assuming that $|z|&lt;1$ and that $z\neq0$,\begin{align*}\frac1{2\pi i}\int_S\frac{f(\phi)\,\mathrm d\phi}{\phi-z}&amp;=\frac1{2\pi i}\int_S\frac{\mathrm d\phi}{\phi(\phi-z)}\\&amp;=\operatorname{Res}_0\left(\frac1{\phi(\phi-z)}\right)+\operatorname{Res}_z\left(\frac1{\phi(\phi-z)}\right)\\&amp;=-\frac1z+\frac1z\\&amp;=0.\end{align*}So, as you can see, you <em>don't</em> get again the function that you started with.</p>

<p>And actually, no, the conjugation is not the restriction to $S$ of a holomorphic function.</p>
"
"2384395","2384474","<p>First, suppose that $G$ is factor-critical. By definition, this means that for any vertex $v$, the subgraph $G-v$ has a perfect matching, call it $M$. Since $M$ is perfect in $G-v$, it must be maximal in $G$, implying that $v$ is avoidable. Since the choice for $v$ was arbitrary, we're done.</p>

<p>Now, suppose that every vertex of $G$ is avoidable. This implies that for any pair of vertices, there is no maximum matching that misses both of them. Therefore, any maximum matching must only miss a single vertex, call it $u$. This implies that $G-u$ has a perfect matching, and hence is factor-critical. Since every vertex is avoidable, this argument holds for arbitrary vertices, proving that $G$ is factor-critical. </p>
"
"2384401","2384596","<p>Look at the corresponding random variables $X$, $Y$, and $Z$ with cdfs $F$, $G$, and $H$ respectively.  Your first inequality says $P(X\ge Y)\ge P(X\le Y)$, the second that $P(Y\ge Z) \ge P(Z\ge Y)$, the third is vacuuous, the fourth that $P(X\ge Y)\ge P(Z\ge Y)$ and the last that $P(Y\ge Z)\ge P(Y\ge X)$.  That is, $X$ beats $Y$ more often than not, $Y$ beats $Z$ more often than not, and you want to know if that implies $X$ beats $Z$ more often than not.  (All three random variables are continuous, so $\ge$ is the same as $&gt;$ in each of the probability expressions, so $P(X&gt;Y) \ge P(X&lt;Y)$ and so on.)</p>

<p>The answer is <em>no</em>, and is famous enough to have a Wikipedia article, <a href=""https://en.wikipedia.org/wiki/Condorcet_paradox"" rel=""nofollow noreferrer"">Condorcet paradox,</a> about it. If each of the 3 rankings $X&gt;Y&gt;Z$, $Y&gt;Z&gt;X$, and $Z&gt;X&gt;Y$ are equally likely, the events $X&gt;Y$ and $Y&gt;Z$, although individually more likely than not to happen, are not independent and their joint occurrence is relatively rare.</p>
"
"2384407","2384414","<p>The problem is this, suppose that for $\Pi_1$ and $\Pi_2$ you have two assignments, $e_1$ and $e_2$ which give ""True"" and ""False"" to $\theta$ respectively.</p>

<p>You have no control over this, because these were taken arbitrarily. So you haven't shown that $\Pi_2\cup\{\theta\}$ is satisfiable. What you have shown is that every finite set of propositions can be satisfied with either $\theta$ or its negation, which is trivially true.</p>

<p>What you need here is to argue that there is a way to satisfy <em>all</em> the formulas in $\Sigma$ at once. And luckily, the condition you have is exactly enough to argue that. Compactness is your only friend.</p>
"
"2384412","2384439","<p>We can see that this sequence is the Fibonnaci sequence with every other term negated:</p>

<p>$$-1, 2, -3, 5, -8, 13, \ldots$$</p>

<p>So if two terms of this sequence added to another term of the sequence, we'd either have:</p>

<p>$$F_n + F_m = F_k$$ for three Fibonnaci numbers, or we'd have</p>

<p>$$F_n - F_m = F_k$$ for three Fibonnaci numbers, but then taking the $F_m$ across, we'd have again the first case. So, we just need to consider the equation:</p>

<p>$$F_n + F_m = F_k$$</p>

<p>WLOG, let's say $n &gt; m$. Then we know that the only $m$ that allows the sum to be Fibonacci is $n-1$, else the sum falls too short. In other words, the only valid sum is:</p>

<p>$$F_n + F_{n-1} = F_{n+1}$$</p>

<p>To hammer the point home one more time, suppose $j &lt; n - 1$. Then we have:</p>

<p>$$F_n &lt; F_n + F_j &lt; F_{n+1}$$</p>

<p>So $F_n + F_j$, being strictly between two consecutive Fibonnaci numbers, can't itself be part of the sequence.</p>
"
"2384420","2384427","<p>Your argument is perfectly correct. </p>

<p>In general, you can convert a quadratic form 
$$
Ax^2 + 2B xy + C y^2 + 2Dx + 2Ey + F
$$
into a matrix 
$$
M = \pmatrix{
A &amp; B &amp; D\\
B &amp; C &amp; E \\
D &amp; E &amp; F
}
$$
and the determinant of the matrix and trace can both be computed from the quadratic form, from which you can work out things like how many positive/negative eigenvalues there are. </p>
"
"2384422","2384490","<p>I always like to think of these type of ODE's in terms of the product rule.
\begin{equation}x=y'e^{\sin(x)}+y\cos(x)e^{\sin(x)}=\left(ye^{\sin(x)}\right)'
\end{equation}
So integrating both sides and dividing by $e^{\sin(x)}$ yields\begin{equation}y=e^{-\sin(x)}\left(\frac{1}{2}x^2+c\right).
\end{equation}</p>
"
"2384432","2384435","<p>Why don't you just use the definition?
$$
q(e_1)=e_1\cdot Ae_1=e_1\cdot\begin{pmatrix}1\\4\end{pmatrix}=1
$$
and
$$
q(e_2)=e_2\cdot Ae_2=e_2\cdot\begin{pmatrix}4\\-4\end{pmatrix}=-4.
$$
You don't need the eigenvalues and eigenvectors of $A$ to compute the value of the quadratic form at some points.<p>
Otherwise you computed
$$
q(x,y)=x^2+8xy-4y^2.
$$
You can also use this to compute
$$
q(e_1)=q(1,0)=1^2+8\cdot1\cdot0-4\cdot 0^2=1
$$
and
$$
q(e_2)=q(0,1)=0^2+8\cdot 0\cdot 1-4\cdot 1^2=-4.
$$</p>
"
"2384433","2384451","<p>Let's assume the indices run from $1$ to $2$ just so that you'll see what is going on. I'll assume that $g_{\mu \nu} = g_{\nu \mu}$. Then</p>

<p>$$ T(q^1, q^2, \dot{q}^1, \dot{q}^2, t) = \frac{1}{2} \left( g_{11} \dot{q}^1 \dot{q}^1 + g_{12} \dot{q}^1 \dot{q}^2 + g_{21} \dot{q}^2 \dot{q}^1 + g_{22} \dot{q}^2 \dot{q}^2 \right) + g_{01} \dot{q}^1 + g_{02} \dot{q}^2 + g_{00} = \frac{1}{2} \left(g_{11} \dot{q}^1 \dot{q}^1 + g_{22} \dot{q}^2 \dot{q}^2 \right) + g_{12} \dot{q}^1 \dot{q}^2 + g_{01} \dot{q}^1 + g_{02} \dot{q}^2 + g_{00}  $$</p>

<p>and so, for example,</p>

<p>$$ \frac{\partial T}{\partial \dot{q}^1} = g_{11} \dot{q}^1 + g_{12} \dot{q}^2 + g_{01}. $$</p>
"
"2384440","2384456","<p>dimension error usually means that there is too much to process or display, you should consult your guide</p>
"
"2384444","2384454","<p>Let the equation be $$\dfrac xa+\dfrac yb=1$$</p>

<p>So, $(a,0);(0,b)$ is divided in $1:2$ at $(-5,4)$ </p>

<p>i.e., $-5=\dfrac{a\cdot2+0\cdot1}{2+1}$ etc.</p>
"
"2384445","2384467","<p>Indeed, you can't use Yoneda in <em>that</em> spot; but $C(W\cdot F,\sim)\to Set^{J^{op}}(W,C(F-,\sim))$ <em>is</em> the right way around to use Yoneda. If you look at the formula that follows the mention of Yoneda on the nLab page, $$W(j)\to C(F(j),W\cdot F)$$ you'll see that this is precisely signature of the morphism associated with $id_{W\cdot F}$.</p>

<p><strong>Edit:</strong> After thinking about this for a few minutes, perhaps your confusion is that you think the isomorphism $$C(W\cdot F,c)\cong Set^{J^{op}}(W,C(F-,c))$$ is intended to be something like the isomorphism $$W(j)\cong Set^{J^{op}}(J(-,j),W).$$ This is not the case. The claim is not that the former isomorphism is an instance of Yoneda; if it were, weighted colimits would be uninteresting because they would be an intrinsic part of every category. Rather, the existence of the former is the substantial fact that a particular set-valued functor is representable; its existence has nothing to do with Yoneda. The instance of Yoneda invoked in the nLab article is rather $$Set^{J^{op}}(W,C(F-,W\cdot F))\cong Set^C(C(W\cdot F,\sim),Set^{J^{op}}(W,C(F-,\sim))),$$ which is used to state that there is a universal arrow $\phi$ from $W$ to $$C(F-,\sim):C\to Set^{J^{op}}.$$</p>

<p>Notice that this is entirely analogous to the isomorphism $$C(\mathrm{colim}\;F,-)\cong C^D(F,\Delta-)$$ (where $F:D\to C$ and $\Delta:C\to C^D$ is the diagonal functor), where Yoneda tells us that the existence of this isomorphism gives rise to a unique $\xi:F\to\Delta(\mathrm{colim}\; F)$--i.e., the colimiting cone.</p>
"
"2384448","2384629","<blockquote>
  <p>From my understanding for all statements $E(x_1, \dots, x_n)$, whose parameters are in an arbitrary collection $X$, it should <em>trivially</em> follow that $X$ agrees with $E(x_1, \dots, x_n)$.</p>
</blockquote>

<p>No, not at all.  I think you perhaps have a misunderstanding: using parameters doesn't mean you can't <em>also</em> have other variables that are quantified.  For a very simple example, let $a$ be any set and $X=\{a\}$.  Let $E(x)$ be the statement $\exists y[x\in y]$.  Then $E(a)$ is true (take $y=\{a\}$) but $E^X(a)$ is false (since $a\not\in a$).</p>

<p>Note also that any statement without free variables is also a ""statement with parameters in $X$"".  Its number $n$ of parameters just happens to be $0$.  Or, you can consider it as a statement with any number of parameters, except that its parameters don't actually occur anywhere in it (so their value does not affect the truth of the statement).  The operative part of the phrase ""with parameters in $X$"" is ""in $X$"": you are not allowed to make parameters take values that are not in $X$.</p>

<blockquote>
  <p>Is
  $$E = \forall x\exists y[P(x) \in y]$$ a statement that doesn't agree with, say, $V_3$, where $V_\alpha$ is the von Neumann stage of order $\alpha$, but agrees with, say, $V_\omega$?</p>
</blockquote>

<p>This one is actually a bit subtle.  The problem is that ""$P(x)$"" is not actually a valid term in the language of set theory.  Rather, the expression $z=P(x)$ is an abbreviation for $$\forall w[w\in z\Leftrightarrow\forall v[v\in w\Rightarrow v\in x]].$$  If you want to express your statement $E$ in the language of set theory, there are a few different ways you can do it, which are equivalent in the universe but not necessarily when relativized to a set $X$.  For instance, you might write $E$ as $$\forall x\exists y\forall z[z=P(x)\Rightarrow z\in y]$$ (where $z=P(x)$ is an abbreviation as above).  Or you might write it as $$\forall x\exists y\exists z[z=P(x)\wedge z\in y].$$  These two statements are equivalent in the universe, since for any $x$, there is a unique $z$ which satisfies $z=P(x)$.  However, in a set like $X=\{a\}$, they are not equivalent: the first is true, since taking $x=a$, there does not exist any $z\in X$ such that $(z=P(a))^X$ is true, so $\forall z[z=P(a)\Rightarrow z\in y]$ is vacuously true (and we can set $y=a$).  But for the exact same reason, the second is false.</p>

<p>As for your question, it turns out that actually both versions are true in $V_\omega$ and false in $V_3$, so both statements agree with $V_\omega$ but not with $V_3$.  Even the first version fails in $V_3$ because if you set $x=\{\emptyset\}$, then $z=P(x)$ is an element of $V_3$ but it is not an element of any element of $V_3$.</p>
"
"2384452","2385010","<p>The correct statement is:</p>

<blockquote>
  <p>Let $E/F$ be a finite field extension such that $E\neq F$.  Then there is a field $E'$ such that $F\subseteq E'\subset E$ and there is no field $K$ such that $E'\subset K\subset E$.</p>
</blockquote>

<p>To prove it, consider the set $S$ of all degrees $[E':F]$ of intermediate fields $E'\subset E$.  Then $S$ is a nonempty set of natural numbers, all of which are less than $[E:F]$.  Let $n$ be the greatest element of $S$, and choose $E'\subset E$ such that $[E':F]=n$.  Then if $E'\subset K\subset E$ we would have $$[K:F]=[K:E'][E':F]&gt;[E':F]=n,$$ which is a contradiction since $n$ is the greatest element of $S$.</p>
"
"2384470","2384478","<blockquote>
  <p>Does it mean that $sâ[0,2)$?</p>
</blockquote>

<p>Essentially, yes, but that's not necessarily good way to think about it.</p>

<p>A unitary representation of $\mathbb{Z}$ is a group homomorphism from $\mathbb{Z}$ (under addition) to the complex plane (under multiplication) whose image lies on the unit circle. It's determined by the value it takes at $1$, which can be any point on the circle. </p>

<p>Points on the circle are naturally parameterized by the central angle, which varies from $0$ to $2\pi$. You can take out the factor of $\pi$ and parameterize by values between $0$ and $2$.</p>
"
"2384472","2384480","<p>You have two random variables $Y$ and $U$, where $Y$ is some random variable with probability density function $g(y)$ and $U$ is uniform in $(0,1)$. To simplify the confusing notation, consider the event $$E:=\left\{y: U\le \frac{f(y)}{cg(y)}\right\}$$ (formally we would need some more notation to define this event). Hence, by Bayes rule (conditional probability) you have that $$P(E)=\sum_{y}P(E\mid y)P(Y=y)$$ However, here $Y$ is a continuous random variable, hence in the place of the summation sign $\sum$ you have an integral sign $\int$ and in the place of the probabilities $P(Y=y)$ you have the probability density function $g(y)$ (times the infitesimal $d(y)$), so the above expression becomes $$P(E)=\int P(E\mid y)g(y)dy$$ and putting back the definition of $E$ you get your expression. Note also, that you are right about the fact that $P\left(U\le f(y)/cg(y)\right)$ is an integral, so what you have, is indeed a double integral. You do away with the second integral in the step $$P\left(U\le \frac{f(y)}{cg(y)}\right)=\int_0^{\frac{f(y)}{cg(y)}}du=\left.u\right|_0^{\frac{f(y)}{cg(y)}}=\frac{f(y)}{cg(y)}-0=\frac{f(y)}{cg(y)}$$ </p>
"
"2384476","2384550","<p>$f$ is differentiable at $(x_0,y_0)$ if the difference $\,(x-x_0)(y-y_0)\,$ between $f(x,y)-f(x_0,y_0)$ and its linear approximation $y_0(x-x_0)+x_0(y-y_0)$ is $o\bigl(\lVert(x-x_0,y-y_0)\rVert\bigr)$.</p>

<p>To see this, set $x-x_0=r\cos\theta$, $y-y_0=r\sin\theta$ $\;(r&gt;0,\;0\le \theta&lt;2\pi)$. This difference is $(x-x_0)(y-y_0)$, so we have
$$\frac{\lvert(x-x_0)(y-y_0)\rvert}{\lVert(x-x_0,y-y_0)\rVert} =\frac{r^2\lvert\cos\theta\sin\theta\rvert}r=r\lvert\cos\theta\sin\theta\rvert\le r\to 0.$$</p>
"
"2384481","2384487","<p>Alternatively, solve the line directly:</p>

<p>$$
\begin{cases}
y=4x -1 \\
y = 9 - 3x
\end{cases}$$</p>

<p>Thus we sovle for $x=\frac{10}{7}$. (The point is $(\frac{10}{7},\frac{33}{7})$, but you'll only need solve $x$ or solve $y$, no need for both.)</p>

<p>Notice that ratio wise, we could just focus on one of $x$ or $y$ axis, since it is a line (linear), focus on $x$-axis below:</p>

<p>$$\frac{m}{n}=\frac{\frac{10}{7}-1}{2-\frac{10}{7}}=\frac{3}{4}$$</p>

<p><strong>EDIT</strong></p>

<p>Segments over a line always have the same ratio for its projection onto axis or another line. For the following picture, we have</p>

<p>$$\frac{x_1}{y_1}=\frac{x_2}{y_2}, \text{ and thus } \frac{x_1}{x_2} = \frac{y_1}{y_2}$$
<a href=""https://i.stack.imgur.com/aCtOZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aCtOZ.png"" alt=""enter image description here""></a></p>
"
"2384482","2384529","<p>Yes, there are such sets.</p>

<p>Apply the same construction as before to obtain $F_2$. Now, in order to obtain $F_1$, you can use the same process, but this time instead of using $U_\varepsilon$, you use $U_\varepsilon'=U_\varepsilon\cap F_2$. It is still a set that contains $E_1$ and it is true that $m^*(E_2)+\varepsilon\geqslant m(U_\varepsilon')$. Of course, $U_\varepsilon'$ may not be open, but it is still measurable. So, the intersection $F_1$ of all these sets is a:</p>

<ul>
<li>measurable set;</li>
<li>subset of $F_2$;</li>
<li>set whose measure is $m^*(E_1).$</li>
</ul>
"
"2384489","2384495","<p>To solve the second inequality just expand and use AM-GM again
\begin{equation}\sum_{cyc}a^2(b+c)=(a^2b+b^2c+c^2a)+(a^2c+c^2b+b^2a)\geq 3abc+3abc=6abc.
\end{equation}</p>
"
"2384492","2384504","<p>The multiplicative groups of $\mathbb{Z}/9 \mathbb{Z}$ and $\mathbb{Z}/ 17\mathbb{Z}$ are indeed cyclic. </p>

<p>More generally, the multiplicative group of $\mathbb{Z}/p^k \mathbb{Z}$ is cyclic for any <em>odd</em> prime $p$.</p>

<p>If you are supposed to know this result, just invoke it. If you do not know this result, possibly you are expected to do this via a direct calculation. </p>

<p>To this end, you'd need to identify a generating element in each case. 
For example, for $9$ you have, trying $2$ as generator, $2^1= 2$, $2^2=4$, $2^2 =8=-1$, $2^4=-2$, $2^5=-4$, $2^6 = 1$.</p>

<p>Thus, $2$ indeed generates the multiplicative group of $\mathbb{Z}/9 \mathbb{Z}$, which has as its elements only the classes co-prime to $9$, that is, the six elements we got above. </p>

<p>However, the set  $\mathbb{Z}/9 \mathbb{Z}$ (of nine elements) with multiplication, is not a group at all. For example, the class $0$ can never have a multiplicative inverse (neither have $3$ nor $6$).   </p>
"
"2384503","2384508","<p>HINT: use that $$x^2+y^2+z^2\geq xy+yz+zx$$
after my hint above we have $$a^2b^2+b^2c^2+c^2a^2\geq a^2bc+ab^2c+abc^2$$</p>
"
"2384518","2384526","<p>Its dimension is $2$. it is a plane.</p>

<p>to find the basis, write $S $ as</p>

<p>$$S=\{(x,y,z)\in \Bbb R^3 \;\;: z=-x-y\} $$</p>

<p>the elements of $S $ are of the form
$$(x,y,-x-y)=$$
$$x (1,0,-1)+y (0,1,-1) =$$</p>

<p>$$x\vec {e_1}+y\vec {e_2} .$$</p>

<p>now prove that these two vectors are not dependent.</p>

<p>to complete them to a base of $\Bbb R^3$, take the normal vector to the plane $S : 1.x+1.y+1.z=0$, which is $$\vec {e_3}=(1,1,1) .$$</p>

<blockquote>
  <p>or</p>
</blockquote>

<p>Put $$\vec {e_3}=\vec {e_1} \land \vec {e_2} $$</p>
"
"2384523","2384875","<p>You have computed the eigenvalues of $A$ to be $\{-1, 1, 1\}$.  The repeated eigenvalue may be an obstacle to diagonalization.  In this case, the geometric and algebraic multiplicities of the eigenvalue $1$ are different, so A is not diagonalizable.  You continued as if $A$ were diagonalizable, so this is the mistake in your approach.</p>

<p>You have added a follow-on question in comments (instead of to your question, so you should not be surprised if other answers do not address it).  I see that others have demonstrated Jordan normal form and induction.  Another method is binary decomposition of the exponent and repeated squaring to get power-of-$2$ powers of $A$: \begin{align*}
A^1 &amp;= \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{pmatrix}  \text{,} \\
A^2 &amp;= A^1 \cdot A^1 = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{pmatrix}  \text{,} \\
A^4 &amp;= A^2 \cdot A^2 = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 2 &amp; 1 &amp; 0 \\ 2 &amp; 0 &amp; 1 \end{pmatrix}  \text{,} \\
A^8 &amp;= A^4 \cdot A^4 = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 4 &amp; 1 &amp; 0 \\ 4 &amp; 0 &amp; 1 \end{pmatrix}  \text{,} \\
A^{16} &amp;= A^8 \cdot A^8 = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 8 &amp; 1 &amp; 0 \\ 8 &amp; 0 &amp; 1 \end{pmatrix}  \text{,} \\
A^{30} = A^{11110_{\,2}} &amp;= A^{16}\cdot A^8 \cdot A^4 \cdot A^2 = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 15 &amp; 1 &amp; 0 \\ 15 &amp; 0 &amp; 1 \end{pmatrix}  \text{.}
\end{align*}
Of course, after computing $A^4$ or $A^8$, perhaps one would notice the pattern...</p>

<p>There is a slightly faster way to get there using the above method.  $A^{-1}$ is easy enough to compute (using the minors method, for example).  $$
A^{-1} = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\ -1 &amp; 1 &amp; 0 \end{pmatrix}
$$  Then, $A^{30} = (A^{15})^2 = (A^{16} \cdot A^{-1})^2$.  This replaces three multiplies with a multiply and an inverse.  Continuing to think about this leads to the computationally intractable problem of optimal <a href=""https://en.wikipedia.org/wiki/Addition-chain_exponentiation"" rel=""nofollow noreferrer"">addition chains</a>.</p>
"
"2384531","2384553","<p>\begin{equation}
\sum_{n=1}^{\infty}\frac{1}{2n(2n+1)(2n+2))}=\frac{1}{4}\sum_{1}^{\infty}\frac{1}{n(n+1)(2n+1)}=\frac{1}{4}[3-4\ln2]
\end{equation}</p>
"
"2384547","2384585","<p>If $f(x) =   \sum_{n=1}^{\infty} {{\sin(nx^2)}\over 1+n^3}$, then $f(x)$ converges absolutely for all $x\in \mathbb{R}$ by the comparison test, as $|\frac{sin(nx^2)} {1+n^3}|\leq \frac{1}{1+n^3}$.
And also $f'(x)=\sum_{n=1}^{\infty}{{\ 2nx \cos(nx^2)}\over 1+n^3}$. This is because for all $n \in \mathbb{N}$ the derivative of $f_m(x) =   \sum_{n=1}^{m} {{\sin(nx^2)}\over 1+n^3}$ is $f_m'(x)=\sum_{n=1}^{m}{{\ 2nx \cos(nx^2)}\over 1+n^3}$
And since $|\frac{2nx \cos(nx^2)}{1+n^3}| \leq |\frac{2xn}{1+n^3}| \leq |\frac{2x}{n^2}$| and $\sum_{n=1}^{\infty}|{{2x}\over n^2}|$ converges, it follows from the Weierstrass M-test that the sequence $(f'_m)$ converges absolutely uniformly on any bounded interval and $\lim_{n\to\infty} f_n'(x)=f'(x)$. Thus, by the uniform limit theorem it follows that $f'(x)=\lim_{n\to\infty} f_n'(x)$ is continuous, and thus it follows that $f(x)$ is continuously differentiable.</p>
"
"2384562","2384575","<p>Up to isomorphism there are many, many different infinite nonabelian groups with each of its elements having infinite order except the identity.</p>

<p>But free groups are special. Perhaps the most special property of the free group 
$$\mathbb{Z} * \mathbb{Z} = \langle a \rangle * \langle b \rangle
$$ 
is that it is the only 2-generator group (up to isomorphism) having the property that for any group $G$ and any function
$$h : \{a,b\} \to G
$$
there is a unique homomorphism
$$H : \mathbb{Z} * \mathbb{Z} \to G
$$
such that $H(a)=h(a)$ and $H(b)=h(b)$. In other words, a homomorphism with domain $\mathbb{Z} * \mathbb{Z}$ is uniquely and freely characterized by where it maps each of the two elements $a,b$.</p>

<p>So, for example, to define a homomorphism $H : \mathbb{Z} * \mathbb{Z} \to \mathbb{Z}$, pick what you want for $H(a)$, for example 
$$H(a) = 42
$$
Then pick what you want for $H(b)$, for example 
$$H(b)=-34890567
$$
There is a unique homomorphism $H : \mathbb{Z} * \mathbb{Z} \to \mathbb{Z}$ with those two values. You can compute any value of $H$ that you want very easily, for example
$$H(aba)= H(a) + H(b) + H(a) = 42 -34890567 +42 = -34890483
$$</p>

<p>As for your title question, perhaps the first time you meet a free group, it will seem unfamiliar to you. But free groups are so special that mathematicians have given them their own name and their own notation. So the only real answer to your title question is: free groups are what they are, just as say infinite cyclic groups are what they are, or other very important named groups.</p>
"
"2384568","2384600","<p>As pointed out above by k.stm, observe that $\left\{ 1\right\} = f^{-1}(1)\not\cong f^{-1}(i) = \left\{ 1/2 , 3/2\right\} $ in which $\not\cong $ means that there is no bijection. </p>

<p>You can follow other ideas as well. For instance, if $g:(0,2)\to \mathbb{S} ^1 $  is a covering map, then it is a universal cover. In particular, it is isomorphic to $\mathbb{R}\to\mathbb{S} ^1 $, $t\mapsto e ^{2\pi i t} $. Thereby the fibres of $g$ need to have infinity elements.
That is to say, every covering map $g:(0,2)\to \mathbb{S} ^1 $ needs to be such that $g^{-1}(x) $ has infinity elements (of course, for every $x\in \mathbb{S} ^1$).</p>
"
"2384570","2384576","<p>For an obvious reason (don't mind), only a hint is provided: you can take $f_{0} := \{ \varnothing, \Omega \}$ and $f_{2} := 2^{\Omega}$. </p>
"
"2384579","2384597","<p>Currently they are not the same because there are some typo's. The first line characterises the surface of a half sphere of radius $r'$ and $z \geq 0$. Therefore it should read:
$$
\sigma = (r \cos \theta,r \sin \theta,\sqrt{r'^2 - r^2}) = (r' \cos \theta \sin \phi,r' \sin \theta \sin \phi, r' \cos \phi)
$$
Equivalence follows from realising $r = r' \sin \phi$ and hence $\sqrt{r'^2 - r^2} = r' \cos \phi$.</p>
"
"2384587","2384611","<p>Let $t=1-\frac1x$.  Then, we have $\sqrt x =\left(1-t\right)^{-1/2}$.  </p>

<p>Applying the generalized binomial theorem reveals</p>

<p>$$\left(1-t\right)^{-1/2}=\sum_{k=0}^\infty (-1)^k\binom{-1/2}{k}t^k\tag1$$</p>

<p>for $|t|&lt;1$ ($x&gt;1$).  </p>

<p>We can write the term $(-1)^k\binom{-1/2}{k}$ as</p>

<p>$$\begin{align}
(-1)^k\binom{-1/2}{k}&amp;=\frac{(1/2)(3/2)(5/2)\cdots ((2k-1)/2)}{k!}\\\\
&amp;=\frac{(2k-1)!!}{2^kk!}\\\\
&amp;=\frac{2^k\,k!\,(2k-1)!!}{2^k\,k!\,2^k\,k!}\\\\
&amp;=\frac{(2k)!}{4^k\,(k!)^2}\\\\
&amp;=\frac{1}{4^k}\binom{2k}{k}\tag2
\end{align}$$</p>

<p>Substituting $(2)$ into $(1)$ yields the coveted result</p>

<p>$$\begin{align}
\left(1-t\right)^{-1/2}&amp;=\sum_{k=0}^\infty \frac{1}{4^k}\binom{2k}{k}t^k\\\\
&amp;=\sum_{k=0}^\infty \frac{1}{4^k}\binom{2k}{k} \left(1-\frac1x\right)^k
\end{align}$$</p>

<p>for $x&gt;1$.</p>
"
"2384595","2384619","<p>By a standard diagonal argument, you may extract a sequence $\{f_n\}_{n \in \mathbb{N}}$ (this is a subsequence of your original bounded sequence) that converges pointwise on $\mathbb{Q} \cap [0,1]$. </p>

<p>Let's show that $\|f_n - f_m\| \to 0$. Let $\varepsilon &gt;0$ be given. Well, fix an $x$ and choose $q_k$ such that $|x-q_k|&lt;\varepsilon$. Let $M$ be the bound on the $f_n$ in the $\| \cdot \|$ norm.</p>

<p>Then $|f_n(x) - f_m(x)| \le |f_n(x) - f_n(q_k)| + |f_n(q_k) - f_m(q_k)| + |f_m(q_k) - f_m(x)|.$</p>

<p>Now,
$|f_n(x) - f_n(q_k)|\le \displaystyle\left|\int_{q_k}^x f_n'(y)dy\right|\le M\varepsilon$,
and likewise, 
$|f_m(q_k) - f_m(x)|\le M\varepsilon$.</p>

<p>Finally, $|f_n(q_k) - f_m(q_k)|$ gets arbitarily small by the diagonal argument.</p>

<p>This concludes the proof. Use the fact that we're working with Banach spaces to obtain a limit function.</p>

<p>Edit: It is no harder to show that the identity operator on $C^k([0,1])$ to $C^{k-1}([0,1])$, with norms defined akin to your $C^1([0,1])$ norm, is a compact operator as well. You might find it instructive to do it.</p>
"
"2384606","2384615","<p>continuing your inequality$$|z_{ 1 }+z_{ 2 }+\cdots +z_{ n }|\le \sum _{ i=1 }^{ n } |z_{ i }|\le \sum _{ i=1 }^{ n } Re\left| { z }_{ i } \right| +\sum _{ i=1 }^{ n } Im|z_{ i }|=3n$$</p>
"
"2384610","2384661","<p>Assume that point $A$ is at the origin and that the circle has radius $r$. Furthermore, let the point $B=(X,Y)$. Then the desired point, say $P(x,y)$ is given by</p>

<p>$$
P=(r\cos \theta,r\sin \theta)\\
\theta=\tan^{-1}\frac{Y}{X}
$$</p>

<p>EDIT: Another solution comes to mind. Let $R=\sqrt{X^2+Y^2}$, then</p>

<p>$$P(x,y)=\frac{r}{R}B(X,Y)$$</p>
"
"2384612","2384625","<p>Let $\phi(x)$ be a suitable test function ($\phi(x)\in C^\infty$ and has compact support on $(-\infty,\infty)$).  </p>

<p>Then, write</p>

<p>$$\begin{align}
\lim_{y\to 0^+}\int_{-\infty}^\infty \phi(x)\frac{1}{x+iy}\,dx&amp;=\lim_{y\to 0^+}\int_{-\infty}^\infty \phi(x)\frac{x-iy}{x^2+y^2}\,dx\\\\
&amp;=\lim_{y\to 0^+}\int_{-\infty}^\infty \phi(x)\frac{x}{x^2+y^2}\,dx-i\lim_{y\to 0^+}\int_{-\infty}^\infty \phi(x)\frac{y}{x^2+y^2} \,dx\\\\
\end{align}$$</p>

<hr>

<p>To evaluate the first limit, $\lim_{y\to 0^+}\int_{-\infty}^\infty \phi(x)\frac{x}{x^2+y^2}\,dx$, we first write$$\begin{align}\int_{-\infty}^\infty \phi(x)\frac{x}{x^2+y^2}\,dx&amp;=\int_{-\infty}^{-\epsilon}\phi(x)\frac{x}{x^2+y^2}\,dx\\\\&amp;+\int_{-\epsilon}^\epsilon \phi(x)\frac{x}{x^2+y^2}\,dx\\\\&amp;+\int_{\epsilon}^\infty \phi(x)\frac{x}{x^2+y^2}\,dx\tag 1\end{align}$$Inasmuch as $\phi$ is of compact support and $\phi(x)\frac{x}{x^2+y^2}$ is continuous, we have the limits$$\lim_{y\to 0^+}\int_{-\infty}^{-\epsilon} \phi(x)\frac{x}{x^2+y^2}\,dx=\int_{-\infty}^{-\epsilon} \frac{\phi(x)}{x}\,dx \tag2$$and$$\lim_{y\to 0^+}\int_{\epsilon}^\infty \phi(x)\frac{x}{x^2+y^2}\,dx=\int_{\epsilon}^\infty \frac{\phi(x)}{x}\,dx\tag3$$Next, we use Taylor's theorem to write $\phi(x)=\phi(0)+\phi'(0)x+o(x)$ for $x\in [-\epsilon,\epsilon]$.Then,$$\begin{align}\int_{-\epsilon}^\epsilon \phi(x)\frac{x}{x^2+y^2}\,dx&amp;=\color{red}{\int_{-\epsilon}^\epsilon \phi(0)\frac{x}{x^2+y^2}\,dx}+\color{blue}{\int_{-\epsilon}^\epsilon \phi'(0)\frac{x^2}{x^2+y^2}\,dx}+\color{orange}{\int_{-\epsilon}^\epsilon \frac{o(x^2)}{x^2+y^2}\,dx}\\\\
&amp;=\color{red}{0}+\color{blue}{\phi'(0)\left(2\epsilon +2y\arctan\left(\frac{\epsilon}{y}\right)\right)}+\color{orange}{o(\epsilon)}\tag 4
\end{align}$$Letting $y\to 0^+$ in $(4)$ we obtain$$\lim_{y\to 0^+}\int_{-\epsilon}^\epsilon \phi(x)\frac{x}{x^2+y^2}\,dx=2\phi'(0)\epsilon+o(\epsilon) \tag5 $$Finally using $(2)-(5)$, and letting $\epsilon\to 0^+$ yields$$\lim_{y\to 0^+}\int_{-\infty}^\infty \phi(x)\frac{x}{x^2+y^2}\,dx=\lim_{\epsilon\to 0^+}\left(\int_{-\infty}^{-\epsilon} \frac{\phi(x)}{x}\,dx +\int_{\epsilon}^\infty \frac{\phi(x)}{x}\,dx  \right)\equiv \text{PV}\left(\int_{-\infty}^\infty \frac{\phi(x)}{x}\,dx\right)$$</p>

<p>as was to be shown!</p>

<hr>

<p>To evaluate the second limit, $\lim_{y\to 0^+}\int_{-\infty}^\infty \phi(x)\frac{y}{x^2+y^2} \,dx$, we write</p>

<p>$$\begin{align}
\lim_{y\to 0^+}\int_{-\infty}^\infty \phi(x)\frac{y}{x^2+y^2} \,dx&amp;=\lim_{y\to 0^+}\int_{-\infty}^\infty \phi(yx)\frac{1}{x^2+1} \,dx\tag6
\end{align}$$</p>

<p>whence applying the Dominated Convergence Theorem to $(6)$ yields the coveted result</p>

<p>$$\begin{align}
\lim_{y\to 0^+}\int_{-\infty}^\infty \phi(x)\frac{y}{x^2+y^2} \,dx&amp;=\int_{-\infty}^\infty \lim_{y\to 0^+}\left(\phi(yx)\right)\frac{1}{x^2+1} \,dx\\\\
&amp;=\phi(0)\int_{-\infty}^\infty \frac{1}{x^2+1}\,dx\\\\
&amp;=\pi \phi(0)
\end{align}$$</p>

<p>as expected!</p>

<hr>

<p>And we are done!</p>
"
"2384617","2384657","<p><strong>Hints:</strong></p>

<ol>
<li>Using the martingale property show that $$\mathbb{E}((M_t^{\tau_n}-M_t^{\tau_{n-1}})^2) = \mathbb{E}((M_t^{\tau_n})^2)-\mathbb{E}((M_t^{\tau_{n-1}})^2).$$</li>
<li>By step 1, we have $$\mathbb{E} \left( \sum_{n=1}^k |M_t^{\tau_n}-M_{t}^{\tau_{n-1}}|^2 \right) = \sum_{n=1}^k \bigg[ \mathbb{E}((M_t^{\tau_n})^2)-\mathbb{E}((M_t^{\tau_{n-1}})^2) \bigg].$$ Show that the right-hand side equals $$\sum_{n=1}^k \mathbb{E}(M_t^2 1_{\{\tau_{n-1} \leq t &lt; \tau_n\}}) + \mathbb{E}(M_{\tau_k}^2 1_{\{t \geq \tau_k\}}).$$</li>
<li>Let $k \to \infty$ using the monotone convergence theorem.</li>
</ol>
"
"2384630","2384638","<p>You are correct that it is $-\log_2(\frac{1}{N})$. However recall the identity
$$\log_b(x^r)=r\log_b(x).$$</p>
"
"2384632","2388478","<p>So far, you have shown that the operation $\Omega \mapsto \Omega_s$, which provides a set with the same volume as $\Omega$, but symmetric to a given hyperplane decreases the perimeter. </p>

<p>In order to profe the isoperimetric inequality in this form, you need to do the following:<br>
(This proof mimics the strategy used when dealing with arbitrary measurable sets (of finite perimeter). There are possibly other approaches more suitable for dealing with sets with $C^1$ boundary.) </p>

<ol>
<li>Show that a perimeter minimizing set (for given volume) actually exists. Call that minimizer $M$ Here you need some sort of compactness property of the space of considered sets. Currently I cannot provide you an easy argument. </li>
<li>As you already proved (for a special case), symmetrizing a set with respect to a given hyperplane decreases the perimeter. Thus $|\partial M_s|\le |\partial M| $ and as $|\partial M|$ is already minimial $|\partial M_s|= |\partial M| $.</li>
<li>We now need to show, that the above implies $M=M_s$, i.e. $M$ is symmetric with respect to the chosen hyperplane.<br>
I think if you only consider sets with $C^1$ boundary, this follows from the computation.<br>
In the general case, you need to first show that the above implies that $M$ is convex.   If $M$ is convex, you can parametrize it in the same form as your $\Omega$, with $\varphi_1$ convex, $\varphi_2$ concave (thus they are differentialbe almost everywhere). An explicit calculation of the perimeter then shows that $\varphi_1= -\varphi_2+c$ for some constant $c\in \mathbb R$. But this means that up to translation $M$ is equal to $M_s$</li>
<li>Since the hyperplane with repect to which we symmetrized in the beginning was arbitrary, it follows that $M$ is symmetric with respect to all hyperplanes. But then $M$ is a ball.</li>
</ol>
"
"2384637","2384717","<p>$B$ is ""The set of all functions that are limits of uniformly convergent sequences of members of $A$"". We are working in a metric space $C([0,1])$ or $C(X)$ for $X$ compact e.g.) so $B = \overline{A}$: Constant sequences show that $A \subseteq B$ and limits of sequences from $A$ that are not in $A$ already are clearly in $A'$. So $A \cup B \subseteq A \cup A'$, also $A' \subseteq B$ because a limit point of $A$ can always be gotten as the limit of a sequence from $A$.</p>

<p>And the closure of a set $A$ is itself closed (the smallest closed set around $A$).</p>
"
"2384639","2385009","<p>From your description, I am not sure <em>exactly</em> what you are doing.
But maybe I can give you some helpful ideas.</p>

<p>1) For normal data, the standard CI for the population mean $\mu$ uses
Student's t distribution which is symmetrical. The 95% CI is of the
form $\bar X \pm t^*S/\sqrt{n},$ where $t^*$ cuts 2.5% from the upper
tail of $\mathsf{T}(\nu= n-1)$.</p>

<p>By contrast, the standard CI for the population variance $\sigma^2$ uses
the chi-squared distribution which is not symmetrical. The 95% CI is of
the form $\left(\frac{(n-1)S^2}{U}, \frac{(n-2)S^2}{L}\right),$ where
$L$ and $U$ cut 2.5% of the probability from the lower and upper tails,
respectively, of $\mathsf{Chisq}(\nu = n-1).$</p>

<p>Unless your bootstrapping procedure corrects for the bias in the case
of the variance (with its skewed distribition), you cannot expect an accurate result.</p>

<p>2) It is not exactly 'fair' to compare CIs (using Student's t and
chi-squared distributions) based on the assumption data are normal
with results of <em>nonparametric</em> bootstrap CIs. The assumption that
data are normal provides 'information' for the t and chi-squared
intervals that is not used in nonparametric bootstrapping.</p>

<p>3) In making bootstrap CIs for variation, I have found it better
to find CIs for $\sigma$ rather than $\sigma^2,$ possibly because the former
has the same units as the data.  Also, for scale parameters, I have found
that it often works better to bootstrap ratios rather than differences.</p>

<p>4) Finally, along lines of @HansEngler's suggestion, I will generate $n = 1000$ observations from $\mathsf{Norm}(\mu = 100, \sigma=15),$ find the chi-sqared CI for $\sigma$ and compare it with
a bias-corrected nonparametric bootstrap. </p>

<p>First, here are my fake data and the corresponding
standard CI for $\sigma,$  which turns out to be $(9.55, 10.43).$
[I have provided <code>set.seed</code> statements, so that you can replicate
the exact simulations I have used.]</p>

<pre><code>set.seed(1234)
n = 1000;  mu = 100;  sg = 10;  x = rnorm(n, mu, sg)
a = mean(x); s = sd(x);  a;  s
## 99.73403    # sample mean
## 9.973377    # sample SD
# CI for sg
UL = qchisq(c(.975,.025), 999); UL
## 1088.487  913.301
sqrt((n-1)*var(x)/UL)
##  9.554619 10.430810  # CI for sg (includes 9.973, as it must)
</code></pre>

<p>Now for the (bias corrected) nonparametric bootstrap CI: I will bootstrap the <em>ratio</em>
$R = S/\sigma.$ If I knew the distribution of $R,$ then I could find
$L$ and $U$ with $P(L \le R = S/\sigma \le U) = 0.95$ so that 
$P(S/U \le \sigma \le S/L) = .95$ and a 95%
CI for $\sigma$ would be of the form $(S/U,\, S/L).$ By bootstrapping
$R,\,$ I can estimate $U$ and $L.$  I use
the observed $s = 9.97$ temporarily as a proxy for unknown $\sigma.$
Suffixes <code>.re</code> indicate bootstrapped quantities. [On this site
with relatively few experienced R users,
I try to use only the most fundamental R functions; I will leave it
to you to write more elegant R code, which is obviously possible.]  </p>

<pre><code>set.seed(1235)
B = 10^5;  r = numeric(B)
for(i in 1:B) {
  s.re = sd(sample(x,n,repl=T))
  r[i] = s.re/s }
L.re = quantile(r, .025);  U.re = quantile(r, .975)
c(s/U.re, s/L.re)  
##    97.5%      2.5% 
## 9.537866 10.462202 
</code></pre>

<p>So the nonparametric bootstrap CI is $(9.54, 10.46),$ which
is not a bad match for the standard chi-squared CI $(9.55, 10.43)$
obtained above. [The large sample size ($n$ = 1000) has mostly
obviated my comment in (2) above, which still stands for smaller $n$.
Even so, it is possible that the information that the data are normal
accounts for the fact that the normal-based CI is a bit shorter.]</p>

<p><em>Note:</em> If you do a <em>parametric</em> bootsrap CI, you may get
closer to the result for the chi-squared CI. In parametric
bootstrapping, one re-samples from a parametric distribution (here normal)
with parameters <em>suggested by</em> the data (rather than from the data themselves). 
In particular, in the code above one would substitute the code 
<code>s.re = sd(rnorm(n, a, s))</code> for the line with <code>sample</code>. With this change
and bootstrap seed 1066, I got the 95% <em>parametric bootstrap</em> CI $(9.55, 10.43).$</p>
"
"2384642","2384650","<p>We have $y=e^{rx}$, $y'=re^{rx}$, $y''=r^2e^{rx}$. Now, we just want to solve 
$$ y''-2y'-3y=0.$$
I.e.,
$$ r^2e^{rx}-2re^{rx}-3e^{rx}=0.$$
We know that $e^{rx}\ne 0$ for $r\in \mathbf{R}$, so we divide through by $e^{rx}$ and find 
$$ r^2-2r-3=(r-3)(r+1)=0.$$
Thus, $r=3,-1$. </p>
"
"2384677","2384687","<p>For the first integral, write 
$$\int \frac{dz}{15 + 2z -z^2} =\int \frac{dz}{(5-z)(3+z)} 
= \int \frac{A}{5-z}dz + \int \frac{B}{3+z} dz
$$ 
and then find $A$ and $B$. Then integrate each one separately. </p>

<p>For the second one, do a $u$-substitution: let $u=\sec \theta$. Then $du =\sec \theta\tan \theta \:d\theta$. </p>
"
"2384689","2384762","<p>The fundamental group must be countable in your situation. Here's a proof, which has a good visualization in my head from which I concocted it, although in the end its easier to write out the proof than to draw the picture.</p>

<p>From your hypotheses, $X$ has a countable basis $\{U_i\}_{i=1}^\infty$ consisting of path connected open sets for which the inclusion induced homomorphism $\pi_1(U_i) \to \pi_1(X)$ is trivial. Also, given two basis elements $U_i,U_j$, their intersection consists of countably many path components, denote them 
$$U_i \cap U_j = \cup_{k=1}^\infty V_{ij}^k
$$
Pick points $p_i \in U_i$ and $q_{ij}^k \in V_{ij}^k$. Declare one of the $p$'s to be the base point, say $p_1 \in U_1$.</p>

<p>For any closed path $\gamma : [0,1] \to X$ based at $p_1$, by the Lebesgue number lemma we may subdivide
$$0=x_0 &lt; x_1 &lt; ... &lt; x_M=1
$$
so that for each $m=1,...,M$ the path $\gamma[x_{m-1},x_m]$ has image in one of the $U_i$'s, call it $U_{i_m}$. We'll assume $i_1=i_M=1$.</p>

<p>Let's do a preliminary path homotopy on $\gamma$, which will achieve the following effect: denoting $y_m = \frac{x_{m-1}+x_m}{2}$ which is the midpoint of the interval $[x_{m-1},x_m]$, we may assume that $\gamma(y_m) = p_{i_m}$ for $2 \le m \le M-1$. To achieve this, cut $\gamma$ at $y_m$ and then insert a new path which first travels along some path in $U_{i_m}$ from $\gamma(y_m)$ to $p_{i_m}$ and then backwards along the same path.</p>

<p>Next, note that we have a nonempty intersection $U_{i_{m-1}} \cap U_{i_m} \ne \emptyset$ because that set contains the point $\gamma(x_m)$. Let $V_{i_{m-1} i_m}^{k_m}$ be the path component of that intersection that contains $\gamma(x_m)$.</p>

<p>Now I'll construct a countable collection of closed ""model paths"" based at $p_1$, and I'll pick out one of those paths which is path homotopic to $\gamma$.</p>

<p>For each $i,j,k$ such that $V_{ij}^k \ne \emptyset$ let $\delta_{ij}^k$ be the concatenation of a path in $U_i$ from $p_i$ to $q_{ij}^k$ with a path in $U_j$ from $q_{ij}^k$ to $p_j$. Since the inclusions from $U_i$ and $U_j$ into $X$ induce trivial maps on fundamental groups, it follows that the path homotopy class of $\delta_{ij}^k$ is well-defined. There are countably many of the $\delta$'s, and so the collection of paths obtained by concatenating a finite sequence of the $\delta$'s is countable. These are the ""model paths"".</p>

<p>So now we just have to show that $\gamma$ is path homotopic to the path
$$\delta_{i_1i_2}^{k_2} * ... * \delta_{i_{m-1}i_m}^{k_m}
$$
For that purpose, for each $m=1,...,M-1$ we pick a path $\eta_m$ in $V_{i_{m-1}i_m}^{k_m}$ from the point $\gamma(x_m)$ to the point $q_{i_{m-1}i_m}^{k_m}$, and then we cut $\gamma$ at $x_m$ and insert a copy of $\eta_m \bar\eta_m$. It follows that $\gamma$ is path homotopic to
$$\underbrace{(\gamma[x_0,x_1] \, \eta_1 \, \bar\eta_1 \, \gamma[x_1,y_2])}_{\delta_{i_1i_2}^{k_2}} * \underbrace{\gamma[y_2,x_2] \, \eta_2 \,  \bar\eta_2 \, \gamma[x_2, y_3])}_{\delta_{i_2i_3}^{k_3}} * ... * \underbrace{(\gamma[y_{m-1},x_{m-1}] \, \eta_{m-1} \, \bar\eta_{m-1}\, \gamma[x_{m-1,}x_m])}_{\delta_{i_{m-1}i_m}^{k_m}}
$$</p>
"
"2384712","2384812","<ol>
<li><p>Looking at the <a href=""https://en.wikipedia.org/wiki/Compact_operator_on_Hilbert_space#Spectral_theorem"" rel=""nofollow noreferrer"">spectral theorem for compact operators in a Hilbert space</a>, we get that the only possible accumulation point of the spectrum of a compact operator in $\ell^2$ is $0$, and hence there are <strong>no compact operators</strong> where the spectrum is contains an interval (1a) or the unit circle (1b).</p></li>
<li><p>Consider any orthogonal partition $V_1 \oplus V_2$ of $\ell^2$, and let $Tx = 1\cdot x_1 + 2\cdot x_2$ where $x=x_1 + x_2$ with $x_i \in V_i, I=1,2$. </p></li>
</ol>
"
"2384719","2384730","<p>Let $\theta = 2 \phi$, then the thing to be proven is:</p>

<blockquote>
  <p>Prove that $$\frac{1 + \sin(2\phi) - \cos(2\phi)}{1 + \sin(2\phi) + \cos(2\phi)} = \tan(\phi)$$</p>
</blockquote>

<p>Then use:</p>

<p>$$\sin(2\phi) = 2 \sin \phi \cos \phi$$
$$\cos(2\phi) = \cos^2 \phi - \sin^2 \phi$$</p>

<p>and:</p>

<p>$$\sin^2 \phi + \cos^2 \phi = 1$$</p>
"
"2384723","2384804","<p>If you think at $\sin(xy)$ be expressed as a combination of a FINITE number of  sums and powers of $\sin(x)$ and $\sin(y)$ , the answer is certainly no.</p>

<p>With infinite series, one can imagine a formal answer (more a joke than serious) </p>

<p>Starting from :
$$\sin(xy)=\sum_{k=0}^\infty \frac{(-1)^k x^{2k+1}y^{2k+1}}{(2k+1)!}$$
$$a=\sin(x)\quad\to\quad x=\sin^{-1}(a)=\sum_{n=0}^\infty \frac{\left(\frac{1}{2} \right)_n a^{2n+1}}{(2n+1)n!}=\sum_{n=0}^\infty \frac{\left(\frac{1}{2} \right)_n \sin^{2n+1}(x)}{(2n+1)n!}$$
And similarly 
$$y=\sum_{n=0}^\infty \frac{\left(\frac{1}{2} \right)_n \sin^{2n+1}(y)}{(2n+1)n!}$$
$\left(\frac{1}{2} \right)_n$ is the Pochhammer symbol $=\frac{\Gamma(n+\frac{1}{2})}{\Gamma(\frac{1}{2})}$.</p>

<p>Finally, the monster :
$$\sin(xy)=\sum_{k=0}^\infty \frac{(-1)^k \left(\sum_{n=0}^\infty \frac{\left(\frac{1}{2} \right)_n \left(\sin(x)\right)^{2n+1} }{(2n+1)n!} \right)^{2k+1}\left(\sum_{n=0}^\infty \frac{\left(\frac{1}{2} \right)_n \left(\sin(y)\right)^{2n+1} }{(2n+1)n!} \right)^{2k+1}}{(2k+1)!}$$</p>
"
"2384749","2384757","<p>A sphere with one point removed is homeomorphic to the plane. So a sphere with $k$ points removed is a plane with $k-1$ points removed. This is homotopically $k-1$ loops attached at a point, and the fundamental group is the free group on $k-1$ elements.</p>
"
"2384760","2394155","<p><em>I am by no means an expert in this field, so if there is anything wrong with my answer or if I interpreted something incorrectly please let me know!</em></p>

<p>According to the paper <em>An $L^p$-estimate for the gradient of solution of second order divergence equations</em> by Meyers (1963), <a href=""https://eudml.org/doc/83302"" rel=""nofollow noreferrer"">https://eudml.org/doc/83302</a>, the (weak) Laplacian is an isomorphism between $W^{1,p}_0(\Omega)\to W^{-1,p}(\Omega)$ for certain $p$. The result can be found in Theorems 1 and 2 on Page 198 by taking $A=\mathbf{I}$, the identity operator. Simplified (by using the characterization of $W^{-1,p}(\Omega)$), the theorem states the following:</p>

<p>Let $n\geq 2$ and let $\Omega$ be a <strong>bounded</strong> connected open set in $\mathbb{R}^n$. Then there exists <strong>some</strong> $p_0 &gt; 2$ such that for all $p\in[2,p_0)$ we have that if $f\in W^{-1,p}(\Omega)$ then there is a unique weak solution, $u$, to $\Delta u = f$ with $u\in W^{1,p}_0(\Omega)$ and there is a constant $C$ such that
$$\|u\|_{W^{1,p}_0(\Omega)}\leq C\|f\|_{W^{-1,p}(\Omega)}.$$
Hence $\Delta^{-1}:W^{-1,p}(\Omega)\to W^{1,p}_0(\Omega)$ is well-defined, linear, and continuous. In particular, we deduce that $\Delta\circ \Delta^{-1} = \mathrm{id}_{W^{-1,p}(\Omega)}$ and $\Delta^{-1}\circ\Delta=\mathrm{id}_{W^{1,p}_0(\Omega)}$, with both maps continuous. That is to say that $\Delta:W^{1,p}_0(\Omega)\to W^{-1,p}(\Omega)$ is an isomorphism as desired.</p>
"
"2384770","2385655","<p>I think you can always consider the adjoint in the inner product and that solves it. </p>

<p>$&lt;Av, v&gt; = 0 \Rightarrow &lt;Av, v&gt; = &lt;v, Av&gt; \Rightarrow A$ is self adjoint, so it is diagonalizable over the reals. 
It is easy to prove the only posible eigenvalue is $0$, so you would have $A = Px0xP^{-1} $ which implies $A = 0$</p>
"
"2384771","2384949","<p>The 'meaning' of interval estimates is a controversial topic on
applied statistics. So there is no universally accepted answer to your
important question. </p>

<p>Let's just use a proposed sample of size $n = 31$ from a normal population with
unknown population mean $\mu$ and unknown variance $\sigma^2.$ </p>

<p>Then $T = \frac{\bar X - \mu}{S/\sqrt{n}} \sim \mathsf{T}(n-1),$ so that
$P(-2.042 \le T = \frac{\bar X - \mu}{S/\sqrt{n}} \le 2.042 )=0.95.$
Here $\bar X$ and $S$ are the sample mean and variance, respectively.</p>

<blockquote>
  <p>Manipulating inequalities in the event, we get
  $P(\bar X - 2.042\frac{S}{\sqrt{n}} \le \mu \le \bar X + 2.042\frac{S}{\sqrt{n}}) = 0.95.$ This is purely a probability statement. Specifically, it is a
  probability statement about the behavior of the random variable $\bar X$ and $S$: the random interval $(\bar X - 2.042\frac{S}{\sqrt{n}}, \bar X + 2.042\frac{S}{\sqrt{n}})$ has a 95% probability of covering (including) the unknown constant $\mu.$ </p>
</blockquote>

<p>Now suppose we take the sample and obtain $\bar X = 21.3$ and $S^2 = 1.44.$ Then the random interval becomes $\left(21.3 - 2.042(0.1249), (21.3 + 2.042(0.1249)\right)$ or $(20.860, 21.740).$</p>

<p>But now we are dealing with observed quantities. According to the usual
frequentist interpretation of probability, this is no longer a probability
statement: Either the interval $(20.860, 21.740)$ includes $\mu$ or it
does not. Accordingly, the interval $(20.860, 21.740)$ is called a 95% <em>confidence</em> interval.</p>

<p>The confidence interval is a statement about the <em>data.</em> Over the long run,
we will obtain data so that
the manipulation in the emphasized paragraph will produce an interval
that includes the true population $\mu$ in 95% of such experiments. </p>

<p>The reason for calling the interval estimate a 'confidence' interval
instead of a 'probability' interval has to do with a strict interpretation
by frequentist statisticians of the word 'probability'. </p>

<p>Bayesian statisticians treat $\mu$ as a random variable, begin with a
'prior' distribution on $\mu$, combine the data with the prior distribution
to get a 'posterior' distribution, and use the posterior distribution to
get a <em>probability</em> interval for $\mu$ (some say a <em>credible</em> interval). If the prior distribution is
""flat"" (containing little information), then the Bayesian and frequentist
interval estimates will be <em>numerically</em> very similar. But philosophies as to
the ""meaning"" of the interval estimate differ.</p>

<p>Both frequentists and Bayesians have their critics. Strictly speaking,
frequentists are are not saying anything about the experiment at hand--only
about what 'works' over the long run. A Bayesian is addressing the experiment
at hand, but needs to explain how
the prior distribution was obtained and what effect it has on the interval estimate.  </p>
"
"2384772","2384786","<p>It's not true if $x$ is isolated in $X$. You have, specifically:</p>

<blockquote>
  <p>If $x$ is isolated in $E$, then ($x$ is isolated in $X$ if and only if $x$ is not a limit point of $E^c$.)</p>
</blockquote>

<p>$(\Leftarrow)$ If $x$ is isolated in $E$ and not a limit point of $E^c$, then there is a neighborhood $N_1$ of $x$ which contains no point of $E\setminus \{x\}$ and a neighborhood $N_2$ of $x$ which contains no point of $E^c$. </p>

<p>And hence $N_1\cap N_2=\{x\}$ is a neighborhood of $x$ which contains no other point of $X$.</p>

<p>$(\Rightarrow)$ If $x$ is an isolated point of $X$, then $\{x\}$ is an open set of $X$, and is not a limit point of $E^c$ because $\{x\}$ does not contain any point of $E^c$.</p>
"
"2384781","2384808","<p>Let $\omega$ the congruence class of $x$ in $\mathbf F_{27}=\mathbf Z/3\mathbf Z[x]/(x^3+2x+1)$. This polynomial splits completely over $\mathbf F_9$:
$$x^3+2x+1=(x-\omega)(x-\omega-1)(x-\omega+1).$$</p>

<p>This means that $\omega\pm1$ are the other roots of the polynomial. Indeed
$$(\omega+1)^3+2(\omega+1)+1=\omega^3+1+2\omega+2+1=\omega^3+2\omega+1=0,$$
and similarly for $\omega-1$.</p>
"
"2384782","2384791","<p>$$\dfrac{\sin(\theta+A)}{\sin(\theta+B)}=\cdots=\dfrac{\tan\theta\cos A+\sin A}{\tan\theta\cos B+\sin B}$$  Dividing numerator &amp; the denominator by $\cos\theta$ </p>

<p>$$\implies\dfrac{(\tan\theta\cos A+\sin A)^2}{(\tan\theta\cos B+\sin B)^2}=\dfrac{2\sin A\cos A}{2\sin B\cos B}$$</p>

<p>$$\iff\dfrac{(\tan\theta+\tan A)^2}{(\tan\theta+\tan B)^2}=\dfrac{\tan A}{\tan  B}$$</p>

<p>Dividing numerator of both sides by $\cos^2A$</p>

<p>and the denominator  of both sides by $\cos^2B$</p>

<p>Now simplify assuming $\tan A\ne\tan B$</p>
"
"2384785","2384788","<p>HINT:</p>

<p>$$1+\sqrt x=t\implies x=(t-1)^2,dx=2(t-1)dt$$</p>
"
"2384796","2384835","<p>The eigenvalues of the matrix 
$$
A_n=\begin{pmatrix}
1 &amp; n\\
n &amp; 1
\end{pmatrix}
$$
are the solution to $(1-x)^2=n^2$, or $x=1\pm n$. The eigenvector for eigenvalue $1\pm n$ is $v_\pm =(1,\pm1)^T/\sqrt{2}$. Construct the orthogonal matrix
$$
O=\frac{1}{\sqrt{2}}\begin{pmatrix}
1 &amp; 1\\
1 &amp; -1
\end{pmatrix}
$$
Then $O^TA_n O=\mathrm{diag}(1+n, 1-n):=D_n$, so $A_n=OD_n O^T$. Using this and defining $\tilde{y}=O^Ty:=(\tilde{X}, \tilde{Y})^T$, your differential equation turns into
$$ \begin{cases}
\tilde{X}'=(1+n)\tilde{X}\\
\tilde{Y}'=(1-n)\tilde{Y}
\end{cases}\Longrightarrow \tilde{X}=A\exp[(1+n)x], \qquad \tilde{Y}=B\exp[(1-n)x]$$
transforming back
$$
y=O\tilde{y}=\frac{e^x}{\sqrt{2}}\begin{pmatrix}
Ae^{nx}+Be^{-nx}\\
Ae^{nx}-Be^{-nx}
\end{pmatrix}
$$
so the solution does indeed depedn on $n$. What you are asked to prove is wrong.</p>
"
"2384797","2384815","<p><strong>Hint:</strong> Find the maximum of the function $$f(x)=x^4e^{-x}$$</p>

<p>You should get that any $$C \geq \frac{4^4}{e^4}$$ is a solution.</p>
"
"2384799","2384836","<p>Let $U \subset Y$ be open. For every $A \subset X$ such that $f\lvert_A$ is a bijection $A \to U$, the condition says</p>

<p>$$f(A) = U = \operatorname{int} f(A) \subset f(\operatorname{int} A),$$</p>

<p>and since $\operatorname{int} A \subset A$, by the injectivity of $f\lvert_A$ it follows that $\operatorname{int} A = A$, i.e. $A$ is open. $f^{-1}(U)$ is the union of such $A$, hence itself open.</p>
"
"2384809","2385164","<p>The concept has many names, including harmonic quadruple, <a href=""https://en.wikipedia.org/wiki/Projective_harmonic_conjugate"" rel=""nofollow noreferrer"">harmonic conjugates</a>, harmonic throws, and certainly some more. It's a property about four points on a line (which in some situations may be the complex âlineâ $\mathbb C^1$), or more specifically two pairs of points, which can be described in various ways. A common one is the following incidence configuration, taken from Wikipedia:</p>

<p><a href=""https://commons.wikimedia.org/wiki/File:Pappusharmonic.svg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XLpxC.png"" alt=""Incidence construction""></a></p>

<p>The points $\{\{A,B\},\{C,D\}\}$ form a harmonic quadruple. Another way to express this is via the <a href=""https://en.wikipedia.org/wiki/Cross-ratio"" rel=""nofollow noreferrer"">cross-ratio</a>: harmonic quadruples have cross ratio $-1$ (or $2$ or $\frac12$ depending on the order of the points).</p>

<p>Harmonic quadruples are essential building blocks in many approaches to projective geometry. On the one hand, the configuration above is pure incidence, with no metric information, and as a result any transformation which preserves collinearity will preserve harmonic quadruples. On the other hand, harmonic quadruples can be used to express all elementary arithmetic operations, namely addition, subtraction, multiplication, division, squaring. Not square roots. By expressing I mean that if you fix a projective basis (three points $0$, $1$ and $\infty$) on a line, you can express arithmetic on that line using constructions resulting in harmonic conjugates.</p>

<p>So how does this apply to your situation? Well, apart from incidence information you have two additional concepts, namely parallels ($LK\parallel VW$) and midpoints ($M$ is midpoint of $VW$). But these two can be translated into the language of harmonic conjugates. Parallel lines are lines meeting at infinity. And if $(A,B;C,\infty)=-1$, i.e. these four points are a harmonic quaquadruple, then $C$ is the midpoint of $AB$. You can see this if you imagine moving the point $C$ further and further to the right in the above configuration, and as you do, $MN$ will approach parallelism with $AB$, and $D$ will become the midpoint of $AB$.</p>

<p><a href=""https://i.stack.imgur.com/JSnLq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JSnLq.png"" alt=""Your configuration""></a></p>

<p>So to show that $M$ is the midpoint of $VW$ you need to show $(V,W;M,\infty)=-1$, where $\infty$ is the point of infinity on the line $VW$.
Unfortunately the witness construction given above can't be found in your construction straight away. We have to pick a slight detour. There are probably many different alternatives one could pick, but I'll go for this one: Label the point where $AB$ intersects $KL$ as $P$. Then $(A,B;M,P)=-1$ as implied by the witness construction, using $C,K,L,O$ as the other points.</p>

<p>Now I did mention earlier on that harmonic quadruples are preserved by projective transformations. Which implies that a central projection from one line to another will preserve it. Use $K$ as that center of projection, and project points from $AB$ to $VW$. This takes $A$ to $W$, $B$ to $V$, $M$ stays on the intersection and $L$ becomes $\infty$ since $KP\parallel VW$. This leads to $(W,V;M,\infty)=-1$ q.e.d.</p>
"
"2384810","2385091","<p>One interval has endpoints $\bar X \pm 1.28 \dfrac 1 {\sqrt{100}}$ and the other $\bar Y \pm 1.28 \dfrac1 {\sqrt{50}}.$</p>

<p>The intervals are disjoint if $\bar X + 1.28 \dfrac 1 {\sqrt{100}} &lt; \bar Y - 1.28 \dfrac 1 {\sqrt{50}}$ or $\bar Y + 1.28 \dfrac 1 {\sqrt{50}} &lt; \bar X - 1.28 \dfrac 1 {\sqrt{100}}.$</p>

<p>That happens if $\bar Y - \bar X &lt; 1.28 \left( \dfrac 1 {\sqrt{100}} + \dfrac 1 {\sqrt{50}} \right)$ or $\bar X - \bar Y &gt; 1.28 \left( \dfrac 1 {\sqrt{100}} + \dfrac 1 {\sqrt{50}} \right).$ In other words
$$
|\bar X - \bar Y| &gt; 1.28 \left( \dfrac 1 {\sqrt{100}} + \dfrac 1 {\sqrt{50}} \right).
$$
This is equivalent to
$$
\frac{|\bar X - \bar Y|}{\dfrac 1 {\sqrt{100}} + \dfrac 1 {\sqrt{50}}} &gt; 1.28.
$$</p>

<p>You have $\bar X - \bar Y \sim N\left(0, \dfrac 1 {100} + \dfrac 1 {50} \right) = N\left( 0, \dfrac 3 {100} \right),$ so that
$$
\frac{\bar X - \bar Y}{ \sqrt{3/100} } \sim N(0,1).
$$
\begin{align}
&amp; \operatorname{standard deviation}\left( \frac{\bar X - \bar Y}{\dfrac 1 {\sqrt{100}} + \dfrac 1 {\sqrt{50}}} \right) = \frac{\operatorname{standard deviation}\left( \bar X - \bar Y \right)}{\dfrac 1 {\sqrt{100}} + \dfrac 1 {\sqrt{50}}} \\[10pt]
= {} &amp; \dfrac{\sqrt{\dfrac 3 {100}}}{\dfrac 1 {\sqrt{100}} + \dfrac 1 {\sqrt{50}}} = \frac{\sqrt 3}{1 + \sqrt 2} = \sqrt 6 - \sqrt 3.
\end{align}
So
\begin{align}
&amp; \Pr\left( \frac{|\bar X - \bar Y|}{\sqrt{\dfrac 1{100}} + \sqrt{\dfrac 1 {50}}} &gt; 1.28 \right) = \Pr\left( \left. \frac{|\bar X - \bar Y|}{\sqrt{\dfrac 1{100}} + \sqrt{\dfrac 1 {50}}} \right/ (\sqrt6-\sqrt3)  &gt; \frac{1.28}{\sqrt6-\sqrt3} \right) \\[10pt]
= {} &amp; \Pr\left( |Z| &gt; \frac{1.28}{\sqrt6-\sqrt 3} \right) \text{ where } Z\sim N(0,1).
\end{align}</p>

<p>If the variance were unknown but the variances of the two populations were equal then we would be working with a t-distribution.</p>
"
"2384850","2384860","<p>It is an application of the chain rule
\begin{equation}c'(0)f:=(f\circ c)'(0)=\left((f\circ \phi^{-1})\circ (\phi \circ c)\right)'(0)=\sum_{i=1}^m \frac{d}{dt}(\phi \circ c)^i \frac{\partial (f\circ \phi^{-1})}{\partial u^i}=\sum_{i=1}^n\dot{x}^i \frac{\partial}{\partial \phi^i}f,
\end{equation}
where in the last equality the definition of $\frac{\partial}{\partial \phi^i}$ was used.
To get your formula just think the $f$ away.</p>
"
"2384853","2385207","<p>I found it awkward to present the following argument without at least <em>some</em> reference to $\ln z$; so I essentially ""invented"" it here at equation (4), then developed a couple of basic properties such as $e^{\ln z} = z$.  But it doesn't take much ""logarithm theory"" to answer this question.</p>

<p>In $D = \Bbb C \setminus \Bbb R^- = \Bbb C \setminus (-\infty, 0]$, we may without ambiguity take</p>

<p>$z = re^{i\theta}, \tag 1$</p>

<p>since $\theta$ is restricted to the interval $(-\pi, \pi)$.  Furthermore, since the point $r = 0$ is excluded from $D$ as well, $\ln r$ is a well-defined <em>real</em> function on $D$.  We may therefore define the complex-valued function on $D$</p>

<p>$g(z) = \ln r + i\theta; \tag 2$</p>

<p>we note that</p>

<p>$e^{g(z)} = e^{\ln r + i\theta} = e^{\ln r} e^{i \theta} = re^{i\theta} = z. \tag 3$</p>

<p>Next, we observe that $z^{-1}$ is holomorphic in $D$, and we <strong><em>define</em></strong> the function $\ln z$ in $D$ <em>via</em> the formula</p>

<p>$\ln z = \displaystyle \int_1^z s^{-1} ds, \tag 4$</p>

<p>where the integral is taken over any path in $D$ which joins $1$ and $z \in D$; since $z^{-1}$ is holomorphic the integral is path-independent, and $\ln z$ is holomorphic in $D$.  We show $e^{\ln z} = z$.  Consider the holomorphic function</p>

<p>$F(z) = z^{-1} e^{\ln z}; \tag 5$</p>

<p>since</p>

<p>$\ln 1 = \displaystyle \int_1^1 s^{-1} ds = 0, \tag 6$</p>

<p>we have</p>

<p>$F(1) = 1; \tag 7$</p>

<p>also,</p>

<p>$F'(z) = -z^{-2}e^{\ln z} + z^{-1}(\ln z)' e^{\ln z} = -z^{-2}e^{\ln z} + z^{-2}e^{\ln z} = 0, \tag 8$</p>

<p>since $(\ln z)' = z^{-1}$ from (4).  I fowllows fro (8) that $F(z)$ is a constant and so by (7) we see that</p>

<p>$z^{-1}e^{\ln z} = F(z) = 1 \tag 9$</p>

<p>for all $z \in D$;  hence</p>

<p>$e^{\ln z} = z. \tag {10}$</p>

<p>We combine (2) and (10):</p>

<p>$e^{\ln z} = e^{g(z)}, \tag {11}$</p>

<p>or</p>

<p>$e^{(g(z) - \ln z)} = 1; \tag {12}$</p>

<p>if follows that</p>

<p>$g(z) = \ln z + 2n\pi i \tag{13}$</p>

<p>for some $n \in \Bbb Z$; but</p>

<p>$g(1) = 0 = \ln 1, \tag{14}$</p>

<p>so $n = 0$ and</p>

<p>$\ln r + i \theta = g(z) = \ln z \tag{15}$</p>

<p>in $D$.  (15) shows that $\ln r + i \theta$ is holomorphic in $D$ and thus
the harmonic conjugate of $\theta = \arg(z)$ in $D$ is $\ln r$.</p>
"
"2384867","2384881","<p>If $n$ is an integer, and $nG$ is properly contained in $G$, then by your hypothesis, $nG$ must be free abelian with some basis $S$.  Then $\frac{1}{n}S$ is a basis for $G$, and you are done.</p>

<p>Otherwise, $nG = G$ for all $n \in \mathbb{N}$.  This means that it makes sense to divide things in $G$ by integers.  Fixing a given $g \neq 1_G$ in $G$, the homomorphism $\mathbb{Q} \rightarrow G$ given by </p>

<p>$$\frac{a}{b} \mapsto \frac{a}{b} \cdot g$$</p>

<p>is well defined and injective.  Then $G$ contains a nonfree subgroup.</p>
"
"2384868","2385004","<p>This is immediate from basic scheme theory, specifically the theorem that the ring of global sections of the structure sheaf on $\operatorname{Spec} A$ is naturally isomorphic to $A$.  Let $X=\operatorname{Spec} A$ and $\mathcal{O}_X$ be its structure sheaf.  Since $X$ is discrete, $X=\{\mathfrak{m}_1\}\cup\dots\cup\{\mathfrak{m}_n\}$ is an open cover by disjoint open sets and so the gluing axiom says that $\mathcal{O}_X(X)\cong\prod_i \mathcal{O}_X(\{\mathfrak{m}_i\})$ via the restriction maps.  But $\mathcal{O}_X(\{\mathfrak{m}_i\})$ is exactly the localization $A_\mathfrak{m_i}$, so $A\cong \mathcal{O}_X(X)\cong\prod_i A_{\mathfrak{m}_i}$.</p>

<p>[There are lower-tech ways to prove it, but this is my favorite and it makes the result ""obvious"" once you've developed a little intuition for schemes.]</p>
"
"2384869","2384877","<p>$$f(z) = \frac{e^{mz}}{(1-e^{-z})^{n+1}}=\frac{e^{(m+n+1)z}}{(e^{z}-1)^{n+1}} \tag{1}$$
and
$$ \text{Res}\left(f(z),z=0\right)=\frac{1}{2\pi i}\oint_{\gamma}f(z)\,dz \tag{2}$$
where $\gamma$ is a simple closed curve enclosing the origin. Since $z\mapsto e^z-1=z+O(z^2)$ is a conformal map, by the substitution $e^{z}-1=w$ we get:
$$ \text{Res}\left(f(z),z=0\right) = \frac{1}{2\pi i}\oint_{\gamma}\frac{(w+1)^{m+n}}{w^{n+1}}\,dw \tag{3}$$
which equals the coefficient of $w^{n}$ in the binomial expansion of $(w+1)^{m+n}$, i.e. $\binom{m+n}{n}=\binom{m+n}{m}$.</p>

<p>The proof of the <a href=""https://en.wikipedia.org/wiki/Lagrange_inversion_theorem"" rel=""nofollow noreferrer"">Lagrange inversion formula</a> exploits a similar trick.</p>
"
"2384872","2384882","<p>By inspection: $\;\displaystyle\phi =  \color{red}{1 + \frac{1}{1 + \frac{1}{1 + \frac{1}{...}}}} = \color{blue}1 + \frac{1}{\color{red}{1 + \frac{1}{1 + \frac{1}{...}}}} = \color{blue}{1}+\frac{1}{\color{red}{\phi}}\,$. </p>

<p>For a formal proof, consider the recurrence $\;\displaystyle\phi_0=1, \;\phi_{n+1}=1+\frac{1}{\phi_n}\,$ and define $\,\phi\,$ as its limit.</p>
"
"2384876","2384889","<p>I believe you are asking if integration can be defined in a way other than setting an identity (e.g., with a limit like differentiation). This is how the <a href=""http://mathworld.wolfram.com/RiemannIntegral.html"" rel=""nofollow noreferrer"">Riemann integral</a> is defined. The integral is taken as the limit of a Riemann sum:</p>

<p><a href=""https://i.stack.imgur.com/nlMfC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nlMfC.png"" alt=""Riemann integral""></a></p>

<p>$$\begin{align}
\int_a^b f(x) \, dx &amp;= \lim_{n\to\infty}\sum_{i=1}^{n}h \cdot w \\
&amp;= \lim_{n\to\infty}\sum_{i=1}^{n} f\left(a+iw\right) \cdot \frac{b-a}{n} \\
&amp;= \lim_{n\to\infty}\sum_{i=1}^{n} f\left(a+i\frac{b-a}{n}\right) \cdot \frac{b-a}{n} \\
\end{align}$$</p>

<p>If you are an introductory calculus student, this is the type of definite integration that you use. The identities from indefinite integration come from the understanding that determining an antiderivative and evaluating it between two points yields the same value as the limiting process.</p>

<p>As you enter higher levels of math, Riemann integration fails or simply makes no sense. This is when other forms of integrationâsuch as <a href=""https://en.m.wikipedia.org/wiki/Lebesgue_integration"" rel=""nofollow noreferrer"">Lebesgue integration</a>, which might be the easiest for you to graspâmust come into play.</p>
"
"2384878","2384928","<p>More generally $\, c&gt;0\,\Rightarrow\, ca\bmod cn\, =\, c(a\bmod n)\quad $ <a href=""https://math.stackexchange.com/a/2059937/242"">[mod Distributive Law]</a></p>

<p><strong>Proof</strong> $\ $ Scaling remainder $\,a\bmod n\,$ by $\,c\,$ and invoking remainder <em>uniqueness</em> yields</p>

<p>$$\begin{align}  a\bmod n\ =\, &amp;\ \ a-qn\\
\Rightarrow\  0\ \le\, &amp;\ \ a-qn\,\ &lt;\,\ n\\
\Rightarrow\  0\ \le\, &amp;\,ca-qcn &lt; cn\\
\Rightarrow\  ca\bmod cn\ =\, &amp;\, ca-qcn\, =\, c(a\!-\!qn)\, =\, c(a\bmod n)\\
\end{align}$$</p>
"
"2384890","2384907","<p>This isn't <em>really</em> a question on metric spaces, so much as it is a question on set theory and operations on sets. Here's a quick proof sketch of a slightly more general result.</p>

<p>Let $x\in A\cap (\bigcup_{\beta \in B}X_\beta)$. Then $x\in A$ and $x\in X_\gamma$ for some $\gamma\in B$. Thus, $x\in A\cap X_\gamma$, and so $x\in \bigcup_{\beta\in B}A\cap X_\beta$. Suppose conversely that $x\in \bigcup_{\beta\in B} A\cap X_\beta$. Then $x\in A$ and $X_\delta$ for some $\delta\in B$. Then $x\in A$ and $x\in \bigcup_{\beta\in B}X_\beta$. So, $x\in A\cap (\bigcup_{\beta\in B}X_\beta)$.</p>

<p>Thus, $A\cap (\bigcup_{\beta\in B}X_\beta)=\bigcup_{\beta\in B}A\cap X_\beta$.</p>
"
"2384895","2384927","<p>Correct but little long.</p>

<p>For $x\ne 0,$</p>

<p>$$|\sin (\frac {1}{x})|\le 1 \implies$$</p>

<p>$$|\sin (x)\sin (\frac {1}{x})|\le |\sin(x)|$$</p>

<p>$$\implies$$
$$ -|\sin (x)|\le \sin (x)\sin (\frac {1}{x})\le |\sin (x)|$$</p>

<p>now squeeze.</p>
"
"2384909","2384952","<p>I think your net indeed works. Let $U$ be a basic product open neighbourhood of $1_{\mathbb{R}}$. This means that we have that the set $U$ is determined by a finite set of open neighbourhoods of $1$, say $U_{r_1}, \ldots, U_{r_N}$, for finitely many points $r_1, \ldots r_N$ of $\mathbb{R}$ such that $$U = \{f \in \mathbb{R}^{\mathbb{R}}: \forall 1 \le i \le N: f(r_i) \in U_{r_i}\}$$
You could also write this set as $$\prod_{r \in \mathbb{R}} U_r$$ where all $U_r = \mathbb{R}$ if $r \notin \{x_{r_0},\ldots, x_{r_N}\}$ but it's the same set.</p>

<p>Now define $E_0 = \{x_{r_1}. \ldots x_{r_N}\} \in I$.</p>

<p>If $E \ge E_0$ or equivalently $E_0 \subseteq E$, then $1_E \in U$, as $1_E$ is $1$ on all points of $E_0$ specifically and those determine membership of $U$.
So the net converges to $1_{\mathbb{R}}$.</p>
"
"2384910","2385680","<p>First of all, I would say that it is a bit strange to say that '$P(x) \land \forall x \ Q(x)$ is equivalent to $\forall x (P(x) \land \ Q(x))$ assuming the $x$ in $P(x)$ in the first formula is bound', given that that $x$ is <em>not</em> bound in that formula!  And the equivalence of formulas can always be determined on the basis of those formulas, without referring to some implicit larger context.</p>

<p>Anyway, your criticism is on the mark.  That is, if we do consider a formula where the $x$ is actually bound, we can of course bind it with an existential. As such: Is $\exists x (P(x) \land \forall x \ Q(x))$ (or the equivalent $\exists x P(x) \land \forall x \ Q(x)$ equivalent to $\forall x (P(x) \land \ Q(x))$?  And clearly it is not, as your example clearly demonstrates.</p>

<p>So, yes, the claim is weird and unusual, and even if we are charitable and try to make sense of it, it is plainly false.</p>

<p>About question 2) As <em>stand-alone</em> formulas, we can make sense of what it would take for them being equivalent, as for any free variable we can consider possible variable-assignments.  That is, with the $x$ being free in the first formula (again, as a stand-alone formula), we would say something like:</p>

<p>$P(x) \land \forall x \ Q(x)$ is equivalent to $\forall x (P(x) \land \ Q(x))$ if and only if for every any interpretation $I$ and variable-assignment $v$ that maps $x$ to some object $d$ in the domain $D$ of $I$:</p>

<p>$I,v \vDash P(x) \land \forall x \ Q(x)$ iff $I,v \vDash \forall x (P(x) \land \ Q(x))$</p>

<p>Does that biconditional hold?  No. We can take your example here. Consider interpretation $I$  with domain $\mathbb{N}$, $P(x)$: $x$ is even and $Q(x)$: $x \ge 0$, and pick variable-assignment $v$ with $v(x) = 12$</p>

<p>Then indeed we have that $I,v \vDash P(x) \land \forall x \ Q(x)$, but we don't have that $I,v \vDash \forall x (P(x) \land \ Q(x))$</p>

<p>So indeed, as stand-alone formulas (and thus with $x$ ion $P(x)$ being free in first formula), $P(x) \land \forall x \ Q(x)$ is not equivalent to $\forall x (P(x) \land \ Q(x))$</p>

<p>So yes, as an answer to question 2) your approach to showing non-equivalence as stand-alone formulas is also good!</p>
"
"2384915","2385796","<p>I can't find an example which is not behind a paywall, but in </p>

<blockquote>
  <p>R. De Sapio and G. Walschap, Diffeomorphism of total spaces and equivalence of bundles, J. Top., Volume 39, Issue 5, September 2000, Pages 921-929</p>
</blockquote>

<p>One finds an argument (attributed to Haefliger and Levine) that there is an embedding of $S^{11}$ into $\mathbb{R}^{17}$ for which the normal bundle is not isomorphic to a product, but whose total space is diffeomorphic to the product. 
 Later on in the same paper (pg 927, first paragraph), there are other related examples.</p>
"
"2384923","2384933","<p>Since $\mathbb{F}$ is a finite field, every element of $\mathbb{F}$ has finite order. This is clear in the case of $\mathbb{F}_2$, where $1=-1$. So let  of $\text{char }\mathbb{F}\neq 2$. Let $x_0,x_1,x_2,\ldots,x_n$ be the nonzero elements of $\mathbb{F}$. Without loss of generality, we can order $x_0,x_1,x_2,\cdots,x_n$ so that $x_0=1$, $x_1=-1$ and $x_{2m}=x_{2m+1}^{-1}$ for $m \geq 1$. Inverses in a group or ring are unique. There are also no repeats in this list as no element besides $\pm 1$ can be its own inverse: if there were such an element, say $x$, then $x^{-1}=x$. But as $x \neq 0$, this implies $x^2=1$. But then $x^2-1=0$ which is to say $(x-1)(x+1)=0$ so that $x \in \{1,-1\}$. But then each element is paired with its inverse in the product so that </p>

<p>$$x_0x_1x_2\cdots x_n=x_0x_1 \cdot 1 \cdot 1 \cdot \cdots \cdot 1=x_0x_1=1 \cdot -1 = -1$$</p>
"
"2384929","2384995","<p>If $M\subset Frac(R)$ is a nonzero submodule which is free, then it must be cyclic, since $M\otimes_R Frac(R)=Frac(R)$ is free of rank $1$ over $Frac(R)$.  Or by a more elementary argument, if $x=\frac{a}{b}$ and $y=\frac{c}{d}$ are two distinct nonzero elements of $M$ with $a,b,c,d\in R$, then $bcx=ady$, which is a nontrivial relation between $x$ and $y$.  So $x$ and $y$ cannot both be part of a free generating set for $M$, and $M$ cannot be generated freely by more than one element.  So this is equivalent to your previous question: the answer is that $R$ satisfies your condition iff $R$ is a field or a DVR.</p>
"
"2384930","2384935","<p>Hint: $$\frac{ax+b}{x^2} = \frac{a}{x}+\frac{b}{x^2}.$$ Therefore, $$\frac{A}{x}+\frac{ax+b}{x^2}=\frac{A'}{x}+\frac{b}{x^2}.$$</p>
"
"2384932","2386135","<p>We seek to evaluate</p>

<p>$$\sum_{l=0}^m (-4)^l {m\choose l} {2l\choose l}^{-1}
\sum_{k=0}^n \frac{(-4)^k}{2k+1} {n\choose k} {2k\choose k}^{-1}
{k+l\choose l}.$$</p>

<p>We start with the inner term and use the Beta function identity</p>

<p>$$\frac{1}{2k+1} {2k\choose k}^{-1}
= \int_0^1 x^k (1-x)^k \; dx.$$</p>

<p>We obtain</p>

<p>$$\int_0^1 [z^l]
\sum_{k=0}^n {n\choose k} (-4)^k x^k (1-x)^k \frac{1}{(1-z)^{k+1}}
\; dx
\\ = [z^l] \frac{1}{1-z}
\int_0^1 \left(1-\frac{4x(1-x)}{1-z}\right)^n \; dx
\\ = [z^l] \frac{1}{(1-z)^{n+1}}
\int_0^1 ((1-2x)^2-z)^n \; dx
\\ = \sum_{q=0}^l {l-q+n\choose n}
[z^q] \int_0^1 ((1-2x)^2-z)^n \; dx 
\\ = \sum_{q=0}^l {l-q+n\choose n}
{n\choose q} (-1)^q \int_0^1 (1-2x)^{2n-2q} \; dx
\\ = \sum_{q=0}^l {l-q+n\choose n}
{n\choose q} (-1)^q 
\left[-\frac{1}{2(2n-2q+1)} (1-2x)^{2n-2q+1}\right]_0^1
\\ = \sum_{q=0}^l {l-q+n\choose n}
{n\choose q} (-1)^q \frac{1}{2n-2q+1}.$$</p>

<p>Now we have</p>

<p>$$ {l-q+n\choose n} {n\choose q} (-1)^q \frac{1}{2n-2q+1}
\\ = \mathrm{Res}_{z=q}
\frac{(-1)^n}{2n+1-2z} 
\prod_{p=0}^{n-1} (l+n-p-z) \prod_{p=0}^n \frac{1}{z-p}.$$</p>

<p>Residues sum to zero and since  $\lim_{R\to\infty} 2\pi R \times R^n /
R /  R^{n+1} = 0$ we  may evaluate the  sum using the negative  of the
residue at $z=(2n+1)/2.$ We get</p>

<p>$$\frac{1}{2} (-1)^n
\prod_{p=0}^{n-1} (l+n-p-(2n+1)/2) 
\prod_{p=0}^n \frac{1}{(2n+1)/2-p}
\\ = (-1)^n
\prod_{p=0}^{n-1} (2l+2n-2p-(2n+1)) 
\prod_{p=0}^n \frac{1}{2n+1-2p}
\\ = (-1)^n
\prod_{p=0}^{n-1} (2l-2p-1)
\frac{2^n n!}{(2n+1)!}
\\ = (-1)^n \frac{1}{2l+1}
\prod_{p=-1}^{n-1} (2l-2p-1)
\frac{2^n n!}{(2n+1)!}
\\ = (-1)^n \frac{2^n n!}{(2n+1)!} \frac{1}{2l+1}
\prod_{p=0}^{n} (2l-2p+1)
\\ = (-1)^n \frac{2^{2n+1} n!}{(2n+1)!} \frac{1}{2l+1}
\prod_{p=0}^{n} (l+1/2-p)
\\ = (-1)^n \frac{2^{2n+1} n! (n+1)!}{(2n+1)!} \frac{1}{2l+1}
{l+1/2\choose n+1}.$$</p>

<p>We obtain for our sum</p>

<p>$$(-1)^n 2^{2n+1} {2n+1\choose n}^{-1}
\sum_{l=0}^m (-4)^l {m\choose l} \frac{1}{2l+1} {2l\choose l}^{-1}
{l+1/2\choose n+1}.$$</p>

<p>We now  work with the  remaining sum without  the factor in  front. We
obtain</p>

<p>$$\int_0^1 [z^{n+1}] \sqrt{1+z}
\sum_{l=0}^m {m\choose l} (-4)^l x^l (1-x)^l (1+z)^l \; dx
\\ = [z^{n+1}] \sqrt{1+z} 
\int_0^1 (1-4x(1-x)(1+z))^m \; dx
\\ = [z^{n+1}] \sqrt{1+z} 
\int_0^1 \sum_{q=0}^m {m\choose q} (1-2x)^{2m-2q}
(-1)^q (4x(1-x))^q z^q \; dx
\\ = \sum_{q=0}^m {m\choose q} {1/2\choose n+1-q}
\int_0^1 
(1-2x)^{2m-2q}
(-1)^q (4x(1-x))^q \; dx
\\ = \sum_{q=0}^m {m\choose q} {1/2\choose n+1-q}
\int_0^1 
(1-2x)^{2m}
\left(1-\frac{1}{(1-2x)^2}\right)^q \; dx
\\ = \sum_{q=0}^m {m\choose q} {1/2\choose n+1-q}
\sum_{p=0}^q {q\choose p} (-1)^p \int_0^1 (1-2x)^{2m-2p} \; dx
\\ = \sum_{q=0}^m {m\choose q} {1/2\choose n+1-q}
\sum_{p=0}^q {q\choose p} (-1)^p \frac{1}{2m-2p+1}.$$</p>

<p>Re-writing then yields</p>

<p>$$\sum_{p=0}^m (-1)^p \frac{1}{2m-2p+1}
\sum_{q=p}^m {m\choose q} {1/2\choose n+1-q} {q\choose p}.$$</p>

<p>Observe that</p>

<p>$${m\choose q} {q\choose p} =
\frac{m!}{(m-q)! \times p! \times (q-p)!}
= {m\choose p} {m-p\choose m-q}$$</p>

<p>so that we find</p>

<p>$$\sum_{p=0}^m {m\choose p} (-1)^p \frac{1}{2m-2p+1} 
\sum_{q=p}^m {m-p\choose m-q} {1/2\choose n+1-q}
\\ = \sum_{p=0}^m {m\choose p} (-1)^p \frac{1}{2m-2p+1} 
\sum_{q=0}^{m-p} {m-p\choose m-p-q} {1/2\choose n+1-p-q}
\\ = \sum_{p=0}^m {m\choose p} (-1)^p \frac{1}{2m-2p+1} 
\sum_{q=0}^{m-p} {m-p\choose q} {1/2\choose n+1-p-q}.$$</p>

<p>Continuing we obtain</p>

<p>$$\sum_{p=0}^m {m\choose p} (-1)^p \frac{1}{2m-2p+1} 
\sum_{q=0}^{m-p} {m-p\choose q} [z^{n+1-p}] z^q \sqrt{1+z}
\\ = \sum_{p=0}^m {m\choose p} (-1)^p \frac{1}{2m-2p+1} 
[z^{n+1-p}] \sqrt{1+z} \sum_{q=0}^{m-p} {m-p\choose q} z^q
\\ = \sum_{p=0}^m {m\choose p} (-1)^p \frac{1}{2m-2p+1} 
[z^{n+1-p}] (1+z)^{m-p+1/2}
\\ = \sum_{p=0}^m {m\choose p} (-1)^p \frac{1}{2m-2p+1} 
{m-p+1/2\choose n+1-p}
\\ = (-1)^m \sum_{p=0}^m {m\choose p} (-1)^p \frac{1}{2p+1} 
{p+1/2\choose n+1-m+p}
\\ = (-1)^m 
\sum_{p=0}^m {m\choose p} (-1)^p \frac{1}{2} \frac{1}{m-n-1/2}
{p-1/2\choose n+1-m+p}
\\ = (-1)^m \frac{1}{2m-2n-1}
\sum_{p=0}^m {m\choose p} (-1)^p 
{p-1/2\choose n+1-m+p}.$$</p>

<p>Concluding with a closed form we establish at last</p>

<p>$$(-1)^m \frac{1}{2m-2n-1}
\sum_{p=0}^m {m\choose p} (-1)^p [z^{n+1-m}] z^{-p} (1+z)^{p-1/2}
\\ = (-1)^m \frac{1}{2m-2n-1} [z^{n+1-m}] (1+z)^{-1/2}
\sum_{p=0}^m {m\choose p} (-1)^p z^{-p} (1+z)^p
\\ = (-1)^m \frac{1}{2m-2n-1} 
[z^{n+1-m}] (1+z)^{-1/2} \left(1-\frac{1+z}{z}\right)^m
\\ = \frac{1}{2m-2n-1} [z^{n+1}] (1+z)^{-1/2}.$$</p>

<p>We finish by re-introducing the factor in front to obtain</p>

<p>$$(-1)^n 2^{2n+1} {2n+1\choose n}^{-1} \frac{1}{2m-2n-1}
{-1/2\choose n+1}
\\ = (-1)^n 2^{2n+1} {2n+1\choose n}^{-1} \frac{1}{2m-2n-1}
\frac{1}{(n+1)!} \prod_{q=0}^{n} (-1/2 -q)
\\ = (-1)^n 2^{n} {2n+1\choose n}^{-1} \frac{1}{2m-2n-1}
\frac{1}{(n+1)!} \prod_{q=0}^{n} (-1 -2q)
\\ =  2^{n} {2n+1\choose n}^{-1} \frac{1}{2n+1-2m}
\frac{1}{(n+1)!} \prod_{q=0}^{n} (1 +2q)
\\ =  2^{n} {2n+1\choose n}^{-1} \frac{1}{2n+1-2m}
\frac{1}{(n+1)!} \frac{(2n+1)!}{2^n n!}.$$</p>

<p>Yes indeed this is</p>

<p>$$\bbox[5px,border:2px solid #00A000]{
\frac{1}{2n+1-2m}.}$$</p>

<p>Here  I have  chosen  to document  the  simple steps  as  well as  the
complicated ones to aid all types of readers.</p>
"
"2384938","2384958","<p>Part of the motivation for this approach is the more general Chernoff approach to large deviation bounds, where, by Markov's inequality, one has (for $t&gt;0$):</p>

<p>$$\mathbb{P}(X&gt;x) = \mathbb{P}(e^{tX}&gt;e^{tx}) = \le \frac{\mathbb{E}[e^{tX}]}{e^{tx}} = \exp(-[tx-C(t)])$$</p>

<p>with $C(t)=\log \mathbb{E}[e^{tX}]$, the cumulant-generating function of $X$. </p>

<p>To proceed, one then calculates the convex conjugate of $C$:</p>

<p>$$r(x) = \sup_{t&gt;0}[tx-C(t)]$$</p>

<p>to deduce that $\mathbb{P}(X&gt;x) \le \exp(-r(x))$ (by taking the $t$ which attains the supremum above).</p>

<p>The idea of this standard proof of Hoeffding's inequality is to:</p>

<ul>
<li>upper bound $\mathbb{P}(X&gt;x)$, by ...</li>
<li>lower bounding $r(x)$, by ...</li>
<li>upper bounding $C(t)$ - which is what the proof does.</li>
</ul>

<p>The choice to do this by a Taylor expansion is partially motivated by the fact that $C(t)$ is convex (this is true for all cumulant-generating functions), and as such, we have information about its derivatives and curvature.</p>
"
"2384941","2384950","<p><strong>hint</strong></p>

<p>$$\frac {x}{x-1}=1+\frac {1}{x-1} $$</p>

<p>We need prove that</p>

<p>$$(\forall A&gt;2 )\;\;\;(\exists\eta&gt;0)\;\;(\forall x&gt;1)$$
$$0 &lt;x-1 &lt;\eta \implies 1+\frac {1}{x-1}&gt;A $$</p>

<p>this last condition is equivalent to</p>

<p>$$ x-1 &lt;\frac {1}{A-1}.$$</p>

<p>So we can take  $\eta=\frac {1}{A-1} $.</p>
"
"2384956","2384969","<p>If we write $f(x+iy) = u(x,y)+iv(x,y) $ where $u,v$ are real, we can compute the Jacobian in the usual way as
$$ \begin{vmatrix} u_x &amp; u_y \\ v_x &amp; v_y \end{vmatrix} = u_x v_y-u_yv_x. $$
The CauchyâRiemann equations give that $u_x=v_y$ and $u_y=-v_x$, so this is equal to $u_x^2+u_y^2$.</p>

<p>On the other hand, the formula for coordinate transformations gives $\partial_z = \frac{1}{2}(\partial_x - i\partial_y)$, so
$$ f'(x+iy) = \frac{1}{2}\left( u_x+v_y+i(u_y-v_x) \right) = u_x+iu_y, $$
by the CauchyâRiemann equations again, and then $\lvert f'(x+iy) \rvert^2 = u_x^2+u_y^2 $. So the Jacobian is $\lvert f'(z) \rvert^2$ as required.</p>
"
"2384963","2385044","<p>For a physicist the first and second part of your chosen path take, in the limit, place infinitely far away from the origin. Hence there is no force working and both integrals have a zero contribution. Only the last integral, where you move from infinitely far away in the $z$-direction toward the point $(a,b,c)$ you integrate over a non-zero force and you will find the answer you are looking for.</p>

<p>As a mathematician you would work out all of the integrals for explicitly finite values of $(x_0,y_0,z_0)$ and only after having the result take the proper limits. </p>

<p>In either case the contribution of the integral in the last line will go to zero.</p>

<p>There is however an slight issue and that is concerned with the force calculation in the last line. The force has a magnitude and a direction, which is not properly included. You need to use $\vec{F} = |F| \hat{F}$ for a radial force, which gives on this particular path: 
$$
\vec{F} = \frac{A}{(x^2+y_0^2+z_0^2)} \frac{1}{\sqrt{x^2+y_0^2+z_0^2}} \left( \begin{array}{l} x \\ y_0 \\ z_0 \end{array}\right)
$$
and hence the integral should read:
$$
\int_{x_0}^a \vec{F} \cdot d \vec{x} = \int_{x_0}^a \frac{x}{(x^2+y_0^2+z_0^2)^{\frac{3}{2}}} ~dx
$$
and something similar for the other two paths.</p>

<p>Note that for the result you want to obtain should not depend on the path you take. You therefore strictly speaking only need one of the three coordinates $(x_0,y_0,z_0)$ to go to infinity and the other two can remain finite.</p>
"
"2384966","2385029","<blockquote>
  <p>When a submodule $N$ of a $R$-module $M$ is a direct summand of $M$?</p>
</blockquote>

<p>There is not really any criterion (one significantly simpler than the definition) to spot summands in general. It all depends on the module structure of $M$. There is an important equivalent condition, though: $N$ is a summand of $M$ if and only if there is an idempotent homomorphism $e:M\to M$ such that $e(M)=N$.</p>

<blockquote>
  <p>or of $R$-module $R$?</p>
</blockquote>

<p>At this point things are easier since the idempotent homomorphisms $R\to R$ are given by idempotent elements of $R$. So a right ideal $T$ is a summand of $R_R$ iff there is an idempotent element $e\in R$ such that $eR=T$.</p>

<blockquote>
  <p>For instance, if $M$ is semisimple then $N$ is semisimple, does this say that $N$ is direct summand of $M$ (or of $R$-module $R$)?</p>
</blockquote>

<p>Well, <em>yes</em>, because a characterization of semisimple modules is that <em>every</em> submodule is a direct summand. I'm not sure which definition of ""semisimple"" you are using, so I don't know if you need help seeing this.</p>
"
"2384971","2384977","<p>If completely regular includes $T_1$, then the standard open map counterexample already works: let $f: \mathbb{R} \to \{0,1\}$, where $\{0,1\}$ has the indiscrete/trivial topology, given by $f(x) = 0$ for $x$ rational and $f(x) =1$ otherwise. Then $f$ is continuous and open, but $\{0,1\}$ is not $T_1$ or any higher $T_i$-axiom. </p>

<p>Another (non-continuous) example is the identity map from $\mathbb{R}$ to $\mathbb{R}_K$ (the $K$-topology from Munkres, which is the smallest topology on the reals that is finer than the usual topology and which makes $K = \{\frac{1}{n}: n \in \mathbb{N}\}$ closed, and which is not regular or completely regular).</p>
"
"2384975","2385086","<p>I presume that it is clear that there can be no direction for the electric field to point in because of symmetry and that one need to be careful very careful with the singularity.</p>

<p>In your first approach, this works out because the shift of ${\bf x}$ to the origin, you impose the symmetry directly on the integral and the electric field from opposite sides cancels immediately.</p>

<p>In the second approach there is you need to be careful with the expansion you use for $\frac{1}{|{\bf x}-{\bf x'}|}$, which is valid only for $|{\bf x}| \neq |{\bf x}'|$. In the case of equality it does not converge, but you actually don't need it because you can use a spherical Gauss surface to compute the radially directed electric field.</p>

<p>If you take the integral $\int_0^\mu$ with $\mu&lt;|{\bf x}|$ you obtain $\frac{4 \pi}{3} \mu^3 \frac{1}{x}$ which is just the potential due to all charge within a sphere of radius $\mu$ and contributes $\frac{4 \pi \mu^3}{3 x^2} \hat{x}~~~$ to the electric field. </p>

<p>The outside integral $\int_\nu^R$ with $|{\bf x}|&lt;\nu$ gives a contribution $\frac{4 \pi}{3} (R^3 - \nu^3)$ to the potential, which does not depend on $x$ at all and hence does not result in any electric field. This is true for any homogenous spherical shell or more generally and radially symmetric charge distribution and I am sure it has been told/explained in one of the lectures. </p>

<p>So what is with the discrepancy between these two different outcomes? If we consider the direction you get for the electric field in the second approach, you notice that is is directed radially outwards with respect to the origin of the system. This would, however suggest that the direction of the electric field that is experienced at the point ${\bf x}$ would depend on the chosen reference frame. This is clearly unphysical and hence there can not or better there should not be such an electric field.</p>

<p>There is however a difference between the first and second approach and it is a rather subtle one. In the first case you consider the point ${\bf x}$ in the centre of a homogenous sphere of radius $R$ and take the limit $R \rightarrow \infty$. In the second case you consider a sphere of radius $R$ that is not centered around ${\bf x}$. </p>

<p>As long as you perform the calculation on an identical charge density distributions (finite values of $R$) with respect to the point ${\bf x}$, both methods will give the same answer. If you consider a spherical distribution centered at ${\bf x}$ the field will be zero in the first approach and the second approach has a correction from a the shell far away at distance $R$. The reason being that there are no complete spherical shells near the border. The electric field contribution of a small charge density element there is proportional to $\frac{1}{R^2}$, but at the same time the shell also has a volume proportional to $R^2$. This combination will give a finite contribution that will counteract the electric field from the spherical charge distribution for $|{\bf x'}| &lt; |{\bf x}|$.</p>

<p>In the alternative version where there is a homogenous spherical charge density of radius $R$ but ${\bf x}$ is not placed in the center, it is the first approach that needs to be corrected for the border region and gets an overall non-zero electric field contribution. For any finite charge density this problem never occurs, but for infinite charge densities the results can depend on how you take the limit.</p>

<p>From the physics point of view we would argue that the electric field in the infinite system is not allowed to depend on the choice of reference frame. This does, however, not imply that there is no electrostatic force in the system, but merely that it has no particular direction and therefore is isotropic. It is a pressure like force that would like to increase the distance between all charge density elements by means of a homogenous expansion.</p>
"
"2384987","2384991","<p>The total number of rolls follows a geometric law of parameter $p=\dfrac{4}{6}=\dfrac{2}{3}$. Therefore the expected number of rolls is $1/p=\color{red}{1.5}$.</p>

<p>Therefore the expected total is $(1/p-1)\cdot 1.5 + 4.5=\color{red}{5.25}$, because $1.5$ is the mean of a roll between $1$ and $2$ (and you have an average of $1/p -1$ rolls between $1$ and $2$) and $4.5$ is the mean of the last roll (between $3$ and $6$).</p>

<p>By the way you can easily verify your results for this kind of problem with a simple python code:</p>

<pre><code>import random as random

nb_trials = 10000
tot = 0

for i in range(nb_trials):
    sum_value = 0
    b = True
    while b:
        a = random.randint(1,6)
        if a &gt;=3:
            b = False
        sum_value += a
    tot += sum_value 

average = tot * 1.0 / nb_trials
print(average)
</code></pre>

<p><a href=""https://tio.run/##RU9BDoIwELzvK/YIahBC4sEEj77AO1ligSbQklIwvr7uAsJcOjsz7WyHr2@tyUPQ/WCdR0fmbXukcWMApiq909SNWGCWMsBbzzwFqK1DjdpItlHRnozvgIxx6suZukktaVEqZi83qWX4tLpTWK1ZAbG7tiZyaOOj7HKLd1/XHHkU@XHj/@aTS9WuHr3nAmmRZWUeDgeAZuWokd3EPGGWpHjF/Q8wOFlgS8Uh/AA"" rel=""nofollow noreferrer"" title=""Python 3 â Try It Online"">Try it online!</a></p>
"
"2384988","2385024","<p>The question is asked in a possibly confusing way. In the abelian case, you have an isomorphism of complex Lie groups (i.e., both biholomorphic and a group isomorphism) with $\mathbf{C}^m\times(\mathbf{C}^*)\times T$.</p>

<p>In the nilpotent case you can then both ask about the biholomorphic type, or about a result really about complex Lie group classification.</p>

<p>The abelian case is an easy consequence of the fact that the group has to be quotient of its universal cover by a discrete normal subgroup (which has to be central, since any discrete normal subgroup in a connected group is central).</p>

<p>So in the nilpotent case we can mimic this: the connected nilpotent complex Lie groups are quotients of a simply connected complex nilpotent Lie group $G$ by a discrete subgroup $\Gamma$ of its center $Z_G$ (which is isomorphic to $\mathbf{C}^k$ for some $k$). </p>

<p>In the abelian case, one considers the real hull $V$ of $\Gamma$. Then $V\cap iV$ ""corresponds"" to the $T$ part, and $V+iV$ corresponds to the $\mathbf{C}^n\oplus T$ part. Here we can define these subspaces, but in general we cannot deduce a decomposition of the whole group. So I don't see a better way to state the structural fact than describing as a quotient as above. (See two examples below.)</p>

<p>To get the biholomorphic type (forgetting the group structure) is, however, doable. Indeed, the exponential conjugates the group law of $G$ to the Baker-Campbell-Hausdorff law of its Lie algebra $\mathfrak{g}$. In particular, we have $\exp(z)\exp(x)=\exp(z+x)$ when $z$ is central. So the left action of $Z_G$ in itself it conjugate to the left action of its Lie algebra $\mathfrak{z}$ by addition. Thus $G/\Gamma$ is biholomorphic to $\mathfrak{g}/\Gamma$, which is part of the abelian case since one now considers addition; this answers positively your last question. </p>

<hr>

<p>First example: $G$ is the 3-dimensional complex Heisenberg group. Then $Z_G$ is 1-dimensional. One can either mod out (a) by $\{0\}$, (b) by a cyclic subgroup of $Z_G$, or (c) by a lattice in $Z_G$. In all the cases, $G/\Gamma$ does not split as a non-trivial direct product (as a complex Lie group). [Actually, the Lie algebra, as a 6-dimensional real Lie algebra, does not split as a direct product, so in none of the cases there is a decomposition even as a 6-dimensional real Lie group] (a) is a single case; (b) is also a single case up to isomorphism, since the automorphism group of the Heisenberg group acts transitively on the center minus 0. (c) Yields may cases, namely the lattice is well-defined modulo similarity, still leaving a continuum of non-isomorphic complex Lie groups (they can be checked to also not be non-isomorphic as real Lie groups, maybe up to pairs).</p>

<p>Second example: start from the direct product $H$ of the latter with $\mathbf{C}$. So the center $Z$ is now isomorphic to $\mathbf{C}^2$, and has a ""distinguished"" 1-dimensional subspace $D$, namely the intersection with the derived subgroup.</p>

<p>Then $\Gamma$ is isomorphic to $\mathbf{Z}^k$ for some $k\in\{0,1,2,3,4\}$. When $\Gamma$ splits as product of $\Gamma\cap D$ with another subgroup, this yields a product of one of the preceding case with an abelian complex Lie group. I think it happens precisely when the real hull $V$ of $\Gamma$ is such that $\Gamma\cap D$ is a lattice in $V\cap D$. But for $k\ge 2$ there are many other cases, for instance, when $\Gamma$ is a lattice ($k=4$) but has zero (or cyclic) intersection with $V$. Or when $V$ is a real plane ($k=2$) intersecting $D$ in a real line, and $\Gamma$ does not meet $D$.</p>
"
"2384994","2385456","<p>Solving functional equations is not necessarily impossible, but in general quite  bit of work. It would be therefore good idea to give people a more compelling reason to want to invest some time in helping you solve such a problem. Specially since you actually ask for a generic solution with arbitrary $p$. So why do you actually want to solve this particular problem? If it is of the type, let's see whether I/we can you underestimate the difficulties of such a question.</p>

<p>Anyway, for the case $p=1$ there is the general solution $f(x) = c \frac{ \arctan x}{x}$ for any constant $c$.</p>
"
"2385011","2385064","<p>In a derangement, no letter is left in its original position.  You only considered the possibility that no pair of letters was left in its original position.</p>

<p>There are 
$$\frac{6!}{2!2!2!}$$
distinguishable arrangements of the letters of the word $aabbcc$.</p>

<p>In a derangement, no letter is left in its original position.  Therefore, we must exclude those cases in which one or more letters is left in its original position.</p>

<p>Define $A_1$ to be the set of arrangements in which the first $a$ is left in its original position.  Define $A_2$ to be the set of arrangements in which the second $a$ is left in its original position.  Define $B_1$, $B_2$, $C_1$, and $C_2$ analogously for the letters $b$ and $c$.</p>

<p><em>one letter is left in its original position</em>:  Consider $|A_1|$.  Since the first $a$ is in its original position, we are left with an arrangement of the set $\{a, b, b, c, c\}$, so 
$$|A_1| = \frac{5!}{2!2!}$$
By symmetry, 
$$|A_1| = |A_2| = |B_1| = |B_2| = |C_1| = |C_2|$$
Hence, there are a total of 
$$\binom{6}{1}\frac{5!}{2!2!}$$
arrangements in which one letter is in its original position.</p>

<p><em>two letters are in their original positions</em>:  We have two cases to consider.  The letters are the same, or they are different.</p>

<p>two identical letters are in their original positions:  Consider $|A_1 \cap A_2|$.  If both $a$s are in their original positions, we are left with an arrangement of the set $\{b, b, c, c\}$.  Hence,
$$|A_1 \cap A_2| = \frac{4!}{2!2!}$$
By symmetry, 
$$|A_1 \cap A_2| = |B_1 \cap B_2| = |C_1 \cap C_2|$$
Hence, there are 
$$\binom{3}{1}\frac{4!}{2!2!}$$
arrangements in which two identical letters are in their original positions.</p>

<p>two different letters are in their original positions:  Consider $|A_1 \cap B_1|$.  We are left with an arrangement of the set $\{a, b, c, c\}$.  Hence, 
$$|A_1 \cap B_1| = \frac{4!}{2!}$$
There are $\binom{3}{2}$ ways to pick two different letters to be fixed points and two ways to choose the position of the fixed point for each of those letters.  Hence, there are 
$$\binom{3}{2}\binom{2}{1}^2\frac{4!}{2!}$$
arrangement with two different letters in their original positions.</p>

<p><em>three letters are in their original positions</em>:  We have two cases to consider.  Either a pair of identical letters and one other letter are in their original positions, or three different letters are in their original positions.  </p>

<p>a pair of identical letters and one other letter are in their original positions:  Consider $|A_1 \cap A_2 \cap B_1|$.  Since both $a$s and the first $b$ are fixed, we have an arrangement of the set $\{b, c, c\}$.  Hence,
$$|A_1 \cap A_2 \cap B_1| = \frac{3!}{2!}$$
By symmetry, there are three ways of picking the repeated letter, two ways of choosing the letter for the other fixed point, and two ways of choosing the position of the fixed point for that letter.  Hence, there are 
$$\binom{3}{1}\binom{2}{1}\binom{2}{1}\frac{3!}{2!}$$
arrangements in which there are three letters in their original positions, in which two of the fixed points are identical letters.</p>

<p>three different letters are in their original positions:  Consider $|A_1 \cap B_1 \cap C_1|$.  Since the first $a$, first $b$, and first $c$ are fixed points, we are left with an arrangement of $\{a, b, c\}$.  Hence, 
$$|A_1 \cap B_1 \cap C_1| = 3!$$ 
By symmetry, there are two ways to pick the fixed point for each of the three letters.  Hence, there are 
$$\binom{2}{1}^33!$$
arrangements in which three different letters are fixed points.</p>

<p><em>four letters are in their original positions</em>:  Again, there are two cases.  Either two pairs of identical letters are fixed points, or one pair of identical letters and two different letters are fixed points. </p>

<p>two pairs of identical letters are in their original positions:  Consider 
$|A_1 \cap A_2 \cap B_1 \cap B_2|$. Since both $a$s and both $b$s are in their original positions, we are left with an arrangement of $\{c, c\}$.  Hence, 
$$|A_1 \cap A_2 \cap B_1 \cap B_2| = \frac{2!}{2!} = 1$$
By symmetry, there are $\binom{3}{2}$ ways of picking two pairs of identical letters to be fixed points.  Hence, there are 
$$\binom{3}{2}$$
arrangements in which two pairs of identical letters are fixed points.</p>

<p>one pair of identical letters and two different letters are fixed points:  Consider $|A_1 \cap A_2 \cap B_1 \cap C_1|$.  Since both $a$s, the first $b$, and first $c$ are fixed points, we are left with an arrangement of $\{b, c\}$.  Hence, 
$$|A_1 \cap A_2 \cap B_1 \cap C_1| = 2!$$
Since there are three ways of choosing the repeated letter and two ways of choosing the position of the fixed points for the other two letters, there are 
$$\binom{3}{1}\binom{2}{1}^22!$$
arrangements in which a pair of identical letters and two different letters are fixed points.</p>

<p><em>five letters are in their original positions</em>:  Consider $|A_1 \cap A_2 \cap B_1 \cap B_2 \cap C_1|$.  Since both $a$s, both $b$s, and the first $c$ are in their original positions, we have an arrangement of $\{c\}$.  Hence, 
$$|A_1 \cap A_2 \cap B_1 \cap B_2 \cap C_1| = 1$$
By symmetry, there are $\binom{6}{5}$ ways to select the fixed point.  Hence, there are 
$$\binom{6}{5}$$
arrangements with five fixed points.</p>

<p><em>six letters are in their original positions</em>:  Since all the letters are in their original positions, 
$$|A_1 \cap A_2 \cap B_1 \cap B_2 \cap C_1 \cap C_2| = 1$$</p>

<p>Thus, by the Inclusion-Exclusion Principle, the number of derangements of $aabbcc$ is 
$$\frac{6!}{2!2!2!} - \binom{6}{1}\frac{5!}{2!2!} + \binom{3}{1}\frac{4!}{2!2!} + \binom{3}{2}\binom{2}{1}^2\frac{4!}{2!} - \binom{3}{1}\binom{2}{1}\binom{2}{1}\frac{3!}{2!} - \binom{2}{1}^33! + \binom{3}{2} + \binom{3}{1}\binom{2}{1}^22! - \binom{6}{5} + \binom{6}{6}$$ </p>
"
"2385019","2385080","<p>I'm afraid there's quite a bit of confusion going on here. So let me try to address not only your two actual questions, but also some other issues that I see here.</p>

<p><strong>[1]</strong> Generally  speaking, parentheses in logical expressions are used in the same way and for the same purpose as in arithmetical expressions: to show order of operations, or so to speak to show which things go together and which don't. So the answer to the question in your title:</p>

<blockquote>
  <p>What's the difference between $\forall x(P(x))$ and $\forall xP(x)$?</p>
</blockquote>

<p>is that there's no difference, as they mean exactly the same thing: for all values of $x$ (in the given domain or universe) $P(x)$ is true. (Note that this also covers both your Questions 1 and 2.) But...</p>

<p><strong>[2]</strong> Taking things out of context is extremely dangerous!!! If someone asks you:</p>

<blockquote>
  <p>What's the difference between $2+3$ and $(2+3)$?</p>
</blockquote>

<p>anybody will say that they are the same thing, both equal to $5$... Until it turns out that they were taken from longer expressions $\color{green}{2+3\cdot4}$ and $\color{blue}{(2+3)\cdot4}$, where presence or absence of parentheses makes a whole lot of difference!</p>

<p><strong>[3]</strong> And that's why the actual question in the body of your post hardly has anything to do with what you asked in the title. According to your post, you're</p>

<blockquote>
  <p>trying to prove is that $\forall x\, \exists y\, (P(x)\to Q(y))$ is logically equivalent to $(\forall x\, P(x))\to(\exists y\,Q(y))$.</p>
</blockquote>

<p>Note that while $\forall xP(x)$ is the antecedent of the second formula, $\forall x(P(x))$ does not appear anywhere in either of them. So when you used it in the title, where did it come from?</p>

<p><strong>[4]</strong> Moreover, these two statements actually are <strong>NOT</strong> equivalent to each other. Here's a quick counterexample. Let the domain be all integers $\mathbb{Z}$. Let</p>

<p>$$P(x)=[x\text{ is even}] \quad \text{and} \quad Q(y)=[y+1&lt;y].$$</p>

<p>(I'm using brackets as quotation marks.) Then:</p>

<ul>
<li><p>$\forall x\, \exists y\, (P(x)\to Q(y))$ is <em>false</em>. It requires that for any $x$ there exists some $y$ such that $P(x)\to Q(y)$. But if $x=2$, then $P(2)\to Q(y)$ doesn't hold for any $y$, because $P(2)$ is true and $Q(y)$ is false for any $y$.</p></li>
<li><p>But $(\forall x\, P(x))\to(\exists y\,Q(y))$ is <em>true</em>, because the antecedent $\forall x\, P(x)$ is false.</p></li>
</ul>

<p>So if you have been given an exercise where two statements are given and your task is to prove that they are logically equivalent, then maybe you made a typo somewhere?</p>
"
"2385048","2385069","<p>Define $k : \mathbb{R} \to \mathbb{R}$ by $k(x) = h(x) + \frac{1}{2}h(2x)$. Then it is easy to check that</p>

<p>$$ k(x) = \begin{cases}
2x, &amp; 0 \leq x \leq \frac{1}{2} \\
1, &amp; \frac{1}{2} \leq x \leq \frac{3}{2} \\
2(2-x), &amp; \frac{3}{2} \leq x \leq 2 \\
k(x \text{ mod } 2), &amp; \text{otherwise}.
\end{cases} $$</p>

<p>$\hspace{10em}$ <a href=""https://i.stack.imgur.com/vDsJs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vDsJs.png"" alt=""graph of h""></a></p>

<p>Notice that $g$ can be written entirely in terms of $k$:</p>

<p>$$ g(x) = \sum_{n=0}^{\infty} \frac{1}{4^n} k(4^n x). $$</p>

<p>Now let $I_n = 2 \bigcup_{j=0}^{4^n - 1} \frac{1}{4^n}\left( j + \left[ \frac{1}{4}, \frac{3}{4} \right] \right)$ and notice that this is the set of points in $[0, 2]$ where the function $x \mapsto \frac{1}{4^n}k(4^n x)$ attains its maximum. Here comes a crucial observation:</p>

<blockquote>
  <p>For each $a = (a_j)_{j\in\mathbb{N}} \in \{1, 2\}^{\mathbb{N}}$ we define $x(a) := 2 \sum_{j=1}^{\infty} \frac{a_j}{4^j}$. Then $x(a) \in I_n$ for all $n \geq 0$.</p>
</blockquote>

<p>Indeed, this is because $I_n$ contains all real numbers $x \in [0, 2]$ such that the $(n+1)$-th digit in the $4$-ary expansion of $\frac{x}{2}$ is either $1$ or $2$.</p>

<p>In particular, for each $a \in \{1, 2\}^{\mathbb{N}}$ we have</p>

<p>$$ g(x(a))
= \sum_{n=0}^{\infty} \frac{1}{4^n} k(4^n x(a))
= \sum_{n=0}^{\infty} \frac{1}{4^n}
= \frac{4}{3} $$</p>

<p>Since $g(x) \leq \frac{4}{3}$ is always true, it follows that $M = \frac{4}{3}$. Moreover, $x(a) \in D$ for any $a \in \{1, 2\}^{\mathbb{N}}$. So $x : \{1, 2\}^{\mathbb{N}} \to [0, 2]$ is an injective map whose image lies in $D$ and hence $D$ is uncountable.</p>
"
"2385053","2392966","<p>A standard reference for quasiconformal mappings in $\mathbb{C}$ is Lehto/Virtanen, ""Quasiconformal Mappings in the Plane"" - containing also quite some historical references. For the higher-dimensional theory of quasiconformal mappings, I'd refer to VÃ¤isÃ¤lÃ¤'s book ""Lectures on $n$-Dimensinonal Quasiconformal Mappings"". More a modern approach via PDEs, the book ""Elliptic PDEs and Quasiconformal Mappings in the Plane"" of Astala et. al. is the right source for your self-study, containing a lot of historical remarks as well.</p>
"
"2385060","2385124","<p>$$\begin{array}{|c|c|c|c|c|c|c|} 
\text{Pouring #} &amp; \text{Amount poured} &amp; \text{Into Urn} &amp; \text{Urn 1 before} &amp; \text{Urn 2 before} &amp; \text{Urn 1 after} &amp; \text{Urn 2 after}\\ \hline
\text{0} &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\ \hline
\text{1} &amp; 1/2 &amp; 2 &amp; 1 &amp; 0 &amp; 1/2 &amp; 1/2 \\ \hline
\text{2} &amp; 1/3 &amp; 1 &amp; 1/2 &amp; 1/2 &amp; 2/3 &amp; 1/3 \\ \hline
\text{3} &amp; 1/4 &amp; 2 &amp; 2/3 &amp; 1/3 &amp; 1/2 &amp; 1/2 \\ \hline
\text{4} &amp; 1/5 &amp; 1 &amp; 1/2 &amp; 1/2 &amp; 3/5 &amp; 2/5 \\ \hline
\text{5} &amp; 1/6 &amp; 2 &amp; 3/5 &amp; 2/5 &amp; 1/2 &amp; 1/2 \\ \hline
\text{6} &amp; 1/7 &amp; 1 &amp; 1/2 &amp; 1/2 &amp; 4/7 &amp; 3/7 \\ \hline
\text{7} &amp; 1/8 &amp; 2 &amp; 4/7 &amp; 3/7 &amp; 1/2 &amp; 1/2 \\ \hline
\text{...} &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... \\ \hline
\end{array}$$</p>

<p>We can note that for all odd pouring number $n \ge 1$, the ""after"" is perfectly even - $1/2$ to $1/2$ - and that for all even pouring number $n \ge 2$, the ""after"" is $\frac{n+2}{2n+2}$ to $\frac{n}{2n+2}$. This means that for pouring number $n = 1976$, where $1/1977$ of the contents from urn 2 will be poured to urn 1, the contents in urn 1 will be $$\fbox{(1976+2)/(2*1976+2) = 989/1977}$$</p>
"
"2385085","2385090","<p>The expected number of bounces can be modeled with the recursion $E = P \cdot (1 + E) + (1-P) \cdot 0$, or just $E = \frac{P}{1-P}$.</p>

<p>In other words, at each stage of the recursion, there is a probability $P$ that we get one bounce and then start the process over again, and a probability $1-P$ that the bounce fails and no additional bounces are counted. </p>
"
"2385098","2385100","<p>In my opinion, this is a strangely formulated question, as it provides you with information that you do not need.</p>

<p>The definition of independence is $P(A \cap B) = P(A) \cdot P(B)$.
From the equation in your image, we have
$$P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A) \cdot P(B)}{P(B)} = P(A),$$
which only needs independence of $A$ and $B$,
and does not require knowledge any of the probabilities' actual values (besides $P(B)&gt;0$).</p>

<hr>

<p>Side note: Your problem asks you to show independence implies $P(A \mid B) = P(A)$. It is not hard to show the reverse implication holds. Thus $P(A \mid B) = P(A)$ is an equivalent definition of independence, and has an intuitive interpretation: knowledge or ignorance of event $B$ does not affect the probability of event $A$.</p>
"
"2385103","2385109","<p>Let $c$ be the amount of nitrogen in coffee, and let $s$ be the amount of nitrogen in sawdust. That means there is $20c$ carbon in coffee, and $325s$ carbon in sawdust.</p>

<p>Note that when we mix coffee and sawdust, we have $c+s$ nitrogen, and $20c+325s$ carbon. We want the carbon-to-nitrogen ratio to be $27:1$, which means the carbon content must be $27(c+s)$.</p>

<p>We have to therefore set $20c+325s$ and $27c+27s$ as equal.</p>

<p>$$20c+325s = 27c+27s$$</p>

<p>$$298s = 7c$$</p>

<p>$$\fbox{s/c = 7/298}$$</p>
"
"2385104","2385122","<p>Your claim that $7$ does not generate the system is not correct. It should be $6$ that does not.  Any number coprime to the system will do so, which is the result you are looking for.  To show that 
$$\begin{array}{|c|c|c|}
7n &amp; 7 &amp; 14 &amp; 21 &amp; 28 &amp; 35 &amp; 42 &amp; 49 &amp; 56 &amp; 63 &amp; 70 &amp; 77 &amp; 84 \\ 
\mod 12 &amp; 7 &amp; 2 &amp; 9 &amp; 4 &amp; 11 &amp; 6 &amp; 1 &amp; 8 &amp; 3 &amp; 10 &amp; 5 &amp; 0
\end{array}$$</p>
"
"2385107","2388198","<p>It seems like there might be some typos in your question. Firstly, $S_t$ is not a standard Brownian motion since it has a non-zero ""drift term"" and non-unity ""diffusion coefficient"". Secondly, the equation:
$$ dS_t = \mu \,dt + \sigma\,dW_t $$
has solution $$ S_t=\mu t+\sigma W_t +S_0 $$
On the other hand, <a href=""https://en.wikipedia.org/wiki/Geometric_Brownian_motion"" rel=""nofollow noreferrer"">geometric Brownian motion</a> (GBM) satisfies:
$$ dX_t = X_t( \mu \,dt + \sigma\,dW_t ) $$
and has solution (as you found) $$ X_t=X_0\exp\left( \left[\mu-\frac{\sigma^2}{2}\right]t+\sigma W_t \right) $$
In any case, let's apply <a href=""https://math.stackexchange.com/questions/1465324/it%C3%B4s-formula-differential-form"">Ito's lemma</a> to GBM: $$ df(t,X_t)=\partial_t f(t,X_t)\,dt+\partial_xf(t,X_t)\,dX_t+\frac{1}{2}\partial_{xx}f(t,X_t)[dX_t]^2 $$
So we set $U_t= 1/X_t$ (i.e. $U_t=f(X_t)$, where $f(X_t)=X_t^{-1}$. Thus, by Ito's Lemma:
\begin{align}
dU_t &amp;= -X_t^{-2}\,dX_t+\frac{1}{2}(2X_t^{-3})\sigma^2X_t^2\,dt\\
&amp;= \frac{-X_t}{X_t^2}(\mu\,dt + \sigma\,dW_t)+\frac{\sigma^2}{X_t}dt\\
&amp;= \frac{1}{X_t}(-\mu\,dt-\sigma\,dW_t+\sigma^2\,dt)\\
&amp;= U_t([\sigma^2-\mu]dt-\sigma\,dW_t)
\end{align}
This is not the same as what you get; perhaps you are missing a $U_t$ factor?</p>

<hr>

<p>Note: in general, if $Y_T=(X_t)^\alpha$ and $X_t$ is 1D GBM, then
$$ dY_t = Y_t\left[ \left( \alpha\mu +\frac{1}{2}\alpha(\alpha-1)\sigma^2 \right)dt+\alpha\sigma\,dW_t \right] $$</p>
"
"2385111","2385115","<p>You can use the cross product. $|\vec a \times \vec b|=ab\sin\theta$, where $\theta$ is the angle between the two vectors</p>
"
"2385120","2386002","<p>As youâve observed, the projected line segment is not symmetric about the view line to the circleâs centerâthe center shifts. Another reason that you might be having some trouble working this out is that the apparent width of the ellipse also depends on the visual angle subtended by the circle, which varies with both the distance of the viewpoint from the center of the circle and the circleâs radius.  </p>

<p><a href=""https://i.stack.imgur.com/GoR0B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GoR0B.png"" alt=""enter image description here""></a></p>

<p>(Diagram provided by the OP.)</p>

<p>This problem can be reduced to finding the intersection of pairs of lines, which is easily done using homogeneous coordinates. We place the circle on the $x$-$y$ plane, centered at the origin, and Let the viewpoint be $V=(0,d\cos\theta,d\sin\theta)$, ($d&gt;0$). Weâll first look at whatâs going on in the $y$-$z$ plane. In the above diagram, the horizontal direction represents $z$ and the vertical $y$. To reduce clutter, the $x$-coordinate will be suppressed and, at the risk of introducing some confusion, the others will be designated $x'$ and $y'$, respectively.  </p>

<p>The line perpendicular to $\overline{OV}$ that passes through the upper point $A(0,r)$ is $y'=r-x'\cot\theta$, which we can rewrite in normal form as $x'\cos\theta+y'\sin\theta-r\sin\theta=0$. The homogeneous vector that represents this line consists of the coefficients of the latter equation: $$\mathbf l=(\cos\theta,\sin\theta,-r\sin\theta).$$ The line through $V$ and the lower point $B(0,-r)$ is represented by the cross product of the homogeneous coordinates of these points: $$\mathbf m=(d\cos\theta,d\sin\theta,1)\times(0,-r,1)=(r+d\sin\theta,-d\cos\theta,-dr\cos\theta).$$ Their intersection is $$\mathbf l\times\mathbf m=(-dr\sin2\theta,dr\cos2\theta-r^2\sin\theta,-(d+r\sin\theta))$$ which in Cartesian coordinates is $$C=\left({dr\sin2\theta\over d+r\sin\theta},{r^2\sin\theta-dr\cos2\theta\over d+r\sin\theta}\right).$$ The minor axis length of the circleâs image is $AC$, which you can compute using the standard formula for the distance between two points as $${2dr\cos\theta\over d+r\sin\theta}.$$ Compared to your initial guess, thereâs an extra factor of ${d\over d+r\sin\theta}$ that accounts for the asymmetry of the view and the âangular size correctionâ factor. As $d\to\infty$, so that the projection becomes closer and closer to parallel, this correction factor approaches unity.</p>

<hr>

<p>This isnât the whole story, though. Rays from the viewpoint through points on the circle converge as they get closer to the viewpoint, so the other semi-axis of the circleâs projection isnât going to be equal to $r$, either. To work out what this is, first project the center of the ellipse back to the $x$-$y$ plane. This center is the midpoint of $A$ and $C$ and its pre-image can be found by intersecting lines again: $$\begin{align}D &amp;= \frac12\left((0,r)+\left({dr\sin2\theta\over d+r\sin\theta},{r^2\sin\theta-dr\cos2\theta\over d+r\sin\theta}\right)\right) \\ &amp;=\left(\frac{d r \sin (\theta ) \cos (\theta )}{d+r \sin (\theta )},\frac{r \sin (\theta ) (d \sin (\theta )+r)}{d+r \sin (\theta )}\right)\end{align}$$ and its back-projection is $$\overline{VD}\times\overline{OA}=(V\times D)\times(-1,0,0)=\left(0,\frac{d r^2 \sin (\theta ) \cos (\theta )}{d+r \sin (\theta )},\frac{d^2 \cos (\theta )}{d+r \sin (\theta )}\right)$$ which becomes $\left(0,\frac{r^2}d\sin\theta\right)$ in Cartesian coordinates.  </p>

<p>Going back to 3-D coordinates, this is the point $\left(0,\frac{r^2}d\sin\theta,0\right)$. The $x$-coordinates of the points on the circle with this $y$-coordinate are $\pm\sqrt{r^2-y^2}=\pm r\sqrt{1-\left(\frac rd\sin\theta\right)^2}$. Iâll leave finding the projections of these points and the resulting semi-axis length to you (hint: you can use similar triangles).</p>
"
"2385121","2385308","<p>Such a functional may not exist at all!! You would have to assume at least that $b\notin ri(K)$. Consider </p>

<p>$$V= \mathbb{R}^2,\; U=\{(x,y)\in V: x=0\}.$$ Also, put $a=x=(0,0),\; b= (1,0)$ and </p>

<p>$$K=[0,2]\times \{0\}.$$ Assume that a functional $f$ satisfying your conditions exist. So there exists $m,n\in \mathbb{R}$ such that </p>

<p>$$f(x,y)=mx+ny.$$ Note that since $U$ is a subspace and the level set of a linear functional, it can only be $\beta=0.$ This means that $U$ is necesarily the Kernel of $f,$ and so we will have $n=0.$ Hence $f(x,y)=mx.$ We must also have </p>

<p>$$f(x,y)\leq f(b)\;\forall\;(x,y)\in K,$$ which is equivalent to </p>

<p>$$mx\leq f(b)=m \;\forall\; x\in [0,2],$$ a contradiction.</p>

<p>$\textbf{EDIT:}$ Since the question was edited, now I will show that this new statement is not true either. </p>

<p>Choose $$V= \mathbb{R}^2,\; U=\{(x,y)\in V: x=0\}$$ and $K=[-1,1]\times\{0\}.$ Hence $x=(0,0).$ Assume that such a functional $f$ exists. Then necessarily $f(x)=0$ Then, if $f(y_1,y_2)=my_1+ny_2,$ we must have </p>

<p>$$U=\{y \in V: f(y)=0\},$$ or $n=0.$ Then we should have</p>

<p>$$f(y)= my_1\leq 0=f(x)$$ for each $y_1\in [-1,1],$ a contradiction. </p>
"
"2385126","2385143","<p>It follows from the <em>uniqueness</em> of the CRT solution: $\,x\equiv y\pmod{pq}\,\Rightarrow\, x\equiv y $ mod $p$ and mod $q$ so it is a solution, and by CRT the solution is unique $\!\pmod{pq}$.</p>

<p>But there is no need to use CRT since $\,p,q\mid x-y\,\Rightarrow\, {\rm lcm}(p,q)=pq\mid x-y,\,$ by $\gcd(p,q)=1.\ $ This is exactly how CRT uniqueness is usually proved, e.g. <a href=""https://math.stackexchange.com/a/614256/242"">see here.</a></p>
"
"2385127","2385158","<p>It's not possible for a section to satisfy $d\sigma_p=0$. That's because the projection $\pi\colon E\to M$ is a left inverse for it: $\pi\circ\sigma = \operatorname{Id}_M$. This implies $d\pi_{\sigma(p)}\circ d\sigma_p = \operatorname{Id}_{T_pM}$, which implies $d\sigma_p$ is injective for every $p$.</p>
"
"2385130","2385146","<p>You have correctly shown that $x \ge \sqrt[3]3.\ $  Now if $x^n \ge 3$ and $x \gt \sqrt [3]3, \lfloor x^{n+1}\rfloor= \lfloor x^n\cdot x\rfloor\ge\lfloor x^n\rfloor+\lfloor x^n(x-1)\rfloor \ge \lfloor x^n \rfloor +1$ so any higher $x$ will work as well, so $\sqrt[3]3$ is the smallest $x$</p>
"
"2385132","2386048","<p>$1-xy$ is right invertible <em>by an element of $S$</em>, but why should that element happen to be in $R$? Maybe there aren't any right inverses in$R$ after all!</p>

<p>That is the gap in the reasoning you gave.</p>
"
"2385133","2385136","<p>No: imagine rotating by any angle about the axis which points in the direction of the vector $v$.  Every such rotation fixes $v$.  Explicitly, these are the rotations given by matrices of the form $$\begin{pmatrix} \cos \theta &amp; -\sin \theta &amp; 0 \\ \sin \theta &amp; \cos \theta &amp; 0 \\ 0 &amp; 0 &amp; 1\end{pmatrix}.$$</p>
"
"2385139","2385151","<p>Seems correct to me</p>

<p>\begin{array}{c:c}p &amp; r &amp; q &amp; \neg r &amp; (Â¬r \to p) &amp; (r \to q) &amp; (Â¬r \to p) \wedge (r \to q)
\\ \hdashline 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0
\\ \hdashline 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0
\\ \hdashline 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0
\\ \hdashline 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1
\\ \hdashline 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1
\\ \hdashline 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1
\\ \hdashline 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0
\\ \hdashline 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1
\end{array}</p>

<p>Edit: Ah.  Trevor has spotted what may have happened.</p>
"
"2385150","2385160","<p>Power of $a$ in the numerator = $\dfrac{-5}{2} \cdot 2 = -5$</p>

<p>Power of $a$ in the denominator = $3$</p>

<p>Overall power of $a$ in the final equation = $-5 - 3 = -8$</p>

<p>Power of $b$ in the numerator = $2$</p>

<p>Power of $b$ in the denominator = $\dfrac{1}{4} \cdot 6 = \dfrac{3}{2}$</p>

<p>Overall power of $b$ in the final equation = $2 - \dfrac{3}{2} = \dfrac{1}{2}$</p>

<p>Hence the answer is $a^{-8} \cdot b^{\frac{1}{2}} = \boxed{\dfrac{\large{b}^{\frac{1}{2}}}{\large{a}^8}}$</p>
"
"2385152","2385157","<p>The only place I remember seeing special notation is in Steven Roman's ""Advanced Linear Algebra"".</p>

<blockquote>
  <p>Let $A \in \mathcal{M}_{n \times m}(\mathbb{F}),\, A=[a_{ij}].$ If $B \subseteq\{1,\ldots,n\}$ and $C\subseteq\{1,\ldots,m\},$ then 
  $\,A[B,C]\,$ denotes the submatrix of $A$ that results in removing all rows that are not in $B$ and all columns that are not in $C.$</p>
</blockquote>

<p>I don't know if this notation is widely used, but I hope it helps.</p>
"
"2385159","2385230","<p>I will only answer question 1 and only from the model theorist's perspective: $C(X)$ is not elementary equivalent to $C(Y)$ (i.e. we can find a sentence in the language of rings with is true in one but false in the other). This implies that they are also not isomorphic (your remark above demonstrates that they are not ""canonically"" isomorphic). </p>

<p>Let $\rho(x) =(\exists y)(x\cdot y=1)$; $\varphi(x)=(\exists y)(y^{2}=x)$; 
 and $\psi(x)= \varphi(x) \wedge (\forall y \neq \mathbf{0})(\varphi(y)\to\neg\varphi(x-y))$. $\rho(x)$ is the collection of elements which have an inverse, $\varphi(x)$ is the collection of elements which are squares, and $\psi(x)$ is the collection of elements which are both squares and for any other (non-zero) square, their difference is not a square (we think of $\psi(x)$ as elements which are positive and close to $0$. For instance, the identity function from $X \to X$ satisfies this condition). </p>

<p>Claim 1: Let $f \in C(X)$ or $f \in C(Y)$. Then, $f$ is a square iff $f \geq 0$. </p>

<p>Claim 2: If $f \in C(X)$ and $\psi(f)$ is true, then $\exists x_0$ such that $f(x_0) = 0$. This follows from the fact that $X$ is compact. However, this is not true for $g \in C(Y)$. Consider $g = id_Y$. </p>

<p>Claim 3: If $f \in C(X)$ and $\psi(f)$ is true, then $f$ does not have a multiplicative inverse in $C(X)$ (since $f(x_0) = 0$ for some $x_0$). However, this is not true in $C(Y)$ as remarked above. </p>

<p>Claim 4: $C(X)\models\forall x(\psi(x)\to\neg\rho(x))$ and $C(Y) \models \exists x(\psi(x) \wedge \rho(x))$. </p>

<p>Therefore, the two structures are not elementary equivalent in the language of rings. </p>

<hr>

<p>Furthermore, we can define a partial order on these structures (in the ring language) by saying that $f \leq g \iff \varphi(g -f)$. Now, $C(X)$ is archimedean with respect to this partial order (and the unique image of $\mathbb{Z}$ mapped into this structure) while $C(Y)$ is not. </p>
"
"2385167","2385359","<p>A function has an asymptote of the form $y =  mx + b$ if 
$$\lim_{x \to \infty} |f(x) - (mx + b)| = 0$$
or 
$$\lim_{x \to -\infty} |f(x) - (mx + b)| = 0$$
If $m = 0$, the asymptote is horizontal.  Otherwise, it is oblique.</p>

<p>However, the domain of $f(x) = \log (4 - x^2)$ is $(-2, 2)$ since we require that the argument of the logarithm be positive, which implies 
\begin{align*}
4 - x^2 &amp; &gt; 0\\
4 &amp; &gt; x^2\\
2 &amp; &gt; |x|
\end{align*}
Thus, $|x|$ cannot approach infinity, so the function has neither a horizontal nor an oblique asymptote.</p>

<p>Observe that 
$$\lim_{x \to 2^-} (4 - x^2) = 0$$
and that 
$$\lim_{x \to -2^+} (4 - x^2) = 0$$
Therefore,
$$\lim_{x \to 2^-} \log(4 - x^2) = -\infty$$
and 
$$\lim_{x \to -2^+} \log(4 - x^2) = -\infty$$
Thus, $f(x) = \log(4 - x^2)$ has vertical asymptotes at $x = 2$ and $x = -2$.</p>

<p>A graph of $f$ is shown below for base $e$.  If you meant base $10$, multiply each $y$-coordinate of the graph by 
$$\frac{\log_e x}{\log_e 10}$$
Note that the choice of base does not affect the location of the vertical asymptotes.</p>

<p><a href=""https://i.stack.imgur.com/omJ2j.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/omJ2j.jpg"" alt=""graph_of_logarithmic_function""></a></p>
"
"2385172","2385179","<p>$\mathbb{Z}/p\mathbb{Z}=\mathbb{Z}_p$ for any integer $p$. Suppose $p$ is prime. Now to prove that $\mathbb{Z}_p$ is a field it suffices to prove that $\mathbb{Z}_p$ is an integral domain, because any finite integral domain is a field. So to obtain a contradiction let's suppose $a$ is a zero divisor in $Z_p$. Then $a\neq0$, i.e. $p\nmid a$, and choose $b\in \mathbb{Z}_p$ with $b\neq 0$, i.e. $p\nmid b$, such that $ab=0$ or rather $p\mid ab$. But since $p$ is prime $p\mid a$ or $p\mid b$ which is a contradiction. So $\mathbb{Z}_p$ is an integral domain which is finite. Hence it is a field.</p>

<hr>

<p>Or we can straightaway prove that any nonzero member of $\mathbb{Z}_p$ has an inverse whenever $p$ is prime. In fact in general the invertible members of $\mathbb{Z}_n$ are precisely those members that are coprime with $n$. Prove this as an exercise.</p>

<hr>

<p>In your case we need to prove that $1,2$ have inverses in $\mathbb{Z}_3$. So observe that $$1._3 1=1$$ and $$2._3 2=1.$$ So every nonzero member of $\mathbb{Z_3} $ has an inverse. </p>
"
"2385176","2385194","<p>You're almost done. If you factor out $\frac{1}{2abc}$ from the expression you obtained, you get
$$\frac{1}{2abc}\left(\frac{a^2+b^2-c^2}{c^2}+\frac{a^2+c^2-b^2}{b^2} + \frac{b^2+c^2-a^2}{a^2}\right).$$
So you just need to prove $\frac{a^2+b^2-c^2}{c^2}+\frac{a^2+c^2-b^2}{b^2} + \frac{b^2+c^2-a^2}{a^2}\ge 3$. Writing
\begin{align}&amp;\frac{a^2+b^2-c^2}{c^2}+\frac{a^2+c^2-b^2}{b^2} + \frac{b^2+c^2-a^2}{a^2} \\
&amp;= \left(\frac{a^2}{b^2}+\frac{b^2}{a^2}\right) + \left(\frac{a^2}{c^2}+\frac{c^2}{a^2}\right) + \left(\frac{b^2}{c^2}+\frac{c^2}{b^2}\right) - 3\end{align}
it is enough to show that each of the terms in the parentheses is at least $2$. Why is that true?</p>
"
"2385189","2385198","<p>Put the vectors $v_1,\dotsc,v_4$ into the columns of a matrix
$$
A=\left[\begin{array}{rrrr}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 4 &amp; 2 \\
0 &amp; -1 &amp; -3 &amp; 0
\end{array}\right]
$$
The columns of $A$ corresponding to pivots in $\DeclareMathOperator{rref}{rref}\rref(A)$ form a basis of $\Bbb R^3$. In our case, 
$$
\rref(A)=\left[\begin{array}{rrrr}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; -6 \\
0 &amp; 0 &amp; 1 &amp; 2
\end{array}\right]
$$
Hence $\{v_1,v_2,v_3\}$ is a basis of $\Bbb R^3$.</p>
"
"2385196","2385213","<p>Here ""without loss of generality"" is in the context of describing fields as splitting fields of polynomials.  So what it means is that if $E$ is any field extension of $F$ that is the splitting field of some polynomial $f(x)\in F[x]$, then there exists a polynomial $f_0(x)\in F[x]$ such that $E$ is also the splitting field of $f_0(x)$ and all the roots of $f_0(x)$ in $E$ are distinct.  (In fact, the discussion shows something stronger: the roots of this $f_0(x)$ are the same as the roots of $f(x)$, just with the repeats removed.)</p>
"
"2385205","2389533","<p>To answer your question directly: <strong>yes</strong>, your answer is logically correct and it is very clear. </p>

<hr>

<p>If you want the proof to be slightly less lengthy, the simple algebra in (2) could have been written in just one line: the equality $1/p+1/q=1$ implies that
$$
(p-1)q=(p-1)\cdot\frac{1}{1-1/p}=p.
$$</p>

<p>Also, if you want to save your labor to type/write the integral sign, use the norm notation $\|\cdot\|_p$.</p>
"
"2385206","2385211","<p>No. If $\lim_{x\to\infty}f(x)=L$ and $L\ne0$, then $\int_0^\infty f(t)\ dt$ does not converge.</p>

<p>To see this, suppose without loss of generality that $L&gt;0$. Since $\lim_{x\to\infty}f(x)=L$, there exists $M&gt;0$ such that $f(x)&gt;L/2$ for all $x&gt;M$. Then $\int_M^\infty f(t)\ dt \geq \int_M^\infty (L/2)\ dt = \infty$ shows that
$\int_0^\infty f(t)\ dt$ diverges.</p>

<p>Similarly, we cannot have a function whose absolute minimum is $1$ be integrable. However, the function
$$
f(x) = \begin{cases}
x+1 &amp; \text{if}\ 0 \leq x &lt;1,\\
-x+3 &amp; \text{if}\ 1\leq x &lt; 3,\\
0 &amp; \text{if}\ 3 \leq x,
\end{cases}
$$
has a local minimum of $1$ at $x=0$, and
$$
\int_0^\infty f(t)\ dt = \int_0^3 f(t)\ dt = 3.5
$$
converges.</p>
"
"2385212","2385316","<p>We have $(z^5+3z^3+4z^2+1)e^\frac{1}{z^2}=(z^5+3z^3+4z^2+1)\sum_{n=0}^\infty\frac{1}{n!}z^{-2n}=(z^5+3z^3+4z^2+1)(1+z^{-2}+\frac{1}{2}z^{-4}+\frac{1}{6}z^{-6}+\dots)=\frac{5}{3}z^{-1}+\dots$</p>

<p>so $\text{Res}_0(f)=\frac{5}{3}$, hence $\oint_{|z|=1}f(z)dz=2\pi i\cdot\frac{5}{3}$</p>
"
"2385229","2385237","<p>Since $v(x)\geq 1$ for all $x\in\mathbb X$ and $\|v\|_v=1$, we get $v\in B(\mathbb X)_+$.<p>
<strong>Claim:</strong> $v$ is in the interior of $B(\mathbb X)_+$.<p>
<strong>Proof:</strong> <p></p>

<blockquote class=""spoiler"">
  <p>Let be $\varepsilon\in(0,1)$. Suppose there exists $f\in B(\mathbb X)$ such that $\|f-v\|_v&lt;\varepsilon$ and there exists $x_0\in\mathbb X$ such that $f(x_0)&lt;0$. Then $$|f(x_0)-v(x_0)|=v(x_0)-f(x_0)&gt;v(x_0)$$and we conclude$$\|f-v\|_v\geq \frac{|f(x_0)-v(x_0)|}{v(x_0)}&gt;\frac{v(x_0)}{v(x_0)}=1&gt;\varepsilon.$$This is a contracation and therefore$$\{f\in B(\mathbb X)~:~\|f-v\|_v&lt;\varepsilon\}\subset B(\mathbb X)_+$$and the interior of $B(\mathbb X)_+$ is not empty.</p>
</blockquote>
"
"2385233","2385333","<p>It is not true in general that $\ker r$ is a normal subgroup of $r$, although it is a subgroup.</p>

<p>It is true that $\ker r$ surjects onto $C$. Since the action of $B$ on $A$ is defined using an action of $C$ on $A$ and the map $p$, it follows that $i(A)$ acts trivially on $A$.</p>

<p>So for $c \in C$, there exists $b \in B$ with $p(b)=c$. Let $r(b) = a$. Then</p>

<p>$$r(i(a^{-1})b) = ri(a^{-1})\,\, {}^{i(a^{-1})}r(b) = a^{-1}a=1,$$
and $p(i(a^{-1})b) = c$.</p>
"
"2385247","2385259","<p><strong>Hint:</strong> Notice that $$xy^2+y+11 \mid x(xy^2+y+11)-y(x^2y+x+y)=11x-y^2$$</p>

<p>Then you have three cases to check: $11x-y^2=0$, $11x-y^2&lt;0$, and $11x-y^2&gt;0$.</p>
"
"2385250","2385279","<p>Start finding the shorter diagonal $AC$</p>

<p>If we have $A(3,-4)$ and $C(1,2)$ then $AC=\sqrt{40}=2\sqrt{10}$</p>

<p>Call $O$ the midpoint of $AC$ and $B$ and $D$ the other vertices. From Pythagoras theorem $OB=\sqrt{BC^2-OC^2}=\sqrt{50-10}=\sqrt{40}=2\sqrt{10}$</p>

<p>The larger diagonal is $BD=4\sqrt{10}$ so the area of the rhombus is $\mathcal{A}=40$ and the height of the rhombus is $h=\dfrac{\mathcal{A}}{BC}=\dfrac{40}{5\sqrt{2}}=4\sqrt{2}$</p>

<p>$h=4\sqrt 2$</p>

<p><a href=""https://i.stack.imgur.com/59waS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/59waS.png"" alt=""enter image description here""></a></p>
"
"2385253","2385261","<p>The indefinite integral is 
$$\int \frac{1}{x^2-2 x-3} \, dx=\frac{1}{4} \left[\log |3-x|-\log (x+1)\right]+C$$
Indeed the fraction can be split in the sum of two fractions</p>

<p>$\dfrac{1}{(x-3) (x+1)}=\dfrac{a}{x-3}+\dfrac{b}{x+1}=\dfrac{(a+b) x+a-3 b}{(x-3) (x+1)}$</p>

<p>which is equal to the given  fraction if </p>

<p>$\left\{ \begin{gathered}
  a + b = 0 \hfill \\
  a - 3b = 1 \hfill \\ 
\end{gathered}  \right.\rightarrow$ 
$\left\{ \begin{gathered}
  a = \frac{1}{4} \hfill \\
  b =  - \frac{1}{4} \hfill \\ 
\end{gathered}  \right.\rightarrow \dfrac{1}{(x-3) (x+1)}=\dfrac{1}{4}\left(\dfrac{1}{x-3}-\dfrac{1}{x+1}\right)$</p>

<p>If we write the given definite integral as you did
$$\int _0 ^3\frac{dx}{x^2 - 2x - 3} + \int _3 ^4\frac{dx}{x^2 - 2x - 3}=\\=\frac{1}{4} \left[\log |3-x|-\log (x+1)\right]_0^3+\frac{1}{4} \left[\log |3-x|-\log (x+1)\right]_3^4$$
we see that the first tends to $+\infty$ as $x\to 3$ because of the logarithm</p>

<p>So the integral diverges</p>

<p>Hope this helps</p>
"
"2385257","2385291","<p>You have $\frac{A}{B+x*C} = \frac x2$</p>

<p>You can pass $\frac x2$ to the other side, it gives you $\frac{A}{B+x*C} - \frac x2 = 0$</p>

<p>Now you simply put every term on the same denominator, by multiplying your left member by 2 (both upper and lower terms) and your right member by $B+x*C$ ; it gives you $\frac{2A}{2(B+x*C)} - \frac{x(B+x*C)}{2(B+x*C)} = 0$</p>

<p>You can now simplify it : $\frac{2A-x(B+x*C)}{2(B+x*C)} = 0$ (note that it only exists for $2(B+x*C) \neq 0$)</p>

<p>Finally, you should know that $\frac{2A-x(B+x*C)}{2(B+x*C)} = 0$ <strong>ONLY IF</strong> $2A-x(B+x*C) = 0$.</p>

<p>You can now solve it, but be sure to verify that your x isn't a solution of $2(B+x*C) = 0$, it would mean that you have a division by 0, which is impossible...</p>

<p>I hope I correctly solved your problem =)</p>
"
"2385260","2385267","<p>Any unbounded harmonic function on $\mathbb{R}^n$ shows why the limiting condition is necessary; for example, $u(x_1, ..., x_n) = x_1$. The issue here is that there is another part of the boundary, the point at infinity. This is easier to visualize if you think of $\mathbb{R}^2$ and the Riemann sphere, for example.</p>

<p>This should indicate how to proceed with the proof. Spoilers below.</p>

<blockquote class=""spoiler"">
  <p> So the maximum is attained on the boundary, and the value at infinity is zero after we extend the function to make sense there. If $f$ is identically zero, we're done. Else, the maximum doesn't occur at infinity. </p>
</blockquote>
"
"2385276","2385285","<ol>
<li><p>$f$ is a probability function $ \iff c \sum_{k=1}^n k^2=1$</p></li>
<li><p>$E(X)=c \sum_{k=1}^n k^3$</p></li>
</ol>

<p>Your turn !</p>
"
"2385286","2385313","<p>A standard deviation refers to a probability distribution, but of course we don't actually know what's the probability distribution for the $\text{km}$ you'll jog on a given.
We can naively estimate this distribution as follows.</p>

<p>Let $N$ be your sample size, meaning your log has $N$ entries (or days). Here, the $i$-th entry is the number $x_i$ of $\text{km}$ you ran on day $i$, including days when you ran no $\text{km}$ at all $($in this case, $x_i=0)$. To each value $v$ on your log we'll associate the probability</p>

<p>$$p_v=\frac1N\cdot\#\{\text{entries with value $v$ in your log}\}.$$</p>

<p>The expected value of the number of $\text{km}$ ran on a given day will then be</p>

<p>$$\mu=\sum_v\,v\cdot p_v$$</p>

<p>and the standard deviation will be</p>

<p>$$\sigma=\sqrt{\sum_v^{}\,p_v\cdot {(v-\mu)}^2}$$</p>

<hr>

<p><strong>Example:</strong> Suppose your $2$-month log has $61$ days, with the following distribution:</p>

<ul>
<li>$9$ days with $0$ $\text{km}$ jogs</li>
<li>$38$ days with $1$ $\text{km}$ jogs</li>
<li>$9$ days with $2$ $\text{km}$ jogs</li>
<li>$5$ days with $3$ $\text{km}$ jogs</li>
</ul>

<p>Then our set of values $v$ is $\{0,1,2,3\}$ and we have</p>

<p>$$
\begin{array}{cc}
p_0=\frac{9}{61}&amp;&amp;p_1=\frac{38}{61}&amp;&amp;
p_2=\frac{9}{61}&amp;&amp;p_3=\frac{5}{61}
\end{array}
$$</p>

<p>Our mean jog has a length of</p>

<p>$$\mu=0\cdot\frac{9}{61}+1\cdot\frac{38}{61}+2\cdot\frac{9}{61}+3\cdot\frac{5}{61}=\frac{71}{61}\simeq 1.164$$</p>

<p>kilometres, and our standard deviation will hence be</p>

<p>\begin{align}
\sigma
&amp;=\sqrt{
\frac{9}{61}\cdot{\left(0-\frac{71}{61}\right)}^2+
\frac{38}{61}\cdot{\left(1-\frac{71}{61}\right)}^2+
\frac{9}{61}\cdot{\left(2-\frac{71}{61}\right)}^2+
\frac{5}{61}\cdot{\left(3-\frac{71}{61}\right)}^2}\\
&amp;=\sqrt{
\frac{9\cdot 71^2}{61^3}+
\frac{38\cdot 10^2}{61^3}+
\frac{9\cdot 51^2}{61^3}+
\frac{5\cdot 112^2}{61^3}}\\
&amp;=\sqrt{
\frac{45369+3800+23409+62720}{61^3}
}\\
&amp;=\sqrt{\frac{135298}{61^3}}\simeq 0.772
\end{align}</p>

<p>kilometres.</p>

<hr>

<p>The standard deviation is a measure of how closely you follow your schedule in the following sense:
the closer your values $v$ are to the mean $\mu$, the smaller the standard deviation $\sigma$.
For instance, if you had run $1$ $\text{km}$ every day, then $\sigma=0$.</p>

<p>The thing is, the standard deviation in principle does not really care that your schedule is $1$ $\text{km}$ per day...
If you had run $0$ $\text{km}$ every day, or $3$ $\text{km}$ day, then you'd also have $\sigma = 0$.</p>

<p>With this in mind, what you <em>can</em> do is take a page from least squares and calculate how far from your scheduled $1$ $\text{km}$ your jogs are, on average.
In symbols, we'll be looking at</p>

<p>$$\epsilon=\sqrt{\frac{1}{N}\sum_{i=1}^N\,{\left(x_i-1\right)}^2}.$$</p>

<p>Grouping the $x_i$ by their values $v$, it turns out that</p>

<p>$$\epsilon=\sqrt{\sum_v^{}\,p_v\cdot {(v-1)}^2}.$$</p>

<p>Looks familiar, huh?
It's like the calculation for $\sigma$, except here we're forcing $\mu=1$ (our scheduled value).
Using the numbers for our previous example, we'd get</p>

<p>\begin{align}
\epsilon
&amp;=\sqrt{
\frac{9}{61}\cdot{\left(0-1\right)}^2+
\frac{38}{61}\cdot{\left(1-1\right)}^2+
\frac{9}{61}\cdot{\left(2-1\right)}^2+
\frac{5}{61}\cdot{\left(3-1\right)}^2}\\
&amp;=\sqrt{
\frac{9\cdot 1}{61}+
\frac{38\cdot 0}{61}+
\frac{9\cdot 1}{61}+
\frac{5\cdot 4}{61}}\\
&amp;=\sqrt{
\frac{38}{61}} \simeq 0.789
\end{align}</p>

<p>Now, $\epsilon$ measures how closely you follow your schedule in the following sense:
the closer your values $v$ are to your intended schedule value $($ $1$ in this case$)$, the smaller the standard deviation $\sigma$.
If you had run $1$ $\text{km}$ every day, you'd still get $\epsilon=0$.</p>

<p>Moreover, we don't run into the same problems as we did with $\sigma$ before.
If you had run $n$ $\text{km}$ every day, then you'd have $\epsilon=|n-1|$ -- you can't get better than this for these cases!</p>
"
"2385295","2391153","<p>$$2f(x)=2\cos^2\theta+2\cos^2(x+\theta)-2\cos x\cdot\cos(x+\theta)$$</p>

<p>$$=1+\cos2\theta+1+\cos2(x+\theta)-[\cos(2x+\theta)+\cos\theta]$$</p>

<p>To be independent of $x,$  clearly we need $$\cos2(x+\theta)-\cos(2x+\theta)=0$$</p>

<p>Using Prosthaphaeresis Formula,
$$\cos2(x+\theta)-\cos(2x+\theta)=-2\sin\dfrac\theta2\sin\left(2x+\dfrac{3\theta}2\right)$$</p>

<p>So, we need $\sin\dfrac\theta2=0$</p>

<p>Can you take it from here?</p>
"
"2385321","2385326","<p>You have a total of $100$ observations, so the median is just the average of the $50$th and the $51$st observation (when observations are ranked in ascending order, as they are here). </p>

<p>In which interval do you find these observations? Are the lengths of the intervals of any importance for this question? </p>

<p>The lengths of the intervals - actually only the length of the interval that contains these observations - may be of an importance when you want to calculate the exact value of the median (and not it's location, in terms of intervals, as you do here). </p>
"
"2385330","2385364","<p>Not that I am aware of. However, in 1952 Israel Halperin published his <em>Introduction to the Theory of Distributions</em> (University of Toronto Press), based upon Schwartz's lectures on that subject.</p>
"
"2385338","2385351","<p>As the disc is returned to the bag it will be the <em>same</em> probability of a red disc coming out each time. As it is <em>less than</em> four red discs, we need to find the probability of $0, 1, 2$ and $3$ red discs being chosen. Your values of $p$ and $q$ are indeed correct, however I'm a bit confused as to why you've chosen $n = 8$. There are $10$ trials, and as such I have chosen $n = 10$ and $r = 0, 1, 2$ and $3$, and putting this into the formula $$^nC_r\times p^{r}\times q^{n-r}$$ and adding up the four values for r you should get your answer.</p>

<p>Hope this helps :)</p>

<p>EDIT: Answer:</p>

<blockquote class=""spoiler"">
  <p>$(^{10}C_0 \times 0.4^0 \times 0.6^{10-0}) + (^{10}C_1 \times 0.4^1 \times 0.6^{10-1})...$<br> I work out the answer to be 0.382...</p>
</blockquote>
"
"2385339","2385538","<p>The condition $b^2-4ac\ge 0$ implies there are two roots (distinct or repetitive). </p>

<p>Assume the two roots are $x_1$ and $x_2$. Then:
$$a(x-x_1)(x-x_2)=ax^2+(\underbrace{-ax_1-ax_2}_{b})x+\underbrace{ax_1x_2}_{c}=0.$$
The given conditions:
$$\begin{cases}a+b+c\ge 0 \\ a-b+c\ge 0 \\
a-c\ge 0 \end{cases} \Rightarrow 
\begin{cases}a-ax_1-ax_2+ax_1x_2\ge 0 \\ a+ax_1+ax_2+ax_1x_2\ge 0 \\ a-ax_1x_2\ge 0 \end{cases} \Rightarrow
\begin{cases}(1-x_1)(1-x_2)\ge 0 \\ (1+x_1)(1+x_2)\ge 0 \\ x_1x_2\le 1\end{cases}$$</p>

<p>Thus $x_1,x_2\in[-1,1]$, otherwise the system of the three inequalities do not hold true. </p>
"
"2385346","2385363","<p>Given a simple undirected connected graph $G = (V,E)$. Edge $e \in E$ is called a cut edge if $G^\prime = (V,E-\{e\})$ has at least two non-empty connected components.</p>

<p><em>Theorem.</em> The edge $e \in E$ is a cut edge $\Leftrightarrow$ $e$ does not belong to a circuit in $G$.</p>

<p>$\Rightarrow$ Suppose $e \in E$ does belong to a circuit in $G$. Then, this circuit can be described as $e_1,\ldots,e_k$, with $e = e_i$ for some $1 \leqslant i \leqslant k$. Remove any $e_i$ in this circuit still allows the traversal $e_{i+1},\ldots,e_k,e_1,\ldots,e_{i-1}$, so $G$ is still connected, hence $e$ can not be a cut edge. Contradiction.</p>

<p>$\Leftarrow$ Suppose that after removing $e = \{v_1,v_2\}$ then $G$ is still connected. Then, consider the trail from $v_1$ to $v_2$. Adding $e$ to this trail creates a circuit (so there is a circuit in the original graph), hence a contradiction.</p>
"
"2385348","2386327","<hr>

<p>For each positive real number $a\in\mathbb{R}_{&gt;0}$ and continuously differentiable function $\phi\in C^{1}{\left[0,a\right]}$, define the function $\mathcal{F}$ via the double integral,</p>

<p>$$\mathcal{F}{\left[\phi\right]}{\left(a\right)}:=\int_{0}^{2a}\mathrm{d}x\int_{0}^{\sqrt{2ax-x^{2}}}\mathrm{d}y\,\frac{x\left(x^{2}+y^{2}\right)\phi^{\prime}{\left(y\right)}}{\sqrt{4a^{2}x^{2}-\left(x^{2}+y^{2}\right)^{2}}}.\tag{1}$$</p>

<hr>

<p>Given $a\in\mathbb{R}_{&gt;0}\land\phi\in C^{1}{\left[0,a\right]}$, we find by changing the order of integration</p>

<p>$$\begin{align}
\mathcal{F}{\left[\phi\right]}{\left(a\right)}
&amp;=\int_{0}^{2a}\mathrm{d}x\int_{0}^{\sqrt{2ax-x^{2}}}\mathrm{d}y\,\frac{x\left(x^{2}+y^{2}\right)\phi^{\prime}{\left(y\right)}}{\sqrt{4a^{2}x^{2}-\left(x^{2}+y^{2}\right)^{2}}}\\
&amp;=\int_{0}^{a}\mathrm{d}y\int_{a-\sqrt{a^{2}-y^{2}}}^{a+\sqrt{a^{2}-y^{2}}}\mathrm{d}x\,\frac{x\left(x^{2}+y^{2}\right)\phi^{\prime}{\left(y\right)}}{\sqrt{4a^{2}x^{2}-\left(x^{2}+y^{2}\right)^{2}}}\\
&amp;=\int_{0}^{a}\mathrm{d}y\,\phi^{\prime}{\left(y\right)}\int_{a-a\sqrt{1-\left(\frac{y}{a}\right)^{2}}}^{a+a\sqrt{1-\left(\frac{y}{a}\right)^{2}}}\mathrm{d}x\,\frac{x\left(x^{2}+y^{2}\right)}{\sqrt{4a^{2}x^{2}-\left(x^{2}+y^{2}\right)^{2}}}.\tag{2a}\\
\end{align}$$</p>

<p>Rescaling the integration variables by $a$, we then find</p>

<p>$$\begin{align}
\mathcal{F}{\left[\phi\right]}{\left(a\right)}
&amp;=\int_{0}^{a}\mathrm{d}y\,\phi^{\prime}{\left(y\right)}\int_{a-a\sqrt{1-\left(\frac{y}{a}\right)^{2}}}^{a+a\sqrt{1-\left(\frac{y}{a}\right)^{2}}}\mathrm{d}x\,\frac{x\left(x^{2}+y^{2}\right)}{\sqrt{4a^{2}x^{2}-\left(x^{2}+y^{2}\right)^{2}}}\\
&amp;=\int_{0}^{1}\mathrm{d}t\,\phi^{\prime}{\left(at\right)}\int_{1-\sqrt{1-t^{2}}}^{1+\sqrt{1-t^{2}}}\mathrm{d}u\,\frac{a^{3}u\left(u^{2}+t^{2}\right)}{\sqrt{4u^{2}-\left(u^{2}+t^{2}\right)^{2}}};~~~\small{\left[\left(x,y\right)\mapsto\left(au,at\right)\right]}\\
&amp;=a^{3}\int_{0}^{1}\mathrm{d}t\,\phi^{\prime}{\left(at\right)}\int_{1-\sqrt{1-t^{2}}}^{1+\sqrt{1-t^{2}}}\mathrm{d}u\,\frac{u\left(u^{2}+t^{2}\right)}{\sqrt{4u^{2}-\left(u^{2}+t^{2}\right)^{2}}}\\
&amp;=a^{3}\lim_{b\to1^{-}}\int_{0}^{b}\mathrm{d}t\,\phi^{\prime}{\left(at\right)}\int_{1-\sqrt{1-t^{2}}}^{1+\sqrt{1-t^{2}}}\mathrm{d}u\,\frac{u\left(u^{2}+t^{2}\right)}{\sqrt{4u^{2}-\left(u^{2}+t^{2}\right)^{2}}}.\tag{2b}\\
\end{align}$$</p>

<p>Now, it's obvious from the last line above that the integration over $u$ is independent of the parameter $a$, but it also happens to be independent of the other variable $t$, as we shall see shortly.</p>

<hr>

<p>Define the auxiliary function $J:\left[0,1\right)\rightarrow\mathbb{R}$ via the definite integral,</p>

<p>$$J{\left(t\right)}:=\int_{1-\sqrt{1-t^{2}}}^{1+\sqrt{1-t^{2}}}\mathrm{d}u\,\frac{u\left(u^{2}+t^{2}\right)}{\sqrt{4u^{2}-\left(u^{2}+t^{2}\right)^{2}}}.\tag{3}$$</p>

<p>Supposing $t\in\left[0,1\right)$ and defining the auxiliary parameter $\sqrt{1-t^{2}}=:r\in\left(0,1\right]$, we obtain</p>

<p>$$\begin{align}
J{\left(t\right)}
&amp;=\int_{1-\sqrt{1-t^{2}}}^{1+\sqrt{1-t^{2}}}\mathrm{d}u\,\frac{u\left(u^{2}+t^{2}\right)}{\sqrt{4u^{2}-\left(u^{2}+t^{2}\right)^{2}}}\\
&amp;=\int_{1-r}^{1+r}\mathrm{d}u\,\frac{u\left(u^{2}+1-r^{2}\right)}{\sqrt{4u^{2}-\left(u^{2}+1-r^{2}\right)^{2}}};~~~\small{\left[\sqrt{1-t^{2}}=:r\right]}\\
&amp;=\int_{1-r}^{1+r}\mathrm{d}u\,\frac{2u\left(u^{2}+1-r^{2}\right)}{2\sqrt{4r^{2}-\left(1+r^{2}-u^{2}\right)^{2}}}\\
&amp;=\frac12\int_{1-r}^{1+r}\mathrm{d}u\,\frac{2u\left(u^{2}+1-r^{2}\right)}{\sqrt{\left[2r+\left(1+r^{2}-u^{2}\right)\right]\left[2r-\left(1+r^{2}-u^{2}\right)\right]}}\\
&amp;=\frac12\int_{1-r}^{1+r}\mathrm{d}u\,\frac{2u\left(u^{2}+1-r^{2}\right)}{\sqrt{\left[\left(1+r\right)^{2}-u^{2}\right]\left[u^{2}-\left(1-r\right)^{2}\right]}}\\
&amp;=\frac12\int_{\left(1-r\right)^{2}}^{\left(1+r\right)^{2}}\mathrm{d}v\,\frac{\left(v+1-r^{2}\right)}{\sqrt{\left[\left(1+r\right)^{2}-v\right]\left[v-\left(1-r\right)^{2}\right]}};~~~\small{\left[u^{2}=v\right]}\\
&amp;=\frac12\int_{0}^{1}\mathrm{d}w\,4r\,\frac{2\left(2rw-r+1\right)}{\sqrt{\left[4r\left(1-w\right)\right]\left(4rw\right)}};~~~\small{\left[v=4rw+\left(1-r\right)^{2}\right]}\\
&amp;=\int_{0}^{1}\mathrm{d}w\,\frac{2rw-r+1}{\sqrt{w\left(1-w\right)}}\\
&amp;=\int_{0}^{1}\mathrm{d}w\,\frac{1-r}{\sqrt{w\left(1-w\right)}}+\int_{0}^{1}\mathrm{d}w\,\frac{2rw}{\sqrt{w\left(1-w\right)}}\\
&amp;=\int_{0}^{1}\mathrm{d}w\,\frac{1-r}{\sqrt{w\left(1-w\right)}}+\left[2r\sqrt{w\left(1-w\right)}\right]_{w=0}^{w=1}+\int_{0}^{1}\mathrm{d}w\,\frac{r}{\sqrt{w\left(1-w\right)}};~~~\small{I.B.P.s}\\
&amp;=\int_{0}^{1}\frac{\mathrm{d}w}{\sqrt{w\left(1-w\right)}}\\
&amp;=\pi.\tag{4}\\
\end{align}$$</p>

<hr>

<p>Finally, continuing with the main calculation from where we left off in the last line of $(2b)$ above, we obtain using result $(4)$ that</p>

<p>$$\begin{align}
\mathcal{F}{\left[\phi\right]}{\left(a\right)}
&amp;=a^{3}\lim_{b\to1^{-}}\int_{0}^{b}\mathrm{d}t\,\phi^{\prime}{\left(at\right)}\int_{1-\sqrt{1-t^{2}}}^{1+\sqrt{1-t^{2}}}\mathrm{d}u\,\frac{u\left(u^{2}+t^{2}\right)}{\sqrt{4u^{2}-\left(u^{2}+t^{2}\right)^{2}}}\\
&amp;=a^{3}\lim_{b\to1^{-}}\int_{0}^{b}\mathrm{d}t\,\phi^{\prime}{\left(at\right)}\,J{\left(t\right)}\\
&amp;=a^{3}\lim_{b\to1^{-}}\int_{0}^{b}\mathrm{d}t\,\phi^{\prime}{\left(at\right)}\,\pi\\
&amp;=\pi\,a^{3}\int_{0}^{1}\mathrm{d}t\,\phi^{\prime}{\left(at\right)}\\
&amp;=\pi\,a^{2}\int_{0}^{a}\mathrm{d}y\,\phi^{\prime}{\left(y\right)};~~~\small{\left[at=y\right]}\\
&amp;=\pi\,a^{2}\left[\phi{\left(a\right)}-\phi{\left(0\right)}\right].\blacksquare\\
\end{align}$$</p>
"
"2385356","2385498","<p>Fubini's theorem requires you to have two (not necessarily distinct) measure spaces. I think that if you spit this out explicitly, your life will be much easier. Here, they are $([0,T],\lambda)$, where $\lambda$ is the 1-dimensional Lebesgue measure and, $(\Omega, \mathscr F, \mathbb P)$ the probabiity space on which your BM is defined.</p>

<p>The stochastic process $X$ is a function of two variables $X:[0,T]\times\Omega\to\mathbb R,\;\;$ $X:(t,\omega)\mapsto X_t(\omega)$. $\mathbb E$ denotes integration with respect to the $\omega\in \Omega$ variable. So, 
\begin{align}
\mathbb E\int_0^Tf(t,X_t(\omega))dX_t(\omega)&amp;
=\int_\Omega\int_0^Tf(t,X_t(\omega))dX_t(\omega)d\mathbb P(\omega)\\
&amp;=\int_0^T\int_\Omega f(t,X_t(\omega))dX_t(\omega)d\mathbb P(\omega)\\
&amp;=\int_0^T\mathbb E\left[f(t,X_t(\omega))dX_t(\omega)\right].
\end{align}</p>

<p>This is all provided the hypotheses of Fubini's theorem hold, of course.</p>

<hr>

<p>This answer is formal as $dX_t$ is not actually a measure... but you'll get into those details if you decide to pursue stochastic analysis further.</p>
"
"2385365","2385404","<p>How about
$$R=\{a/(1+6b):a,b\in\Bbb Z\}?$$
This has two maximal ideals, generated by $2$ and $3$, yet is indecomposable (only idempotents are $\pm1$).</p>
"
"2385372","2385799","<p>This is the solution of the problem. I think that it is self explaining, but if you have any question feel free to ask</p>

<p>hope this helps
<a href=""https://i.stack.imgur.com/5D2qh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5D2qh.png"" alt=""enter image description here""></a></p>
"
"2385374","2385388","<p>we have $$P(a\cos^3(\theta);a\sin^3(\theta))$$ and $$y=mx+n$$ the straight line, then we plug in the coordsinate of P:
$$y=m(x-a\cos^3(\theta))+a\sin^3(\theta)$$  (I)
the given equation has the form
$$y=-\frac{\sec(\theta)}{\csc(\theta)}x+\frac{a}{\csc(\theta)}$$
then is $$m=\frac{\csc(\theta)}{sec(\theta)}$$ the searched slope
then you must plug in $m$ in (I)</p>
"
"2385380","2385421","<p><strong>$k$ is even</strong><br>
Assume that the edge connectivity is $1$, and we remove one edge to make the graph disconnected. Then, apart from the two end points of the removed edge, all vertices still have an even degree. Therefore, in each of the two connected components there is exactly one vertex of odd degree, and this violates the <a href=""https://en.wikipedia.org/wiki/Handshaking_lemma"" rel=""nofollow noreferrer"">handshaking lemma</a> (applied to a single connected component). Therefore we conclude that the edge connectivity cannot be $1$.</p>

<p><strong>$k$ is odd:</strong><br>
Consider the complete graph on $k+1$ vertices (each vertex then has degree $k$). We shall add an additional vertex to it in order to construct a counterexample. Take all except $2$ of the vertices, and divide them into pairs. For each pair, take the edge between the two, and let it go through the new vertex instead of directly between the two vertces in the pair. The new vertex now has degree $k-1$, while all the other vertices still have degree $k$. Take two of these graphs, and add an edge between the two ""new"" vertices. You now have a graph of edge connectivity $1$, where each vertex has degree exactly $k$, and therefore degree divisible by $k$.</p>
"
"2385382","2385398","<p>I am afraid this is physicists' notation.</p>

<p>You have $z=xy$.
Then $x=z/y$ and $y=z/x$. So
$$\left(\frac{\partial x}{\partial y}\right)_z=-\frac{z}{y^2},$$
$$\left(\frac{\partial y}{\partial z}\right)_x=\frac{1}{x}$$
and
$$\left(\frac{\partial z}{\partial x}\right)_y=y.$$
Multiplying these together gives
$$-\frac{yz}{xy^2}=-\frac{xy^2}{xy^2}=-1.$$</p>

<p>As for interpreting what the likes of
$\left(\frac{\partial x}{\partial y}\right)_z$ actually mean, good luck!</p>
"
"2385386","2385402","<p>Note:
$$\frac{\partial}{\partial x}\left( \left(\sqrt{x^2+y^2}\right)^{-1}\right)=(-1)\left(\sqrt{x^2+y^2}\right)^{-2}\frac{\partial}{\partial x}\sqrt{x^2+y^2}$$
$$\frac{\partial}{\partial x}\left( \frac{1}{\sqrt{x^2+y^2}}\right)=\frac{0-1\cdot \frac{\partial}{\partial x}\sqrt{x^2+y^2}}{\left(\sqrt{x^2+y^2}\right)^2}$$</p>
"
"2385400","2385413","<p>\begin{align}
\sum_{m=5}^{n}\frac{3}{m^2+3m+2}
&amp;=\sum_{m=5}^{n}3\left(\frac{1}{m+1}-\frac{1}{m+2}\right)\\
&amp;=3\left(\frac16-\frac17+\frac17-\frac18+\frac18-\frac19+\ldots+\frac{1}{n+1}-\frac{1}{n+2}\right)\\
&amp;=3\left(\frac16-\frac{1}{n+2}\right)\\
&amp;=\frac12-\frac{3}{n+2}
\end{align}</p>

<p>Infinite case is just $\frac12$</p>
"
"2385403","2385425","<p>The statement of part 4 can be through of like this. Given any real continuous function $f$ on $K$, we can find a sequence of $h_{n}\in B$ such that $h_{n}\to f$ uniformly. </p>

<p>Uniformly closed means that for any uniformly convergent sequence $h_{n}\in B$, $\lim_{n\to\infty} h_{h}\in B$. Thus, we conclude via part 4 that for any real contionus function $f$ on $K$, $f\in B$, which is the desired result.</p>
"
"2385406","2385412","<p>HINT: prove the $f(x)$ has the form $$f(x)=2\,x \left( ab+ac+bc+{x}^{2} \right) $$</p>
"
"2385411","2385468","<p>Terence Tao's ""<a href=""http://www.hindbook.com/index.php/analysis-i-3-e"" rel=""nofollow noreferrer"">Analysis I</a>"".
Title says ""Analysis"", but it really starts off with a rigorous treatment of sets and numbers.</p>
"
"2385414","2385466","<p>You asked for ""some help"", so here's a little. </p>

<p>First, it depends on $x$. </p>

<p>If $x$ is $\pm 1$, then every orbit is a point. </p>

<p>No matter what $x$ is, every pure-real quaternion is an orbit. </p>

<p>If $x$ is a pure vector quaternion, then you might as well pick $x = \mathbf i$ (via a change of coordinates if necessary), at which point you can explicitly compute to see that for any quaternion $q = r + u$, where $r$ is real and $u$ is pure-vector, the $r$ part is invariant, so you need only look at what happens to $u$. For $u = \mathbf i$, you have a fixed point; for $u = \mathbf j, \mathbf k$, you get an orbit: $\{\mathbf j, -\mathbf j\}$ or $\{\mathbf k, -\mathbf k\}$. And in general, for $a\mathbf j + b\mathbf k$, you get a similar two-point orbit. </p>

<p>That all happens because multiplication by $i$ represents a 90-degree rotation on the sphere of unit pure-vector quaternions. For a mixed quaternion like $\cos t + \mathbf i \sin t $, you might get much more complicated orbits, I expect, but I haven't worked out the details. I suspect that Rodrigues' formula might prove useful. </p>
"
"2385416","2385426","<p>The problem is that $\arg(z^2)\not\equiv 2\arg(z)$. In fact $\arg(z^2)=2\arg(z)+2k\pi$, for some $k\in \{-1,0,+1\}$. </p>

<p>So from $\arg(z^2)=\arg((-z)^2)$, all you can conclude about $2\arg(z)$ and $2\arg(-z)$ is that they differ by a multiple of $2\pi$, since the values of $k$ which make the above equation true may be different for $z$ and $-z$.</p>
"
"2385429","2385434","<p>The first approach is the easiest. The equation you are looking at is equivalent to 
$$(a-c)\vec x + (b-d) \vec y = 0$$
If $x, y$ are not parallel then they are linearly independent, which, by definition, implies $(a-c)=0=(b-d)$</p>
"
"2385442","2385451","<p>First of all, you may need to use a function called <code>atan2(x,y)</code> or the like, in order to distinguish between $(x,y)=(3,4)$ and $(x,y)=(-3,-4)$, for example, and also avoid problems with division by zero.</p>

<p>Alternatively, avoid trigonometry altogether and write $\frac{x}{\sqrt{x^2 +y^2}}$ and $\frac{y}{\sqrt{x^2 +y^2}}$ instead of $\cos\theta$ and $\sin\theta$.</p>
"
"2385445","2385476","<p>You almost got it!
(I will assume $f=l$ in your question).</p>

<p>Setting
$p(x) = \lVert l \rVert \cdot \lVert x \rVert$
is a good idea.</p>

<p>Note that the righthand side of the equivalence
$\lvert l(x) \rvert \leq \lVert l \rVert \cdot \lVert x \rVert \Leftrightarrow \lvert Re(l(x)) \rvert \leq \lVert l \rVert \cdot \lVert x \rVert$
is always true.
thus, you know that the right-hand side has to be true.
This implies $\| Re(l) \|\leq \| l \|$.</p>

<p>For the other inequality,
define $p(x)=\|Re(l)\| \cdot \|x\|$
and procede similarly.</p>
"
"2385446","2385501","<p>(Note: the following is more of a roadmap, and details are ommitted)</p>

<p>We denote by $ f^{-n}([0,1]) $
the $n$-th iterated pre-image of $[0,1]$.
the solution set can be alternatively described as
$$
 \tag{1}
  K= \bigcap_{n\in\mathbb N} f^{-n}([0,1])
$$
It is also obvious, that $f^{-(n+1)}([0,1])\subset f^{-n}([0,1])$.</p>

<p>Now we need to get an idea what $K$ looks like.
Consider $f^{-1}([0,1])$.
It is easy to prove that
$$
 f^{-1}([0,1])= [0,\frac13]\cup[\frac23,1].
$$
As a next step, one can calculate
$$
 f^{-2}([0,1])=f^{-1}(f^{-1}([0,1]))= [0,\frac19]\cup[\frac29,\frac13]\cup[\frac23,\frac79]\cup[\frac89,1].
$$</p>

<p>One can see, that we always delete the (open) center third of an interval, when we procede
from $f^{-n}([0,1])$ to $f^{-(n+1)}([0,1])$.
This is the same process as in Cantor set (<a href=""https://en.wikipedia.org/wiki/Cantor_set"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Cantor_set</a>).</p>

<p>It follows that $K$ is Cantor set.</p>

<p>An alternative approach is to consider the digital expansion of $x\in[0,1]$ in base 3
and observe how the digits change when applying $f$.</p>
"
"2385454","2385497","<p>$$g (x)=\frac {2\frac {f (x)}{x}+1}{\frac {f (x)}{x}+2} $$</p>

<p>so if $\lim_0f'(x)=L $ then 
$$\lim_0g (x)=\frac {2L+1}{L+2} $$</p>
"
"2385455","2385462","<p>They're all excellent for their own uses. There is no number system which is objectively ""better"" than any other.</p>

<p>When working directly with computers and low-level programming, number systems based on powers of $2$ are easier to use because they translate easier to bits of $0$ and $1$.</p>

<p>If you want to talk about numbers to members of the general public, you should use base ten, because otherwise you run a great risk of not being understood properly.</p>

<p>If you're looking at a watch, you should think in base $60$, you never think that $10.31$ hours have passed since midnight; you think that the time is $10:18:36$.</p>

<p>The time and space you save by being ""more efficient"" isn't worth the confusion you cause by picking the wrong system for the situation.</p>
"
"2385464","2385481","<p>As pointed out in the comments, there is an error in (2); you should have something like
$$0 = ((a+c)+(b+d))^2 = (a+c)^2+(b+d)^2 + 2(a+c)(b+d)$$
giving
$$(a+c)^2+(b+d)^2 = -2ab-2ad-2cb-2cd$$
But the RHS here is exactly what appeared in (1).</p>

<p>So substituting (2) into (1) gives
$$a^2_{n+1} + b^2_{n+1} + c^2_{n+1} + d^2_{n+1} = 2 (a^2_{n} + b^2_{n} + c^2_{n} + d^2_{n}) + (a_n+c_n)^2+(b_n+d_n)^2$$</p>

<p>But squares are always non-negative so
$$a^2_{n+1} + b^2_{n+1} + c^2_{n+1} + d^2_{n+1} \ge 2 (a^2_{n} + b^2_{n} + c^2_{n} + d^2_{n})$$
which is (3).</p>

<p>Then by induction or similar, it follows that 
$$\begin{align}
a^2_{n+1} + b^2_{n+1} + c^2_{n+1} + d^2_{n+1}
&amp;\ge 2 (a^2_{n} + b^2_{n} + c^2_{n} + d^2_{n}) \\
&amp;\ge 2 \cdot 2 (a^2_{n-1} + b^2_{n-1} + c^2_{n-1} + d^2_{n-1}) \\
&amp; \ge \cdots \\
&amp; \ge 2^{n} (a^2_{1} + b^2_{1} + c^2_{1} + d^2_{1}) \\
\end{align}$$</p>
"
"2385465","2385686","<p>From the equation $y^2=4ax$ we solve for $x$ and we get $x=\dfrac{y^2}{4a}$ means that the points of the parabola are all in the form $P\left(\dfrac{y^2}{4a},\;y\right)$.</p>

<p>A chord $OP$ has midpoint $M\left(\dfrac{y^2}{8a},\;\dfrac{y}{2}\right)$ which means that $y_M^2=\dfrac{y^2}{4}$ that is $y^2=4y_M^2$</p>

<p>In a similar way we show that $x_M=\dfrac{y^2}{8a}$ then $y^2=8ax_M$</p>

<p>So we can conclude that $4y_M^2=8ax_M$ and finally $y_M^2=2ax_M$</p>

<p>as $P$ runs on to the parabola, $M$ runs on the curve $y^2=2ax$</p>

<p>I hope it is clear. (Actually I have used hidden parametrization)</p>
"
"2385469","2385479","<p>Let us suppose that $P \in \mathbb{C}[x]$ has degree $k$, i.e., it has complex roots $\alpha_1,\ldots,\alpha_k$ with
$$
P(x)=c\prod_i(x-\alpha_i)
$$
where the $\alpha_i$ are distinct complex and $c\neq 0$. In addition, we know that
$$
P(x)-1=c\prod_i(x-\alpha_i-1).
$$
In particular, the coefficient of $x^{k-1}$ of $P(x)$ verifies
$$
-c\sum_i \alpha_i = - c\sum_i (\alpha_i+1).
$$
This is impossible whenever $k\ge 2$.</p>

<hr>

<p>Edit: The condition of $\alpha_i$ being distinct is necessary. Indeed, the polynomial $P(x):=x^k$ has all roots real and equal to $0$, and $P(a)=0 \implies P(a+1)=1$ for all $a \in \mathbf{R}$.</p>
"
"2385484","2386035","<p>Let $I$ be your integral.  One kind of bound is to observe that $|x_1|\le \|x\|$, resulting in $$I\le \int_{\mathbb R^d} \|x\|^n p(x)\,dx = \frac{2\pi^{d/2}}{\Gamma(d/2)} \int_0^\infty r^{n+d-1} \exp( -r^{4/3}) \,dr.$$  On substituting $t=r^{4/3}$ we get 
$$I\le\frac{3\pi^{d/2}}{2\Gamma(d/2)}\int_0^\infty t^{3(n+d)/4 - 1} e^{-t} \,dt = \frac{3\pi^{d/2}\Gamma(3(d+n)/4)}{2\Gamma(d/2)}.$$</p>

<p>Is this good enough for you?</p>
"
"2385491","2385512","<p>The two dashes do not necessarily represent the same object, in the first place because the categories $\mathcal C$ and $\mathcal D$ are not necessarily the  same.   </p>

<p>Actually, for any objects  $X$ in $\mathcal D$, $Y$ in $\mathcal C$, you have a bijection
$$\Phi_{X,Y}\colon\operatorname{Hom}_{\,\mathcal C}\bigl(F(X),Y\bigr)\longrightarrow\operatorname{Hom}_{\mkern2mu\mathcal D}\bigl(X,G(Y)\bigr)$$</p>
"
"2385492","2385539","<p><em>Correction</em>:</p>

<p>$x(n+1)= \begin{bmatrix}2 &amp; 1\\0 &amp; 2 &amp;\end{bmatrix}x(n)+\begin{bmatrix}2\\-1 &amp;\end{bmatrix}\ \ \ \ $ (Eq. 1)</p>

<p>Then the fixed point is given by ($x(n+1)=x(n)=x_*$):</p>

<p>$x_*= \begin{bmatrix}2 &amp; 1\\0 &amp; 2 &amp;\end{bmatrix}x_*+\begin{bmatrix}2\\-1 &amp;\end{bmatrix}$</p>

<p>Therefore
$\begin{bmatrix} 1&amp; 1\\0 &amp; 1 &amp;\end{bmatrix}x_*=\begin{bmatrix}-2\\1 
&amp;\end{bmatrix}$</p>

<p>and hence $x_*=\begin{bmatrix}-3\\1 &amp;\end{bmatrix}$</p>

<p>Redefine zero: $X(n)=x(n)-x_*$ (substitute in Eq. 1)</p>

<p>then
$X(n+1)= \begin{bmatrix}2 &amp; 1\\0 &amp; 2 &amp;\end{bmatrix}X(n)$</p>

<p>The eigenvalue equation is $(\lambda-2)^2=0$ with $\lambda_{1,2}=2$</p>

<p>let $K_1=\begin{bmatrix}1\\0 &amp;\end{bmatrix}$</p>

<p><em>I do not know what the rest of the exercise is.</em></p>
"
"2385495","2385503","<p>Here is a simpler approach:</p>

<p>Since $(k, \phi(m)) = 1$, write $uk + v \phi(m)=1$ for $u,v \in \mathbb Z$.</p>

<p>Then, if $(x,m)=1$ we have $x=x^1=x^{uk + v \phi(m)}={(x^u)}^k (x^{\phi(m)})^v \equiv {(x^u)}^k \bmod m$.</p>

<p>This proves that the map $x \mapsto x^k$ is surjective and so is a bijection.</p>

<p>In other words, every $x$ with $(x,m)=1$ has a unique $k$-th root mod $m$.</p>
"
"2385499","2385742","<p>Formally, $[Y|X=x]$ does not make sense. However, under some mild assumptions the probability measure $P(\,\cdot\,|X=x)$ exists for each $x$, so it makes sense to discuss the distribution of $Y$ with respect to this measure. This is what is really meant, and it is not surprising that a statistician would take this shortcut. After all, if you don't know any measure theory then what I have said might not mean anything to you, whereas talking about ""conditional distributions"" should have some intuition behind it - for example, if $Z$ is independent of $X$, the conditional distribution of $X^2-3XZ$ given $X=x$ should clearly be the same as the distribution of $x^2-3xZ$.</p>
"
"2385514","2385623","<p>There is a way to approximate quite well the value </p>

<p>$$k H_k\approx \frac{1}{2}+k \left(\gamma -\log \frac{1}{k}\right)$$
where $\gamma\approx 0.577216$ is Euler constant and $\log$ are natural logarithms</p>

<p>Here is a table which shows </p>

<p>$$
\begin{array}{r|r|r}
k &amp; \textit{actual value} &amp; \textit{approximation}\\
\hline
 10 &amp; 29.289683 &amp; 29.298008 \\
 20 &amp; 71.954793 &amp; 71.958959 \\
 30 &amp; 119.84961 &amp; 119.85239 \\
 40 &amp; 171.14172 &amp; 171.14380 \\
 50 &amp; 224.96027 &amp; 224.96193 \\
 60 &amp; 280.79222 &amp; 280.79361 \\
 70 &amp; 338.29857 &amp; 338.29976 \\
 80 &amp; 397.23834 &amp; 397.23938 \\
 90 &amp; 457.43135 &amp; 457.43228 \\
 100 &amp; 518.73775 &amp; 518.73859 \\
\end{array}
$$</p>
"
"2385520","2385522","<p>$a^2-1=0$ is equivalent to $a^2-1=(a-1)(a+1)=0$ since the domain is integral, $a=1$ or $a=-1$.</p>
"
"2385527","2385592","<p>Look at the diagram below:</p>

<p><a href=""https://i.stack.imgur.com/gsviP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gsviP.png"" alt=""enter image description here""></a></p>

<p>Note that the area of the right triangle $ABO$ is:
$$S_{\Delta ABO}=\frac{1}{2}AO\cdot BO=\frac{1}{2}\cdot 2a\cdot a=a^2$$
$$S_{\Delta ABO}=\frac{1}{2}AB\cdot OC=\frac{1}{2}\cdot a\sqrt{5}\cdot 1=\frac{a\sqrt{5}}{2}.$$
Equate them:
$$a=\frac{\sqrt{5}}{2}.$$</p>

<p>The equation of the line:
$$\frac{y-2a}{0-2a}=\frac{x-0}{a-0} \Rightarrow y=-2x+2a.$$</p>

<p>Note that there are four symmetrical cases of the diagram above. Can you figure out other cases?</p>
"
"2385534","2385644","<p>Because your statement of the Tarski-Vaught test is wrong. Here's what it really says:</p>

<blockquote>
  <p>Let $M$ be a substructure of $N$. Then $M\preceq N$ if and only if for every first-order formula $\psi(x,\overline{b})$ with parameters $\overline{b}$ from $M$, if $N\models \exists x\,\psi(x,\overline{b})$, then there exists $a\in M$ such that $N\models \psi(a,\overline{b})$.</p>
</blockquote>

<p>This differs from your statement in two important ways. First, the formula $\exists x\,\psi(x,\overline{b})$ can be <em>any</em> formula that starts with an existential quantifier. It is <em>not</em> required to be an existential ($\Sigma_1$) formula, i.e. $\psi$ is not required to be quantifier-free. Second, the truth of $\psi(a,\overline{b})$ is checked in $N$, not in $M$. In some sense, this is the whole point of the Tarski-Vaught test: the definition of $M\preceq N$ requires you to compare truth in $M$ to truth in $N$, while the Tarski-Vaught test is a criterion for $M\preceq N$ that only mentions truth in $N$, never in $M$.</p>

<p>So the statement ""the Tarski-Vaught test holds"" is really quite different than the statement ""$M\preceq_{\Sigma_1} N$"". If you try to prove by induction that $M\preceq_{\Sigma_1} N$ implies $M\preceq_{\Sigma_n} N$ for all $n$, you'll run into trouble right away! (Because it's not true.)</p>
"
"2385546","2385649","<p>You're right! When you solve the linear diophantine equation $ax+by=c$, this equation has infinite solutions if and only if $gcd(a,b)|c$.</p>

<p>To find all of these solutions, compute $gcd(a,b)$ using the Euclidean algorithm (you only need to find one solution $(x_0,y_0))$. Once you have $(x_0,y_0)$, the other solutions comes in the form:</p>

<p>$$x=x_0+\frac {bk}{gcd(a,b)}$$
$$y=y_0-\frac {ak}{gcd(a,b)}$$</p>

<p>For all integers $k$</p>

<p>Answering your question, it depends on the values of $a$ and $b$, given that for some natural numbers $a,b$ the $gcd(a,b)$ could or could not divide $c$. For instance: $ax+by=12$</p>

<p>If $a=3$ and $b=9$, the equation $3x+9y=12$ has solutions:
$$x=1+3k$$
$$y=1-k$$
For all integers $k$</p>

<p>But, if $a=5$ and $b=10$, then $gcd(a,b)=gcd(5,10)=5$ which doesn't divide $c$. Therefore, the equation has no solutions</p>

<p>Hope it helps!</p>

<hr>

<p>To find all natural solutions restrict values of k for which $x&gt;0$ and $y&gt;0$:</p>

<p>$$x&gt;0 \Rightarrow k&gt;\frac {-x_ogcd(a,b)}{b}$$
$$y&gt;0 \Rightarrow k&lt;\frac {y_ogcd(a,b)}{a}$$</p>

<p>In conclusion: </p>

<p>If $gcd(a,b)$ doesn't divide c, there are no natural solutions</p>

<p>If $gcd(a,b)$ divides c, exists a pair of natural solutions $(x_0,y_0)$ and the equation has a finite set of solutions of the form:</p>

<p>$$x=x_0+\frac {bk}{gcd(a,b)}$$
$$y=y_0-\frac {ak}{gcd(a,b)}$$</p>

<p>For all integers $k$ such that $k\in (\frac {-x_ogcd(a,b)}{b},\frac {y_ogcd(a,b)}{a})$</p>

<p>In other words, the number of natural solutions of the diophantine equation $ax+by=c$ where $c$ is given, is equal to the number of integers in the interval $(\frac {-x_ogcd(a,b)}{b},\frac {y_ogcd(a,b)}{a})$</p>
"
"2385556","2385600","<p>Using the <a href=""https://en.wikipedia.org/wiki/Vectorization_(mathematics)"" rel=""nofollow noreferrer"">vectorisation</a> and <a href=""https://en.wikipedia.org/wiki/Kronecker_product"" rel=""nofollow noreferrer"">Kronecker product</a> operations, we can re-write the equation as:</p>

<p>$$
  \sum_i (B_i^T \otimes A_i)\mathbf{vec}(X)=\mathbf{vec}(C)
$$</p>

<p>from which we can get the (exact or least-squares) solution to this problem using:</p>

<p>$$
  \mathbf{vec}(X)=\left( \sum_i (B_i^T \otimes A_i) \right)^\dagger \mathbf{vec}(C)
$$</p>

<p>and performing the inverse-vectorise operation. Here $M^\dagger$ denotes the <a href=""https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse"" rel=""nofollow noreferrer"">Moore-Penrose psuedoinverse</a> of the matrix $M$.</p>
"
"2385561","2385566","<p>Simplifying the fraction after applying L'Hopital, you should get $$\frac{2x^2}{1+x^2}$$ instead of $$\frac{2x}{1+x^2}$$</p>

<p>Then to get $$\lim_{x \to \infty} \frac{2x^2}{1+x^2}$$ you can again apply L'Hopital and get $$\lim_{x \to \infty} \frac{2x^2}{1+x^2}=\lim_{x \to \infty} \frac{4x}{2x}=2$$</p>
"
"2385563","2385594","<p>I assume the characteristic is different of $3$.
You know that $dim(Ker(L-I))+dim(Im(L-I)=dim(V)$.</p>

<p>Let $x\in Ker(L-I)\cap Im(L-I)$, $L(x)=x$ and $x=L(y)-y$ this implies that  </p>

<p>$x=L(y)-y$</p>

<p>$L(x)=x=L^2(y)-L(y)$, </p>

<p>$L^2(x)=x=L^3(y)-L^2(y)=y-L^2(y)$. </p>

<p>You obtain $3x=L(y)-y+L^2(y)-L(y)+y-L^2(y)=0$ and $x=0$. </p>

<p>You deduce that $Ker(L-I)\cap Im(L-I)=\{0\}$ and henceforth $Ker(L-I)\oplus Im(L-I)=V$.</p>
"
"2385570","2385585","<p>It's simply because your diagram assumes you stay with probability $1/2$. In that case the probability of switching <strong>and</strong> getting the car is indeed $2/6$, and the probability of switching <strong>and</strong> not getting the car is $1/6$.</p>

<p>But if you always switch, you're twice as likely to get each of these outcomes - instead of a $1/2$ probability of switching, you have probability $1$ of switching - so they become $2/3$ and $1/3$ respectively.</p>
"
"2385588","2385821","<p>As you mentioned already, the key is the behavior of $f$ around the origin $x= 0$. $f_{n,k}$ should be integrable </p>

<ul>
<li>for all $k,n\in\mathbb N$.</li>
<li>for $k\in\mathbb R\setminus\mathbb N$ and $n&lt;k+1$, $n\in\mathbb N$.</li>
</ul>

<p>For all other $n,k$, the singularity at the origin becomes non-integrable.</p>

<p>Since the function is symmetric around $x=0$, it is sufficient to investigate integrability for $x\in(0,1)$ (the 1 being an arbitrary upper bound). Hence, we can drop the modulus in the exponent. Now we can show by induction and the chain rule that 
$$\frac{d^n}{dx^n} e^{-x^k} = e^{-x^k}\sum_{i=I(n,k)}^n a_i x^{k-i},$$
for appropriate coefficients $a_i$, and a possibly negative $I(n,k)$. The factor $ e^{-x^k}$ in front of the sum does not affect the integrability at $x= 0$, since $ e^{-0^k} = 1$ for all $k\in\mathbb N$.</p>

<p>The critical situation arises once the sum contains negative exponents: $x^{k-i}$ is integrable if and only if $k-i &gt; -1$. If $k\in\mathbb N$, this can never happen, since once $k=i$, further derivatives cannot lower the exponent in $x^{k-i}$ any further (e.g., the third, fourth, ..., derivative of $x^2$ is zero). If $k\notin \mathbb N$, negative exponents in the sum will occur eventually. The smallest exponent in the sum is $k-n$. In this case, $f_{n,k}$ is integrable if and only if $k-n&gt;-1$. Once $n$ exceeds this bound, the singularity stemming from $x^{k-n}$ is non-integrable.</p>
"
"2385599","2385751","<p>so we have to prove that $$\cos(\alpha)\cos(\gamma)=\frac{2(c^2-a^2)}{3ac}$$
we have $$\cos(\gamma)=\frac{2b}{a}$$ (I)
and $$\frac{\sin\left(\alpha-\frac{\pi}{2}\right)}{\sin\left(\frac{\pi}{2}+\gamma\right)}=\frac{-cos(\alpha)}{\cos(\gamma)}=\frac{a}{2c}$$
further we have
$$b^2+AD^2=\frac{a^2}{4}$$(II)
and $$AD^2=\frac{c^2}{2}+\frac{b^2}{2}-\frac{a^2}{4}$$ (III) therefore we get
$$a^2-c^2=3b^2$$
and $$\cos(\alpha)\cos(\gamma)=-\frac{2b^2}{ac}=\frac{2(c^2-a^2)}{3ac}$$ if $$3b^2=a^2-c^2$$</p>
"
"2385602","2385661","<p>Suppose we have the following linear matrix equation in $\mathrm X \in \mathbb R^{n \times p}$</p>

<p>$$\rm A  X = B$$</p>

<p>where matrices $\mathrm A \in \mathbb R^{m \times n}$ and $\mathrm B \in \mathbb R^{m \times p}$ are given. <a href=""https://en.wikipedia.org/wiki/Vectorization_(mathematics)"" rel=""nofollow noreferrer"">Vectorizing</a> both sides, we obtain a system of $m p$ linear equations in $n p$ unknowns</p>

<p>$$\left( \mathrm I_p \otimes \mathrm A \right) \, \mbox{vec} (\mathrm X) = \mbox{vec} (\mathrm B)$$</p>

<p>or, less economically,</p>

<p>$$\begin{bmatrix} \mathrm A &amp; &amp; &amp; \\ &amp; \mathrm A &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp;  &amp; &amp; \mathrm A\end{bmatrix} \begin{bmatrix} \mathrm x_1\\ \mathrm x_2\\ \vdots\\ \mathrm x_p\end{bmatrix} = \begin{bmatrix} \mathrm b_1\\ \mathrm b_2\\ \vdots\\ \mathrm b_p\end{bmatrix}$$</p>
"
"2385605","2390092","<p>So I have solved the problem now, after getting the points to be interpolated you simply just construct a Lagrange interpolation polynomial using these points.</p>
"
"2385608","2385642","<p>To answer your questions:</p>

<p>a) This is a fact about the complex numbers which is worth remembering: for any $n = 1,2,3,\dots$ the roots of the equation $x^n = 1$ are
$$
x = \exp(2 \pi ki/n), \quad k = 0,1,2,\dots,n-1
$$
(note: $\exp(i \theta) = e^{i\theta} = \cos(\theta) + i \sin \theta$).
These roots are called the ""$n$th roots of unity"".  All $n$ of these roots are distinct.</p>

<p>b) Note that $T:M_n(\Bbb C) \to M_n(\Bbb C)$ is <strong>not</strong> the transformation that one would normally associate with the matrix $A$. The transformation that you're thinking of is $L_A: \Bbb C^n \to \Bbb C^n$ given by $x \mapsto Ax$.  Yes, both transformations involve multiplication by $A$, but the spaces involved are different.  $T$ is a map between two vector spaces of dimension $n^2$ whereas $L_A$ is a transformation between spaces of dimension $n$.</p>

<p>c) There is no reason to suppose that $A$ was $18 \times 18$.  In fact, we could make a $6 \times 6$ matrix with the required properties, for example, by using the <a href=""https://en.wikipedia.org/wiki/Companion_matrix"" rel=""nofollow noreferrer"">companion matrix</a> associated with the given polynomial.</p>

<p>In fact, if we simply take $A = \omega I$ where $\omega$ is any solution to $\omega^6 - \omega^3 + 1 = 0$, then we have a satisfactory matrix which could work for any $n$.</p>

<hr>

<p>To your comment below:</p>

<p>To see 2) in detail, what we've really said is that if $g$ is any polynomial such that $g(A) = 0$, then we have $g(T) = 0$.  This is true because for any $B$, 
$$
[g(T)](B) = g(A)B = 0B = 0
$$
<em>(However, since the polynomials satisfying $g(A) = 0$ are just the multiples of $f$, it was enough to consider $f(T))$</em></p>

<p>In 3), we show that if $g$ is any polynomial such that $g(T) = 0$, then we have $g(A) = 0$.  In particular: if $g(T) = 0$, then we can say that for <em>every</em> $B$, we have
$$
[g(T)](B) = g(A)B = 0
$$
Now, it's up to you to show that because $g(A)B = 0$ for every $B \in M_n$, it must be the case that $g(A) = 0$.</p>

<p>Between parts 2) and 3), we have shown that a polynomial $g$ will satisfy $g(T) = 0$ if and only if it satisfies $g(A) = 0$.  Since we're looking at the same set of polynomials, of course the minimal polynomial (the lowest common divisor to this set) will be the same.</p>
"
"2385613","2385618","<p>It is clear that $a^{m+1}=a^m.a$. On the other hand, if $a^{m+n}=a^m.a^n$, then$$a^{m+n+1}=a^{m+n}.a=a^m.a^n.a=a^m.a^{n+1}.$$</p>
"
"2385621","2385714","<p>Here are <em>two</em> (slightly more general) approaches.</p>

<hr>

<p>If you are familiar with the complex representation of $\cos(x)$, namely</p>

<p>$$\cos(x)=\frac{e^{ix}+e^{-ix}}2,$$</p>

<p>then it becomes pretty straight forward. Then you have</p>

<p>$$\cos^n(x)=\frac1{2^n}\sum_{k=0}^n{n\choose k}e^{ixk}e^{-ix(n-k)}=\frac1{2^n}\sum_{k=0}^n{n\choose k}e^{ix(2k-n)}.$$</p>

<p>Now the $N$-th derivative is</p>

<p>$$(\cos^n(x))^{(N)}=\frac{i^N}{2^n}\sum_{k=0}^n{n\choose k}(2k-n)^Ne^{ix(2k-n)}.$$</p>

<p>But to reformulate this in a nice real-valued form might be hard. So here another approach. We generalize the problem to find the $N$-th derivative of</p>

<p>$$f(x)=\sum_{m,n}a_{mn}\cos^m(x)\sin^n(x)$$</p>

<p>with $m$ and $n$ ranging over $\Bbb Z$ and the $a_{mn}$ choosen appropriately, so the sum stays finite. At first, note</p>

<p>$$f'(x)=\sum_{m,n}a_{mn}(m\cos^{m-1}(x)(-\sin^{n+1}(x))+n\sin^{n-1}(x)\cos^{m+1}(x))$$</p>

<p>by only applying chain and product rule. We see further that this is the same as</p>

<p>\begin{align}
f'(x)&amp;=-\sum_{m,n}ma_{mn}\sin^{n+1}(x)\cos^{m-1}(x)+\sum_{m,n}na_{mn}\cos^{m+1}(x)\sin^{n-1}(x)\\
&amp;=-\sum_{m,n}(m+1)a_{m+1,n-1}\sin^n(x)\cos^m(x)+\sum_{m,n}(n+1)a_{m-1,n+1}\cos^m(x)\sin^n(x)\\
&amp;=\sum_{m,n}a^{(1)}_{mn}\cos^m(x)\sin^n(x)
\end{align}</p>

<p>with $a^{(1)}_{mn}=(n+1)a_{m-1,n+1}-(m+1)a_{m+1,n-1}$. Now you can iteratively compute $a^{(N)}_{mn}$ for $N\geq 2$ to get your final answer. For your task you want to start with $a_{mn}=a^{(0)}_{mn}=1$ for $(m,n)=(9,0)$ and zero everywhere else. Maybe one can see a nice pattern emerging when computing the coefficients. I will have to look at it later.</p>
"
"2385640","2385645","<p>You are counting certain instances multiple times. Say you select $A_1$, then $B_1$, and then $A_2$. This exact same selection happens if you select $A_2$, then $B_1$, and then $A_1$. (See edit below for more details).</p>

<p>Another approach would be to count how many combinations there are without having at least one $A$ and one $B$, and then subtracting from the total. There are $\binom{4}{3}$ ways to not have any $A$'s, $\binom{3}{3}$ ways to not have any $B$'s, and it's impossible to not have any of either, so we get
$$
\frac{\binom{6}{3}-\binom{4}{3}-\binom{3}{3}}{\binom{6}{3}} = \frac{15}{20} = \frac{3}{4}. 
$$</p>

<hr>

<p>EDIT:
In your count, you choose one $A$, then one $B$, and then one card remaining. Enumerating, we obtain:</p>

<p>$A_1B_1A_2$</p>

<p>$A_1B_1B_2$</p>

<p>$A_1B_1B_3$</p>

<p>$A_1B_1C$</p>

<p>$A_1B_2A_2$</p>

<p>$A_1B_2B_1$</p>

<p>$A_1B_2B_3$</p>

<p>$A_1B_2C$</p>

<p>...and so forth. However we have included $A_1B_1B_2=A_1B_2B_1$ twice. Continuing in this way, we will double count all counts of the form $A_iB_jA_k$ and all counts of the form $A_iB_jB_k$. There are $\binom{2}{1}\binom{3}{1}\binom{1}{1}=6$ of the first type and $\binom{2}{1}\binom{3}{1}\binom{2}{1}=12$ second type. Thus we have $6/2+12/2=9$ extra counts. This is consistent because $24-9=15$.</p>
"
"2385643","2385648","<p>The first thing that comes in mind is $338=2\cdot169$ and $288=2 \cdot 144$. Knowing that $12^2=144$ and $13^2=169$ you can finish the computations.</p>
"
"2385654","2386033","<p>Eric Wofsey's answer shows that regulatiry for sets implies regularity for classes, provided you have enough other axioms (like replacement) available. In the particular case you asked about, where the class $A$ consists of transitive sets, you can avoid part of Eric's argument, namely the part where he produces a transitive set $T$.  </p>

<p>In detail: Given a nonempty class $A$ of transitive sets, begin by considering some $y\in A$. If you're very lucky, $y\cap A=\varnothing$ and you're done. So suppose from now on that $y\cap A$ is a nonempty set.  (It's a set, not a proper class, because $y$ is a set.)  By regularity for sets, $y\cap A$ has an element $z$ with $z\cap y\cap A=\varnothing$.  So we have $z\in A$, and if we can show $z\cap A=\varnothing$ then we'll be done.</p>

<p>Now we use that all elements of $A$ are transitive. In particular $y$ is transitive, so, from $z\in y$, we get $z\subseteq y$ and thus $z\cap y=z$. But then what we already knew, that $z\cap y\cap A=\varnothing$, simplifies to $z\cap A=\varnothing$, and the proof is complete.</p>
"
"2385666","2385697","<p>There are two objects being considered here - the <em>sample space</em> $\Omega$, and an unnamed <em>state space</em> which I shall call $E$, which in this case is assumed to be a Banach space. (It is fairly unhelpful that the author does not give the state space a name, and probably contributes to your confusion.) Our random variable is $X:\Omega\to E$.</p>

<p>Random variables need not take values in Banach spaces. Indeed, at its most basic definition, a random variable is just a measurable map from a probability space to a measurable space. However, for almost all practical purposes, we require some topology on the state space. Do you want any kind of notion of convergence? Then you need some kind of topology.</p>

<p>(Edit, to make something clear from the comments: whenever you have a topological space, you also have a measurable space by considering the Borel $\sigma$-field. Unless otherwise specified, you should always assume this is the $\sigma$-field considered when we have a topological space being considered in measure/probability theory.)</p>

<p>In this case, we are looking at the expectation of a random variable, which we know is simply the integral of the measurable function $X$. But what is the integral? Any definition of an integral requires linear combinations and limits, so at the least we will require $E$ to be a topological vector space. Assuming $E$ is Banach simply makes things a little cleaner - the topology of a normed vector space is easier to understand, and making it complete (i.e. Banach) merely guarantees that the space possesses enough limits to avoid running into problems.</p>
"
"2385673","2386093","<p>As phrased, it seems to be talking about the precursor terms to the Morse-Thue sequence:</p>

<p>$0$<br>
$01$<br>
$0110$<br>
$01101001$<br>
$0110100110010110$  </p>

<p>You get each of these terms by appending the binary complement of the previous term to itself. So, once you have enough digits, the last eight digits just flip back &amp; forth between complementary patterns of ones and zeroes.</p>

<p>Thus:<br>
The last eight digits of the 2008$^{th}$ term in the sequence are the same as  </p>

<ul>
<li>the last eight digits of the 2006$^{th}$ term</li>
<li><p>the last eight digits of the 2004$^{th}$ term<br>
:</p></li>
<li><p>the last eight digits of the 4$^{th}$ term</p></li>
</ul>

<hr>

<p>The Morse-Thue word itself is infinite, so has no ""last 8 digits"". The 8 digits in positions 2001 to 2008 can be determined by checking whether the binary representation of each contains an odd or an even count of the digit $1$. (The word starts with $0$ in position zero.)</p>

<p>$2008_{dec} = 11111011000_{bin}$ has $7$ instances of digit $1$ so the Morse-Thue digit is $1$.  </p>

<p>The preceding $7$ position values of interest differ only in the last $4$ binary digits so are easy to calculate by difference from the above.</p>
"
"2385679","2385687","<p>You just can't apply <a href=""https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule"" rel=""nofollow noreferrer"">L'Hopital rule</a>, since $\frac{-x+1}{x^{-2}}$ is not an indeterminate form as $x\to \infty$.</p>
"
"2385682","2385692","<p>This is a good question. Without realizing my assumptions, I interpret $(64x^3 \div 27a^{-3})^\frac{-2}{3}$ as $(\frac{64x^3}{27a^{-3}})^\frac{-2}{3}$. I realize now there is an implicit pair of parentheses around the expression $27a^{-3}$. Clearly, the authors of that question assume parentheses around that term as well. </p>

<p>It is always good to be aware of the assumptions you bring to a problem, so thank you for bringing this up. In general, however, I wouldn't expect to see a lot of problems with division signs like that. Most rational functions (in calculus, for example) are written as fractions; higher math seems to eschew the grade-school division sign.  </p>
"
"2385685","2389882","<p>I will reduce the question to some known results. For positive numbers $a,b$ we define the geometric mean $G(a,b)$ and the power mean of order $s$ denoted by $M_s(a,b)$ as folows
$$G(a,b)=\sqrt{ab},\qquad M_s(a,b)=\left(\frac{a^s+b^s}{2}\right)^{1/s}$$
Now, with $a=x$, $b=1/x$, and writing $M_s$ and $G$ instead of $M_s(a,b)$ and $G(a,b)$ for simplicity, the proposed inequality takes the form
$$\frac{M_1^p-G^p}{M_p^p-G^p}\ge 2^{1-p}$$
So, this is a particular case of the more general inequality</p>

<p>$$\frac{M_s^p-G^p}{M_t^p-G^p}\ge 2^{p/t-p/s}$$
Which is valid for $0&lt;s&lt;t$, $p\ge \max(2s,2(s+t)/3)$. A detailed proof and more results on this topic can be found in the paper of Omran Kouba entitled:
""<em>Bounds for the ratios of differences of power means in two arguments</em>""</p>

<p>This paper can be found <a href=""http://dx.doi.org/10.7153/mia-17-66"" rel=""nofollow noreferrer"">here</a> or <a href=""https://arxiv.org/abs/1006.1460"" rel=""nofollow noreferrer"">here</a>.</p>
"
"2385693","2385734","<p>Let's say $\pi \colon P \rightarrow M$ is a fiber bundle. The fiber $\pi^{-1}(q)$ through $q \in M$ is a submanifold of $P$ (diffeomorphic to $G$ in your case, but this is not really relevant for what follows). Choose a point in the fiber $p \in \pi^{-1}(q)$. A tangent vector at $p \in P$ which is tangent to the fiber is just a tangent vector which belongs to the tangent space of the fiber $\pi^{-1}(q)$ (identified as a subspace of the tangent space to the total space $T_pP$). Such a tangent vector is represented by an equivalence class $[\alpha]$ of smooth curves $\alpha \colon I \rightarrow \pi^{-1}(q)$ with $\alpha(0) = p$ (that is, curves in $P$ that pass through $p$ and stay on the fiber). Since $\pi(\alpha(t)) \equiv q$ is a constant curve on $M$, by differentiating we get that</p>

<p>$$ d\pi|_{p}(\dot{\alpha}(0)) = 0 $$</p>

<p>so the tangent vector $\dot{\alpha}(0)$ lies in $\ker(d\pi|_p)$. By using the local model of a fiber bundle, you can see that the converse also holds so the tangent space to the fiber $\pi^{-1}(q)$ at $p$ is just the kernel $\ker(d\pi|_p)$.</p>
"
"2385698","2386138","<p>You have the correct expressions for the pmf at those arguments.</p>

<p>$X\sim\mathcal{Ber}(p)$ is a typical notation indicating that random variable $X$ follows a Bernoulli distribution. &nbsp; That is that: $X$ is the indicator of success in a single succeed or fail trial, which has the given expected rate for success, $p$. &nbsp; Some authors may use ""$\mathcal{Bern}$"", ""$\mathcal{Bernoulli}$"", or simply ""$\mathcal{B}$"". </p>

<p>Where $f_X()$ is the <em>probability <strong>mass</strong> function</em> for $X$, then:</p>

<p>$$X\sim\mathcal{Ber}(p) ~\iff~ f_X(x) = p^x\,(1-p)^{1-x}~\mathbf 1_{x\in\{0,1\}}\\ f_X(0)=1-p\\ f_X(1)=p\qquad$$</p>

<p>For a single coin toss, you will also need to indicate what are you <em>counting as a success</em>, and what value is the <em>success rate</em>, $p$. &nbsp; Some of this has to be done with words. &nbsp; In short:</p>

<blockquote>
  <p>Let $X$ be the count for obtaining heads in the toss of an unbiassed coin, so $X\sim\mathcal {Ber}(1/2)$.</p>
</blockquote>
"
"2385717","2385886","<p>Let $f(n)=1/n^2\ln n$ for $n\geq 2$.  Then $f(n)$ is decreasing. Now $0&lt;2^nf(2^n)=1/(2^n\cdot n\ln 2)&lt;1/2^n,$ and $\sum_{n&gt;1}1/2^n$ converges, so   $\sum_{n&gt;1} f(n)$ converges.</p>

<p>Let $g(n)=[n(\ln n)(\ln \ln n)]^{-1}$ for $n\geq 2.$ Then $g(n)$ is decreasing for $n\geq 3.$  And $$2^n g(2^n)=    [(n\ln 2)(\log n +\ln \ln 2)]^{-1}&gt;[2\ln 2]^{-1}[n\ln n]^{-1}&gt;0$$ for $n\geq 3.$  Applying the Cauchy condensation test to $[n\ln n]^{-1}$ shows that $\sum_{n&gt;2}[n\ln n]^{-1}$ diverges, so $\sum_{n&gt;1}g(n)$ diverges.</p>

<p>Note: It often helps to look for ways to simplify an intermediate step. E.g.  by comparing $2^ng(n)$ to $[2\ln 2]^{-1}[n\ln n]^{-1} ,$ as there is less calculation and detail when applying the condensation test to the latter expression.</p>

<p>Note: By repeated use of the condensation test, each of the series $\sum_{n&gt;n_1} [n(\ln n)^A]^{-1},\; \;$ $\sum_{n&gt;n_2} [n(\ln n) (\ln \ln  n)^A]^{-1},\;\; $ $\sum_{n&gt;n_3} [n(\ln n)( \ln \ln n) (\ln\ln\ln n)^A]^{-1}$...(et cetera)... (where $n_1,n_2,n_3$ are large enough that the terms of the series are all defined).... converges for $A&gt;1$ and diverges for $A\leq 1.$</p>
"
"2385718","2385724","<p>You have $P(X\ge1|N=4)=0.9984$ where $X$ is the number of correct transmissions and $N$ is the number of total transmissions. So you can work out $P(X=0|N=4) = 1-P(X\ge1|N=4)$. But also you have $P(X=0|N=4) = q^{4}$ because the transmissions are independent, where $q$ is the probability of a single transmission being incorrect (hence the probability of one transmission being correct is $1-q$ as those are the only two possibilities).</p>

<p>From there, you have a geometric distribution, since you keep sending the message until it is received correctly. The expected number of trials for a geometric distribution is $1/p$, where $p$ is the probability of success.</p>

<p>Answer below for verification:</p>

<blockquote class=""spoiler"">
  <p> $q^{4} = 0.0016 \Rightarrow q=0.2 \Rightarrow p=0.8$ so you need $1/0.8=1.25$ transmissions on average to transmit a message correctly.</p>
</blockquote>
"
"2385722","2385725","<p>Let $B(0,0)$ and $C(a,0)$.</p>

<p>Hence, $BC=a$ and we need to prove that there exists $A(x,y)$ such that $AB=c$ and $AC=b$.</p>

<p>You can write equations of two circles and  prove that there are intersection points.</p>

<p>For example $x^2+y^2=c^2$ and $(x-a)^2+y^2=b^2$.</p>

<p>Thus, $-2ax+a^2+c^2=b^2$ or $x=\frac{a^2+c^2-b^2}{2a}$ and
$$y^2=c^2-\left(\frac{a^2+c^2-b^2}{2a}\right)^2$$ or
$$y^2=\frac{(a+b+c)(a+b-c)(a+c-b)(b+c-a)}{4a^2},$$
which says that there are two intersection points.</p>
"
"2385729","2385775","<p>HINT: Note that $$(a+x)^n = a^n \left(1+\frac{x}{a}\right)^n.$$ If $|x|&lt;|a|$ then the expansion of $(1+y)^n$ for $|y|&lt;1$ can be applied.</p>
"
"2385733","2385737","<p>$k=n+1$, using the (n+1)-periodicity then for all $m$, \begin{align} f(m) &amp;= f(m+n+1)\\ &amp; =f((m-1)(n+1) + 1) = f(1)
\end{align}</p>
"
"2385740","2385744","<p>To answer these questions, you have to find a constant $\delta$ for every $M$. One way I like to think about this is making $\delta$ a function of $M$. What $\delta(M)$ can you choose so that $x &lt; \delta(M)$ for all $M$?</p>

<p>Note particularly that this is a positive limit, so we know that $x &gt; 0$, which is stronger than $|x| &gt; 0$. Then:</p>

<p>$$\frac{1}{2x} &gt; M \implies \frac{1}{2M} &gt; x$$</p>

<p>So, let $\delta(M) = \frac{1}{2M}$. </p>
"
"2385746","2385765","<p>If you take $p=4$, then you have $p!=24&gt;16=2^p$, so you only need to prove that for $n\geq 4$, you have $n!\geq 2^n$.</p>

<p>So, you already have step 1 of the induction proof, so let's suppose that you have for $k\geq 4$ that</p>

<p>$$k!\geq2^k.$$</p>

<p>Then</p>

<p>$$(k+1)!=k!(k+1)\geq2^k(k+1)\geq2^k(4+1)\geq2^k2=2^{k+1}.$$</p>
"
"2385750","2385754","<p><strong>Hint:</strong></p>

<p>Since $e^{2x}+e^{-2x}+2=(e^x+e^{-x})^2$ we have $$\int_0^1\sqrt{e^{2x}+e^{-2x}+2}\;dx=\int_0^1\left(e^{x}+e^{-x}\right)dx$$</p>
"
"2385755","2385778","<p>Consider the functor $\text{Hom}(M, -)$ where $M$ is, to keep things concrete, an $R$-module and Hom takes values in abelian groups. This functor is exact iff $M$ is projective but commutes with colimits iff $M$ is finitely generated projective. So a counterexample is given by any projective module which is not finitely generated, for example an infinitely generated free module $\bigoplus_{i=1}^{\infty} R$. </p>
"
"2385766","2385777","<p>The magical characteristic of the Fast Fourier Transform is that it turns a product of factors in a sum of factors</p>

<p>$$N=p\cdot q\to (p+q)N$$
$$N=2^n\to (n\cdot 2)N=2N\log_2N$$
$$N=p^mq^nr^k\cdots\to(mp+nq+kr+\cdots)N$$</p>

<p>because the whole process can be decomposed as a sequence of stages of complexity $pN$ where $N$ is the number of coefficients and $p$ a factor of $N$.</p>

<p>Prime $N$ indeed make it quadratic $O(N^2)$.</p>
"
"2385770","2385776","<p>Let's set $h(x) = f^2(x) + g^2(x)$. Then</p>

<p>$$ h'(x) = 2 f(x) f'(x) + 2 g(x) g'(x) = 2f(x) g(x) - 2 g(x) f(x) \equiv 0$$</p>

<p>so $h$ is constant. Since $h(2) = f^2(2) + g^2(2) = 4^2 + (f'(2))^2 = 4^2 + 4^2 = 32$, we get $h(24) = 32$.</p>
"
"2385773","2385805","<p>Statement A: ""every sequence in $A$ that converges, converges in $A$ $\implies A$ is closed""</p>

<p>is equivalent to Statement B: ""$A$ is not closed $\implies$ there is converging sequence in $A$ that converges to a point not in $A$"".</p>

<p>So to prove that, he assumes $A$ is not closed, and then he mechanically creates a $\{x_n\}$ so that $x_n \rightarrow p \not \in A$.  By creating that sequence, he will have proven statement B. And therefore he will have proven statement A.  (which is half of the proof).</p>

<p>$A$ not closed $\implies$ $A^c$ not open $\implies$ there is a point $p \in A^c$ so that $p$ is not an interior point of $A^c$ $\implies$ every neighborhood $N_{\frac 1n}(p)$ will have a point $x_n \in N_{\frac 1n}(p)$, $x_n \in (A^c)^c = A$  $\implies $ $\{x_n\}\subset A$,  $x_n \rightarrow p \not \in A$.</p>

<p>So statement B has been proven.</p>
"
"2385774","2385779","<p>Yes, factor out the $\frac{1}{\ln \ln 2}$ and the comparison $\frac{1}{2^n n} &lt; \frac{1}{2^n}$ will work, because $$\frac{1}{2^n n} &lt; \frac{1}{2^n} \implies 2^n n &gt; 2^n \implies n &gt; 1$$ and  $\sum \frac{1}{2^n}$ is a convergent geometric series.</p>
"
"2385792","2385809","<p>Do integration by part of the second term.</p>

<p>The first term when you integrate becomes $\frac{d(F^2)}{dx}$.</p>

<p>The second term after integration by parts with $u = \frac{x}{3}$ and $dv = \frac{dF}{dx}$</p>

<p>You get $\frac{xF}{3} - \frac{F}{3}$ the last term in this and the last term in the original equation cancels out and you get the final expression .</p>

<p>That is all to it.</p>
"
"2385808","2386087","<p>There is a bit more general statement which can be useful when checking if an operation defines a group or not.</p>

<p>Your statement is that $e$ is identity with respect to $*$, but this can be relaxed to only require that $e$ is left identity. More precisely:</p>

<blockquote>
  <p>Let $(G,*)$ be a semigroup, i.e. $*$ is associative binary operation
  on $G$. If the following holds:</p>
  
  <p>$(1)$ $\ (\forall a\in G)\ e*a = a,$</p>
  
  <p>$(2)$ $\ (\forall a\in G)(\exists b\in G)\ b*a = e,$</p>
  
  <p>then $(G,*)$ is a group.</p>
</blockquote>

<p><em>Proof.</em> We first prove that every element in $a$ is invertible in $G$. Let $a\in G$ and let $b,c\in G$ such that $b*a = e$ and $c*b = e$ (those exist by $(2)$). Then,</p>

<p>$$a*b \stackrel{(1)}= e*(a*b) = (c*b)*(a*b) = c*((b*a)*b) =c*(e*b) = c*b = e,$$ so we see that left inverse guaranteed by $(2)$ is in fact two-sided inverse.$^{[1]}$</p>

<p>Now, we want to prove that $e$ is two-sided identity. Let $a\in G$. Then we have $$a*e = a*(a^{-1}*a) = (a*a^{-1})* a = e * a = a$$ and we are done.</p>

<p>Doesn't seem much of an improvement, but when you have non-commutative operation, it can be tedious to <em>calculate</em> first both left and right identity and then both left and right inverse. This here says if you have calculated both left identity and left inverse for all elements, then you don't have to worry about the other side.</p>

<hr>

<p>$[1]$ Here it is essential that <em>every</em> element of $G$ has left inverse. It is very common in mathematics to encounter monoids in which some elements have left inverse, some right, some both, and some neither. For example, consider set of functions on some infinite set $X$. Then injective functions are those that have left inverse, surjective have right inverse and bijective have two-sided inverse. Since $X$ is infinite, there are injective functions that are not bijective (for example, $n\mapsto n + 1$ on $\mathbb N$).</p>
"
"2385812","2385827","<p>One way is :$$x=\sqrt2+\sqrt3+\sqrt 5\\x-\sqrt5=\sqrt2+\sqrt3\\(x-9\sqrt5=\sqrt2+\sqrt3)^2\\x^2+5-2\sqrt5x=2+3+2\sqrt6\\x^2-2\sqrt5x=2\sqrt6\\(x^2-2\sqrt5x=2\sqrt6)^2\\x^4+20x^2-4\sqrt5x^3=24\\(x^4+20x^2-24)^2=(4\sqrt5x^3)^2$$ Is there an easier way  ?</p>
"
"2385816","2385902","<p>Assume that that $(f_i)_{i \in I} \to g$ where $f_i \in X$. We want to show that $g \in X$. Assume, for contradiction, that $g \not \in X$. Then, there exists $x_1, x_2$ such that $x_1 &lt; x_2$ and $g(x_1) &gt; g(x_2)$. Now, let $g(x_1) &gt; b &gt; g(x_2)$.  Let $U_b = \{f \in [0,1]^{\mathbb{R}}: f(x_1) &gt;b\} \cap \{f \in [0,1]^{\mathbb{R}}: f(x_2) &lt;b\} $. Now, $U_b$ is the intersection of two basic opens and therefore it is open in $[0,1]^{\mathbb{R}}$. Notice that $g \in U$. However, $X \cap U = \emptyset$. Therefore, $(f_i)_{i \in I}$ does not converge to $g$ (since our net fails to meet every open set containing $g$) and we have a contradiction. </p>
"
"2385828","2385841","<p>Whereas the bivariate Normal yields elliptical contours (or a circle given zero correlation), the trivariate case yields the intuitive 3D equivalent, namely the surface of an ellipsoid (or that of a sphere given zero correlations). </p>

<p>So, <em>at each point in time</em> (reducing you to 3D), a contour plot of the pdf $f(x,y,z)$ = constant would look something like this:</p>

<p><a href=""https://i.stack.imgur.com/iHpVQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/iHpVQ.png"" alt=""enter image description here""></a></p>

<p>where parameter $\rho_{x,y}$ alters the 'orientation' of the ellipsoid in the $x$-$y$ plane <em>etc</em></p>
"
"2385832","2385856","<p>If you take the natural log of your equation you can arrange it to read</p>

<p>$$x^2-1 = \ln x.$$</p>

<p>The two sides are easy to graph and intersect at $x=1$.  Noting that the left side is always concave up and the right is always concave down shows there is a second solution (around 0.45.)</p>
"
"2385836","2385843","<p>The idea is to say that $k = \sum_{j=1}^k 1$</p>

<p>$$\sum_{1\leq k \leq n} k^2 = \sum_{1\leq k \leq n} k \left(\sum_{j=1}^k 1\right)= \sum_{1\leq j \leq k \leq n} k$$</p>
"
"2385838","2385984","<p>Recall that the <em>trace map</em> is the function $\DeclareMathOperator{tr}{tr}\tr:\Bbb M_n(\Bbb R)\to\Bbb R$ given by $\tr(A)=\sum_k a_{kk}$. One easily shows that $\tr$ is linear. In this problem, we are interested in the kernel of $\tr$, which you call $W$ but I will refer to as $\mathfrak{sl}_n(\Bbb R)=\ker(\tr)$. Since $\tr$ has rank one, the <a href=""https://proofwiki.org/wiki/Rank_Plus_Nullity_Theorem"" rel=""nofollow noreferrer"">rank-nullity theorem</a> implies that 
$$
\dim\mathfrak{sl_n}(\Bbb R)=\dim\Bbb M_n(\Bbb R)-\dim\DeclareMathOperator{image}{image}\image(\tr)=n^2-1
$$
Now, we wish to describe $\mathfrak{sl}_n(\Bbb R)^\perp$ where we are viewing $\Bbb M_n(\Bbb R)$ as an inner-product space with inner product given by
$$
\langle A, B\rangle=\sum_{i,j}a_{ij}b_{ij}
$$
The <a href=""https://proofwiki.org/wiki/Dimension_of_Orthogonal_Complement_With_Respect_to_Bilinear_Form"" rel=""nofollow noreferrer"">dimension formula for orthogonal complements in finite-dimensional vector spaces</a> implies that
$$
\dim\mathfrak{sl}_n(\Bbb R)^\perp=\dim\Bbb M_n(\Bbb R)-\dim\mathfrak{sl}_n(\Bbb R)=n^2-(n^2-1)=1
$$
Thus $\mathfrak{sl}_n(\Bbb R)^\perp$ is a one-dimensional subspace of $\Bbb M_n(\Bbb R)$. Can you compute a basis of $\mathfrak{sl}_n(\Bbb R)^\perp$?</p>
"
"2385840","2385842","<p>Every power series converges uniformly in a circle of radius $r&lt;R$, where $R$ is the convergence radius. For an entire function, $R=\infty$, for the Taylor series around any $z_0.$<br>
Edit: that would be the answer to the question in the title. The question in the body is different:<br>
Let $M(R)=\sup_{|z|=R}f(z)$. Then, $$f^{(n)}(z)=\frac{n!}{2\pi i}\oint_{|t|=R}\frac{f(t)}{(t-z)^{n+1}}\,dt,$$ and that means
$$\left|\frac{f^{(n)}(z)}{n!}\right|\le\frac{R\,M(R)}{(R-|z|)^{n+1}}.$$
So that's converging locally uniformly, too.</p>
"
"2385844","2385923","<p>This should be a fairly straightforward problem using what we know about graphic degree sequences, but it probably involves analyzing several cases.  It turns the exact formula you are looking for was already worked out 40 years ago in Moshe Rosenfeld's thesis.  You can find the relevant info (Theorem 1) on p.263 of</p>

<p>M. Rosenfeld, ""Independent Sets in Regular Graphs"", <em>Israel J. Math.</em> (1964) Vol 2:4, pp.262â272.</p>

<p>The <a href=""https://page-one.live.cf.public.springer.com/pdf/preview/10.1007/BF02759743"" rel=""nofollow noreferrer"">preview link</a> is enough to see the statement of the theorem, which yields</p>

<p>$$F(v,d) = \min\{\lfloor v/2\rfloor, v-d\},$$</p>

<p>except that of course the function is undefined when $v,d$ are both odd.  The theorem also gives the precise bounds for the minimum independence number.</p>
"
"2385847","2385872","<p>The Laplace operator with Dirichlet boundary conditions on a bounded domain is self-adjoint with compact resolvent: see  e.g. <a href=""https://en.wikipedia.org/wiki/Laplace_operator#Spectral_theory"" rel=""nofollow noreferrer"">Laplace operator - Spectral theory</a>.  Thus it does have eigenfunctions, in fact a complete orthonormal set of them.  But it is unlikely that you'll be able to express them in closed form.</p>
"
"2385848","2386043","<p>The equivalence you hypothesise I think is correct:</p>

<p>Suppose $$\exists x \in X: \forall O \in \mathcal{T}_X: x \in O \implies |X|  =|O|$$ </p>

<p>holds, and $\mathcal{B}$ is any base. Then some base member contains $x$ and for this one we are done. This is a trivial direction.</p>

<p>If the condition does not hold, then any $x \in X$ has a ""small"" open neighbourhood $N_x$ with $|N_x| &lt; |X|$. The set $\mathcal{B} = \{O \in \mathcal{T}_X: |O| &lt; |X|\}$ is a base for $X$:  let $x \in O$, $O$ open. Then $N_x \cap O$ is ""small"" as $|N_x \cap O| \le |N_x| &lt; |X|$ and sits between $x$ and $O$. This $\mathcal{B}$ refutes the requirement for the base.</p>

<p>So these are indeed equivalent.</p>

<p>I don't see any other essentially different one. </p>

<p>If I may ask: why define this notion? What's your interest? Euclidean spaces obey it (but these are strongly homogeneous). 
In fact in any normed vector space $X$, every point has arbitrarily small neighbourhoods that are homeomorphic to $X$.</p>

<p>$\omega_1$ as an ordered space (the first uncountable ordinal) is locally countable, so does not obey it. Neither do discrete spaces of size 2 or more, as the singleton base shows.</p>
"
"2385852","2385936","<p>No, it does not.  Note that every open set $U$ is in $\mathcal{A}$ since $U = U \cup \emptyset$ and $\emptyset$ is closed; and similarly, every closed set $K$ is in $\mathcal{A}$.  Now, if $\mathcal{A}$ were an algebra, that would imply every intersection of an open set and a closed set in $\mathbb{R}$ would also be a union of an open set and a closed set.</p>

<p>We now give a counterexample to show this is not true.  Consider the open set $U = (0, \infty)$ and the closed set $K = \{ 0 \} \cup \{ \frac{1}{n} : n \in \mathbb{N}_+ \}$.  Then $U \cap K = \{ \frac{1}{n} : n \in \mathbb{N}_+ \}$.  Suppose we had $U \cap K = V \cup L$ where $V$ is open and $L$ is closed.  Then for each $n$, $\frac{1}{n} \notin V$ since otherwise $U \cap K = V \cup L$ would contain an open neighborhood of $\frac{1}{n}$, which is a contradiction.  Therefore, $V = \emptyset$, and it follows that $L = \{ \frac{1}{n} : n \in \mathbb{N}_+ \}$.  However, then $L$ is not closed, giving the desired contradiction.</p>
"
"2385860","2389095","<p>For those of you who can make do with an approximate solution. I found
one using small-angle approximations and Euler angles.</p>

<p>First, $T_{w \leftarrow c}$ can be expressed using <a href=""https://en.wikipedia.org/wiki/Euler_angles#Rotation_matrix"" rel=""nofollow noreferrer"">Tait-Bryan angles</a> in the
$YXZ$ order (yaw about $y$, pitch about $x'$, roll about $z^{\prime\prime}$).</p>

<p>In this order, condition (2) (maitaining the up direction) can be respected if we
restrict the roll angle to $0$.</p>

<p>For $\theta$ is the yaw angle about the $y$ axis and $\phi$ is the pitch angle
about the $x'$ axis, the corresponding transformation matrix is the following:</p>

<p>\begin{equation}
T_{w \leftarrow c} =
\begin{bmatrix}
  C_{\theta} &amp; S_\theta S_\phi &amp; S_\theta C_\phi \\
  0          &amp; C_\phi          &amp; - S_\phi        \\
  -S_\theta  &amp; C_\theta S_\phi &amp; C_\theta C_\phi
\end{bmatrix}
\end{equation}</p>

<p>But because the notation is getting tedious, from now on</p>

<p>\begin{equation}
T       = T_{w\leftarrow c} \\
p_{1_c} = p_{i_{camera}}    \\
p_{2_c} = p_{f_{camera}}    \\
p_w     = p_{world}.
\end{equation}</p>

<p>Recalling condition (1),</p>

<p>\begin{equation} \label{eq:T}
p_w = T_2 p_{2_c} = T_1 p_{1_c}
\end{equation}</p>

<p>Now, since we both know $p_{1_c}$ (the corresponding mouse position at the last
frame) and $p_{2_c}$ (the mouse position at the current frame). We know that</p>

<p>\begin{equation} \label{eq:d}
p_{1_c} = D p_{2_c}
\end{equation}</p>

<p>And thus,</p>

<p>\begin{equation}
T_2 p_{2_c} = T_1 D p_{2_c} \\
\Rightarrow T_2 = T_1 D \\
\end{equation}</p>

<p>Great! If I can solve (or approximate) $D$, I can find $T_2$!</p>

<p>Since $p_{1_c}$ and $p_{2_c}$ are ""not far away"" from each other. We can assume
that they are some small $\Delta \theta$ and small $\Delta \phi$ away from one
another.</p>

<p>\begin{equation}
D =
\begin{bmatrix}
  C_{\Delta\theta}  &amp; S_{\Delta\theta} S_{\Delta\phi} &amp; S_{\Delta\theta} C_{\Delta\phi} \\
  0                 &amp; C_{\Delta\phi}                  &amp; - S_{\Delta\phi}                \\
  -S_{\Delta\theta} &amp; C_{\Delta\theta} S_{\Delta\phi} &amp; C_{\Delta\theta} C_{\Delta\phi}
\end{bmatrix}
\end{equation}</p>

<p>Using <a href=""https://en.wikipedia.org/wiki/Small-angle_approximation"" rel=""nofollow noreferrer"">small-angle approximations</a>, we know</p>

<p>$$
\sin x \approx x \\
\cos x \approx 1 - \frac{x^2}{2}
$$</p>

<p>Knowing this and $p_{1_c} = D p_{2_c}$, we can get second degree polynomial
equations for $\Delta\theta$ and $\Delta\phi$. In fact, from the second row of
the matrix, we find that</p>

<p>$$
0 = - \frac{y_2}{2} \Delta\phi^2 -z_2 \Delta\phi + (y_2 - y_1) \\
$$</p>

<p>For which we can find two solutions. We will choose the one that respects the
small-angle assumption.</p>

<p>$$
\Delta\phi = \frac{-b \pm \sqrt{b^2 - 4 a c}}{2a} 
$$</p>

<p>And finally, from the first row of the matrix we get,</p>

<p>$$
0 = - \frac{x_2}{2} \Delta\theta^2 - (\Delta\phi y_2 + z_2 ( 1 - \frac{\Delta\phi ^2}{2})) \Delta\theta + (x2 - x1)
$$</p>

<p>For which we can find two solutions. And by choosing the one that respects the
small-angle assumption, we have successfully approximated $D$!</p>

<p>$$
\Delta\theta = \frac{-b \pm \sqrt{b^2 - 4 a c}}{2a} 
$$</p>

<p>Now that $D$ is approximated, we can calculate $T_2$ by adding $\Delta\theta$
and $\Delta\phi$ to the angles of $T_1$.</p>

<p>We're done!</p>
"
"2385865","2386090","<p>The given infinite product equals</p>

<p>$$ P=\exp\sum_{n\geq 1}\left(\frac{\log(4n+1)}{4n+1}-\frac{\log(4n-1)}{4n-1}\right)=\exp\sum_{n\geq 1}\frac{\chi(n)\log(n)}{n}. $$
with $\chi$ being the non-principal <a href=""https://en.wikipedia.org/wiki/Dirichlet_character"" rel=""nofollow noreferrer"">Dirichlet's character</a> $\!\!\pmod{4}$.<br>
In compact form, $P=\exp\left(-L'(\chi,1)\right)$. By Frullani's Theorem $\log(n)=\int_{0}^{+\infty}\frac{e^{-x}-e^{-nx}}{x}\,dx$, hence:</p>

<p>$$ P=\exp\left(-L'(\chi,1)\right) = \exp\int_{0}^{+\infty}\left(\frac{\pi}{4}e^{-x}-\arctan e^{-x}\right)\frac{dx}{x} $$
By considering the Taylor series of $\frac{\pi(1-z)}{4}-\arctan(1-z)$ around the origin we may easily derive the approximation $\color{blue}{P\approx 0.82}$. The previous integral representation recalls Binet's <a href=""http://mathworld.wolfram.com/BinetsLogGammaFormulas.html"" rel=""nofollow noreferrer"">second $\log\Gamma$ formula</a>. Indeed, by the Malmsten-Kummer Fourier series, for any $z\in(0,1)$ we have:</p>

<p>$$ \log\Gamma(z) = \left(\tfrac12 - z\right)(\gamma + \log 2) + (1 - z)\ln\pi
- \frac12\log\sin\pi z  + \frac{1}{\pi}\sum_{n=1}^\infty \frac{\sin 2\pi n z \cdot\log n} n$$</p>

<p>hence an explicit representation for $P$ can be simply found by plugging in $z=\frac{1}{4}$:</p>

<p>$$\boxed{ P = \color{blue}{\left(\frac{\Gamma\left(\frac{1}{4}\right)^4}{4\pi^3 e^{\gamma}}\right)^{\frac{\pi}{4}}}\approx 0.82456334.}$$</p>
"
"2385866","2386134","<p>As a rule of thumb you can build the graphs with $n$ vertices from the graphs of $n - 1$ vertices. Add one vertex to each graph and find all unique ways to connect it. It entails finding ""unique"" groups of vertices and connecting the new vertex to one or more vertices in these groups.</p>

<p>More practically: <a href=""https://math.stackexchange.com/questions/77200/is-there-a-simple-algorithm-to-generate-unlabeled-graphs"">this question</a> is almost the same as yoursâit just doesn't specify connectedness. The answer to use <a href=""http://users.cecs.anu.edu.au/~bdm/nauty/"" rel=""nofollow noreferrer"">Nauty</a> still holds, though: the <code>geng</code> program takes an option <code>-c</code> to only output connected graphs:</p>

<pre><code>geng -c 3 | listg -e
&gt;A geng -cd1D2 n=3 e=2-3
&gt;Z 2 graphs generated in 0.00 sec

Graph 1, order 3.
3 2
0 2  1 2

Graph 2, order 3.
3 3
0 1  0 2  1 2
</code></pre>
"
"2385869","2385899","<p>Your proof looks correct besides the small mistake that layman spotted out. As layman said, it's impressing that you went as far as finding $T,S\in\mathcal{L}(V)$ with $TSv\neq STv$ for every $v$ when $\dim V&gt;2$. I'd just like to point out two small things:</p>

<p>1- In your proof for the case $\dim V=2$, you didn't have to bother yourself calculating the images of $v_2$ by $ST$ and $TS$. Since $STv_1\neq TSv_1$ this shows that $ST\neq TS$.</p>

<p>2- You could use generalize your construction in the case of $\dim V=2$. Define $S$ and $T$ for $v_1$ and $v_2$ just as you did, and then for $v_3,\dots,v_n$ (if the dimension is larger than $2$) define their values in whatever way (say $Sv_i=Tv_i=v_i$ for $i\ge 3$, or $Sv_i=Tv_i=0$ for $i\ge 3$, or whatever). You will still have $TSv_1=0$ and $STv_1=v_2$, showing that $TS\neq ST$.</p>
"
"2385874","2385883","<p>HINT:</p>

<p>$$ \left(\begin{array}{c}n\\ k\end{array}\right)=\frac{n!}{r!(n-r)!} \\n=i+j-1 ,r=j \to (n-r)=i+j-1-i=j-1\\\frac{(i+j-1)!}{i!(j-1)!}= \left(\begin{array}{c}i+j-1\\ i\end{array}\right)=\left(\begin{array}{c}i+j-1\\ j-1\end{array}\right)$$this mean the ""how many solution for ""$x_1+x_2+...+x_j=i$  where $x_i \in \mathbb{Z}^+$</p>

<p>or , you can look at pascal triangle ,and find $2^k$  forms \begin{array}{rccccccccc}
n=0:&amp;    &amp;    &amp;    &amp;    &amp;  1\\\noalign
n=1:&amp;    &amp;    &amp;    &amp;  1 &amp;    &amp;  1\\\noalign
n=2:&amp;    &amp;    &amp;  1 &amp;    &amp;  2 &amp;    &amp;  1\\\noalign
n=3:&amp;    &amp;  1 &amp;    &amp;  3 &amp;    &amp;  3 &amp;    &amp;  1\\\noalign
n=4:&amp;  1 &amp;    &amp;  4 &amp;    &amp;  6 &amp;    &amp;  4 &amp;    &amp;  1\\\noalign
\end{array}</p>

<p>for example $$x_1+x_2=0 \to (0,0) $$is only solution $$\to 2^k=1 \to k=1\\i=0,j=2$$ or 
$$x_1+x_2=3 \to (3,0),(2,1),(1,2),(0,3) \to 2^k=4 \to k=2\\j=2,i=3$$
or 
$$x_1+x_2=7 \to 8-solution \space 2^k=8 \to k=3 \\j=2,i=7$$or 
$$x_1+x_2+...+x_8=1 \to 8-solution \space 2^k=8 \to k=3 \\j=8,i=1$$
and so on </p>
"
"2385878","2385945","<p>Let $g(x) = xf(x).$ Then $\int_0^1g(x)x^{2n}\, dx=0$ for $n\ge $ some $N.$ In particular</p>

<p>$$\tag 1 \int_0^1g(x)(x^{2N})^m\, dx=0,\,\, m=1,2,\dots.$$</p>

<p>Now by Stone-Weierstrass, polynomials in $x^{2N}$ are dense in $C[0,1].$ Hence there is a sequence of polynomials $p_k$ such that $p_k(x^{2N}) \to g(x)$ uniformly on $[0,1].$ But note $g(0)=0.$ It follows that $p_k(x^{2N})-p_n(0) \to g(x)$ uniformly on $[0,1].$ Since each $p_k(x^{2N})-p_n(0)$ is a finite linear combination of the monomials $(x^{2N})^m,\, m=1,2,\dots,$ we have by $(1)$</p>

<p>$$\int_0^1 g(x)[p_k(x^{2N})-p_n(0)]\,dx = 0$$</p>

<p>for all $k.$ Therefore $\int_0^1 g(x)^2\,dx =0,$ which implies $g\equiv 0,$ hence $f\equiv 0.$</p>
"
"2385881","2385895","<p><strong>Hint:</strong> A map $T:\Bbb C^n \to \Bbb C^n$ is invertible if and only if it has a trivial kernel, which is true if and only if $\|Tv\| = 0 \implies \|v\| = 0$.</p>
"
"2385903","2385921","<p>The answer is $n$. </p>

<p>First notice that the existence of distinct $A, B$ such that $A, B, A \cup B, A \cap B$ are of the same color is equivalent to existence of $A, B$ of the same color such that $A \subsetneq B$. </p>

<ol>
<li><p>The coloring for $p=n+1$ is as follows: let any $A$ have the color $|A| \in \{ 0, 1, \ldots, n \}$. Then obviosuly there are no $A \subsetneq B$ of the same color.</p></li>
<li><p>Suppose we have a coloring with $\leqslant n$ colors. Then at least two sets of $\varnothing, \{ 1 \}, \{ 1, 2 \}, \ldots, \{ 1, 2, \ldots, n \}$ must have the same color, and one is a strict subset of the other.</p></li>
</ol>
"
"2385906","2385914","<p><a href=""https://ncatlab.org/nlab/show/maximal+compact+subgroup#ExamplesForLieGroups"" rel=""nofollow noreferrer"">https://ncatlab.org/nlab/show/maximal+compact+subgroup#ExamplesForLieGroups</a> states that the maximal compact subgroup of E7 is $SU(8)/\mathbb{Z}_2$. It should be clear then that $SO(13)$ is a subgroup of E7 iff $SO(13)$ is a subgroup of $SU(8)/\mathbb{Z}_2$. This sounds very likely not the case but off the top of my head I'm not 100% certain how to show it.</p>

<p>Edit: $A_{13}$ is clearly a subgroup of $SO(13)$. Checking with <a href=""https://en.wikipedia.org/wiki/Representation_theory_of_the_symmetric_group"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Representation_theory_of_the_symmetric_group</a>, it sounds like $A_{13}$ needs at least 12 dimensions to be faithfully represented (maybe 13, I'm not sure I'm reading it correctly -- but it doesn't matter) even in the complex numbers, so $A_{13}$ is not a subgroup of $SU(8)$. Thus $A_{13}$ is a subgroup of $SO(13)$ but not the maximal compact subgroup of E7, so E7 doesn't contain $SO(13)$.</p>

<p>I think that in general as long as one of your lie groups is compact, comparing with the maximal compact subgroup -- which is well understood -- is a good first step to establishing existence as a subgroup.</p>
"
"2385927","2385944","<p>The function $y=x^n$ isn't defined for $x&lt;0$.</p>

<p>Thus, we'll only discuss the case when $x&gt;0$.</p>

<p>Since greatest integer of any number $\in (0,1)=0$. Hence, $\lfloor x \rfloor$, when $x\to 0^+$ is equal to $0$.</p>

<p>Thus \begin{align}
\lim_{x\to 0} \frac{(\log_e x^n)-\lfloor x\rfloor}{x}
&amp;=\lim_{x\to 0} \frac{(\log_e x^n)}{x}\\
&amp;=\lim_{x\to 0} \frac{n (\log_e x)}{x}\\
&amp;=n \cdot \lim_{x\to 0} \frac{(\log_e x)}{x}\to n \cdot -\infty \to -\infty
\end{align}</p>

<p>Thus, the limit doesn't exist.</p>
"
"2385934","2385941","<p>No.  Try  $a_{2^k} = 1/k^2$, $a_n = 1/n^2$ if $n$ is not a power of $2$.<br>
Then $2^k a_{2^k}^2 = 2^k/k^4 \to \infty$, so $\sum_n n a_n^2 = \infty$.</p>
"
"2385940","2385954","<p>Note that we can use the trigonometric identities, $\sin(\theta)=2\sin(\theta/2)\cos(\theta/2)$ and $\frac12(1-\cos(\theta))=\sin^2(\theta/2)$, to write $x(\theta)$ and $y(\theta)$ as</p>

<p>$$\begin{align}
&amp;x(\theta)=k^2(\theta/2 -\sin(\theta/2)\cos(\theta/2))\tag1\\\\
&amp;y(\theta)=k^2 \sin^2(\theta/2)\tag2
\end{align}$$</p>

<p>Then, it is easy to see that we have </p>

<p>$$\frac{dy}{dx}=\cot(\theta/2) \tag3$$</p>

<p>Finally, using $(1)$ and $(3)$ it is straightforward to show that</p>

<p>$$\left(1+\left(\frac{dy}{dx}\right)^2\right)y=k^2$$</p>

<p>as was to be shown!</p>
"
"2385946","2385950","<p>Think about what the left hand side does: It rounds $n/b$ up to the nearest integer.</p>

<p>Now think about what the right hand side does: It just computes $n/b$ and then increases it by $x/b$. </p>

<p>So the difference (not accounting for the increase) in the outputs is less than $1$. This should suggest a good choice of $x/b$, or alternatively a good choice of $x$, given $b$.</p>

<p>But note that $x$ must depend on $b$; if it's too small relative to $b$, it will not work.</p>
"
"2385949","2391232","<p>When going through Jack's nice answer I did some intermediate steps to better see what's going on. Here is a somewhat more elaborated version, which might also be convenient for other readers.</p>

<blockquote>
  <p>We  obtain
  \begin{align*}
\color{blue}{\sum_{k=0}^{2n}}&amp;\color{blue}{(-1)^k\binom{4n}{2k}\binom{2n}{k}^{-1}}\\
&amp;=\sum_{k=0}^{2n}\binom{4n}{2k}(2n+1)\int_0^1(1-x)^kx^{2n-k}\,dx\tag{1}\\
&amp;=(2n+1)\int_0^1x^{2n}\sum_{k=0}^{2n}\binom{4n}{2k}\left(-\frac{1-x}{x}\right)^k\,dx\tag{2}\\
&amp;=(2n+1)\int_0^1x^{2n}\cdot\frac{1}{2}
\left(\left(1+i\sqrt{\frac{1-x}{x}}\right)^{4n}+\left(1-i\sqrt{\frac{1-x}{x}}\right)^{4n}\right)\,dx\tag{3}\\
&amp;=\frac{2n+1}{2}\int_0^1\left(\sqrt{x}+i\sqrt{1-x}\right)^{4n}+\left(\sqrt{x}-i\sqrt{1-x}\right)^{4n}\,dx\tag{4}\\
&amp;=(2n+1)\int_{0}^{\frac{\pi}{2}}
\left[(\cos \theta+i\sin \theta)^{4n}+(\cos\theta-i\sin \theta)^{4n}\right]\cos \theta\sin \theta\,d\theta\tag{5}\\
&amp;=(2n+1)\int_{0}^{\frac{\pi}{2}}
\left[e^{4ni\theta}+e^{-4ni\theta}\right]\cos \theta\sin \theta\,d\theta\tag{6}\\
&amp;=(2n+1)\int_{0}^{\frac{\pi}{2}}\cos(4n\theta)\sin(2\theta)\,d\theta\tag{7}\\
&amp;=\frac{2n+1}{2}\int_{0}^{\frac{\pi}{2}}\left[\sin( (4n+2)\theta)-\sin ((4n-2)\theta))\right]\,d\theta\tag{8}\\
&amp;=\frac{2n+1}{2}\left[-\frac{1}{4n+2}\cos((4n+2)\theta)
+\frac{1}{4n-2}\cos((4n-2)\theta\right]_0^{\frac{\pi}{2}}\\
&amp;=\frac{2n+1}{2}\left(\frac{1}{2n+1}-\frac{1}{2n-1}\right)\\
&amp;\color{blue}{=\frac{1}{1-2n}}
\end{align*}</p>
</blockquote>

<p><em>Comment:</em></p>

<ul>
<li><p>In (1) we write the reciprocal of a binomial coefficient using the <em><a href=""https://en.wikipedia.org/wiki/Beta_function"" rel=""nofollow noreferrer"">beta function</a></em>
\begin{align*}
\binom{n}{k}^{-1}=(n+1)\int_0^1z^k(1-z)^{n-k}\,dz
\end{align*}</p></li>
<li><p>In (2) we do some rearrangements in order to apply the binomial theorem.</p></li>
<li><p>In (3) we consider the even function derived from  $(1+z)^{2n}$
\begin{align*}
\sum_{k=0}^{n}\binom{2n}{2k}z^{2k}=\frac{1}{2}\left((1+z)^{2n}+(1-z)^{2n}\right)
\end{align*}
Replacing  $n$ with $2n$  and $z$ with $i\sqrt{\frac{1-x}{x}}$ and the application of de Moivre's theorem in (6) becomes plausible.</p></li>
<li><p>In (4) we do some simplifications.</p></li>
<li><p>In (5) we substitute $x=\cos ^2\theta, dx=-2\cos\theta\sin\theta\,d\theta$.</p></li>
<li><p>In (6) we apply De Moivre's theorem  and in (7) and (8) trigonometric sum formulas.</p></li>
</ul>
"
"2385952","2385972","<p>Given $t$ in the domain of $X$, take $M$ so that $M&gt;\frac{X(t)}{k}$. This is possible because $X$ is finite and $k$ is fixed. Then, for all $c\geq M$, we have
$1_{X&lt;kc}(t) = 1$ because $X(t)&lt;Mk\leq ck$. Therefore $\lim_{c\to\infty}1_{X&lt;kc}(t)=1$ for each $t$, which means the pointwise limit $\lim_{c\to\infty}1_{\{X&lt;kc\}}$ is $1$.</p>
"
"2385953","2385978","<p>I think it's better to know geometry good enough.</p>

<p>For example if you know that if $CD$ is an altitude of $\Delta ABC$ and $\measuredangle ACB=90^{\circ}$ then $CD^2=AD\cdot BD$, so you can prove the following inequality immediately.</p>

<p>For all $x&gt;0$ prove that:
$$(e^x-1)\ln(1+x)\geq x^2,$$
otherwise it's much harder.</p>

<p>But to know trigonometry you need very good.  </p>
"
"2385961","2385971","<p>If $0\in S$, then $R_S$ is the zero ring, yes, because $\frac00$ is equivalent to any other fraction, as you have seen, so the ring necessarily contains only one element.</p>

<p>The map $R\to R_S$ given by $x\mapsto \frac x1$ is injective iff $S$ contains no zero divisors. If $a\in S$ is a zero divisor, and $b\in R$ is such that $ab=0$, then $\frac b1=\frac01$. But if $b\notin S$, $R_S$ is not necessarily the zero ring.</p>

<p>Your $\frac ab$ doesn't make sense because we don't know that $b\in S$.</p>

<p>I think I covered most of it. Let me know if there is something I missed.</p>

<p>Finally, the $s(ad-bc)=0$ requirement I like to think of as ""you can expand both fractions so that numerators are equal and denominators are equal"". It's a lot more intuitive in my opinion. Just remember that you're only allowed to use elements of $S$ when expanding.</p>
"
"2385968","2385979","<p>This is all fake because you are manipulating a divergent series as if it were convergent. If we are allowed this, we can show that $\sum_{n=1}^\infty (-1)^n$ is equal to any real number.</p>

<p>Indeed, suppose $a$ is a real number. Then
$$
\sum_{n=1}^\infty (-1)^n = -1 + 1 -1+1-1+1-+\cdots
= (-1+1)+(-1+1)+\cdots
= 0
= 0a
= ((-1+1)+(-1+1)+\cdots)a
= ((1-1)+(1-1)+\cdots)a
=(1+((-1+1)+(-1+1)+\cdots))a
=(1+0)a
=a.
$$</p>
"
"2385976","2386841","<p>The fact that $t_i=0$ for some combinations of $n$ and $k$, and hence becomes independent of the values of $p_j$ already indicates that it is the sum over $q$ that makes this happen. To prove this we start with the general binomial expansion:
$$
(a + b)^m = \sum_{q=0}^m \binom{m}{q} b^{m-k} a^q
$$
for some integer $m \geq 1$ and apply the operation $a \frac{\partial}{\partial a}$ several times, say $l \geq 0$ times, on both the left and right hand side. This gives us 
$$
\left[a \frac{\partial}{\partial a}\right]^l (a + b)^m = \sum_{q=0}^m \binom{m}{q} b^{m-k} \left[a \frac{\partial}{\partial a}\right]^l a^q = \sum_{q=0}^m \binom{m}{q} b^{m-k} q^l a^q
$$
which is generally true for all $a,b$ and integers $l,m$. It is easy to prove/convince your self that for $l&lt;m$ the left hand side is divisible by $(a+b)^{m-l}$, and therefore if we insert $a=-1$, $b=1
$, and restrict $0 \leq l &lt; m$ that we get the general result
$$
\sum_{q=0}^m (-)^q \binom{m}{q} q^l = 0
$$
and from that it follows immediately that
$$
\sum_{q=0}^m (-)^q \binom{m}{q} (j+q)^l = 0
$$
for any value of $j$ and $0 \leq l &lt; m$. Setting $m = k n - j$ and $l=i-1$ we find that the $j$th term of $t_i$ is zero whenever $0 \leq i-1 &lt; k n - j$ and hence we get $$j &lt; kn-i+1$$
The sum over $j$ however is restricted by
$$1 \leq j \leq kn - k + 1 &lt; kn -i+1$$
for all $1 \leq i&lt;k$, and therefore every term in the summation over $j$ is identical zero, which gives the results that $t_i=0$ for $1 \leq i &lt; k$.</p>

<p>Note that neither the restriction $n \leq k$ nor the actual values of the $p_i$ are required for this to be true, but it might be imposed by the underlying problem of interest. </p>
"
"2385988","2386003","<p>If, as I suspect, you need a pedagogical way to convince (not only to prove) that $0.\bar 9=1$, I suggest also this other approach. You can use both, of course.</p>

<p>Ask your pupils what is the smallest positive number. Make them remember that $0$ is not positive, and that we are speaking of real numbers (or numbers with digits after the point), so $1$ is not the answer.</p>

<p>Make them note that this very small number is divided by $2$, we get an even smaller number. So, much like there are no biggest number because any big number can be multiplied by $2$ (a fact your pupils should easily accept), there is also no smallest positive number.</p>

<p>After this, explain that if $1-0.\bar 9$ were positive, it would be the smallest positive number, a thing that can not exist, so it must be $0$.</p>
"
"2386015","2386638","<blockquote>
  <p>When looking at the expression
  \begin{align*}
S_n=\sum_{k=0}^n kn\tag{1}
\end{align*}
  the symbol $n$ on <em>both</em> sides identifies one and the same variable $n$. This means that <em>substitution</em> of the symbol $n$ by $n+1$ has to be done at each occurrence of $n$ at <em>both</em> sides simultaneously.</p>
  
  <p>We obtain
  \begin{align*}
S_{n+1}&amp;=\sum_{k=0}^{n+1}k(n+1)=(n+1)\sum_{k=0}^{n+1}k\tag{2}\\
&amp;=\left(\sum_{k=0}^n k(n+1)\right)+(n+1)^2
\end{align*}</p>
</blockquote>

<p>Note that in (2) we do <em>not</em> have the same situation as in (1), namely
\begin{align*}
S_n=\sum_{k=0}^n a_k
\end{align*}
but rather a <em>generalisation</em> in the form
\begin{align*}
S_n=\sum_{k=0}^n a_{k,n}
\end{align*}
and again we obtain by substituting $n+1$ for $n$
\begin{align*}
S_{n+1}=\sum_{k=0}^{n+1} a_{k,n+1}
\end{align*}</p>
"
"2386030","2386037","<p>Assume $a\ne 0$.</p>

<p>Sum $S $ of roots is then integer.</p>

<p>$$S=-\frac{a+1}{a}=-1-\frac {1}{a} \in \Bbb Z$$</p>

<p>$\implies  a=1$ or $a=-1$.</p>

<p>for $a=1$, roots are $0,-2$</p>

<p>for $a=-1$ , no root.</p>

<p>If $a=0$, the root is $x=1$.</p>

<p>Finally, $$a\in\{0,1\} $$</p>
"
"2386042","2386046","<p>It's true, but another way:</p>

<p>In any metric space, finite or not, all singletons are closed. So finite sets are closed.</p>

<p>In a finite metric space, any subset has a finite (hence closed) complement. So all sets are open.</p>
"
"2386050","2386081","<p>In any matroid that comes from a partial order, the feasible sets are closed under intersections, since the lower sets in a poset are closed under intersections.  As an example where this fails, consider the antimatroid on the set $S=\{a,b,c\}$ in which every subset except $\{c\}$ is feasible.  This antimatroid cannot come from a partial order since $\{a,c\}$ and $\{b,c\}$ are feasible but their intersection is not.</p>
"
"2386055","2386070","<p>Fix a symmetric connection $\partial$ to use as an origin for $\mathcal A$. Any connection $\nabla \in \mathcal A$ can be written $\nabla = \partial + \Gamma$ for some $(1,2)$-tensor $\Gamma$, which can be naturally decomposed as $$\Gamma = \mathrm{Sym}(\Gamma) + \frac 1 2 \tau^\nabla$$ where $\tau^\nabla = 
2 \mathrm{Alt}(\Gamma)$ is the torsion of $\nabla$. Since the sets of symmetric and antisymmetric tensors are vector spaces giving a direct sum decomposition of $T^1_2$, we can take the linear projection on to the symmetric tensors along the antisymmetric ones; i.e. ""taking the symmetric part"". </p>

<p>Since the torsion  of a connection is well-defined (i.e. does not depend on our choice of $\partial$), the antisymmetric subspace also does not depend on $\partial$; so this gives us an honest affine projection $P:\mathcal A \twoheadrightarrow \mathcal S$ which we can write as $$P(\nabla) = \nabla - \frac 1 2 \tau^\nabla,$$ or more explicitly $$P(\nabla)_X Y = \nabla_X Y - \frac 1 2\tau^\nabla(X,Y) = \frac 1 2 \left( \nabla_X Y + \nabla_Y X + [X,Y] \right).$$ There will of course be many, many different projections: this is just a fact of linear algebra. I would argue that this one is the most geometrically natural, however: it's the unique symmetric connection having the same geodesics as $\nabla$. </p>

<p>It's probably also natural/canonical in some rigorous sense: I can't think of any other projection $\mathcal A \twoheadrightarrow \mathcal S$ we could write down without making some arbitrary choice.</p>
"
"2386057","2386064","<p>Assume the series $\sum p (i) $ convergent and its sum is $1$.</p>

<p>then</p>

<p>$$\sum_{i=0}^{+\infty}p (i)=1$$
$$p (0)+p (1)+p (2)+....=$$
$$p (0)+p (1)+...p (s)+p (s+1)+...=$$
$$\sum_{i=0}^sp (i )+\sum_{i=s+1}^{+\infty}p (i) $$</p>

<p>thus</p>

<p>$$\sum_{i=s+1}^{+\infty}p (i)=1-\sum_{i=0}^sp (i) $$</p>
"
"2386061","2386089","<p>As you have guessed, this is just a typo: $m$ should be the order of $\beta$, not the order of $\alpha$.</p>
"
"2386076","2386127","<p>You are right: If the empty space counts as connected, then it needs to be explicitly excluded from the claim.</p>
"
"2386077","2386124","<p>Let $n=1$, $d = p^p - 1$, and $\alpha = 1$. The roots of $f$ are precisely the nonzero elements of $\mathbb{F}_{p^p}$, the degree $p$ extension of $\mathbb{F}_p$.</p>
"
"2386079","2386083","<p>For all $A$ and $B$, the null set is indeed a subset of $A\times B$, so it fits the definition of a morphism $A\to B$. What else is there to say?</p>
"
"2386099","2386167","<p>In general, no, not without a lot of additional information about the functional form of $P_{XY|Z}(x,y|z)$.</p>
"
"2386102","2387291","<p>Short answer: Yes, there is a representational face lattice. The tridiagonal Birkhoff polytope of dimension d is the polytope of tridiagonal, doubly stochastic $(d+1) \times (d+1)$ matrices $A$. We begin motivating the face representation system by numbering the $f_{d+2}$ vertices of $\Omega^t_{d+1}$ by reading off the values of the superdiagonal entries from upper left to lower right. As the possible values of these superdiagonal entries are 0 and 1, and there can be no pair of consecutive 1's, each vertex is numbered as the <strong>fibinary</strong> representation of the integers from 0 to $f_{d+2} - 1$ (where $f_n$ is the Fibonacci series). (The fibinary integer representation, which has been used in programming competitions, uses each possible string of 0's and 1's without leading zeroes and with no consecutive pair of 1's, listed in lexicographical order.) We'll retain leading zeroes, so that each vertex is represented by a string of d 0's and 1's - with no consecutive pair of 1's allowed (and each such string represents a vertex). We'll refer to the location of each 0 and 1 as a (fibinary) place, analogous to a decimal place.</p>

<p>Now, we will allow variables to occur in each place instead of 0's and 1's (retaining the rule about no consecutive pair of 1's). The variable X has possible values 0 or 1. $\Omega^t_{d+1}$ is represented by a string of d X's, meaning that $\Omega^t_{d+1}$ is the convex hull of all of the $f_{d+2}$ vertices - the solution space of the string of $d$ X's.</p>

<p>Consider the facets of $\Omega^t_{d+1}$ given by setting superdiagonal entries equal to zero. The facet given by $a_{i,i+1} = 0$ is represented by a string of X's, except with a 0 in the $i_{th}$ place - this facet is the convex hull of all vertices that have 0 in the $i_{th}$ place of their fibinary representation. Call these facets $Z_i$, $i$ = 1 to $d$.</p>

<p>Next consider the facets given by setting the main diagonal entries equal to zero. Because A is doubly stochastic, symmetric and tridiagonal, for 2 $\le$ $i$ $\le$ $d$, $a_{i,i} = 0$ if and only if ($a_{i-1,i} = 1$ or $a_{i,i+1} = 1$). The facet $a_{i,i} = 0$ is represented by a string of X's, except with YY in places $i - 1$ and $i$. Call these facets $Y_i$, $i$ = 2 to d. A string of Y's (of length two or more) is evaluated as a single <strong>zebra-striped</strong> field. That is, a Y-field represents alternating 1's and 0's, and thus has two possible solutions (beginning with 0 and beginning with 1, and alternating for the rest of the field). We have now represented all $2d - 1$ facets of $\Omega^t_{d+1}$.</p>

<p>More generally, the representations of all $\Omega^t_{d+1}$  faces are all the strings of $d$ symbols which may be 0, 1, X or Y, with Y occurring in fields comprising two or more places, with no pair of consecutive 1's and with 1 not adjoining any variable field. Each 'X' is called an X-field, and X and Y-fields are called variable fields. Each '0' and '1' is called a numeric field. (Adjoining Y-fields are separated by a comma. Field separation between X-fields, between X and Y-fields, and between an X or Y-field and a numeric field are implied; a comma is not needed.) The numeral 1 cannot adjoin a variable field (i.e., X or Y-field), because it is known that the adjacent place to 1 in any solution must be 0 (hence, the variable field would not be variable). Finally, we define a variable region as a maximal sub-string of adjoining variable fields with no numeric fields included.</p>

<p>Regarding the first sentence of the preceding paragraph, we will first prove by math induction that the representation of each $\Omega^t_{d+1}$ face is a string as described. The representations of $\Omega^t_{d+1}$ itself and its facets (dimension $d - 1$) are strings of $d$ symbols as described. Assume that all faces of dimension $d$ through $d - k$ have representations that are strings of d symbols which are 0, 1, X or Y, with Y occurring in fields of two or more places, with no pair of consecutive 1's, and with 1 not adjoining any variable field. To prove: faces of dimension $d - k - 1$ have such representations too. We'll evaluate all non-empty intersections of a ($d - k$)-face and a facet of $\Omega^t_{d+1}$, and thus evaluate all ($d - k - 1$)-faces along with (possibly) some lower dimensional faces.</p>

<p>Let F be a ($d - k$)-face of $\Omega^t_{d+1}$ (we'll use the same name for a face and its representation). Begin by considering non-empty intersections of F and facet $Z_i$, where F is not a face of $Z_i$. The $i_{th}$ place of F can't be a 0 (this would imply that F is a face of $Z_i$), and it can't be a 1 (this would produce an empty intersection). If the $i_{th}$ place of F is an X, the intersection $F \cap Z_i$ has the same representation as F, but with a 0 in the $i_{th}$ place - the representation criteria are met. If the $i_{th}$ place of F is a Y, we know that the $i_{th}$ place is within a Y-field (we'll call this F's Y-field); setting the $i_{th}$ place to 0 determines a zebra-striped numeric string of 0's and 1's in $F \cap Z_i$ to occupy the places of F's Y-field. We have some cases to consider:</p>

<p>Case 1: F's Y-field covers an even number of places. In Case 1a, the zebra-striped numeric string in $F \cap Z_i$ occupying the places of F's Y-field begins with 0 and ends with 1. If there are no additional variable fields adjoining F's Y-field to the right in the variable region, no further changes are necessary - $F \cap Z_i$ has the same representation as F, except with the zebra-striped numeric string beginning with 0 occupying the same places as F's Y-field. The criteria are met because the right end of F's Y-field is either the end of the representation, or a numeric region begins just to the right, starting with 0. If there are additional variable fields to the right of F's Y-field in the variable region, first consider the case that they are all even-length. In that case, each variable field to the right must be converted to a zebra-striped numeric string beginning with 0 and ending with 1, because the variable field just to its left was converted to a zebra-striped numeric string ending in 1. Again, the criteria are met because the right end of F's Y-field is either the end of the representation, or a numeric region begins just to the right, starting with 0. Say that one or more of the additional variable fields to the right of F's Y-field in the variable region is odd-length. Moving to the right from F's Y-field, each subsequent variable field must be converted to a zebra-striped numeric field beginning with 0, up to and including the first odd-length variable field encountered, which ends in a 0. Thus, the criteria are met.
In Case 1b, the zebra-striped numeric string in $F \cap Z_i$ occupying the places of F's Y-field begins with 1 and ends with 0. This case proceeds exactly like Case 1a, except moving to the left from F's Y-field instead of to the right. We conclude that the criteria are met in this case.</p>

<p>Case 2: F's Y-field covers an odd number of places. For Case 2a, both ends of the zebra-striped string occupying the places of F's Y-field are 0's. In this case, the representation of $F \cap Z_i$, is the same as that of F, except with the zebra-striped numeric string replacing the Y-field; the representation criteria are met.
For Case 2b, both ends of the zebra-striped numeric string occupying the places of F's Y-field are 1's. The argument for Case 2b is essentially a combination of Case 1a and 1b; conversion to zebra-striped numeric strings must occur both to the left and to the right. We conclude that the criteria are met in this case.</p>

<p>Now, consider non-empty intersections of F and facet $Y_i$, where F is not a face of $Y_i$. The $i_{th}$ and preceding place of F can't be 'YY', '01', or '10' (these would imply that F is a face of $Y_i$), and it can't be '00' (this would produce an empty intersection). If the $i_{th}$ and preceding place of F is 'XX', the intersection $F \cap Y_i$ has the same representation as F, but with a 'YY' in the $i_{th}$ and preceding place - the representation criteria are met (add a comma if this new Y-field adjoins another Y-field). 
If the $i_{th}$ and preceding place of F is 'XY', then the $i_{th}$ place is at the beginning of a Y-field in F. Solutions producing a 0 in the $i_{th}$ place imply that place $i - 1$ must be a 1 in the intersection (by definition of $Y_i$); solutions producing a 1 in the $i_{th}$ place imply that place $i - 1$ is 0 (by the prohibition on consecutive 1's). Thus, in $F \cap Y_i$, the string from place $i - 1$ to the end of the Y-string (in F) is zebra-striped. Thus, the representation of $F \cap Y_i$ has the same representation as F, but with the Y-field starting in the $i_{th}$ place extended back to begin in place $i - 1$ - the representation criteria are met (add a comma if this expanded Y-field adjoins another Y-field to the left). If the $i_{th}$ and preceding place of F is 'YX', then a similar argument applies; the representation of $F \cap Y_i$ has the same representation as F, but with the Y-field ending in place $i - 1$ extended forward to end in the $i_{th}$ place - the representation criteria are met (add a comma if this expanded Y-field adjoins another Y-field to the right).</p>

<p>Next case: the $i_{th}$ and preceding place if F is '0X'. The $i_{th}$ place of $F \cap Y_i$ must be 1, and the $(i+1)_{th}$ place must then be 0. If was already a 0 in F, then we've determined $F \cap Y_i$ and the criteria are met. If there is a variable field beginning with the $(i+1)_{th}$ place of F, we proceed with a similar argument as Case 1a of $F \cap Z_i$; conclude that the criteria are met. If the $i_{th}$ and preceding place of F is 'X0', we make a similar argument, working to the left instead of to the right, and conclude that the criteria are met.</p>

<p>Next case: the $i_{th}$ and preceding place if F is '0Y'. The $i_{th}$ place of $F \cap Y_i$ must be 1. A Y-field of F begins in place $i$; if it is even-length it ends in 0, and the representation of $F \cap Y_i$ is the same as F, except with a zebra-striped numeric string beginning with 1 occupying the same places as F's Y-field. If the Y-field of F beginning in place $i$ is odd-length, it ends in 1. We proceed with a similar argument as Case 1a of $F \cap Z_i$; conclude that the criteria are met. If the $i_{th}$ and preceding place of F is 'Y0', we make a similar argument, working to the left instead of to the right, and conclude that the criteria are met.</p>

<p>Having exhausted all cases, we proceed to the converse of the preceding argument by showing that each described representation corresponds to a face:</p>

<p>$\circ$  Each Y-field occupying places $i$ to $j$ is from the intersection of $Y_{i+1}$ to $Y_j$.</p>

<p>$\circ$  Each '010' in places $i$ through $i + 2$ is from the intersection of $Z_i$, $Z_{i+2}$, $Y_{i+1}$, and $Y_{i+2}$ (note that this case also covers longer zebra-striped strings beginning and ending with 0's, such as '01010').</p>

<p>$\circ$  '10' in places $1$ through $2$ is from the intersection of $Z_2$ and $Y_2$.</p>

<p>$\circ$  '01' in places $d - 1$ through $d$ is from the intersection of $Y_d$ and $Z_{d-1}$.</p>

<p>$\circ$  Each other '0' is from the corresponding $Z_i$.</p>

<p>$\circ$  Each 'X' place results from no $Z_i$ or $Y_i$ facet containing a '0' or 'Y' in that place being included in the intersection.</p>

<p>We turn to the evaluation of whether one $\Omega^t_{d+1}$ face is a face of another $\Omega^t_{d+1}$ face. Given representations of faces A and B of $\Omega^t_{d+1}$, A is a face of B if and only if all three of the following are true:</p>

<ol>
<li><p>For each Y-field of B, the corresponding places of A are either all Y's, or zebra-striped numeric.</p></li>
<li><p>For each 0 (1) in B, there is a 0 (1) in the corresponding place of A.</p></li>
<li><p>For each comma in A's representation (between two Y-fields) there is a field separation in the corresponding place of B.</p></li>
</ol>

<p>First we'll assume A is a face of B and prove statements 1 - 3.</p>

<ol>
<li><p>'00' cannot appear within the places of A corresponding to B's Y-field, because any numeric solution to the Y-field must be zebra-striped. Say 'X' appears within the places of A corresponding to B's Y-field. The 'X' and an adjoining place within the places corresponding to B's Y-field could both be 0; the adjoining place is either one end of a variable field, or a 0; thus, a contradiction as any numeric solution to B's Y-field must be zebra-striped. Say both a Y and a 0 appear within the places corresponding to B's Y-field. The last place in the variable region between the Y and the 0 could be a 0, and it adjoins a 0 (at the near end of the numeric region next to the variable region). Likewise, this is a contradiction. The only remaining possibilities are for the corresponding places of A to be all Y's or zebra-striped numeric.</p></li>
<li><p>For each 0 in B, the corresponding place in A can't include the possibility of being solved as 1; thus the corresponding place can't be 1, X or Y; therefore, it must be 0. For each 1 in B, the corresponding place in A can't include the possibility of being solved as 0; thus the corresponding place can't be 0, X or Y; therefore, it must be 1.</p></li>
<li><p>Say there is a comma separating Y-fields in A ending in place $i$ and beginning in place $i + 1$, but there not a field separation between places $i$ and $i + 1$ in B; thus, places $i$ and $i + 1$ in B must be within a Y-field. Hence, there are two possible solutions of places $i$ and $i + 1$ in B; that is '01' and '10'. However, as they are in separate Y-fields in A, places $i$ and $i + 1$ could be '00', leading to a contradiction.</p></li>
</ol>

<p>For the converse, we will assume that statements 1 - 3 are true. We'll show that every $\Omega^t_{d+1}$ facet that B is a face of, A is also a face of, implying that A is a face of B.</p>

<p>$\circ$  Each Y-field in B occupying places $i$ to $j$ is from the intersection of $Y_{i+1}$ to $Y_j$. As the corresponding places of A are either all Y's (thus A is also a face of each of these facets), or zebra-striped numeric (also implying A is a face of each of $Y_{i+1}$ to $Y_j$, plus a face of some Z-facets).</p>

<p>$\circ$  Each '010' in places $i$ through $i + 2$ of B is from the intersection of $Z_i$, $Z_{i+2}$, $Y_{i+1}$, and $Y_{i+2}$. '10' in places $1$ through $2$ (if it occurs in B) is from the intersection of $Z_2$ and $Y_2$. '01' in places $d - 1$ through $d$ (if it occurs in B) is from the intersection of $Y_d$ and $Z_{d-1}$. Each other '0' in B is from the corresponding $Z_i$. Because the numeric places of B are identical in A, A is also a face of each of the facets described in this paragraph that B is a face of.</p>

<p>$\circ$  We have described every $\Omega^t_{d+1}$ facet that B is a face of, and shown that A is also a facet of these. Therefore, A is a face of B.</p>
"
"2386120","2386160","<p>I think the question is crystal clear.</p>

<blockquote>
  <p>If some function $f:\mathbb{R}^+\to\mathbb{R}$ is weakly Riemann-integrable over $\mathbb{R}^+$, meaning that
  $$ \lim_{t\to +\infty} \int_{0}^{t}f(u)\,du = C&lt;+\infty, $$
  is it true that for any $s&gt;0$
  $$ (\mathcal{L} f)(s) = \lim_{t\to +\infty}\int_{0}^{t} e^{-us}f(u)\,du $$
  is finite?</p>
</blockquote>

<p>The answer is <strong>yes</strong> and the proof goes by integration by parts. Since the first limit is finite,
$$ M = \sup_{t\in\mathbb{R}^+}\left|\int_{0}^{t}f(u)\,du\right| $$
is finite as well. For any $s,t&gt;0$, by denoting as $F(t)=\int_{0}^{t}f(u)\,du$, we have:</p>

<p>$$ \int_{0}^{t}e^{-su}f(u)\,du = \left[e^{-su}F(u)\right]_{0}^{t}+s\int_{0}^{t}e^{-su}F(u)\,du $$
where the absolute value of the RHS is bounded by
$$ 2M+Ms\int_{0}^{t}e^{-su}\,du \leq 2M+Ms\int_{0}^{+\infty}e^{-su}\,du=3M $$
no matter what the value of $t$ is. By considering the limit as $t\to +\infty$, the claim follows.</p>
"
"2386122","2386123","<p>We let:</p>

<p>$$x_1 = y \\ x_2 = y'$$</p>

<p>Next</p>

<p>$$x_1' = y' = x_2 \\ x_2' = y'' = -y' + 6 y = - x_2  + 6 x_1$$</p>

<p>We can now write</p>

<p>$$X' = \begin{bmatrix} 0 &amp; 1 \\ 6 &amp; -1 \end{bmatrix}x$$</p>

<p>Can you take it from here?</p>

<p>Note: as an alternate approach, we can find the roots of the characteristic equation $\lambda^2 + \lambda - 6 = 0 \implies \lambda_1 = -3, \lambda_2 = 2.$</p>
"
"2386128","2386742","<p>You need $p\alpha+p-n+1&lt;1$ instead, so that</p>

<p>$$\frac{1}{\alpha}\vert\vert Du\vert\vert_p^p=C_n\int_0^1\frac{1}{r^{p\alpha+p-n+1}}dr=C_n\int_0^1r^{-(p\alpha+p-n+1)}dr&lt;\infty,$$</p>

<p>which comes from ensuring the antiderivative of $r^{-(p\alpha+p-n+1)}$ doesn't blow up at $0$.</p>

<p>Note your expression for $Du$ is correct but with bars around it.</p>
"
"2386171","2386226","<blockquote>
  <p>It can be shown that the mgf of the standard normal = $e^{\frac{t^2}{2}}$ so $m_{w-z}(w-z) =e^{\frac{-t^2}{2}}e^{\frac{-t^2}{2}} =1 $ </p>
</blockquote>

<p>It can be shown that the mgf of the standard normal random variable, such as $Z$ is: $\mathsf M_Z(t) = e^{\frac{t^2}{2}}$ so $$\mathsf M_{W-Z}(t) ~{=\mathsf M_W(t)\,\mathsf M_{-Z}(t) \\=e^{\frac{t^2}{2}}e^{\frac{t^2}{2}} \\= e^{t^2} \\= \mathsf M_{Z}(\pm t\surd 2) \\ = \mathsf E (e^{\pm tZ\surd 2})}$$</p>

<p>Now... What does that mean?</p>
"
"2386173","2386177","<p>Hint: take a circular contour just inside the disk, and use the Argument Principle.</p>
"
"2386182","2386188","<p>You can write that there exists $N$ such that $n&gt;N$ implies that $x_0+\xi_n&lt;y$, you deduce that $f(x+\xi_n)\leq f(y)$ and $lim_nf(x+\xi_n)\leq f(y)&lt;U$. Contradiction.</p>
"
"2386184","2386234","<p>If you have $(a_1,a_2,...,a_n)$ and you choose $a_i$ and $a_j$ with $i&lt;j$, and replace $a_i$ with the modulo and $a_j$ with the product, you may notice that the modulo is less than the minimum of $a_i$ and $a_j$.</p>

<p>So the new tuple is smaller than the initial one via <a href=""https://en.wikipedia.org/wiki/Lexicographical_order"" rel=""nofollow noreferrer"">lexicographical order</a>.</p>

<p>Now use that $\mathbb N^n$ is <a href=""https://proofwiki.org/wiki/Finite_Lexicographic_Order_on_Well-Ordered_Sets_is_Well-Ordering"" rel=""nofollow noreferrer"">well ordered</a> with lexicographical order. So the process cannot infinitely go on, and eventually a zero must appear.</p>
"
"2386187","2386196","<p>If $x-y$ is rational for all $x, y\in E$, then take any $x\in E$ and note that $E\subseteq S := \{q+x : q\in \mathbb{Q}\cap [-1, 1]\}$. Therefore, as $m(S) = m(\mathbb{Q}\cap [-1, 1]) = 0$, we have $m(E) = 0$. The contrapositive of this is that if $m(E) &gt; 0$, then $x-y$ cannot be rational for all $x, y\in E$.</p>
"
"2386191","2386200","<p><strong>If you can use some basic group theory</strong>: A set with an associative binary operation with inverse and symmetric element is a group. There is only one group with three elements.</p>

<p><strong>Otherwise</strong>: Since $e$ is the identity, we have to determine $a*a$, $a*b$, $b*a$ and $b* b$. The inverse of $a$ can be $a$ or $b$.<br>
If it is $a$, then $a*a=e$. Then $a*(a*b)=(a*a)*b=e*b=b$. Therefore $a*b$ is not $a$ or $e$ (because $a*a=e$ and $a*e=a$), so $a*b=b$. But this implies $a=e$, a contradiction. We conclude that $a*b=e$.<br>
Can you finish?</p>
"
"2386195","2386201","<p>For $x \in (a,b)$
$$f'(x)=\lim_{h \to 0}\frac{f(x+h)-f(x)}{h}\geq0$$</p>
"
"2386207","2386209","<p>Uniqueness means:</p>

<p>$$\forall x,y\ \ P(x)\land P(y)\implies x=y$$</p>

<p>If there is no solution, both $P(x)$ and $P(y)$ are false, which makes the implication true <a href=""https://en.wikipedia.org/wiki/Vacuous_truth"" rel=""nofollow noreferrer"">(vacuous truth concept)</a>.</p>

<p>The nice thing is that the opposite:
$$\forall x,y\ \ P(x)\land P(y)\implies x\ne y$$
is also true. </p>

<p>In other words, anything you would say about things that do not exist is true (you could say ""My car is a Ferarri"", and that would be true if you don't have a car).</p>
"
"2386212","2386852","<p>Actually this might be pretty straightforward. Let $1 \to A \overset{\iota}{\to} E \overset{\pi}{\to} G \to 1$ be any extension of $G$ by an abelian group $A$. Since $A$ is normal in $E$, any $e \in E$ determines an automorphism $a \mapsto eae^{-1}$. Moreover, since $A$ is abelian, replacing $e$ by $ea_1$ for some $a_1 \in A$ does not change this automorphism, i.e. the action of $e$ on $A$ only depends on the coset $eA$. Thus, we do in fact have a well-defined action
\begin{align*}
\theta:G \to \mathrm{Aut}(A) &amp;&amp; &amp;&amp; \theta_g(a) = eae^{-1} \text{ for any } e\in \pi^{-1}(g),
\end{align*}
(it is easy to check $g \mapsto \theta_g$ is still a homomorphism)</p>

<p>Next we need something like a 2-cocycle. For every $g \in G$, choose arbitrarily an $e(g)$ in the coset $\pi^{-1}(g)$. Since $e(g_1) e(g_2)$ and $e(g_1g_2)$ are equal under $\pi$, we get a map 
\begin{align*}
\psi : G \times G \to A &amp;&amp; \text{ defined by } &amp;&amp;e(g_1)e(g_2) = \psi(g_1,g_2) e(g_1g_2)
\end{align*}
To see what happens with the 2-cocycle identity, we should compute $e(g_1)e(g_2)e(g_3)$ in two different ways. Because $A$ is not necessarily central in $E$, things come out slightly differently.
\begin{align*}e(g_1)e(g_2)e(g_3) &amp;=\psi(g_1,g_2)e(g_1g_2)e(g_3) \\
&amp;= \psi(g_1,g_2)\psi(g_1g_2,g_3) e(g_1g_2g_3)
\end{align*}
\begin{align*}e(g_1)e(g_2)e(g_3) &amp;=e(g_1)\psi(g_2,g_3)e(g_2g_3) \\
&amp;= e(g_1)\psi(g_2,g_3)e(g_1)^{-1} e(g_1)e(g_2g_3) \\
&amp;= \theta_{g_1}(\psi(g_2,g_3)) \psi(g_1,g_2g_3) e(g_1,g_2g_3)
\end{align*}
so we find that $\psi$ satisfies the identity
$$\psi(g_1,g_2) \psi(g_1g_2,g_3) =\psi(g_1,g_2g_3) \theta_{g_1}(\psi(g_2,g_3))$$
a rather mild variation on the 2-cocycle identity. For convenience, we had may as well require that $e(1)=1$ and that $e(g^{-1}) =e(g)^{-1}$, since this is easy to enforce. This leads to the additional identities
$$\psi(g,g^{-1})=\psi(g,1)=\psi(1,g)=1$$</p>

<p>Now we just need to turn all this around:</p>

<blockquote>
  <p>Let $G$ be a group, $A$ an abelian group. suppose that we have the following data:</p>
  
  <ul>
  <li>A homomorphism $\theta : G \to \mathrm{Aut}(A)$</li>
  <li>a map $\psi : G \times G \to A$ satisfying $\psi(g_1,g_2) \psi(g_1g_2,g_3) =\psi(g_1,g_2g_3) \theta_{g_1}(\psi(g_2,g_3))$ and $\psi(g,g^{-1})=\psi(g,1)=\psi(1,g)=1$ (this second requirement can most likely be dispensed with, at the cost of complicating the formulas).</li>
  </ul>
  
  <p>Define $E$ to be $A \times G$ as a set with product given by
  $$(a_1,g_1)(a_2,g_2) = (a_1 \theta_{g_1}(a_2) \psi(g_1,g_2),g_1g_2).$$
  (This group law is invented by thinking of $(a,g)$ as $ae(g)$ for $g \mapsto e(g)$ some set-theoretic section of $\pi$ defining $\psi$, and multiplying accordingly). There are some routine algebraic checks to show that this defines a group product:</p>
  
  <ul>
  <li>$(1,1)(a,g)=(1 \theta_1(a) \psi(1,g),1g)=(a,g)$</li>
  <li>$(a,g)(1,1) = (a \theta_g(1) \psi(g,1),g1)=(a,g)$</li>
  <li>$(\theta_g^{-1}(a^{-1}), g^{-1})(a,g)=(\theta_g^{-1}(a^{-1})\theta_g^{-1}(a)\psi(g^{-1},g), g^{-1}g)=(1,1)$</li>
  <li>$(a,g)(\theta_g^{-1}(a^{-1}), g^{-1})=(a \theta_g \theta_g^{-1}(a^{-1}) \psi(g,g^{-1}), gg^{-1}) =(1,1)$</li>
  </ul>
  
  <p>and, finally,
  \begin{align*}
[(a_1,g_1)(a_2,g_2)](a_3,g_3) &amp;= (a_1 \theta_{g_1}(a_2)\psi(g_1,g_2),g_1g_2)(a_3,g_3) \\
&amp;=(a_1 \theta_{g_1}(a_2) \psi(g_1,g_2)\theta_{g_1g_2} (a_3)\psi(g_1g_2,g_3), g_1g_2g_3) \\
&amp;=(a_1 \theta_{g_1}(a_2)\theta_{g_1} \theta_{g_2}(a_3) \psi(g_1,g_2)\psi(g_1g_2,g_3), g_1g_2g_3) \\
&amp;=(a_1 \theta_{g_1}(a_2)\theta_{g_1} \theta_{g_2}(a_3) \psi(g_1,g_2g_3) \theta_{g_1}(\psi(g_2,g_3)), g_1g_2g_3) \\
&amp;=(a_1\theta_{g_1}(a_2\theta_{g_2}(a_3)\psi(g_2,g_3))\psi(g_1,g_2g_3),g_1g_2g_3) \\
&amp;=(a_1,g_1)(a_2 \theta_{g_2}(a_3)\psi(g_2,g_3),g_2g_3)\\
&amp;=(a_1,g_1)[(a_2,g_2)(a_3,g_3)]
\end{align*}</p>
  
  <p>It is quite obvious that $(a,g) \mapsto g$ is a surjective homomorphism whose kernel $\{(a,1) : a \in A\}$ is a isomorphic to $A$.</p>
</blockquote>

<p>Strictly speaking, I did not prove that every extension of a group $G$ by an abelian group $A$ comes from the above construction, but this will surely be the case since, as discussed above, every extension $E$ determines such data $\theta$ and $\psi$ once one chooses a section $g \mapsto e(g) : G \to E$. So, one just needs to check the extension constructed out of that data is isomorphic to the original one.</p>
"
"2386216","2386224","<p><strong>First question:</strong>
You can get a partial answer.
If $\beta(a) \leq 1$, then $\gamma(a)\leq 1$ since
$$ \gamma(a) \leq \beta(a) \leq 1.$$
However, there may be some values $a$ such that $\gamma(a)\leq 1$ but $\beta(a)&gt;1$. For instance, if $\gamma(a)=a$ and $\beta(a)=a+1$, then $\beta(a)\leq 1$ for all $a\in(-\infty,0]$, but $\gamma(a)\leq 1$ for all $a\in(-\infty,1]$.</p>

<p><strong>Second question:</strong>
No. If $\gamma(a)=0$ and $\beta(a)=a$, then $\beta(a)\geq 1$ for all $a\in[1,\infty)$, but $\gamma(a)\geq 1$ does not occur for any $a$.</p>
"
"2386217","2386254","<p>There are seven points of data, but six intervals. Ergo, $n=6$. That should solve your problem. </p>
"
"2386218","2386268","<p>Let $x+y+z=3u$, $xy+xz+yz=3v^2$ and $xyz=w^3$.</p>

<p>Hence, our inequality it's
$$\sum_{cyc}(x^2y^2+2xyz+x^2)\geq\sqrt2\sum_{cyc}\left(x^2y+x^2z+\frac{2}{3}xyz\right)$$ or
$$9v^4-6uw^3+6w^3+9u^2-6v^2\geq\sqrt2(9uv^2-w^3),$$
which is a linear inequality of $w^3$,</p>

<p>which says that it's enough to prove our inequality for an extremal value of $w^3$.</p>

<p>Now,we see that $x$, $y$ and $z$ are non-negative roots of the following equation.
$$(X-x)(X-y)(X-z)=0$$ or
$$X^3-3uX^2+3v^2X-w^3=0$$ or
$$X^3-3uX^2+3v^2X=w^3,$$
which says that the graph of $f(X)=X^3-3uX^2+3v^2X$ and the line $Y=w^3$ </p>

<p>have three common points: $(x,f(x))$, $(y,f(y))$ and $(z,f(z))$.</p>

<p>Now, let $u$ and $v^2$ be constants and $w^3$ changes.</p>

<p>We see that $w^3$ gets a maximal value, when the line $Y=w^3$ will touch to the graph of $f$, </p>

<p>which happens for equality case of two variables. </p>

<p>Also, we see that $w^3$ gets a minimal value, when the line $Y=w^3$ will touch to the graph of $f$, </p>

<p>which happens for equality case of two variables, or when $w^3=0$.</p>

<p>Thus, it's enough to prove our inequality in the following cases.</p>

<ol>
<li>$w^3=0$.</li>
</ol>

<p>Let $z=0$.</p>

<p>Hence, we need to prove here that
$$x^2y^2+x^2+y^2\geq\sqrt2(x+y)xy,$$
which is C-S and AM-GM:
$$x^2y^2+x^2+y^2=x^2y^2+\frac{1}{2}(1^2+1^2)(x^2+y^2)\geq$$
$$\geq x^2y^2+\frac{1}{2}(x+y)^2\geq2\sqrt{x^2y^2\cdot\frac{1}{2}(x+y)^2}=\sqrt2(x+y)xy;$$</p>

<ol start=""2"">
<li>$y=z$. </li>
</ol>

<p>We need to prove
$$2(xy+y)^2+(x+y^2)^2\geq2\sqrt2y(x+y)^2,$$
which is AM-GM and C-S:
$$2(xy+y)^2+(x+y^2)^2=2y^2(x+1)^2+(x+y^2)^2\geq$$
$$\geq2\sqrt{2}y(x+1)(x+y^2)\geq2\sqrt2y(x+y)^2.$$
Done!</p>
"
"2386219","2386233","<p>The <a href=""https://en.m.wikipedia.org/wiki/Maximum_modulus_principle"" rel=""nofollow noreferrer"">maximum modulus principle</a> allows us to conclude that $\lvert f\rvert &lt; 1$ on the unit disk $D$, as $\lvert f\rvert$ must achieve its maximum on $\partial D$. As $\lvert f\rvert$ is continuous, it maps $\partial D$ to a compact set in $[0, 1)$ (which therefore has a supremum $m &lt; 1$ that is achieved on $\partial D$; we can then say that $\lvert f\rvert\leq m &lt; 1$ on $D$, so $f(D)\subset m\overline{D}\subsetneq D$). Then, by the <a href=""https://en.wikipedia.org/wiki/Denjoy%E2%80%93Wolff_theorem"" rel=""nofollow noreferrer"">Denjoy-Wolff theorem</a>, as $f$ is not an automorphism of $D$, there is a unique point $z_0$ in $\overline{D}$ such that $f^n(z)\to z_0$ uniformly on compact subsets of $D$. Since $z_0$ must lie inside $\overline{f(D)}\subseteq m\overline{D}$ (and therefore inside $D$), Denjoy-Wolff states that $z_0$ must therefore be the unique fixed point of $f$ in $D$.</p>
"
"2386240","2386469","<p>$$\sqrt{2\mu}\:t=\int\frac{dy}{\sqrt{\frac{1}{y}-\frac{1}{a}}}=\int\sqrt{\frac{ay}{a-y}}dy$$
Change of variable :$\quad y=a \sin^2(Y)\quad$ leading to :
$$\sqrt{2\mu}\:t=2a^{3/2}\int \sin^2(Y)dY = a^{3/2}\left(Y-\frac{1}{2}\sin(2Y )\right)+\text{constant}$$
$Y=\sin^{-1}(\sqrt{\frac{y}{a}})\quad$ After simplification : 
$$t(y)=\frac{1}{\sqrt{2\mu}}\left( a^{3/2}\sin^{-1}\left(\sqrt{\frac{y}{a}}\right)-\sqrt{ay(a-y)} \right)+C$$</p>

<p>Do not try to express the inverse function $y(t)$ as a combination of a finite number of standard functions. There is no convenient special function available for a closed form. Use numerical calculus to compute or draw $y(t)$ .</p>

<p>A parametric form:
$$\begin{cases}
t = \frac{a^{3/2}}{\sqrt{2\mu}}\left(Y-\frac{1}{2}\sin(2Y )\right)+c \\
y=a\sin^2(Y)
\end{cases}$$</p>
"
"2386246","2386381","<p>No, it isn't. The only terms of sort $\square$ in $\lambda\underline{\omega}$ are $*$ and $t_1 \to t_2$ where $t_1$ and $t_2$ are terms of sort $\square$. $\lambda k:\square.k$ is not a term of sort $\square$. In fact, you can easily formulate $\lambda\underline{\omega}$ without even talking about a sort $\square$.</p>

<p>It's trivial to add such terms if you want. Probably the easiest system to consider such thing is a <a href=""https://ncatlab.org/nlab/show/pure+type+system"" rel=""nofollow noreferrer"">Pure Type System</a> aka <a href=""http://www.diku.dk/hjemmesider/ansatte/henglein/papers/barendregt1991.pdf"" rel=""nofollow noreferrer"">generalized type systems</a> introduced also by Barendregt in this paper I believe. You could allow $(\lambda k:\square. k \to k)$ by having sorts $\{*,\square,\square_2\}$, axioms $\{(*:\square),(\square:\square_2)\}$, and rules $\{(*,*),(\square,\square),(\square_2,\square_2)\}$. You can continue on in the obvious manner for higher and higher towers. You can go off in other directions too, not just a vertical tower. The generalized type systems paper referenced above gives examples of systems with a variety of other arrangements of sorts. You could also consider the ""settings"" described in Bart Jacobs' PhD thesis ""Categorical Type Theory"" which provides a bit more flexibility than Pure Type Systems. </p>
"
"2386249","2386256","<p>From trigonometry we know that $\displaystyle \tan(\alpha+\beta) = \frac{\tan\alpha + \tan\beta}{1 - \tan\alpha\tan\beta}.$</p>

<p>If $\alpha$ happens to be the same as $\beta$ then this says $\displaystyle\tan(\alpha+\alpha) = \frac{\tan\alpha + \tan\alpha}{1-\tan\alpha\tan\alpha} = \frac{2\tan\alpha}{1 - \tan^2\alpha}.$</p>

<p>In other words $\displaystyle\tan(2\alpha) = \frac{2\tan\alpha}{1-\tan^2\alpha}.$</p>

<p>That is the double-angle formula for the tangent function.</p>

<p>So $\displaystyle y = \tan(2\theta) = \frac{2\tan\theta}{1-\tan^2\theta} = \frac{2x}{1-x^2}.$</p>

<p>So you're looking for the graph of $y = \dfrac{2x}{1-x^2}.$</p>

<p>(Note that $\tan\theta$ goes all the way from $-\infty$ to $+\infty,$ and therefore so does $x.$) </p>
"
"2386259","2386266","<p>You are close to the <a href=""https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes"" rel=""nofollow noreferrer"">Sieve of Erastosthanes</a>.  If you change the definition of $\hat q(n)$ to be zero for the prime you can just take the or over the primes and get zero for all the primes and one for all the composites.  Your version using $6,12,18$ is trying to cover for the fact that you didn't define it properly for the prime, so if you fix $\hat q(n)$ you don't need that.</p>
"
"2386270","2386280","<p>Suppose $\{a_n\}_{n=1}^\infty$ is a Cauchy sequence. Let $A_n$ be the sequence $a_n, a_{n+1},a_{n+2},\ldots$ If this Cauchy sequence does not converge, then the set $A_n$ is closed. The fact that it is a Cauchy sequence implies the diameter approaches $0.$ But $\bigcap_{n=1}^\infty A_n = \varnothing.$ Thus if a metric space is not complete, then it does not have the nested set property.</p>

<p>Now suppose the space is complete. Let $a_n\in A_n.$ Then the fact that the diameter approaches $0$ means $a_1,a_2,a_3,\ldots$ is a Cauchy sequence. It converges. Since the intersection of arbitrarily many closed sets is closed, the limit is a member of $A_n.$ Thus the intersection contains at least one point. But it cannot contain more than one: Suppose $a\ne b$ are both members of the intersection. Let $\varepsilon = d(a,b)/2.$ For some $n,$ you have $\operatorname{diam} A_n &lt; \varepsilon.$ Hence $A_n$ must exclude either $a$ or $b.$</p>

<p>Thus equivalence holds.</p>
"
"2386271","2386282","<p>Indeed, yours is a reasonable interpretation of the question, it is just not what they appear to have intenended.</p>

<p>You are correct that ${^8C_5}/({^7C_3}+{^8C_5})$ is the <em>conditional</em> probability that the favoured committee will be formed <em>when given</em> the condition (that Mr X will not join without his wife, etc.).</p>

<p>However $1-({^7C_3}/{^9C_5})$ is the probability that the favoured committee will be selected from any among <em>all</em> equally-probable combinations (whether Mr X would choose to join them or not).</p>
"
"2386277","2386288","<p>If you could find a uniform bound looking like</p>

<p>$$\left|\frac{x(x + n)}{n}\right| \le r &lt; 1$$</p>

<p>then you'd be set, after comparing to a geometric series. Notice that</p>

<p>$$\left|\frac{x(x +n)}{n}\right| \le |x|\left(1 + \frac{|x|}{n}\right).$$</p>

<p>Now provided that $|x| &lt; 1$, choose $n$ large enough that this quantity is strictly less than $1$, for example something like $n &gt; x^2 / (1 - |x|)$. All subsequent terms can be compared with a geometric series, and there are only finitely many preceeding terms. Hence the series is convergent for $|x| &lt; 1$. Using a similar technique, you can study what happens when $|x| &gt; 1$.</p>

<p>If $x = 1$, it is clearly divergent. If $x = -1$, we have</p>

<p>$$\sum_{n = 1}^{\infty} \left(\frac{n - 1}{n}\right)^n$$</p>

<p>and the terms tend to $e^{-1}$, not zero.</p>
"
"2386283","2386303","<blockquote>
  <p><strong>I tried an alternative approach to this problem:</strong></p>
  
  <p>I simplify the problem to  $$(\text{count possible places for } R)\cdot(\text{count of permutations of } PEPPE) \\ = 6 \cdot (\text{count of permutations of } PEPPE)$$</p>
</blockquote>

<p>This is a good beginning.  Why not just continue in the same manner?</p>

<p>$$(\text{count places for } R\text{ among the 6})\cdot(\text{count places for all }P\text{ among the remaining 5}) \\ = {^6{\rm C}_1} \cdot {^5{\rm C}_3} $$</p>

<p>$^6{\rm C}_1$ is of course, $6$. &nbsp; There are $6$ ways to select $1$ place from $6$.</p>

<p>${^5{\rm C}_3}$ is $10$. &nbsp; There are $5$ places for the first $P$, $4$ for the second, and $3$ for the last; <em>but</em> wait, which $P$ is which? &nbsp;  We must divide by the $3\cdot 2\cdot 1$ ways to arrange the $P$ among themselves. &nbsp;  So ${^5{\rm C}_3}$ is $(5\cdot 4\cdot 3)/(3\cdot 2\cdot 1)$.</p>

<p>There are $60$ distinct arrangements of the string ""PEPPER""</p>

<p>$$\dfrac 61\cdot\dfrac {5!}{3!~2!} ~=~ \dfrac {6!}{1!~3!~2!}$$</p>

<hr>

<p>Your method of counting neglects that the choices available to $x_4$ depend on what choices were made for $x_5$, and so on. &nbsp; Further there are only $2$ choices available for $x_5$: either one from the identical $P$ or one from the identical $E$. &nbsp; This continues untill either all two $E$ or all three $P$ have been used.</p>
"
"2386286","2386429","<ul>
<li>Let's assume $S^1 \subset X$. Denote by $\gamma$ the loop.</li>
</ul>

<p>$$ \gamma : [0,1] \longrightarrow  S^1 \subset X \\   t \longmapsto e^{2\pi i t}$$</p>

<p>Let $i: X \hookrightarrow \mathbb{R}^2 -\{0\}$, be the inclusion map. We know that $\pi_1(\mathbb{R}^2-{0})$ is generated by the class $[i \circ\gamma]$. Hence $i_{*} : \pi_1(X) \rightarrow\pi_1\left(\mathbb{R}^2-\{0\}\right)$ is surjection, so $\pi_1(X)$ cannot be trivial.</p>

<ul>
<li>Note that the claim is not true if we have just $S^1 \subset \bar{X}$. Indeed, you may take $X= \mathbb{R}^{2}- A$, where $A = B(0,1) \cup \mathbb{R}_{+}\times \{0\}$. This can be seen to be homotopy equivalent to a point.</li>
</ul>
"
"2386314","2387009","<p>The modulus of
$$f(s)\stackrel{\text{def}}{=}\int_{0}^{\pi/2}e^{is\cos x}\,dx = \int_{0}^{\pi/2}e^{is\sin x}\,dx = \int_{0}^{1}e^{isx}\frac{dx}{\sqrt{1-x^2}} =\frac{\pi}{2}\left[J_0(s)+i H_0(s)\right]$$
decays like $\frac{C}{\sqrt{s}}$ for $s\to +\infty$. You may study <em>how</em> to apply Laplace method in the last section of <a href=""http://www.maths.manchester.ac.uk/~gajjar/MATH44011/notes/44011_note3.pdf"" rel=""nofollow noreferrer"">these notes</a>. An equivalent approach is to devise a differential equation fulfilled by $f(s)$ and to derive such bound from such a differential equation.</p>
"
"2386318","2386340","<p>We have 
$$(D-I)^2=D^2-2D+I=0$$
or
$$D^2+I=2D$$
Multiply by $D^T$:
$$D+D^T=2I$$
You have shown that every term of $D$ is $0$ or $1$, so $D=I$.</p>
"
"2386319","2386326","<p>$\alpha+\beta=5$ and $\alpha\beta=3$.</p>

<p>Thus, $$\frac{\alpha}{\beta}+\frac{\beta}{\alpha}=\frac{(\alpha+\beta)^2-2\alpha\beta}{\alpha\beta}=\frac{25-6}{3}=\frac{19}{3}$$
and $$\frac{\alpha}{\beta}\cdot\frac{\beta}{\alpha}=1,$$
which gives the answer:
$$3x^2-19x+3=0.$$</p>
"
"2386321","2386361","<p>The key idea here is that the rows of a matrix constitute a linear system of equations.</p>

<p>Since we have 3 equations and 5 unknowns (columns), our system has an infinitum of solutions, because we could choose two variables to be whatever we want.</p>

<p>It is somewhat an arbitrary rule that mathematicians always choose to say that the pivot entries are bound and the other ones ($x_2$ and $x_5$ in your example above) are ""free"". </p>

<p>I say that it is is arbitrary because if $x_1 + x_2 = 5$ then we could say $x_1 = 5 - x_2$ or equivalently $x_2 = 5 - x_1$. In any event, the number of pivots corresponds to the number of equations you <em>actually</em> have.</p>

<p>A good reference for learning about this is chapter 2 of Strang's <em>Linear Algebra and It's Applications</em>.</p>
"
"2386331","2387242","<p>The easiest approach to determining convergence of this series would be to invoke the Limit Comparison test between this series and $\sum \frac{n}{n^{2p}}=\sum \frac{1}{n^{2p-1}}$, since for the latter we know precisely when it converges of diverges. But if you have to apply the Integral Test directly showing all work, then your solution is mostly correct&hellip; except for some errors and inaccuracies in it.</p>

<blockquote>
  <p>For the integral test to work, the function has to be decreasing</p>
</blockquote>

<p>Absolutely correct! That's why you should immediately conclude that $p&gt;0$, since otherwise the terms are not decreasing. So from now on, we're assuming that $p&gt;0$.</p>

<blockquote>
  <p>Two critical points exist in the reals $x=1$, $x=â1$.</p>
</blockquote>

<p>That depends. From the second factor of $f'(x)$, we can also have $x^2=\frac{1}{1-2p}$. So if $p&lt;1/2$, then we have two more critical points $x=\pm\frac{1}{\sqrt{1-2p}}$. But fortunately that doesn't matter: $x=1$ is still the right-most critical point, and we can check that regardless of $p$ (however, remember that our $p&gt;0$) the derivative is negative on $[2,+\infty)$.</p>

<blockquote>
  <p>We can observe that the integral is indeterminate when $p=1$</p>
</blockquote>

<p>No, that's not what's going on here. Your whole calculation simply doesn't work (doesn't apply) when $p=1$. It's a completely different integral in that case. So instead you should keep the calculation that you did (and you did it correctly!), but with a comment that this is for the case when $p&gt;0$ and $p\neq1$. Then you will determine, as you did, that the series diverges for $p&lt;1$ and converges for $p&gt;1$. But the case $p=1$ has to be redone separately, as the antiderivative in this case is completely different &mdash; a logarithm, not a power function.</p>
"
"2386342","2386344","<p>Hint:</p>

<p>Set $\sqrt{1-x}=u\implies x=1-u^2\implies dx=-2u\ du$</p>
"
"2386348","2386668","<p>What you're doing works, but it is aguably simpler to note that each of the functions you're interested in is a subset of $X\times Y$, so you can get at set of all of them by using Separation on the power set of $X\times Y$.</p>
"
"2386351","2386359","<p>Yep, the author knew the answer beforehand, I would say much longer before.</p>

<p>Because this is just an instance of the Babylonian method of computing square roots, later discovered to be a particular case of Newton's iterative method for the resolution of nonlinear equations.</p>

<p>So with closed eyes, this sequence converges to $\sqrt2$. You can easily verify it be assuming convergence, so that $a_{n-1}$ and $a_n$ become indiscernible, and</p>

<p>$$a=\frac{a+\dfrac2a}{2}$$ or $$a^2=2.$$ As the initial value is $1$, all terms are positive and convergence is to the positive root (if there is convergence, though).</p>

<hr>

<p>There is a simple way to explain that method, also known as Heron's formula.</p>

<p>Let $s$ be the number of which you want to extract the root, and let $a$ be an approximation by default. Then $$a&lt;\sqrt s\implies a':=\frac sa&gt;\sqrt s$$ so that $\dfrac sa$ is another approximation, by excess. Now if we take the arithmetic mean, we get a new approximation which is closer than the worse of the two,</p>

<p>$$a''=\frac{a+a'}2=\frac{a+\dfrac sa}{2}.$$</p>

<p>As can be shown, when you are close to the root, the sequence converges extremely rapidly.</p>

<p>For example,</p>

<p>$$a=\color{green}{1.41}\implies a'=\color{green}{1.41}84397163121\cdots\implies a''=\color{green}{1.41421}9858156\cdots$$ while the true value is
$$\sqrt2=1.4142135623731\cdots$$</p>

<p>The next iteration gives $11$ exact digits.</p>

<hr>

<p>A note on convergence, for the skepticals (the method has been in use for at least two millenia).</p>

<p>Let $$x_n:=\frac{a_n}{\sqrt s}.$$</p>

<p>We have</p>

<p>$$\frac{x_{n+1}-1}{x_{n+1}+1}=\frac{x_n+\dfrac1{x_n}-2}{x_n+\dfrac1{x_n}+2}=\frac{(x_n-1)^2}{(x_n+1)^2}$$
and by induction</p>

<p>$$\frac{x_n-1}{x_n+1}=\left(\frac{x_0-1}{x_0+1}\right)^{2^n}.$$</p>

<p>This is an exact formula for the $n^{th}$ iterate, which proves convergence from any $x_0$ such that</p>

<p>$$\left|\frac{x_0-1}{x_0+1}\right|&lt;1.$$</p>

<p>This holds for all positive $x_0$.</p>

<p>In passing, this also proves quadratic convergence, i.e. the relative error is squared on every iteration.</p>
"
"2386352","2386455","<blockquote>
  <p>If $E$ and $G$ are going, what is the probability $A$ and $C$ are going?</p>
</blockquote>

<p>Since $E$ and $G$ are going, two of the other seven students must be selected.  Only one of these selections includes both $A$ and $C$.  Hence, the probability is 
$$\frac{1}{\dbinom{7}{2}}$$</p>

<blockquote>
  <p>If $A$ is not going, what is the probability $B$ and $C$ are going?</p>
</blockquote>

<p>Since $A$ is not going, four students must be selected from the remaining eight students.  If $B$ and $C$ are going, then two students must be selected from the remaining six students.  Hence, the probability is 
$$\frac{\dbinom{6}{2}}{\dbinom{8}{4}}$$</p>

<blockquote>
  <p>If $A$, $C$, and $D$ are not going, what is the probability that $E$, $B$, and $F$ are not going?</p>
</blockquote>

<p>If all six students do not go, then at most three students can go to the show. Since four students are going to the show, this is impossible.  Hence, the probability is $0$.</p>
"
"2386360","2386503","<p>What Spivak asserts is that, among the numbers $x_1,x_2,\ldots,x_n$, there is one and only one number ($x_i$) at which the function $f_i$ takes the value $1$ and that all other numbers (the $x_j$'s with $j\neq i$) are zeros of $f_i$. He never says that there is no other real (or complex) number $x$ such that $f(x)=1$.</p>
"
"2386372","2386422","<p>To solve $Lu(x)=u(x)-u''(x)=f(x)$ for $u(x)$ (with suitable boundary conditions of decay at infinity), take the Fourier transform of both sides:
$$
\hat{u}(\xi) - (i\xi)^2 \hat{u}(\xi) = \hat{f}(\xi)
.
$$
This gives
$$
\hat{u}(\xi) = \frac{1}{1+\xi^2} \, \hat{f}(\xi)
.
$$
Here we recognize the standard Fourier transform $\hat{Q}(\xi)=1/(1+\xi^2)$,
so $\hat{u}=\hat{Q}\hat{f}$, and therefore $u=Q*f$ (since multiplication of Fourier transforms corresponds to convolution of the original functions).</p>
"
"2386373","2386382","<p>Let $a=p_1^{\alpha_1}p_2^{\alpha_2}...p_n^{\alpha_n}$, $b=p_1^{\beta_1}p_2^{\beta_2}...p_n^{\beta_n}$ and $c=p_1^{\gamma_1}p_2^{\gamma_2}...p_n^{\gamma_n}$, where $\alpha_i\geq0$, $\beta_i\geq0$, $\gamma_i\geq0$ be integers and $p_i$ be different prime numbers.</p>

<p>Hence, we need to prove that
$$p_1^{\max\{\alpha_1,\beta_1,\gamma_1\}}p_2^{\max\{\alpha_2,\beta_2,\gamma_2\}}...p_n^{\max\{\alpha_n,\beta_n,\gamma_n\}}=$$
$$=p_1^{\alpha_1+\beta_1+\gamma_1+\min\{\alpha_1,\beta_1,\gamma_1\}-\min\{\alpha_1,\beta_1\}-\min\{\alpha_1,\gamma_1\}-\min\{\beta_1,\gamma_1\}}\cdot...$$
$$\cdot p_n^{\alpha_n+\beta_n+\gamma_n+\min\{\alpha_n,\beta_n,\gamma_n\}-\min\{\alpha_n,\beta_n\}-\min\{\alpha_n,\gamma_n\}-\min\{\beta_n,\gamma_n\}}.$$</p>

<p>Thus, it's enough to prove that
$$\max\{x,y,z\}+\min\{x,y\}+\min\{x,z\}+\min\{y,z\}=x+y+z+\min\{x,y,z\},$$
which is obvious because we can assume $x\geq y\geq z$.</p>

<p>Done!</p>
"
"2386378","2386485","<p>As an add on to @dxiv comment, it can actually be shown that indeed if $a_n,b_m\neq0$, then $y=\frac {a_n}{b_m}$ is a horizontal asymptote to $P(x)=\frac {F(x)}{G(x)}$.</p>

<p>By definition, a horizontal asymptote exists if and only if $\lim_{x\to\pm\infty}P(x)=k$ for some constant$k\in\mathbb{R}$. </p>

<p>Thus in general, we see:</p>

<p>$$\lim_{x\to\infty}P(x)=\lim_{x\to\infty}\frac{F(x)}{G(x)}=\lim_{x\to\infty}\frac{a_nx^n+a_{n-1}x^{n-1}+...+a_1x+a_0}{b_mx^m+b_{m-1}x^{m-1}+...+b_1x+b_0}$$</p>

<p>Let's consider the case when $m=n$, then for $x\ne0$, we see:</p>

<p>$$\lim_{x\to\infty}P(x)=\lim_{x\to\infty}\frac{a_nx^n+a_{n-1}x^{n-1}+...+a_1x+a_0}{b_nx^n+b_{n-1}x^{n-1}+...+b_1x+b_0}=\lim_{x\to\infty}\frac{a_n+\frac{a_{n-1}}{x}+...+\frac {a_1}{x^{n-1}}+\frac {a_0}{x^n}}{b_n+\frac{b_{n-1}}{x}+...+\frac {b_1}{x^{n-1}}+\frac {b_0}{x^n}}=\frac {a_n}{b_n}$$</p>

<p>So indeed, regarding your above question in the comments, the horizontal asymptote is:</p>

<p>$$\lim_{x\to\infty}\frac{F(x)}{G(x)}=\lim_{x\to\infty} \frac{4x^2+6x+8}{2x^2+4x+4}=\frac 42=2$$</p>

<p>Try considering what happens when $m&gt;n$, or $m&lt;n$.</p>
"
"2386379","2386432","<p>Let $w_1,\ldots,w_r$ be a basis of $W$. Consider the map
$\phi:V\to k^r$ given by $\phi(v)=(\left&lt;w_1,v\right&gt;),
\ldots,\left&lt;w_r,v\right&gt;$. Then (i) show $\phi$ is always nonzero on $W$,
(ii) $\ker\phi=W^\perp$, (iii) $W\cap W^\perp=\{0\}$ and (iv) $W+W^\perp=V$.</p>
"
"2386385","2386850","<p>The trick is to use $\bot \ Elim$. Now, to continue from what you have, you can do:</p>

<p><a href=""https://i.stack.imgur.com/ANb3p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ANb3p.png"" alt=""enter image description here""></a></p>

<p>But note that you never used the subproof on lines 5-7, so this can be simplified to:</p>

<p><a href=""https://i.stack.imgur.com/wADTa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wADTa.png"" alt=""enter image description here""></a></p>

<p>Although conceptually, it may be helpful to keep the original subproof, and (since it shows that $B$ leads to a contradiction $\bot$) derive $\neg B$, and to then combine $\neg B$ with $B \lor P$ (the latter is a <strong>super</strong> common patterns, so remember that one!!):</p>

<p><a href=""https://i.stack.imgur.com/seHz5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/seHz5.png"" alt=""enter image description here""></a></p>

<p>Finally, you can set this up as a proof by cases within a conditional proof, i.e. derive your goal $P \land D$ from each of the cases $B$ and $P$:</p>

<p><a href=""https://i.stack.imgur.com/k2RQZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k2RQZ.png"" alt=""enter image description here""></a></p>
"
"2386392","2386397","<p>Let $x,y\in\mathbb R$ be arbitrary points and let $d:=|x-y|$. Then, there exists $m\in\mathbb N$ such that $d&lt;mr$ (yes?) and so you can find a sequence $(x_i)_{i=0}^{m+1}$ with $x_0=x, x_{m+1}=y$ such that $$|x_0-x_1|=|x_1-x_2|=\dots=|x_m-x_{m+1}|&lt;r$$ This implies that $(x_i,x_{i+1})_{i=0}^{m+1} \in D_r$ and hence, by definition of the transitive closure $tr(D_r)$, if follows that  $(x,y)\in tr(D_r)$. Since $x,y \in \mathbb R$ were arbitrary, the conclusion follows.</p>
"
"2386393","2386403","<p>Since $$z=\frac1{\sqrt{12}}\sqrt{42-3x^2-2y^2},$$ the correct normal vector would be $$
n=\left\langle-\frac{3x}{\sqrt{12}\sqrt{42-3x^2-2y^2}},-\frac{2y}{\sqrt{12}\sqrt{42-3x^2-2y^2}},-1\right\rangle.
$$ Your approach is correct, but keep in mind that there are in principle two such spheres, one touching from the outside (that's the one you need, probably) and one from the inside (though it doesn't fit entirely).<br>
That would mean $$
n=\left\langle-\frac{3x}{12\,z},-\frac{2y}{12\,z},-1\right\rangle=\left\langle-\frac{6}{12},-\frac{6}{12},-1\right\rangle=\left\langle-\frac12,-\frac12,-1\right\rangle.$$ Its norm is $\sqrt{6}/2,$ so $$
n_u=\left\langle-\frac1{\sqrt{6}},-\frac1{\sqrt{6}},-\frac2{\sqrt{6}}\right\rangle.$$ Using that, we get the two solutions $$\langle2,3,1\rangle+\sqrt{6}n_u=\langle2,3,1\rangle+\langle-1,-1,-2\rangle=\langle1,2,-1\rangle$$ and $$\langle2,3,1\rangle-\sqrt{6}n_u=\langle2,3,1\rangle-\langle-1,-1,-2\rangle=\langle3,4,3\rangle.$$</p>
"
"2386396","2386718","<p>Define
$$P_n(x)=1 - {n \choose 2}x^2 + {n \choose 4}x^4 - \ldots$$
$$Q_n(x)={n \choose 1}x - {n \choose 3}x^3 + \ldots$$</p>

<p>I have never heard of these polynomials before, but the polynomials $P_n$ bare some vague resemblances to Chebyschev polynomials $T_n(x)$.</p>

<p>Since
$${n\choose 1}x - {n \choose 3}x^3 + \ldots=\Im(1+ix)^n,$$
$$1 - {n \choose 2}x^2 + {n \choose 4}x^4 - \ldots=\Re(1+ix)^n,$$
writing $(1+ix)$ in polar form yields
$$
\begin{align}
(1+ix)^n=
&amp;
(\sqrt{1+x^2}e^{i\arctan(x)})^n
\\
=&amp;(1+x^2)^{\frac n2}e^{in\arctan(x)}
\end{align}.
$$
This shows that 
$$P_n(x)=(1+x^2)^{\frac n2}\cos\left(n \arctan(x)\right)
\\
Q_n(x)=(1+x^2)^{\frac n2}\sin\left(n \arctan(x)\right).$$</p>

<p>Compare 
$$T_n(x)=\cos(n\arccos(x))$$
$$P_n(x)=(1+x^2)^\frac n2 \cos( n\arctan(x)).$$
Also, the family $P_n$ has the property that
$$P_n(\tan(\theta))=(1+\tan^2(\theta))^{\frac n2}\cos(n\theta)=\frac{\cos(n\theta)}{\cos^{n}(\theta)},$$
mirroring 
$$T_n(\cos(\theta))=\cos(n\theta).$$
Through this relation, we can also express our polynomials as a function of Chebyschev polynomials $T_n$:
$$P_n(\tan(\theta))=\frac{T_n(\cos(\theta))}{\cos^n(\theta)}.$$</p>
"
"2386398","2388988","<p>Choose a prime number $q \equiv 1 \bmod 4p$ not dividing the discriminant of $K$, and let $F$ be the subfield of degree $p$ inside the field of $q$-th roots of unity. Observe that $F$ is real. The extension $KF/K$ is ramified exactly at $q$. The field $L = KF$ does not contain the $q$-th roots of unity since $L \cap {\mathbb Q}(\zeta_q) = F$. If $L$ contains some $n$-th root of unity with $n$ coprime to $q$, then $L$ must contain $K(\zeta_n)$, which is unramified at $q$. But $L/K$ is completely ramified at the primes above $q$: contradiction. </p>
"
"2386409","2386453","<p>As you mention, if you have a surface described by $z=f(x,y)$, the normal vector on $(x_0,y_0)$ can be computed by:
$$\vec{n}=\langle f_x(x_0,y_0), f_y(x_0,y_0),-1 \rangle.$$</p>

<p>But $z=f(x,y)$ can also be expressed as implicitly as $g(x,y,z)=f(x,y)-z=0$. In that case, the gradient of $g$ also gives you the normal vector on $(x_0,y_0,z_0)$, where $z_0=f(x_0,y_0)$:
$$\vec{n}=\langle g_x(x_0,y_0,z_0), g_y(x_0,y_0,z_0), g_z(x_0,y_0,z_0) \rangle.$$</p>

<p>Note that if you use the equation of the ellipsoid to substitute $z$ on your second vector. Then normalize both the first and the second one and you will see that both are equal (up to a sign).</p>

<p>You can read about those and many other ways of computing the normal vector <a href=""http://mathworld.wolfram.com/NormalVector.html"" rel=""nofollow noreferrer"">here</a>.</p>
"
"2386411","2386458","<p><strong>Two issues.</strong></p>

<h1>Notations &amp; terms</h1>

<ul>
<li>rv: random variable</li>
<li>iid: independent identical distribution</li>
<li>pdf: probability density function</li>
<li>jpdf: joint pdf</li>
<li>$X_1,X_2,...,X_n$: $n$ iid</li>
<li>$x_1,x_2,...,x_n$: an observation</li>
<li>$\theta$: the wanted parameter(s)</li>
<li>$\Theta$: parametric space (i.e. <strong>all the values $\theta$ may take</strong>)</li>
</ul>

<blockquote>
  <p>Fact: the parameter ($\theta$) is a number but unknown, not a rv.</p>
</blockquote>

<h1>ISSUSE 1 MLE</h1>

<p><strong>The key to understand MLE is to think the same thing (jpdf) from other side, the PARAMETRIC SPACE side.</strong></p>

<p>Namely, you take the PARAMETRIC SPACE as the domain of the jpdf.</p>

<blockquote>
  <p>Then, we denote this jpdf as $L(\theta;x_1,...,x_n), \theta\in\Theta.$</p>
</blockquote>

<p>That is, for each possible $\theta_0$, there is a corresponding $L(\theta_0;x_1,...,x_n)$ which is a <strong>number</strong> because on the one hand $x_1,...,x_n$ are given, on the other hand $\theta$ is given. </p>

<p>You have known that the following step is maximizing this function.</p>

<p>Wait a minute. What is <em>maximizing</em>?
This word means that you choose the <em>maximum possible</em> (i.e. $L(\theta)$) case  among a bunch of cases (each $\theta$ in $\Theta$ representing a case).</p>

<p>Therefore, the cost function you choose is <em>legal</em>, if the maximizing of this cost function is equivalent to the maximizing of $L(\theta;x_1,...,x_n)$ among the cases $\theta\in\Theta$. </p>

<p>In fact, we often choose the cost function as $\ln L(\theta;x_1,...,x_n)$ because this is convenient for many distributions.</p>

<blockquote>
  <p><strong>Theorem.</strong> Maximizing  $L(\theta)$ is equivalent to maximizing $\ln L(\theta)$, among all $\theta\in\Theta$.</p>
  
  <p>Hints: It it obviously that $\ln x$ is monotone incresing in $x$ when $x&gt;0$.
  Meanwhile, $L(\theta)&gt;0, \theta\in\Theta$.</p>
  
  <p>Take $\theta_0\in\Theta$, s.t. $L(\theta_0)=\max_{\theta\in\Theta}L(\theta).$ Then $\ln L(\theta_0)=\max_{\theta\in\Theta}\ln L(\theta)$.</p>
</blockquote>

<p>In fact, the MLE method is not the BEST. Consider the following example:</p>

<blockquote>
  <p>Consider $(X_1,..,X_n)\stackrel{iid}{\sim}N(\mu,\sigma^2)$.</p>
  
  <p>We can calculate the MLE of $\sigma^2$ is $\hat\sigma^2=\dfrac1n\sum\limits_{i=1}^n(X_i-\bar X)^2$, which is obviously biased.</p>
</blockquote>

<h1>ISSUSE 2 Best way to estimate parameters of a distribution</h1>

<p>On parameters of a distribution, there are point estimation and interval estimation.</p>

<p>What you mentioned is point estimation, in which the BEST estimator is called <strong>UMVUE</strong> (uniformly minimum variance unbiased estimator).</p>

<p>The MLE method is not a approach to get UMVUE directly (as we've shown that MLE is sometimes biased) but an intuitive one. </p>

<p>On UMVUE, there are too many theories. You may google <strong>Lehmann-Scheffe</strong> theorem (a way to find UMVUE) for more details.</p>
"
"2386414","2386427","<p>You have not applied the root test correctly. Although it is quite true that $3/(n^2 + 1)$ is decreasing, that does not imply that the $n$-th root of it is. In fact, </p>

<p>$$\lim_{n \to \infty} \left(\frac 3 {n^2 + 1}\right)^{1/n} = 1$$</p>

<p>and the root test is not applicable to this series. For a similar case, consider the fact that $1/n$ decreases to zero, but</p>

<p>$$\left(\frac 1 n\right)^{1/n} = e^{\log n / n} \to e^0 = 1$$</p>

<p>in the limit.</p>

<hr>

<p>Also, there are a couple other issues. First, if you're using this test, you should have $n^2 - 1$ rather than $+1$. Secondly,</p>

<blockquote>
  <p>$$ \overline\lim_{n \rightarrow \infty} a_n^{1/n} = \left(\frac{3}{n^2+1}\right)^{1/n}&lt;1, \forall n \in[2, \infty)$$</p>
</blockquote>

<p>doesn't make sense at all. You can't have $n$ on the right hand side of this equality, because it's got a very specific meaning on the left hand side. The limit cannot depend on $n$, since $n \to \infty$.</p>

<hr>

<p>For a different approach, just try direct comparison, using the fact that your summands are all smaller than $3/(n^2 - 1)$ for $n &gt; 1$.</p>
"
"2386428","2386436","<p>Note quite right, you are missing out intersection between two sets. Hence, the signs are wrong for some terms.</p>

<p><a href=""https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle"" rel=""nofollow noreferrer"">Inclusion-exclusion Principle:</a></p>

<p>$$\mathbb{P}\biggl(\bigcup_{i=1}^n A_i\biggr)  =\sum_{k=1}^n \left((-1)^{k-1}\sum_{\scriptstyle I\subset\{1,\ldots,n\}\atop\scriptstyle|I|=k} \mathbb{P}(A_I)\right)$$</p>

<p>where the last sum runs over all subsets $I$ of the indices $1,\ldots , n$ which contain exactly $k$ elements, and</p>

<p>$$A_I=\bigcap_{i\in I} A_i$$</p>

<p>denotes the intersection of all those $A_i$ with index in $I$.</p>
"
"2386431","2386466","<p>$AB-BA=0$ is a set of $4$ equations with four unknowns. </p>

<p>$$\begin{bmatrix}a_{12} b_{21} - a_{21}b_{12} &amp; -a_{12} b_{11} + a_{11} b_{12} - a_{22} b_{12} + a{12} b_{22}\\
a_{21} b_{11} - a_{11} b_{21} + a_{22} b_{21} - a_{21} b_{22} &amp; a_{21} b_{12} - a_{12} b_{21}\end{bmatrix} = \begin{bmatrix}0 &amp; 0\\
0 &amp; 0\end{bmatrix}$$</p>

<p>The two equations coming from the diagonal are the same. This is just $tr(AB-BA)=0$.</p>

<p>So, three equations really. Moreover, that equation on the diagonal only has
two unknowns. Therefore it already gives you one unknown in terms of the other. Unless it is identically zero.</p>

<p>Case 1: The diagonal equation is identically zero. Therefore $b_{12}=b_{21}=0$. Since $B$ is non-scalar, it follows that $a_{12}=a_{21}=0$. And the space of solutions is $2$-dimensional, at most.</p>

<p>Case 2: The diagonal equation is not identically zero. In this case $a_{12}$ determines $a_{21}$, or the other way around. This is because only those two unknowns are in present in that equation. Having $a_{12}$ and $a_{21}$, the remaining equations express $a_{11}$ in terms of $a_{21}$ or the other way around.</p>

<p>Therefore the commutant has dimension $2$ at most. Since $aB+b$ has dimension $2$ that is all of it.</p>
"
"2386444","2386844","<p>If $M$ is a manifold, let $nM$ denote the connected sum of $n$ copies of $M$.</p>

<p>First note that if $X \to Y$ is a $k$-sheeted covering, then $\chi(X) = k\chi(Y)$. So if $X$ is a four-sheeted covering of $5\mathbb{RP}^2$, $\chi(X) = 4\chi(5\mathbb{RP}^2)$. Now recall that $\chi(n\mathbb{RP}^2) = n - 2$, so $\chi(X) = 4(5 - 2) = 12$. If $X$ is non-orientable, then $X = n\mathbb{RP}^2$ for some $n$. But $\chi(X) = 12$ and $\chi(X) = \chi(n\mathbb{RP}^2) = n - 2$, therefore $n = 14$. That is, the only non-orientable surface which can be a four-sheeted cover of $5\mathbb{RP}^2$ is $14\mathbb{RP}^2$. Note, we still need to check whether $14\mathbb{RP}^2$ does cover $5\mathbb{RP}^2$ or not.</p>

<p>Here's a useful trick: if $\pi: X' \to X$ is a $k$-sheeted covering, then there is a $k$-sheeted covering $p: X'\# kY \to X\# Y$.</p>

<p>To see this, let $U$ be an evenly covered open disc in $X$, then $X'\setminus\pi^{-1}(U)$ is $X'$ with $k$ disjoint open discs removed. Let $V$ be an open disc in $Y$. Gluing a copy of $Y\setminus V$ to each boundary sphere of $X'\setminus\pi^{-1}(U)$, and a copy of $Y\setminus V$ to the boundary sphere of $X\setminus U$, the $k$-sheeted covering $X'\setminus\pi^{-1}(U) \to X\setminus U$ extends to a $k$-sheeted covering $X'\# kY \to X\# Y$.</p>

<p>Returning to the problem at hand, recall that $3\mathbb{RP}^2 = T^2\#\mathbb{RP}^2$, so $5\mathbb{RP}^2 = T^2\# 3\mathbb{RP}^2$. There is a four-sheeted covering $T^2 \to T^2$ (for example, $(z, w) \mapsto (z^4, w)$), so by the above construction, there is a four-sheeted covering $T^2\# 12\mathbb{RP}^2 \to T^2\# 3\mathbb{RP}^2$, i.e. $14\mathbb{RP}^2 \to 5\mathbb{RP}^2$.</p>

<p>If the deck transformations of the four-sheeted covering map $T^2 \to T^2$ are generated by a $90$ degree rotation (which is the case for the example I gave), you can visualise the construction of the four-sheeted covering of $T^2\# 3\mathbb{RP}^2$ as follows: </p>

<p>$$3\mathbb{RP}^2 \\
\# \\
3\mathbb{RP}^2\ \#\ \ T^2\ \#3\ \mathbb{RP}^2 \\
\# \\
3\mathbb{RP}^2$$</p>

<hr>

<p>These arguments can be used to show the following:</p>

<blockquote>
  <p>Let $X$ and $Y$ be two closed, connected surfaces, either both orientable or both non-orientable. Then there is a $k$-sheeted covering $X \to Y$ if and only if $\chi(X) = k\chi(Y)$.</p>
</blockquote>

<p>As every covering of an orientable space is orientable, the only case left to consider is orientable coverings of non-orientable surfaces.</p>

<blockquote>
  <p>Let $X$ and $Y$ be two closed, connected surfaces, with $X$ orientable and $Y$ non-orientable. Then there is a $k$-sheeted covering $X \to Y$ if and only if $\chi(X) = k\chi(Y)$ and $k$ is even.</p>
</blockquote>

<p>The reason we need $k$ to be even in this statement is that every orientable covering of a non-orientable manifold factors through the orientation double cover.</p>
"
"2386459","2386493","<p>You're correct that each vector $v \in \mathbb{R}^3$ defines a line $$\{av : a \in \mathbb{R}\}.$$ But such lines always go through the origin. Thankfully, we can generalize. If we have a point $p$ and a vector $v$, there's a corresponding line $$\{p+av : a \in \mathbb{R}\},$$ and such lines needn't go through the origin. Indeed, every line can be described in the latter form.</p>
"
"2386463","2386474","<p>The math involved is that of Semigroups and its application to ""sequential circuits"" is named ""algebraic theory of machines"".</p>

<p>Read just to start:</p>

<p>Krohn, Rhodes, Arbib - <em>Algebraic Theory of Machines, Languages and Semigroups</em></p>
"
"2386467","2386478","<p>$$c^2\left(\frac{1}{c}-\frac{1}{s+c}\right)=
\frac{c^2 \left(s+c-c\right)}{c(s+c)}=
\frac{cs}{s+c}=
\frac{s^2(c+s-s)}{s(c+s)}=
s^2\left(\frac{1}{s}-\frac{1}{c+s}\right)$$</p>
"
"2386471","2386486","<ul>
<li>The sample space that you defined is not relevant to the question.</li>
</ul>

<p>Each event in the space must be one possible outcome of the experiment ""throwing hats and picking them at random"". One of these event, for instance is ""man 1 gets hat C, man 2 gets hat A, man 3 gets hat B"", which we can note ""A2B3C1"".</p>

<p>There are 6 such events in your universe (which is in bijection with the set of all permutations of $\{1,2,3\}$): $U=\{A1B2C3,A1B3C2,A2B1C3,A2B3C1,A3B1C2,A3B2C1\}$</p>

<p>The event ""nobody picks his own hat"" is the subset $E=\{A2B3C1,A3B1C2\}$</p>

<ul>
<li>The complement of ""nobody picks his own hat"" is (in english) the sentence that say the opposite of ""nobody picks his own hat"" i.e. ""at least someone picks his own hat"". One is true if and only if the other is false.</li>
</ul>

<p>In maths, it is $U-E=\{A1B2C3,A1B3C2,A2B1C3,A3B2C1\}$</p>

<p>When you run the experiment, one and only one of the two events ""nobody picks his own hat"" and ""at least someone picks his own hat"" will happen: that is the very definition of ""complementing events"".</p>
"
"2386473","2386483","<p>We can express $a=x+y$ where $y\in H_0$ and $x\in H_0^{\bot}$</p>

<p>The distance $d(a,H_0)=\left\|a-y\right\|=\left\|x\right\|$.</p>

<p>Now, for $u\in H_0^{\bot}$ with $\left\|u\right\|=1$, $\langle a,u\rangle=\langle x,u\rangle\leq\left\|x\right\|\left\|u\right\|=\left\|x\right\|$ by Cauchy-Schwartz inequality. Therefore $\left\|x\right\|$ is the maximum you wanted.</p>
"
"2386479","2386484","<p>I don't understand why you're discarding 111,111,111 (or counting 000,000,000 in the first place).</p>

<p>There are $2^9$ $9$-digit numbers consisting only of $1$s and $2$s, but then you need to include the $8$-digit numbers (of which there are $2^8$), and so on down to the $1$-digit numbers. What do you get when you add all those possibilities up?</p>
"
"2386488","2386496","<p>The limit in that case is $\infty$ from the right, and $-\infty$ from the left. The intuition is that as you divide a ""fixed"" positive number by smaller and smaller quantities, the result grows large unboundedly.</p>

<p>In the other hand, if both the numerator and denominator approach $0$ the same argument doesn't work. In fact, you will find examples where both the numerator and denominator approach $0$ and the limit can either exist and be equal to any number (including $0$), or the limit can equal $\pm\infty$, or it may not exist. That's why we say that $0/0$ is an indeterminate.</p>
"
"2386515","2386523","<p>So far so good! All you need to do now is divide through by $n^2$ as follows:
$$ \lim_{n\to \infty}\frac{1}{n}\sum_{k=0}^n \frac{n^2}{n^2+k^2}=\lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^n\frac{1}{1+(\frac{k}{n})^2}.$$
If we think about it, we realize that this is the Riemann Sum corresponding to 
$$ \int_0^1\frac{1}{1+x^2}dx=\arctan x\bigg|_0^1=\arctan 1=\frac{\pi}{4}.$$
To see that this is in fact the Riemann Sum, note that the $\frac{1}{n}$ denotes the ""bin"" size with which we partition our interval of integration. Here, we note that as $0\le k\le n$, $0\le \frac{k}{n}\le1 $. So, our sample points are ranging from $0$ to $1$. As $n\to \infty$, the bin size (or mesh of the partition) tends to $0$, while the number of sample points tends to infinity. This is precisely the setup for the Riemann Integral.</p>
"
"2386519","2386542","<p>Take $t \in I$, $t \neq 0$ (if such a $t$ does not exist, then $I = \{0\} = R \cdot 0$). You have either $I = Rt$ (then you are done) or you have $I \neq Rt$, in which case you get $I = I + Rt = R = R\cdot 1$.</p>
"
"2386524","2386537","<p>What you should use is the case $x\neq0$.</p>

<p><strike>Because the limit of $\cos(\frac1x)$ does not exist when $x$ approaches to $0$, therefore the limit of $f(x)$ does not exist when $x$ approaches to $0$.</strike></p>

<p>Recall the definition of limit: </p>

<blockquote>
  <p>The limit of function $f(x)$ at point $x=c$ exists, if $x$ approaches $c$ resulting $f(x)$ approaches a point $L$. </p>
  
  <p>We then denote this PHENOMENON as $\lim_{x\rightarrow c}f(x)=L$.</p>
</blockquote>

<p>The definition of limit is an active PHENOMENON, not evaluating the function at point $c$.</p>

<p><strong>APPROACH a point does not mean AT that point.</strong></p>

<p>Now back to your problem.</p>

<p>Because $x^2\cos(\frac1x)$ approaches $L=0$ as $x$ approaches $c=0$, therefore $\lim_{x\rightarrow 0}f(x)=0$.</p>
"
"2386530","2389276","<p>This is not a complete answer to your question, but more an argument which should demonstrate that the property of being quotient-dense seems to have very few to do with the overall density of $S$.</p>

<p>Especially, I found two counterexamples to my former conjecture (see comments to your question) that $S=\{s_n\mid n\in\Bbb N\}$ is quotient-dense if and only if the always increasing sequence $s_n$ is sub-exponential. One of them is another counterexample to your own conjecture, but of another kind as the one given in the comments.</p>

<hr>

<p><strong>$s_n$ is sub-exponential, but $S$ is not quotient-dense</strong></p>

<p>This <em>not</em> quotient dense set $S$ is generated by a sequence $s_n&lt;7n$, obviously sub-exponential. To write down the set nicely, I will write $[a,b]_\Bbb N$ for the range of natural numbers from $a$ to $b$, e.g. $[1,6]_\Bbb N$ is short for $1,2,3,4,5,6$. Now we can define</p>

<p>$$S=\{[a_0,b_0]_\Bbb N,\;[a_1,b_1]_\Bbb N,\;[a_2,b_2]_\Bbb N,\;...\}$$</p>

<p>with $a_0=b_0=1$ and recursively $a_i=3b_{i-1}$ and $b_i=\lfloor 3/2\cdot a_i \rfloor$. The following picture shows the generating sequence $s_n$ up to $n\approx 80$ together with the bounding linear function (in gray).</p>

<p><img src=""https://i.stack.imgur.com/RTNPw.png"" width=""400"" /></p>

<p>It remains to show that this sequence is bounded by $7n$ and $S$ is not quotient-dense. The former statement is not hard to show but it is annoying to do nicely (I tried). So, I will only show the latter one:</p>

<p>Note that $a_0=b_0&lt;a_1&lt;b_1&lt;a_2&lt;b_2&lt;\cdots$. We will see that no rational number $x\in(1/3,2/3)$ can be expressed by quotients $p,q\in S$. Lets say $p\in[a_i,b_i]_\Bbb N$ and $q\in[a_j,b_j]_\Bbb N$. Because we want to build a rational $&lt;1$, we can assume $i\leq j$. At first assume $i&lt;j$. Then</p>

<p>$$\frac pq\leq\frac {b_i}{a_j}\leq\frac{b_{j-1}}{a_j}=\frac{b_{j-1}}{3b_{j-1}}=\frac13.$$</p>

<p>In the other case $i=j$ we have</p>

<p>$$\frac pq\geq\frac {a_i}{b_i}=\frac {a_i}{\lfloor 3/2\cdot a_i\rfloor}\geq\frac {a_i}{3/2\cdot a_i}=\frac23.$$</p>

<p>So we indeed have this gap $(1/3, 2/3)$ that we cannot fill. </p>

<hr>

<p><strong>$s_n$ is growing exponentially, but $S$ is quotient-dense</strong></p>

<p>At first note that it suffices to show that any number in $[0,1]$ can be approximated by $s_n/s_m$. Because given a number $x&gt;1$, we can instead approximate $1/x\in(0,1)$ by a sequence $s_{n(i)}/s_{m(i)}$ and then convert this via exchanging nominator and denominator to the sequence $s_{m(i)}/s_{n(i)}$ which converges to $x$.</p>

<p>Choose an enumeration $p_i/q_i,i=1,2,3,...$ of all rational numbers in $(0,1)$. If all of these numbers are representable from $S$, then all of $[0,1]$ can be approximated. Now choose $s_1=1$ (we skip $s_0$ for no real reason) and define recursively</p>

<p>$$s_{2i}=2p_i \cdot s_{2i-1},\qquad s_{2i+1}=2q_i \cdot s_{2i-1}.$$</p>

<p>Note that because $p_i&lt;q_i$, we indeed have $s_{2i}&lt;s_{2i+1}$. Now we have $s_{2i}/s_{2i+1}=p_i/q_i$, hence every rational numbers in $(0,1)$ can be generated from this sequence. We have to show that the sequence is growing exponentially, but this is easy:</p>

<p>$$s_{2i+1}=2q_i\cdot s_{2i-1}\geq 2s_{2i-1}$$</p>

<p>which is an exponentially growing sub-sequence. And because $s_{2i+1}=s_{n(i)}$ with linearly growing $n(i)$, this suffices to imply that $s_n$ itself grows exponentially.</p>

<hr>

<p><strong>Moral of the story</strong></p>

<p>If you want a nice characterization of quotient-dense sets, you should not look at properties like density. This involves your conjecture on $\sum_n s_n&lt;\infty$ and my conjecture about sub-exponential growth.</p>

<hr>

<p><strong>Update</strong></p>

<p>Actually any arbitrarily sparse set $S=\{s_n\mid n\in\Bbb N\}$ can be made quotient-dense by at most ""doubling its density"". Take an enumeration of the rationals $p_n/q_n$ as above. Then we have this new quotent-dense set</p>

<p>$$S':=\{p_ns_n\mid n\in\Bbb N\}\cup\{q_n s_n\mid n\in\Bbb N\}.$$</p>

<p>This new set has ""dense"" clusters of two elements and big gaps between these clusters. So it seems to have more to do with local clustering than global structure.</p>
"
"2386532","2386562","<p>$L^1((0,1))$ consists of the real-valued Lebesgue integrable functions on $(0,1)$. $L^2((-1,1))$ consists of those real-valued Lebesgue measurable functions $f$ on $(-1,1)$ such that $f^2$ is Lebesgue integrable. Implicit here is that the relevant measure is the Lebesgue measure. (In some contexts these are complex-valued instead of real-valued.)</p>

<p>The above is not <em>quite</em> right, because we traditionally want to understand these spaces as normed spaces, but the spaces as described above are in fact pseudonormed spaces. Accordingly those are traditionally denoted as $\mathcal{L}$ spaces, and the $L$ spaces are obtained by taking equivalence classes under equality almost everywhere. But in common parlance this distinction is usually ignored, and instead statements are made about individual functions being in $L^p$, with the qualifier ""almost everywhere"" thrown around when necessary. </p>
"
"2386540","2386545","<p>You need to meet</p>

<p>$$p(x+T)=p(x)+2k\pi$$ or</p>

<p>$$p(x+T)-p(x)=2k\pi.$$ </p>

<p>But for the LHS to be continuous, $k$ must be constant, hence $p(x)$ must indeed be linear.</p>
"
"2386548","2386585","<p>First of all I think there's a typo, it should be $\alpha = pb/(bc-ad)$. Take for example $P(x) = x^3 + x^2 - 3x - 4$ and $\alpha = p= 2$.</p>

<p>Then it's unclear how you got it that way, from your description your approach seem to be correct and I guess that you've miscalculated somewhere.</p>

<p>What you have is:</p>

<p>$$P(\alpha) = a\alpha^3 + b\alpha^2 + c\alpha + d = p$$
$$P(-\alpha) = -a\alpha^3 + b\alpha^2 - c\alpha + d = -p$$</p>

<p>Then adding and subtracting you get after dividing by two:</p>

<p>$$b\alpha^2 + d = 0$$
$$a\alpha^3 + c\alpha = p$$</p>

<p>From the first you get $\alpha^2 = -d/b$ and inserting it into the second you get:</p>

<p>$$c\alpha - a\alpha d/b = p$$
$$\alpha(c-ad/b) = p$$
$$\alpha = p/(c-ad/b) = pb/(bc - ad)$$</p>

<p>Then of course you should sort out the special cases where $\alpha=0$ or  $b=0$...</p>
"
"2386549","2387319","<p>Suppose that there exist two integer number $x,y&gt;0$ such that 
$$
	\sqrt{n}+\sqrt{n+1}&lt;\sqrt{x}+\sqrt{y}&lt;\sqrt{4n+2} \qquad\qquad (1)
$$
From the left hand side of (1) and the Cauchy_Schwarz we have
    $$
	x+y\geq\frac{(\sqrt{x}+\sqrt{y})^2}{2}&gt;\frac{(\sqrt{n}+\sqrt{n+1})^2}{2}&gt;\frac{(\sqrt{n}+\sqrt{n})^2}{2}=2n.
	$$
    Since $x+y&gt;0$ is an integer number, we have $x+y\geq 2n+1$. 
    Taking square both sides of (1) we get
$$
	2n+1+2\sqrt{n}\sqrt{n+1}-(x+y)&lt;2\sqrt{xy}&lt;4n+2-(x+y)\qquad\qquad (2)
$$
    Since $x+y\geq 2n+1$ we have $x+y=2n+1+k$ for some $k\geq 0$. Substituting $x+y$ into (2) we obtain
$$
2\sqrt{n}\sqrt{n+1}-k&lt;2\sqrt{xy}&lt;2n+1-k\qquad\qquad (3)
  $$
    From the above inequality we have $k&lt;2n+1$. Therefore
    $$
k\leq 2n&lt;2\sqrt{n}\sqrt{n+1}.
$$
    From here it implies that the left hand side of (3) is positive. Taking square both sides of (3) we get
    $$
(2\sqrt{n}\sqrt{n+1}-k)^2&lt;4xy&lt;(2n+1-k)^2.
$$
   On the other hand
$$
	(2\sqrt{n}\sqrt{n+1}-k)^2-((2n+1-k)^2-1)=(4n(n+1)+k^2-4k\sqrt{n}\sqrt{n+1})-(4n^2+4n+1+k^2-2k(2n+1)-1)
$$
$$
\quad\quad\quad\quad\quad=2k(2n+1-2\sqrt{n}\sqrt{n+1})
$$
$$
\quad\quad\quad\quad\quad=2k(\sqrt{n}-\sqrt{n+1})^2\geq 0.
$$
Hence
 $$
 (2n+1-k)^2-1\leq (2\sqrt{n}\sqrt{n+1}-k)^2&lt;4xy&lt;(2n+1-k)^2.
 $$
This is an absurd.</p>
"
"2386555","2386575","<p>Triangle's inequality in the definition of a norm is as follows: </p>

<blockquote>
  <p>$$\forall x,y\in X,\ \|x+y\|\le \|x\|+\|y\| $$</p>
</blockquote>

<p>Now, the above inequality follows the convexity of the unit closed ball. Let $\lambda\in [0,1]$ and $\mathbf{x},\mathbf{y}\in \mathbb{B}[0,1]\implies \|\mathbf{x}\|\le 1 $ and $\|\mathbf{y}\|\le 1$. Now consider, 
\begin{align*}
\|\lambda \mathbf{x}+(1-\lambda) \mathbf{y}\| &amp; \le \|\lambda \mathbf{x}\|+\|(1-\lambda)\mathbf{y}\|\\
                            &amp; = \lambda \|\mathbf{x}\|+(1-\lambda)\|\mathbf{y}\|\\
                            &amp; \le \lambda + (1-\lambda)=1
\end{align*} 
Hence, $ \lambda \mathbf{x}+(1-\lambda) \mathbf{y}\in \mathbb{B}[0,1]. $ </p>

<p>For the converse see, 
$$ \frac{\mathbf{x}+\mathbf{y}}{\|\mathbf{x}\|+\|\mathbf{y}\|}=\frac{\|\mathbf{x}\|}{\|\mathbf{x}\|+\|\mathbf{y}\|}\frac{\mathbf{x}}{\|\mathbf{x}\|}+\frac{\|\mathbf{y}\|}{\|\mathbf{x}\|+\|\mathbf{y}\|}\frac{\mathbf{y}}{\|\mathbf{y}\|}\in \mathbb{B}[0,1] \implies \|\mathbf{x}+\mathbf{y}\|\le \|\mathbf{x}\|+\|\mathbf{y}\|$$</p>
"
"2386564","2386577","<p>You have no guarantee that $Z,X$ will be independent (and infact they are not).</p>

<p>Use the Law of Total Probability:
$$\mathsf P(Z=z) ~{= \mathsf P(Z=z,X=-1)+\mathsf P(Z=z,X=1) \\ = \mathsf P(Z-X=z+1, X=-1)+\mathsf P(Z-X=z-1, X=1)}$$</p>

<p><em>Now</em> you have joint probabilities of independent random variables.</p>
"
"2386573","2386594","<p>We will use the form $y, y+32$ as proposed in the question.  </p>

<p>We know that $$\text {lcm}(a,b)=\frac {ab}{\gcd (a,b)}$$</p>

<p>It is clear that $d=\gcd(y,y+32)$ must be a divisor of $32$.  Hence $d\in \{1,2,2^2,2^3,2^4,2^5\}$.</p>

<p>We will proceed case by case.</p>

<p>I.  $d=1$ then we solve $y(y+32)=420\implies y=\{-42,10\}$.  However neither of these are relatively prime to $32$ so No Solution.</p>

<p>II. $d=2$ then we solve $y(y+32)=2\times 420$ which has no integer solutions, so No Solution.</p>

<p>III. $d=4$ then we solve $y(y+32)=4\times 420\implies y=\{-60,28\}$  Both of these work since $\gcd (-60,32)=4=\gcd(28,32)$. (Though perhaps the OP intended to assume $y&gt;0$).  Easy to see that this gives us the solutions $$\boxed {x=\{-79,9\}}$$</p>

<p>IV.  $d=8$ then we solve $y(y+32)=8\times 420$ which has no integer solutions, so No Solution.</p>

<p>V. $d=16$ then we solve $y(y+32)=16\times 420$ which has no integer solutions, so No Solution.</p>

<p>VI $d=32$ then we solve $y(y+32)=32\times 420$ which has no integer solutions, so No Solution.</p>

<p>Thus the only solutions are $$\boxed {x=\{-79,9\}}$$</p>
"
"2386579","2386876","<blockquote>
  <p>The key-word is <em>symmetry</em>. When looking at $$1,2,3,4,5$$ consider
  \begin{align*}
1+2+\color{blue}{3}+4+5 &amp;= (3-2)+(3-1)+\color{blue}{3}+(3+1)+(3+2)\\
&amp;=3+3+\color{blue}{3}+3+3\\
&amp;=5\cdot \color{blue}{3}\\
&amp;=15
\end{align*}</p>
</blockquote>
"
"2386583","2386666","<p>There is a problem with the definition of $f(x)$. </p>

<p>Lets analyse it at $x=0.8$ (this would work for any $|x|&lt;1$):</p>

<ul>
<li><p>By the first definition, $f(0.8)=0.8$. </p></li>
<li><p>By the second definition, $f(0.8)=f(-1.2+2)=-1.2$ since $-1.2&lt;-1$</p></li>
</ul>

<p>Then, as for one value of $x$ we have two different values of $f(x)$, either this is not a function or it is not well defined. </p>

<p>(However, for odd integers greater than $3$, it should be continuous since for those sufficiently large values $f(x)=x-2$)</p>
"
"2386587","2386701","<p>Let's look at this from another perspective for a moment. Let's fix $x=x_k-x_{k+1}$, and ask what are the possible values of $y$ which satisfy $\|x-y\|&lt;\frac1{2^k}$.</p>

<p>For a concrete example, consider $\Bbb R^2$ as $X$ and $\{(r,0)\mid r\in\Bbb R\}$ as $Y$. Now let $x=(0,0.01)$, and tell me how many possible values of $y$ we have to witness that $\inf_{y\in Y}\|x-y\|&lt;\frac12$, for example. There are uncountably many possible values. You simply have no canonical choice of a vector.</p>

<p>So now you need to choose for every $k$ some point $\eta_k$ which witnesses that $x_k-x_{k+1}$ is sufficiently close to $Y$. And since there is no distinguished vector that just the job, there is no way of choosing these $\eta_k$'s uniformly. There is where the axiom of choice kicks in.</p>

<p>Your mistake is that when you write $\inf_{y\in Y}$, then $y$ is a quantified variable, and you can't treat it as a single valued notion. It ranges over <em>all</em> the possible values. </p>
"
"2386588","2386744","<p>Well, this tree will look sort of like the full binary tree $2^{&lt;\kappa}$, except that all of the nodes with infinitely many $1$s are missing.  Let's call your tree $T$.  Then every branch of $T$ can be extended to a branch of $2^{&lt;\kappa}$, i.e. a function $\kappa\to 2$.  But given a function $\kappa\to 2$, not all of its restrictions will be elements of $T$: only those with only finitely many $1$s.  That is, the restriction forgets about all of the values of the function $\kappa\to 2$ after the first $\omega$ $1$s.</p>

<p>Thus the branches of $T$ are in bijection with subsets $S$ of $\kappa$ of order type $\leq\omega$.  Explicitly, given such a subset $S$, you get a branch by taking all initial segments of the characteristic function of $S$ prior to the supremum of its first $\omega$ elements.  Conversely, given a branch $B$, the union of all the elements of $B$ is a function $f:\alpha\to 2$ for some $\alpha\leq\kappa$.  If $\alpha&lt;\kappa$, the fact that $B$ cannot be extended by adding $f$ as another member means that $f$ takes the value $1$, which means $f$ is the characteristic function of a cofinal subset of $\alpha$ of order-type $\omega$.  If $\alpha=\kappa$, then $f$ must similarly be either the characteristic function of a finite subset of $\kappa$ or the characteristic function of a cofinal subset of $\kappa$ of order-type $\omega$.  Combining all the cases together, $f$ comes from a subset of $\kappa$ of order-type $\leq\omega$.</p>

<p>Let us now count how many branches there are (assuming $\kappa$ is an infinite cardinal).  There are $\kappa$ finite subsets of $\kappa$.  The elements of a subset of order-type $\omega$ can be chosen one-by-one in increasing order, and at each step you have $\kappa$ choices (there are still $\kappa$ elements greater than the last one you chose).  So there are $\kappa^{\aleph_0}$ subsets of order-type $\omega$.  In total, then, there are $\kappa+\kappa^{\aleph_0}=\kappa^{\aleph_0}$ branches.</p>
"
"2386602","2386618","<p>Let's consider the first series $\sum_{n=1}^{\infty} (n!)^2 x^{n^2}$. The easiest way to find the radius of convergence is to forget this is a power series and treat $x$ as a constant. Let's assume $x &gt; 0$ so that the terms of the series are positive and we can use any test we wish for the convergence/divergence of a non-negative series. Alternatively, replace $x$ with $|x|$ and check for absolute convergence instead. Using the ratio test, we get the expression
$$ \frac{(n+1)!^2 x^{(n+1)^2}}{(n!)^2 x^{n^2}} = (n+1)^2 x^{2n + 1}. $$
This expression converges if $0 &lt; x &lt; 1$ to $0$ while it diverges if $x \geq 1$. This implies that the series converges if $0 &lt; x &lt; 1$ and diverges if $x \geq 1$. But this is a power series so the only possible value for the radius of convergence is $R = 1$ (because it should converge when $|x| &lt; R$ and diverge when $|x| &gt; R$).</p>
"
"2386605","2386619","<p><strong>hint</strong></p>

<p>$$\forall \to \;\;\exists $$
$$\ge \to \;\;&lt;$$
$$\lor \to \;\;\land $$</p>

<p>so the negation is</p>

<p>$$\exists x \;: x &lt;100 \;\; \land \;\; x\ge 100$$</p>

<blockquote>
  <p>remark</p>
</blockquote>

<p>Your proposition is always true (tautology), thus its negation is always false (contradiction).</p>
"
"2386608","2386623","<p>The ""dimension is equal to the degree of the minimal polynomial"" thing only works when you have a <em>single element</em> that generates the whole extension. Take, for instance, $\alpha = \omega + 2^{1/3}$. It has minimal polynomial
$$
x^6 - 3 x^5 + 6 x^4 - 11 x^3 + 12 x^2 + 3 x + 1
$$
over $\Bbb Q$ and therefore generates an extension of degree $6$. Clearly $\Bbb Q(\alpha)\subseteq \Bbb Q(\omega, 2^{1/3})$, so the degree of $Q(\omega, 2^{1/3})$ cannot be $5$.</p>

<p>Using this, and the multiplicative formula for the extension degrees which you've already used in your question, we can see that if you have several distinct elements that generate a field extension, we <em>multiply</em> the degrees of their respective minimal polynomials instead of adding them.</p>

<p>Or... almost. We multiply the degree of the minimal polynomial of $\omega$ over $\Bbb Q$ with the degree of the minimal polynomial of $2^{1/3}$ over $\Bbb Q(\omega)$ (or vice versa). It might not make a difference in this case, but it is important with other extensions like $\Bbb Q(\sqrt2, \sqrt[4]2+\sqrt2)$, and many other, probably less trivial examples.</p>
"
"2386612","2386671","<p>Your approach is correct, but here's a way to arrive at the same result with less computation.  Considering $A^T A$ as a linear transformation, we have the following factorization.
$$
\newcommand{\R}{\mathbb{R}} \R^3 \overset{A}{\longrightarrow} \R \overset{A^T}{\longrightarrow} \R^3
$$
This shows that the map factors through a $1$-dimensional space, so its rank is $\leq 1$.  Assuming $A$ is not the zero matrix, then the rank of $A^T A$ is $1$, so its kernel has dimension $3-1 = 2$.  Thus two of the eigenvalues must be $0$, and from your trace formula $\DeclareMathOperator{\tr}{tr} \sum_i \lambda_i = \tr(A^T A) = a^2 + b^2 + c^2$, we see that the third eigenvalue must be $a^2 + b^2 + c^2$.  Note that this approach generalizes and can be used for $A \in M_{1 \times n}(\R)$ for any $n$.</p>

<p>The factorization above also indicates how to find the eigenvectors.  The kernel of $A$ is $2$-dimensional, and these will be the eigenvectors with eigenvalue $0$.  (Since $\DeclareMathOperator{\img}{img} \ker(A) = \img(A^T)^\perp$, this agrees with Daniel's answer.)  Since $\ker(A)^\perp = \img(A^T)$, the remaining eigenvector is any nonzero multiple of $A^T$.</p>
"
"2386628","2386873","<p>I think there's been a misinterpretation of the problem.
First of all, to be clear since this isn't written in the post, we have a function $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ defined by
$$f(x,y) = 3 - 3x^2 - y^2 \, .$$
The problem is to find the point in the plane such that $\lvert \nabla f \rvert$ is maximized subjected to the constraint that $x^2+y^2=1$.</p>

<p>However, you've written one of your constraint equations as
$$3 - 3x^2 - y^2 = 0$$
which could also be written as $f(x,y)=0$.
That's a much stronger constraint than what's in the problem, and I don't think it's what the problem intends.</p>

<p>There are a few ways to solve the problem.
We could use Lagrange multipliers or we can parametrize the circle and use simple calculus.
Let's do that latter.
The thing we're trying to maximize is
$$\lvert \nabla f \rvert = \lvert -6x\hat{x} -2y\hat{y} \rvert = \sqrt{36x^2 + 4y^2} \, . $$
To simplify our life, let's maximize $g(x,y) \equiv \lvert \nabla f \rvert^2$ instead:
$$g(x,y) = 36x^2 + 4y^2 \, . $$
Now normally to find an extremum we'd compute $\partial g / \partial x$ and $\partial g / \partial y$, set them to zero, and solve for $x$ and $y$.
However, we can't do that here because we have a constraint.
Fortunately, it's easy to respect the constraint by putting $y^2 = 1 - x^2$, giving
$$g(x) = 36x^2 + 4(1 - x^2) = 4 - 32x^2$$
so
$$dg/dx =0 \longrightarrow x = 0 \, .$$</p>
"
"2386630","2386888","<p>If you have an adjunction where $F\colon \mathcal{C} \to \mathcal{D}$ is left adjoint to $U$, then $U\circ F$ is an <a href=""https://en.wikipedia.org/wiki/Monad_(category_theory)#Monads_and_adjunctions"" rel=""nofollow noreferrer"">monad</a>. And the category of algebras over this monad is equivalent to $\mathcal{D}$.</p>

<p>If the monad $U\circ F$ preseves certain kind of colimits, then $U$ also <a href=""https://math.stackexchange.com/questions/1016618/filtered-colimits-commute-with-forgetful-functors"">does</a>.</p>

<p>So in your case the monad would be the free functor, like free group, free $R$-module. So you just need to check that the free group functor commutes with certain colimits on the level of sets. (I.e. for groups: the colimit of sets over a poset $I$ is the same as the colimit of sets words in these sets over $I$.</p>
"
"2386632","2386950","<p>Fix $t&gt;s$ and let ${\cal C}=\{A_M \cap A_X: A_M\in{\cal F}^M_s, A_X\in{\cal F}^X_\infty \}.$</p>

<p>The martingale property for $M$ means that for $A_M\in{\cal F}^M_s$,
 we have $$\mathbb{E}\left((M_t-M_s){\bf 1}_{A_M}\right)=0.\tag1 $$</p>

<p>Thus, for any set in $\cal C$ the independence of $M$ and $X$ gives<br>
$$\mathbb{E}\left((M_t-M_s){\bf 1}_{A_M \cap A_X}\right)
=\mathbb{E}\left((M_t-M_s){\bf 1}_{A_M}{\bf 1}_{A_X}\right)=
\mathbb{E}\left((M_t-M_s){\bf 1}_{A_M}\right) \mathbb{E}({\bf 1}_{A_X})=0.$$</p>

<p>Since $\cal C$ is a $\pi$-system that generates ${\cal F}^M_s\vee{\cal F}^X_\infty$, the Monotone Class Theorem says that 
$$\mathbb{E}\left((M_t-M_s){\bf 1}_{A}\right)=0\tag2 $$
for every $A\in {\cal F}^M_s\vee{\cal F}^X_\infty$. This means that $(M_t)$ is a martingale with respect to the filtration $({\cal F}^M_t\vee{\cal F}^X_\infty).$</p>
"
"2386634","2386642","<p>If $y=0$ then $x=0$</p>

<p>If $y=2$ then $x=0$ and $x=20/9$ impossible.</p>

<p>Assume $y\ne 0$ and $y\ne 2$. then by division ,</p>

<p>$$\frac {2y (y+3)}{y (y-2)}=\frac {9x}{3x}=3$$</p>

<p>or after simplification,
$$\frac {2 (y+3)}{y-2}=3$$
$$\implies 2 (y+3)=3 (y-2) $$
$$\implies y=12$$
$$3x=y (y-2)=120$$
$$\implies x=40$$</p>
"
"2386643","2386692","<p>Note that $GA*GA'=GB*GB'=GC*GC':=s^2$.</p>

<p>It suffices to prove that $\frac{{GA}^{2}+{GB}^{2}+{GC}^{2}}{s^2}=3$.</p>

<p>Note that $s^2=R^2-OG^2$ and by Leibniz's formula we have ${GA}^{2}+{GB}^{2}+{GC}^{2}+3OG^2=OA^2+OB^2+OC^2=3R^2$. </p>

<p>Substituting the equation in leads to the wanted equation.</p>

<p>(Sorry, my first post, I 'm not good at posting, please forgive.)</p>
"
"2386647","2386673","<p>Notice that the minimal polynomial $m(x)$ of $T$ is a real polynomial of degree $2$ and divides $x^q - 1$. Using the condition that $\det(T) &gt; 0$, we easily find that</p>

<p>$$ m(x) = (x - \omega)(x - \bar{\omega}) = x^2 - 2\cos (\alpha)x + 1 $$</p>

<p>for some <em>complex</em> zero $\omega = e^{i\alpha}$ of $\omega^q - 1$. So $T$ is diagonalizable over $\mathbb{C}$. Let $u$ be a complex eigenvector of $T$ corresponding to $\omega$ and write $u = v + iw$ for <em>real</em> vectors $v$ and $w$. Then both $v$ and $w$ are non-zero since $ T\bar{u} = \overline{Tu} = \overline{\omega u} = \bar{\omega}\bar{u} $ and hence $\{u, \bar{u}\}$ is $\mathbb{C}$-linearly independent.</p>

<p>$$ Tv = \operatorname{Re}(Tu) = \operatorname{Re}(\omega u) = \cos(\alpha)v - \sin(\alpha)w $$</p>

<p>and </p>

<p>$$ Tw = \operatorname{Im}(Tu) = \operatorname{Im}(\omega u) = \sin(\alpha)v + \cos(\alpha)w. $$</p>

<p>Therefore, $B = \{v, w\}$ is a basis of $V$ such that</p>

<p>$$ [T]_B = \begin{pmatrix}
\cos\alpha &amp; -\sin\alpha \\
\sin\alpha &amp; \cos\alpha
\end{pmatrix}. $$</p>

<p>(Or simply invoke the real Jordan normal form of $T$.)</p>
"
"2386650","2386663","<p>Note that $\frac{f'(x)}{f(x)}$ is the derivative of $\ln f(x)$. If $f(x)\to 0$ then $\ln f(x)\to-\infty$, which requires that the derivative of $\ln f(x)$ is unbounded. But this does not necessarily mean that it goes to $\infty$. Try
$$f(x)=\begin{cases}x(2+\sin\tfrac 1x)&amp;\text{if }x&gt;0\\0&amp;\text{if }x=0\end{cases} $$
(The added $2$ is only an icing on the cake to prevent $f(x)=0$ for $x&gt;0$).</p>

<p>Then $f'(x)$ oscillates between negative and positive values, hence $\lim_{x\to0^+}\frac{f'(x)}{f(x)}$ does not exist, not even in the extended reals.</p>
"
"2386652","2386765","<p>If I understand correctly, you are given $s$ and $h$ below, and you are looking for $r$, $\theta$ and $c$.</p>

<p><a href=""https://i.stack.imgur.com/hWYw9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hWYw9.png"" alt=""arc""></a></p>

<p>This means solving the following equations </p>

<p>$$\begin{align}
  s &amp;= r \theta \\
  h &amp;= r - r\cos\theta \\
  c &amp;= 2 r \sin\left( \frac{\theta}{2} \right)
\end{align}$$</p>

<p>If you knew the radius of curvature $r$, then $$c = \frac{h}{\sin\left( \frac{s}{2 r}\right)}$$</p>

<p>Unfortunately, there isn't an analytical solution to the equation $$h = r -r \cos\left( \frac{s}{r} \right)$$ for $r$ in terms of $h$ and $s$.</p>

<p>A second order estimate, for small $\frac{h}{s}$ is $$r \approx \frac{s}{4 \sin\left( \frac{1}{3} \sin^{-1} \left( \frac{3 h}{2 s} \right) \right)}$$</p>

<p><sub>This was found using a Taylor series expansion on $ \frac{h}{s} = \frac{1}{\theta} (1-\cos\theta)$</sub></p>

<p>A higher order estimate is $$\frac{r}{s} \approx \frac{ \sqrt{\frac{5}{18}-\frac{\sqrt{13}}{18}}}{ \sin^{-1} \left(2 (\sqrt{13}-3) \sin \left( \frac{1}{3} \sin^{-1}\left( \frac{h}{s} \sqrt{ \frac{\sqrt{13}}{4}+1} \right) \right) \right)}$$</p>

<p>This last one with $\theta &lt; \frac{\pi}{2}$ the error is less than $0.007\%$. </p>
"
"2386659","2386680","<p>The polynomial  $p(x)=x^4+1$ splits into four linear factors over the algebraicely complete field $\mathbb{C}$:</p>

<p>$p(x)=(x-e^{i\frac{\pi}{4}})(x-e^{-i\frac{\pi}{4}})(x-e^{i\frac{5\pi}{4}})(x-e^{-i\frac{5\pi}{4}})$</p>

<p>and each of the corresponding roots could be taken as $``(-1)^{\frac{1}{4}}""$.</p>

<p>Clearly $e^{i\frac{5\pi}{4}}=-e^{i\frac{\pi}{4}}$ (This might explain why Wolfram found ""the negative of Your root""). However since $\mathbb{C}$ is no ordered field there is no unique fourth root.</p>
"
"2386661","2386694","<p>NO. If two statements are logically equivalent, then their columns in the combined truth-table will be exactly the same. Truth-tables are a way of showing the truth-conditions of statements, and two statements are equivalent if and only if they have the same truth-conditions ... i.e. have the same truth-table!</p>

<p>By 'combined truth-table I mean that both statements would be put in the same truth-table. Because if you don't do that, then superficially it may look like the truth-tables of, for example, $P$ and $P \land (P \lor Q)$ are not the same:</p>

<p>$\begin{array}{c|c}
P&amp;P\\
\hline
T&amp;T\\
F&amp;F\\
\end{array}$</p>

<p>$\begin{array}{cc|c}
P&amp;Q&amp;P\land (P \lor Q)\\
\hline
T&amp;T&amp;T\\
T&amp;F&amp;T\\
F&amp;T&amp;F\\
F&amp;F&amp;F\\
\end{array}$</p>

<p>But using a combined truth-table:</p>

<p>$\begin{array}{cc|c|c}
P&amp;Q&amp;P&amp;P \land (P \lor Q)\\
\hline
T&amp;T&amp;T&amp;T\\
T&amp;F&amp;T&amp;T\\
F&amp;T&amp;F&amp;F\\
F&amp;F&amp;F&amp;F\\
\end{array}$</p>
"
"2386675","2386720","<p><strong>hint</strong></p>

<p>the right hand side is an even polynomial function, so the particular  solution $y_p $  could be  polynomial.</p>

<p>$y_p $ and $y_p''$ could be both even.</p>

<p>Put $$y_p=Ax^2+B $$</p>

<p>then</p>

<p>$$y_p''+y_p=2A+Ax^2+B=1+x^2$$
and
$$A=1 \;\;,\;\;2A+B=1 \;\;, \;B=-1$$</p>
"
"2386689","2386847","<p>Yes, eliminatiion by division always works. But you also need to separately consider the case that the eliminated variable $=0.\,$ Let's compare eliminating vs. isolating $x$ then dividing for the system</p>

<p>$$\begin{align} a x + b y = c\\
 d x + e y = f \end{align}$$</p>

<p>Eliminating $x$ we obtain the equation $\ (bd-ae)\, y = cd-af$</p>

<p>Isolating $x$ and dividing we obtain $\  \ a/d = (by-c)/(ey-f)$</p>

<p>Clearing denominators shows that the latter is equivalent to the former, assuming that $d\neq 0$ (which we can assume wlog). Note that $\,ey-f = dx\neq 0\,$ by $\,d,x\neq 0$.</p>
"
"2386691","2386711","<p>Let us show that the assumption implies that $x\,|\,P(x)$.</p>

<p>In general, we must have $$P(x)=x\;Q(x)+c$$</p>

<p>Where $Q(x)$ is another poynomial with integer coefficients (the quotient) and $c$ is an integer constant, the remainder.  </p>

<p>Now we remark that  $$n\,|\,P(n)\implies n\,|\, c$$</p>

<p>But if $c$ were non-zero this could only be true for finitely many $n$.  As the assumption is that it is true for all positive $n$ then $c$ must be $0$.  Thus $P(x)=x\;Q(x)$ so $P(0)=0$.</p>
"
"2386705","2386717","<p>Ad absurdum:</p>

<p>Let $a,b&gt;0$ be s.t. $m=ap+bq$</p>

<p>Since $m&lt;(p-1)(q-1)$ we also have $a&lt;q-1$ and $b&lt;p-1$.</p>

<p><em>Edit: to prove this, notice that if $a\geq q-1$ then $ap+bq&gt;ap\geq (q-1)p&gt;(p-1)(q-1)&gt;m$, which contradicts $m=ap+bq$.</em></p>

<p>Then $m=p(q-1)-q=ap+bq$, hence $(q-1-a)p=(b+1)q$, where $0&lt;q-1-a&lt;q$ and $0&lt;b+1\leq p$.</p>

<p>This contradicts the fact that $p$ and $q$ are coprime.</p>
"
"2386706","2386868","<p>You are correct!</p>

<blockquote>
  <p>The cocountable and cofinite topologies on $X$ are equal iff $X$ is finite.</p>
</blockquote>

<p><em>Proof:</em>
If $X$ is infinite, then there is a countably infinite subset $D$ of $X$. (This uses the axiom of choice.) Then $D^c$ is cocountable, but not cofinite.
Conversely, if $X$ is finite, then every subset of $X$ is both cofinite and cocountable. This means that the cofinite, cocountable, and discrete topologies coincide.</p>
"
"2386708","2386789","<p>It is tacitly assumed here that $r=\sqrt{x^2+y^2}&gt;0$. Therefore we may as well write
$${\bf F}=\langle-{y\over\sqrt{x^2+y^2}},{x\over\sqrt{x^2+y^2}}\rangle=\langle-\sin\phi,\cos\phi\rangle={\bf u}_{\phi+\pi/2}\ ,$$
where $\phi$ denotes the polar angle of $(x,y)$ and ${\bf u}_\phi:=\langle\cos\phi,\sin\phi\rangle$ denotes the unit vector in direction $\phi$.</p>
"
"2386710","2386715","<p>The first non-mathy definition that <a href=""https://www.merriam-webster.com/dictionary/complement"" rel=""noreferrer"">Merriam-Webster</a> gives for ""complement"" is</p>

<blockquote>
  <p>something that fills up, completes, or makes perfect</p>
</blockquote>

<p>You may also note that the other definitions have a similar connotation.  Things complement each other if, when put together, they make up something that is complete.  For another mathematical example, consider complementary angles, which add up to a ""perfect"" right angle.</p>

<p>In set theory, a set and its complement form a universal set (i.e. if $X$ is the universe, then $A \cup A^c = X$).  Hence the two sets complement each other, in the sense that together, they make a whole.</p>

<p>Addendum:  the <a href=""http://www.oed.com/view/Entry/37642?rskey=pGF9o8&amp;result=1&amp;isAdvanced=false#eid"" rel=""noreferrer"">Oxford English Dictionary</a> (this may require academic access) indicates that ""complement"" was first used in the mathematical sense in Billingsley's 1570 translation of Euclid's <em>Elements</em>.  At that time, it had been in English usage for at least 150 years (the earliest reference in the OED goes back to 1419, as far as I can tell, meaning something to the effect of ""finishing or completing something"").  The word originally comes from Latin: ""complÄmentum that which fills up or completes.""</p>
"
"2386725","2386737","<p>Here's a slightly more general answer to your question.</p>

<p>Given categories and functors as shown:
$$\mathcal{A} \overset{F}{\to} \mathcal{B} \overset{G}{\underset{H}{\rightrightarrows}} \mathcal{C} \overset{K}{\to} \mathcal{D}$$
and a natural transformation $\theta : G \to H$, we can define new natural transformations
$$\theta_F : GF \to HF \quad \text{and} \quad K\theta : KG \to KH$$
by defining
$$(\theta_F)_A = \theta_{F(A)} : GF(A) \to HF(A) \quad \text{and} \quad (K\theta)_B = K(\theta_B) : KG(B) \to KH(B)$$
for all $A \in \mathrm{ob}(\mathcal{A})$ and $B \in \mathrm{ob}(\mathcal{B})$.</p>

<p>These are just definitions of what is meant by '$\theta_F$' and '$K\theta$'; but as you suggest, identity natural transformations are involved: indeed,
$$\theta_F = \theta \star \mathrm{id}_F \quad \text{and} \quad K\theta = \mathrm{id}_K \star \theta$$
where $\star$ is <a href=""https://ncatlab.org/nlab/show/horizontal+composition"" rel=""nofollow noreferrer"">horizontal composition</a> of natural transformations. Horizontal composition is slightly trickier to define, so most authors define $\theta_F$ and $K\theta$ independently.</p>
"
"2386729","2389185","<p>If $\hat u$ is the Laplace transform, then $\hat u(i\omega) = \mathcal F u(\omega)$, and $\mathcal F u$ is the Fourier-transform of $u$. If I understood your post correctly, you are actually working with Fourier transforms most of the time. This is good, since we have a couple of nice properties about norms in this context.</p>

<p>If we start from $\hat y = G\hat u$, but the arguments are $i\omega$ anyway, we can instead look at $\mathcal F(y) = G\mathcal F (u)$.</p>

<p>We could proceed by using the continuity of $\mathcal F^{-1}$ from $L^1$ into the space of continuous functions decaying towards infinity, equipped with the maximum norm (usually you will find this statement only for $\mathcal F$, but since $\mathcal F$ and $\mathcal F^{-1}$ behave exactly the same, this is ok):
$$\|y\|_{L^\infty} = \|\mathcal F^{-1}\mathcal F y\|_{L^\infty}\le\frac1{\sqrt{2\pi}}\|\mathcal F y\|_{L^1} = \frac1{\sqrt{2\pi}}\|G\mathcal F (u)\|_{L^1}\le \|G\|_{H^2}\|\mathcal Fu\|_{L^2}= \|G\|_{H^2}\|u\|_{L^2}.$$
The first inequality comes from the aforementioned <a href=""https://en.wikipedia.org/wiki/Fourier_transform#Uniform_continuity_and_the_Riemann.E2.80.93Lebesgue_lemma"" rel=""nofollow noreferrer"">continuity</a> of $\mathcal F^{-1}:L^1\to (C, \|.\|_{L^\infty})$. The next estimate is due to the <a href=""https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality"" rel=""nofollow noreferrer"">Cauchy-Schwarz</a> inequality. The last step uses that the Fourier transform is an <a href=""https://en.wikipedia.org/wiki/Fourier_transform#Plancherel_theorem_and_Parseval.27s_theorem"" rel=""nofollow noreferrer"">isometry</a> on $L^2$, i.e. it does not change the norm (a property you have used in your question already).</p>

<p><em>Remark 1</em>: In more detail, the first estimate is obtained by the following considerations:
$$\sqrt{2\pi}\cdot\|y\|_{L^\infty} = \sup_{x\in\mathbb R}\left|\int_{\mathbb R}e^{i t x} \mathcal F[y(t)]d t\right|\le \sup_{x\in\mathbb R}\int_{\mathbb R}|e^{i t x} |\cdot|\mathcal F[y(t)]|d t = \int_{\mathbb R}|\mathcal F[y] |dx = \|\mathcal Fy\|_{L^1}.$$
The constant $\sqrt{2\pi}$ comes from the choice of the Fourier transform $\mathcal F y = \frac 1{\sqrt{2\pi}}\int_{\mathbb R} e^{-ixt}y(t) dt$. This makes $\mathcal F$ an isometry on $L^2$, but introduces the constant in the first estimate. If you wish to choose another definition of the Fourier transform you should adapt the constants accordingly.</p>

<p><em>Remark 2</em>: As a mathematician, I would say that what you denote by $\|G\|_{H^2}$ is nothing else than an $L^2$-norm on matrix-values functions. $H^2$ is commonly reserved for a <a href=""https://en.wikipedia.org/wiki/Sobolev_space#The_case_p_.3D_2"" rel=""nofollow noreferrer"">Sobolev space</a>.</p>
"
"2386730","2386792","<p>I think any such proof would revolve around the same ideas. But here's how we can phrase it to stay close enough to your original proof.</p>

<p>After</p>

<blockquote>
  <p>&hellip; we have to prove $H\subset K$ or $K\subset H$</p>
</blockquote>

<p>we can insert the following: If $H\subset K$, we're done. So let's assume that $H\not\subset K$ (in which case our goal is to show that $K\subset H$). Then instead of</p>

<blockquote>
  <p>Let $x\in H$</p>
</blockquote>

<p>it's better to pick $x\in H\setminus K$. Now we can pick any $y\in K$, as you did. And as in your proof (but I just want to give a name to the new element):</p>

<blockquote>
  <p>Then $z=x*y\in H\cup K$ for $H\cup K$ is a subgroup $\implies$ $x*y\in H$ or $x*y\in K$.</p>
</blockquote>

<p>If $z=x*y\in K$, then $x=zy^{-1}\in K$, since both $z\in K$ and $y^{-1}\in K$. But this is impossible because we chose $x\in H\setminus K$. Therefore $z=x*y\in H$. But then $y=x^{-1}z\in H$, since both $x^{-1}\in H$ and $z\in H$.</p>

<p>Thus we've shown that for any element $y\in K$, we have $y\in H$, i.e. $K\subset H$.</p>
"
"2386731","2386750","<p>Let us say that you want the zero of $$f(t)=1.71 e^{-26.18 t}-11.7 e^{-3.82 t}+5$$ Since the exponents do not show a ratio of $2$, $3$ or $4$, the equation cannot reduce to a polynomial.</p>

<p>Only numerical methods will solve the problem.</p>

<p>Whet you can notice is that $$f'(t)=44.694 e^{-3.82 t}-44.7678 e^{-26.18 t}$$ cancels for $x\approx 0.0000738$ and the second derivative test shows that this is a minimum. So, you have a unique solution to the equation.</p>

<p>By inspection, $f(0.1)\approx -2.86045$ and $f(0.5)\approx 3.26746$. So, let us start Newton method using $x_0=0.3$. The iterates would be
$$\left(
\begin{array}{cc}
n &amp; x_n \\
 0 &amp; 0.300000 \\
 1 &amp; 0.209721 \\
 2 &amp; 0.222005 \\
 3 &amp; 0.222287
\end{array}
\right)$$ which is the solution for six significant figures.</p>
"
"2386734","2386736","<p>I guess you want this:</p>

<p>You know $P(A\mid B)=\dfrac{P(A\cap B)}{P(B)}$</p>

<p>Since independent $P(A\mid B)=P(A)$</p>

<p>Hence, $$P(A\cap B)=P(A)P(B)\space\space\space\space\space\blacksquare$$</p>
"
"2386745","2386809","<p>Define</p>

<p>$$A_{k,n} := \left\{x \in \mathbb{R}; m \big(\{y \in [k,k+1); f(y)=x\} \big)&gt; \frac{1}{n} \right\}.$$</p>

<p>Since for any $x \neq z$ the sets $$\{y \in [k,k+1); f(y)=x\} \quad \text{and} \quad \{y \in [k,k+1); f(y)=z\}$$ are disjoint, there can be at most $n$ points $x_1,\ldots,x_n \in \mathbb{R}$ such that $$m(\{y \in [k,k+1); f(y)=x_j\})&gt;\frac{1}{n};$$ thus $\sharp A_{k,n} \leq n$. This implies that</p>

<p>$$S_n := \{x \in \mathbb{R}; m(f^{-1}(\{x\})&gt; \frac{1}{n}\} = \bigcup_{k \in \mathbb{Z}} A_{k,n}$$</p>

<p>is countable, and therefore</p>

<p>$$S = \bigcup_{n \geq 1} S_n$$</p>

<p>is countable; in particular $m(S)=0$.</p>
"
"2386753","2386903","<p>The $2$ is the value of <em>one</em> standard deviation.</p>

<p>So, there is a $0.68$ probability that a mouse has a weight between $8$ and $12$ grams, and thus there is a $0.32$ probability that the weight of a mouse is outside that interval, i.e. below $8$ or above $12$. By symmetry, this means that there is a $0.16$ probability it is below $8$.</p>
"
"2386754","2386816","<p>With each $a \in \mathbb{Z}^n$ associate the half-open box $$B(a) := \prod_{\nu = 1}^n [a_{\nu}, a_{\nu} + 1).$$</p>

<p>If $a$ lies in the ball with radius $R$, then the box $B(a)$ is contained in the ball with radius $R + \sqrt{n}$, since $\operatorname{diam} B(a) = \sqrt{n}$. Each box has volume (Lebesgue measure) $1$, hence the number of points in the ball of radius $R$ is bounded by the volume of the ball with radius $R + \sqrt{n}$, which is</p>

<p>$$C_n (R+\sqrt{n})^n = C_n R^n + n^{3/2}C_n R^{n-1} + \dotsc = C_n R^n + O(R^{n-1}).$$</p>

<p>On the other hand, if $B(a)$ intersects the ball with radius $R - \sqrt{n}$, then $a$ lies in the ball with radius $R$. So the boxes associated with lattice points in the ball with radius $R$ cover the ball with radius $R - \sqrt{n}$, hence there must be at least $$C_n(R-\sqrt{n})^n = C_n R^n - n^{3/2}C_nR^{n-1} + \dotsc = C_nR^n + O(R^{n-1})$$</p>

<p>such boxes.</p>

<p>It follows that the number of lattice points in the ball with radius $R$ is $C_n R^n + O(R^{n-1})$, and thus the number of lattice points in the shell is</p>

<p>$$C_n(R+1)^n + O\bigl((R+1)^{n-1}\bigr) - C_nR^n + O(R^{n-1}) = O(R^{n-1}).$$</p>
"
"2386756","2386855","<p>Let's start with the integral $\;\displaystyle\int \frac {dx}{\cosh(x)^2}=\frac 2{1+e^{-2x}}+C\;$ (with the choice $C=-2$) and use integration by parts :</p>

<p>\begin{align}
I(\alpha) &amp;= \int_0^\infty \frac{x^\alpha}{\cosh^2x}dx\\
&amp;=\left.x^\alpha\left(\frac 2{1+e^{-2x}}-2\right)\right|_0^\infty-\int_0^\infty \alpha\,x^{\alpha-1}\left(\frac 2{1+e^{-2x}}-2\right)\,dx\\
&amp;=2\alpha \int_0^\infty \frac {x^{\alpha-1}\,e^{-2x}}{1+e^{-2x}}\,dx\\
&amp;=2\alpha \int_0^\infty \frac {x^{\alpha-1}}{e^{2x}+1}\,dx\\
&amp;=2\alpha\, 2^{1-\alpha-1}\int_0^\infty \frac {(2x)^{\alpha-1}}{e^{2x}+1}\,d(2x)\\
&amp;= 2^{1-\alpha}\alpha\,\int_0^\infty \frac {t^{\alpha-1}}{e^t+1}\,dt
\end{align}</p>

<p>Now a closed variant of $\zeta$ is the <a href=""https://en.wikipedia.org/wiki/Dirichlet_eta_function"" rel=""nofollow noreferrer"">Dirichlet eta function</a> which may be defined by :
$$\eta(s) := \frac{1}{\Gamma(s)} \int_{0}^{\infty} \frac{t^{s-1}}{e ^ t + 1}\, dt=\left(1-2^{1-s}\right)\zeta(s) $$</p>

<p>All this gives us :
$$I(\alpha) ={2^{1-\alpha}\left(1-2^{1-\alpha}\right)}{\,\Gamma(\alpha+1)}\,\zeta(\alpha)$$</p>
"
"2386757","2386762","<p>The density of $X_1$ is $f(x)=(1/\beta) \mathbb{1}_{[0,\beta]}(x)$ because $X_1$ is uniform on $[0,\beta]$.</p>

<p>Therefore $P(X_1 &lt; \beta - \varepsilon)=\int_0^{\beta - \varepsilon} 1/\beta \  \mathrm{d}x=\frac{\beta - \varepsilon}{\beta}&lt;1$</p>

<p>I don't really know what to say about its meaning, I can only ""translate"" it into English, it means that the probability for $X_1$ to be less than $\beta - \varepsilon$ is $\frac{\beta - \varepsilon}{\beta}$, which is of course less than $1$.</p>
"
"2386767","2386773","<p>Hint: prove by induction on $k$ that for each interval $I$ the set $d^k(I)$ is countable. Then use the fact that a countable union of countable sets is countable. </p>

<p>E.g. $\displaystyle d^2(I) = \bigcup_{J \in d(I)} d(J)$. Each $d(J)$ is countable and $d(I)$ is countable, so this is a union of countably many countable sets, so it's countable. </p>
"
"2386768","2387100","<p>For simplicity, let's use
$$\begin{array}{l}
( p_x , p_y , p_z ) = \vec{P}_0 \\
( d_x , d_y , d_z ) = \vec{d} \\
( a_x , a_y , a_z ) = \vec{a} \\
( b_x , b_y , b_z ) = \vec{b}
\end{array}$$</p>

<p>Let's parametrise the line using $s \in \mathbb{R}$ and the line segment using $t \in \mathbb{R}$, $0 \le t \le 1$:
$$\vec{P}_0 + s \vec{d} = (1 - t) \vec{a} + t \vec{b} = \vec{a} + t \left ( \vec{b} - \vec{a} \right)$$
i.e.
$$\begin{cases}
p_x + s d_x = (1 - t) a_x + t b_x \\
p_y + s d_y = (1 - t) a_y + t b_y \\
p_z + s d_z = (1 - t) a_z + t b_z \end{cases}$$
You have three equations, but only two unknowns. Furthermore, because all real $s$ are acceptable, you only need to solve for $t$, and verify it $0 \le t \le 1$. There are three solutions, subscripted by which coordinate pair is used in the solution:
$$t_{xy} = \frac{ d_y ( a_x - p_x ) - d_x ( a_y - p_y ) }{ d_y ( a_x - b_x ) - d_x ( a_y - b_y ) } \tag{1}\label{1}$$
$$t_{xz} = \frac{ d_z ( a_x - p_x ) - d_x ( a_z - p_z ) }{ d_z ( a_x - b_x ) - d_x ( a_z - b_z ) } \tag{2}\label{2}$$
$$t_{yz} = \frac{ d_z ( a_y - p_y ) - d_y ( a_z - p_z ) }{ d_z ( a_y - b_y ) - d_y ( a_z - b_z ) } \tag{3}\label{3}$$
For numerical accuracy, I suggest you calculate all three denominators first, and calculate $t$ using the formula corresponding to the largest denominator in magnitude (absolute value).</p>

<p>If all three denominators are zero, the line and the line segment do not intersect. (This can also occur if $\vec{P}_0 = \vec{d}$ or $\vec{a} = \vec{b}$ or both.)</p>

<p>In case you wish to check for the intersection between two line segments, here are the corresponding formulae for $s$:
$$s_{xy} = \frac{ a_x ( b_y - p_y ) - b_x ( a_y - p_y ) + p_x ( a_y - b_y ) }{ d_y ( a_x - b_x ) - d_x ( a_y - b_y ) } \tag{4}\label{4}$$
$$s_{xz} = \frac{ a_x ( b_z - p_z ) - b_x ( a_z - p_z ) + p_x ( a_z - b_z ) }{ d_z ( a_x - b_x ) - d_x ( a_z - b_z ) } \tag{5}\label{5}$$
$$s_{yz} = \frac{ a_y ( b_z - p_z ) - b_y ( a_z - p_z ) + p_y ( a_z - b_z ) }{ d_z ( a_y - b_y ) - d_y ( a_z - b_z ) } \tag{6}\label{6}$$</p>

<p>Note that the corresponding formulae for $s$ have the same denominators; that is, $\eqref{1}$ and $\eqref{4}$ have the same denominators, $\eqref{2}$ and $\eqref{5}$ have the same denominators, and $\eqref{3}$ and $\eqref{6}$ have the same denominators. (This means that if a solution for $t$ exists, a solution exists for $s$ also. In OP's case $s$ does not matter, because it is the parameter for an infinite line, and any real $s$ is acceptable.)</p>

<p>If you find $t$, and $0 \le t \le 1$, the line and the line segment intersect at $\vec{v}$,
$$\vec{v} = (1-t) \vec{a} + t \vec{b}$$
i.e.
$$\begin{cases}
v_x = (1-t) a_x + t b_x \\
v_y = (1-t) a_y + t b_y \\
v_z = (1-t) a_z + t b_z \end{cases}$$</p>
"
"2386774","2386907","<p>The multivalued function $y(x)=1\pm\frac{1}{1+e^x}$ suffices the requirements, as $\lim_{x\to \infty}f(x)=1$, $\lim_{x\to -\infty}f(x)=1\pm1=\{0,2\}$. Additionally, the multivalued function is monotone and differentiable.</p>
"
"2386781","2386801","<p>Let $x+\frac{1}{x}=u$.</p>

<p>Hence, $|u|\geq2$ and we have $2u^2+u+m-4=0.$</p>

<p>Let $f(u)=2u^2+u+m-4$.</p>

<p>For existing four real roots of the given equation we need that the equation $f(u)=0$</p>

<p>will be have two real roots $u_1$ and $u_2$.</p>

<p>Let $u_1\leq u_2$.</p>

<p>Since $-2&lt;-\frac{1}{4}&lt;2$ (see on the vertex of the parabola), we need $u_1\leq-2$ and $u_2\geq2$,  </p>

<p>which gives  $f(2)\leq0$ and $f(-2)\leq0$, which gives $m\leq-6$.</p>

<p>Done!</p>
"
"2386786","2386787","<p>It's probably an assignment statement in an algorithm. It means ""the new value of $\hat v$ is computed from the old value using the formula on the right of the arrow"".</p>
"
"2386800","2386965","<p>If you are willing to make an eigenvalue decomposition, you can optimize with respect to ${\bf M = TDT}^{-1}$. Pushing all but $k$ entries along the diagonal in $\bf D$ to $0$. It may not be so nice given the full eigenvalue decomposition can be computationally demanding. </p>

<hr>

<p>A faster approach could be to use some power iteration method to find the $k$ largest modulus eigenvalues (and corresponding eigenvectors) of $\bf M$, then modifying the eigenvalues to make sure they are $\in \mathbb R$ and $\geq 0$, then construct ${\bf M = TDT}^{\dagger}$, Where $\bf T$ and $\bf D$ are the modified new sparse matrices of the eigenvalue decomposition and ${\bf T}^\dagger$ is a pseudo-inverse, for example Moore-Penrose's pseudo inverse.</p>

<hr>

<p>A basic power iteration method described:</p>

<ol>
<li>Start with a random column vector ${\bf v}_0$ and the matrix $\bf M$ you want to find eigenvalues for. Calculate ${\bf v}_1 = {\bf Mv}_0$, set $k=1$</li>
<li>For as long as ${\bf v}_k$ is not parallell enough to ${\bf v}_{k-1}$

<ol>
<li>Calculate ${{\bf v}_{k+1} = \bf Mv}_k$</li>
<li>If needed, renormalize ${\bf v}_{k+1}$, for example to avoid overflow or other numerical issues.</li>
<li>Increase $k$ by one.</li>
</ol></li>
<li>When done the last calculated ${\bf v}_{k+1}$ is an estimate of the eigenvector and each scalar of the Schur/Hadamard (element-wise) division of ${\bf v}_{k+1}$ by ${\bf v}_k$ is an estimate to the largest modulus eigenvalue of $\bf M$.</li>
</ol>

<p>Now you can factor out the eigenvalue and eigenvector you just found and continue finding the next one. One way to do this is described in Horn-Johnson's <em>Matrix Analysis</em>, but I am sure there exist many other ways to do it too. </p>

<hr>

<p>Or if you think it is too inconvenient to factor, you can just make sure you project away the found eigenvector for the next randomized ${\bf v}_0$ because then you are sure that it will not grow any more in the direction of that eigenvector whenever you multiply with $\bf M$ from the left.</p>
"
"2386804","2386834","<p>The Fibonacci numbers are $a_0=0$, $a_1=1$, $a_{n+2}=a_{n+1}+a_n$ for $n\ge0$. We have
$$\begin{align}a_0 &amp;= 0\quad\text{(even)} \\ a_1 &amp;= 1\quad\text{(odd)} \\ a_2 &amp;= a_1+a_0=1\quad\text{(odd)} \\ a_3 &amp;= a_2+a_1=2\quad\text{(even)} \\ a_4 &amp;= a_3+a_2=3\quad\text{(odd)} \\ a_5 &amp;= a_4+a_3=5\quad\text{(odd)} \end{align}$$
It is straightforward from here to prove by induction that $a_k$ is even and $a_{3k+1}$ and $a_{3k+2}$ are odd for all $k\ge0$.</p>
"
"2386806","2386826","<p>Let $B_k :=\bigcup_{n=k}^{\infty} A_n$, so that $G := \limsup A_n = \bigcap_{k=1}^\infty B_k$.</p>

<p>Since $B_k$ converges monotonically to $G$ and $\mu(B_1)$ is finite, we have that $$
\mu(G) = \lim_k \mu(B_k) \geq \frac{1}{2}\,,
$$
where the last inequality follows from the fact that $B_k$ is the union of sets of measure $\geq 1/2$.</p>
"
"2386814","2394015","<p>Additional answer in  response to new query by OP  asking for the same
statistic for a  multiset on $n$ different  elements with multiplicity
$r$ of the first element.  Call this $E_{a,b,r}(n,k).$ We will provide
a  recurrence  that  yields  a  practical  means  of  computing  these
numbers.  We  obtain from  first  principles  and applying  the  Polya
Enumeration Theorem the answer</p>

<p>$$[A_1^r A_2 \cdots A_n] 
Z(P_k)\left(\sum_{p=a}^b [z^p] 
\sum_{q=0}^r z^q A_1^q \prod_{j=2}^n (1+z A_j) \right).$$</p>

<p>This formula  has the  poorest complexity  of the  three that  we will
present  here  and is  used  mainly  to  verify correctness  on  small
arguments of the others that  were obtained.  Applying the exponential
fromula we obtain the OGF</p>

<p>$$G(w) = \exp\left(\sum_{l\ge 1} (-1)^{l-1} \frac{w^l}{l}
\left(\sum_{p=a}^b [z^p] 
\sum_{q=0}^r z^q A_1^{ql} \prod_{j=2}^n (1+z A_j^l)
\right)\right).$$</p>

<p>Splitting into $l=1$ and $l\ge 2$ we see that in the latter case
only the constant term from the product contributes and we obtain</p>

<p>$$G(w) = \exp\left(w\sum_{p=a}^b [z^p] 
\sum_{q=0}^r z^q A_1^q \prod_{j=2}^n (1+z A_j) \right)
\\ \times \exp\left(\sum_{l\ge 2} (-1)^{l-1} \frac{w^l}{l}
\left(\sum_{p=a}^b [z^p] 
\sum_{q=0}^r z^q A_1^{ql}\right)\right).$$</p>

<p>This last formula is already  suitable for practical computations, but
mostly in the same  range as the first one as  the complexity is about
the same.   We set the upper  limit to $l=r$ and  extract coefficients
through  the Taylor  series in  the variables  $A_j.$ OP  asked for  a
recurrence  however  and  we  continue with  this  goal.   <strong>Important
remark.</strong> All  coefficient extraction  in these computations  with the
exception of the initial repertoire is done through Taylor series i.e.
<em>differentiation.</em> While we are dealing with polynomials, we must note
however that  we get  exponential increase when  we expand  these into
constituent terms. The algorithms that we present here require this as
a precondition and would otherwise not work. <P></p>

<p>Now let $[a,b]\cap [0,r] = [c_1, c_2]$ to get</p>

<p>$$G(w) = \exp\left(w\sum_{p=a}^b [z^p] 
\sum_{q=0}^r z^q A_1^q \prod_{j=2}^n (1+z A_j) \right)
\\ \times \exp\left(\sum_{l\ge 2} (-1)^{l-1} \frac{w^l}{l}
\left(\sum_{p=c_1}^{c_2} A_1^{pl}\right)\right).$$</p>

<p>This is</p>

<p>$$G(w) = \exp\left(w\left(- \sum_{p=c_1}^{c_2} A_1^{p}  +
\sum_{p=a}^b [z^p] 
\sum_{q=0}^r z^q A_1^q \prod_{j=2}^n (1+z A_j) \right) \right)
\\ \times \exp\left(\sum_{l\ge 1} (-1)^{l-1} \frac{w^l}{l}
\left(\sum_{p=c_1}^{c_2} A_1^{pl}\right)\right).$$</p>

<p>We put</p>

<p>$$A = - \sum_{p=c_1}^{c_2} A_1^{p}  +
\sum_{p=a}^b [z^p] 
\sum_{q=0}^r z^q A_1^q \prod_{j=2}^n (1+z A_j).$$</p>

<p>and $G(w)$ finally simplifies to</p>

<p>$$G(w) = \prod_{p=c_1}^{c_2} (1+wA_1^p) \times
\exp\left(w A \right).$$</p>

<p>This  closed  form  is  the  first  one  to  be  really  suitable  for
computations, the  complexity here  is of  a lower  order than  of the
first two  we saw and  indeed it  was used in  the Maple code  that we
include below.  We now use it  to establish a recurrence and introduce
$G_k = [w^k]  G(w)$ which is the OGF before  extraction of $[A_1^r A_2
\cdots  A_n].$ Differentiating  and actually  doing the  extraction we
find</p>

<p>$$(k+1) G_{k+1} = 
[w^k]  \sum_{p=c_1}^{c_2} \frac{A_1^p} {1+wA_1^p} G(w)
+ [w^k] A G(w).$$</p>

<p>This yields the recurrence</p>

<p>$$G_{k+1} = \frac{1}{k+1} 
\sum_{m=0}^k  G_{k-m} \sum_{p=c_1}^{c_2} (-1)^m A_1^{(m+1)p}
+ \frac{1}{k+1} A G_k.$$</p>

<p>The base case  here is $G_0 = 1$  and it is important to  note that we
must remove superfluous  terms at every step of  the recursion, namely
those where  $A_1$ is present  with degree more  than $r$ or  $A_2$ to
$A_n$  are  present with  degree  more  than  one.   This is  done  by
restricting  to   the  initial  segment  of   the  appropriate  Taylor
series. We clearly require a CAS to represent the intermediate results
as syntax trees rather than flat lists of coefficients on multivariate
polynomials.  This was  the code.  (We  also verified  that this  will
produce the answers from the companion post when $r=1.$)</p>

<pre>
with(combinat);

pet_cycleind_set :=
proc(n)
option remember;

    if n=0 then return 1; fi;

    expand(1/n*add((-1)^(l-1)*a[l]*pet_cycleind_set(n-l), l=1..n));
end;


pet_varinto_cind :=
proc(poly, ind)
local subs1, subs2, polyvars, indvars, v, pot, res;

    res := ind;

    polyvars := indets(poly);
    indvars := indets(ind);

    for v in indvars do
        pot := op(1, v);

        subs1 :=
        [seq(polyvars[k]=polyvars[k]^pot,
             k=1..nops(polyvars))];

        subs2 := [v=subs(subs1, poly)];

        res := subs(subs2, res);
    od;

    res;
end;

srcgf :=
proc(n, a, b, r)
option remember;
local base, p, gf;

    base :=
    expand(add(z^p*A[1]^p, p=0..r)*mul(1+z*A[q], q=2..n));

    gf := 0;

    for p from a to b do
        gf := gf + coeff(base, z, p);
    od;

    gf;
end;

extract :=
proc(n, r, gf)
local subsgf, q;

    subsgf := gf;
    for q from 2 to n do
        subsgf := coeftayl(subsgf, A[q]=0, 1);
    od;

    coeftayl(subsgf, A[1]=0, r);
end;


ENUM :=
proc(n, k, a, b, r)
option remember;
    extract(n, r,
            pet_varinto_cind(srcgf(n, a, b, r),
                             pet_cycleind_set(k)));
end;


E1 :=
proc(n, k, a, b, r)
option remember;
local c1, c2, ranges, G;

    ranges := sort([a,b,0,r]);
    c1 := ranges[2]; c2 := ranges[3];

    G := exp(w*srcgf(n, a, b, r))
    *exp(add((-1)^(l-1)*w^l/l*add(A[1]^(p*l), p=c1..c2), l=2..r));

    extract(n, r, coeftayl(G, w=0, k));
end;

GCFX :=
proc(n, k, a, b, r)
option remember;
local c1, c2, ranges, G;

    ranges := sort([a,b,0,r]);
    c1 := ranges[2]; c2 := ranges[3];

    G := exp(w*(-add(A[1]^p, p=c1..c2) + srcgf(n, a, b, r)))
    *mul(1+w*A[1]^p, p=c1..c2);

    coeftayl(G, w=0, k);
end;

E2 :=
proc(n, k, a, b, r)
option remember;
    extract(n, r, GCFX(n, k, a, b, r));
end;

GCFR :=
proc(n, k, a, b, r)
option remember;
local c1, c2, ranges, G, res;

    if k = 0 then return 1 fi;

    ranges := sort([a,b,0,r]);
    c1 := ranges[2]; c2 := ranges[3];

    res :=
    1/k*
    add(GCFR(n, k-1-m, a, b, r)*
        add((-1)^m*A[1]^((m+1)*p), p=c1..c2), m=0..k-1)
    + 1/k*GCFR(n, k-1, a, b, r)
    *(-add(A[1]^p, p=c1..c2) + srcgf(n, a, b, r));

    res;
end;

ERED :=
proc(n, k, a, b, r)
option remember;
local c1, c2, ranges, G, res, q;

    if k = 0 then return 1 fi;

    ranges := sort([a,b,0,r]);
    c1 := ranges[2]; c2 := ranges[3];

    res :=
    1/k*
    add(ERED(n, k-1-m, a, b, r)*
        add((-1)^m*A[1]^((m+1)*p), p=c1..c2), m=0..k-1)
    + 1/k*ERED(n, k-1, a, b, r)
    *(-add(A[1]^p, p=c1..c2) + srcgf(n, a, b, r));

    for q from 2 to n do
        res := convert(series(res, A[q], 2), &#96;polynom&#96;);
    od;

    convert(series(res, A[1], r+1), &#96;polynom&#96;);
end;

E := (n, k, a, b, r) -&gt;
extract(n, r, ERED(n, k, a, b, r));

VERIFST2 := n -&gt; {seq(E(n,k,1,n,1)-stirling2(n,k), k=1..n)};
</pre>

<p><strong>Addendum.</strong> There is another possible interpretation of this problem
namely  that  we have  multisets  of  multisets  rather than  sets  of
multisets.   This is  very similar  to the  first version  and may  be
solved quite easily.  PET yields the closed form</p>

<p>$$[A_1^r A_2 \cdots A_n] 
Z(S_k)\left(\sum_{p=a}^b [z^p] 
\sum_{q=0}^r z^q A_1^q \prod_{j=2}^n (1+z A_j) \right).$$</p>

<p>where  $Z(S_k)$  is  the  cycle  index of  the  symmetric  group.  The
derivation using the exponential formula goes through as before and we
obtain</p>

<p>$$G(w) = \prod_{p=c_1}^{c_2}\frac{1}{1-wA_1^p} \times
\exp\left(w A \right).$$</p>

<p>Differentiating to obtain the recurrence now yields</p>

<p>$$(k+1) G_{k+1} = 
[w^k]  \sum_{p=c_1}^{c_2} \frac{A_1^p}{1-wA_1^p} G(w)
+ [w^k] A G(w).$$</p>

<p>and we find</p>

<p>$$G_{k+1} = \frac{1}{k+1} 
\sum_{m=0}^k  G_{k-m} \sum_{p=c_1}^{c_2} A_1^{(m+1)p}
+ \frac{1}{k+1} A G_k.$$</p>

<p>The Maple source  goes like this (code refers to  the previous listing
for common routines). Note that  the distinction between multisets and
sets only becomes active when $r\ge 2.$</p>

<pre>
pet_cycleind_symm :=
proc(n)
option remember;

    if n=0 then return 1; fi;

    expand(1/n*add(a[l]*pet_cycleind_symm(n-l), l=1..n));
end;

ENUM :=
proc(n, k, a, b, r)
option remember;
    extract(n, r,
            pet_varinto_cind(srcgf(n, a, b, r),
                             pet_cycleind_symm(k)));
end;

GCFX :=
proc(n, k, a, b, r)
option remember;
local c1, c2, ranges, G;

    ranges := sort([a,b,0,r]);
    c1 := ranges[2]; c2 := ranges[3];

    G := exp(w*(-add(A[1]^p, p=c1..c2) + srcgf(n, a, b, r)))
    *mul(1/(1-w*A[1]^p), p=c1..c2);

    coeftayl(G, w=0, k);
end;

E2 :=
proc(n, k, a, b, r)
option remember;
    extract(n, r, GCFX(n, k, a, b, r));
end;

ERED :=
proc(n, k, a, b, r)
option remember;
local c1, c2, ranges, G, res, q;

    if k = 0 then return 1 fi;

    ranges := sort([a,b,0,r]);
    c1 := ranges[2]; c2 := ranges[3];

    res :=
    1/k*
    add(ERED(n, k-1-m, a, b, r)*
        add(A[1]^((m+1)*p), p=c1..c2), m=0..k-1)
    + 1/k*ERED(n, k-1, a, b, r)
    *(-add(A[1]^p, p=c1..c2) + srcgf(n, a, b, r));

    for q from 2 to n do
        res := convert(series(res, A[q], 2), &#96;polynom&#96;);
    od;

    convert(series(res, A[1], r+1), &#96;polynom&#96;);
end;

E := (n, k, a, b, r) -&gt;
extract(n, r, ERED(n, k, a, b, r));

VERIFST2 := n -&gt; {seq(E(n,k,1,n,1)-stirling2(n,k), k=1..n)};
</pre>

<p></p>

<p><strong>Remark.</strong> As observed by OP in a personal communication we can get a
simple closed form if the restriction on the multisets having at least
$a$  and  at most  $b$  elements  is  lifted. With  $p_k(n)$  counting
partitions and  $c_k(n)$ counting weak  compositons we get  from first
principles  on classifying  by  the  number $m$  of  instances of  the
element with multiplicity $r$ that are in a set by themselves</p>

<p>$$\sum_{m=0}^{r} \sum_{m_1=0}^{k-1} p_{m_1}(m)
{n-1\brace k-m_1} c_{k-m_1}(r-m).$$</p>

<p>Here we use the convention that $p_0(0) = 1.$
Now the number of weak compositions is given by</p>

<p>$$c_k(n) = [z^n] \prod_{m=1}^k \frac{1}{1-z}
= [z^n] \frac{1}{(1-z)^k} = {n+k-1\choose k-1}$$</p>

<p>and we obtain</p>

<p>$$\sum_{m=0}^{r} \sum_{m_1=0}^{k-1} p_{m_1}(m)
{n-1\brace k-m_1} {k+r-m-m_1-1\choose k-m_1-1}.$$</p>

<p>This formula  gives an  idea of  the difficulties  that arise  when we
restrict the  size of the  sets to be  from the interval  $[a,b].$ The
Maple code is  as follows (we checked for Stirling  numbers when $r=1$
and for  a match of  the data from the  enumeration routine --  PET --
when $r\ge 2.$)</p>

<pre>
p :=
proc(n, k)
option remember;
    if n = 0 and k = 0 then return 1 fi;
    if n &lt;= 0 or k &lt;= 0 or n &lt; k then return 0 fi;

    if n = k or k = 1 then return 1 fi;

    p(n-1, k-1) + p(n-k, k)
end;

EX := (n, k, r) -&gt;
add(add(p(m,m1)*stirling2(n-1,k-m1)
        *binomial(k+r-m-m1-1, k-m1-1), m1=0..k-1),
    m=0..r);
</pre>

<p><strong>Completed answer.</strong> We can actually give a closed from
even for $[a,b]$ not being set to the simple $[1,n].$
Introduce</p>

<p>$$p_{k, a, b}(n) = 
[z^n] Z(S_k)\left(\sum_{q=a}^b z^q\right)
= [z^n] [w^k] \prod_{q=a}^b \frac{1}{1-wz^q}.$$</p>

<p>so this  counts the number  of partitions  into values from  the range
$[a,b]$  using  PET.  Furthermore   introduce  the  marked  generating
function</p>

<p>$$F_{k, b}(z) =
\frac{1}{k!} \left(\sum_{q=1}^b B_q \frac{z^q}{q!}\right)^k.$$</p>

<p>Let the evaluation rule $B_q^*$ be given by</p>

<p>$$B_q = [[a\le q\le b]] \times (1+w+w^2+\cdots+w^{b-q})
\\ + [[1\le q\lt a]] \times (w^{a-q}+\cdots+w^{b-q})
\\ = w^{\max(a-q, 0)}+\cdots+w^{b-q}.$$</p>

<p>This yields the closed form</p>

<p>$${\large (n-1)! \sum_{m=0}^{r} \sum_{m_1=0}^{k-1} p_{m_1,a,b}(m)
[z^{n-1}] [w^{r-m}] \left. F_{k-m_1, b}(z) 
\right|_{B_q^*}.}$$</p>

<p>The implementation  is as follows.  This will produce  instant results
where the version from the exponential formula does not succeed.</p>

<pre>
p :=
proc(n, k, a, b)
option remember;
local subsgf;

    if n = 0 and k = 0 then return 1 fi;

    subsgf :=
    pet_varinto_cind(add(z^q, q=a..b), pet_cycleind_symm(k));

    coeftayl(subsgf, z=0, n);
end;

F := (k, b) -&gt; 1/k!*add(B[q]*z^q/q!, q=1..b)^k;


EX :=
proc(n, k, a, b, r)
option remember;
local curF, m, m1, res, Bsubs;

    if n = 1 then
        return p(r, k, a, b);
    fi;

    Bsubs :=
    [seq(B[q]=add(w^l, l=max(a-q, 0)..b-q), q=1..b)];

    res := 0;

    for m1 from 0 to k-1 do
        curF := subs(Bsubs,  F(k-m1, b));
        if r &gt; 1 then
            curF := coeff(expand(curF), z, n-1);
        else
            curF := coeftayl(curF, z=0, n-1);
        fi;

        for m from 0 to r do
            if r &gt; 1 then
                res := res +
                (n-1)!*p(m, m1, a, b)*
                coeff(curF, w, r-m);
            else
                res := res +
                (n-1)!*p(m, m1, a, b)*
                coeftayl(curF, w=0, r-m);
            fi;
        od;
    od;

    res;
end;


VERIFST2 := n -&gt; {seq(EX(n,k,1,n,1)-stirling2(n,k), k=1..n)};
</pre>

<p><strong>One remaining  simplification.</strong> Here we replace  the computation of
the partition function by PET,  which is instructive, by a recurrence,
which is fast  and efficient.  With $p_{b}(n, k)$  the partitions with
parts of size at most $b$, which has a simple recurrence, we then have
$p_{a,b}(n,k) = p_{b -  a + 1}(n - (a - 1) k,  k).$ This is documented
below.</p>

<pre>
paux :=
proc(n, k, b)
option remember;
    if n = 0 and k = 0 then return 1 fi;
    if n &lt;= 0 or k &lt;= 0 or n &lt; k then return 0 fi;

    if b = 0 then return 0 fi;

    if n = k then return 1 fi;
    if k = 1 then
        if n &lt;= b then
            return 1
        else
            return 0;
        fi;
    fi;

    paux(n-1, k-1, b) + paux(n-k, k, b-1)
end;

p := (n, k, a, b) -&gt; paux(n-(a-1)*k, k, b-a+1);
</pre>

<p><strong>What  have  we  learned.</strong>  The  approach where  we  try  to  use  a
recurrence revealed  itself through extensive investigation  to not be
quite the equal of the closed form.</p>
"
"2386815","2386899","<p>I haven't seen such a construction before. As long as the topology on $2^Y$ is Hausdorff, then following argument works.</p>

<p>Assuming that the topology on $2^Y$ is Hausdorff, then $X\times 2^Y$ is Hausdorff. As a compact subset of $X\times 2^Y$, we know that the graph of $F$, denoted by $\Gamma(F)$, is closed. Therefore it contains its boundary $\partial\Gamma(F)$. This means that $\partial\Gamma(F)$ a closed subset of a compact Hausdorff space. Hence $\partial\Gamma(F)$ is compact and Hausdorff, and consequently it is regular.</p>
"
"2386817","2387049","<p>$$\int_0^\infty \sin(kx){e^{-ax^2}}dx =
{\frac {\sqrt {\pi }}{2\sqrt {a}}{{\rm e}^{-{{{k}^{2}}/({4
a})}}}{\rm erfi} \left({\frac {k}{2\sqrt {a}}} \right) }
$$
where the imaginary error function $\mathrm{erfi}$ may be found <a href=""https://en.wikipedia.org/wiki/Error_function#Imaginary_error_function"" rel=""nofollow noreferrer"">here</a> .</p>
"
"2386818","2387138","<p>Let $X=\{0,1\}$, topologized such that $\{0\}$ is open by $\{1\}$ is not.  Let $k$ be your favorite field, and define a sheaf of rings on $X$ by $\mathcal{O}(X)=k[x]$ and $\mathcal{O}(\{0\})=k$, with the restriction map sending $x$ to $0$.  Then the maximal ideal $\mathfrak{m}=(x)\subset\mathcal{O}(X)$ has $\mathfrak{m}/\mathfrak{m}^2\cong k$ but $\mathfrak{m}_0/\mathfrak{m}_0^2=0$ since $\mathfrak{m}_0=0$.</p>

<p>(Replacing $k[x]$ by a local ring, we can even get a counterexample for a locally ringed space.)</p>
"
"2386822","2386831","<p>Let $k$ be a positive real number. Let $f: \mathbb{R} \to \mathbb{R}$ be given by:
$f(t)=0$ if $t&lt;k$, and $f(t)=\frac{(t-k)^{2}}{4}$ for $t&gt;k$. Now check that:</p>

<ul>
<li>$f'(t)=\sqrt{f(t)}$ for all $t$</li>
<li>$f(0)=0$</li>
<li>$f$ is continuously differentiable at $t=k$ (at all other points this is obvious)</li>
</ul>
"
"2386827","2386988","<p>The total number of crossovers follows a Binomial distribution $B(N,\epsilon)$ A global crossover occurs if that number is odd. The probability of that event (well, its complement) is computed <a href=""https://math.stackexchange.com/questions/1149270/probability-that-a-random-variable-is-even"">here</a>. And it leads to the result you copied.</p>
"
"2386828","2386833","<p>A useful inequality:</p>

<p>$$n!&gt;\left(\frac ne\right)^n$$</p>

<p>which should help you show that it fails the term test at the boundaries.</p>

<p>The derivation of this follows by taking the log of both sides:</p>

<p>$$\ln(n!)&gt;n\ln(n)-n$$</p>

<p>By the definition of the factorial, some log rules, and a Riemann sum,</p>

<p>$$\ln(n!)=\sum_{k=1}^n\ln(k)=\int_0^n\ln\lceil x\rceil~\mathrm dx&gt;\int_0^n\ln(x)~\mathrm dx=n\ln(n)-n$$</p>
"
"2386829","2386886","<p>First, it's not true that the characteristic polynomial of $A'$ (or $B'$) must divide $x^2 - x$. What is true is that the minimal polynomial of $A'$ (or $B'$) must divide $x^2 - x$ and hence $A'$ (and $B'$) is diagonalizable with <strong>possible</strong> eigenvalues $0,1$.</p>

<p>Since $A' + B' = I$, we get 
$$ \operatorname{trace}(A' + B') = \operatorname{trace}(A') + \operatorname{trace}(B') = \dim V + \dim W = \operatorname{trace}(I_n) = n. $$</p>

<p>Since $A'B' = 0$, if $v \in V \cap W$ then $0 = A' B'v = A'v = v$ which shows that $V \cap W = \{ 0 \}$. This, together with $\dim V + \dim W = n$ implies that $\mathbb{R}^n = V \oplus W$. Finally, the eigenvalues of $cB' + dI$ are $d$ with multiplicity $\dim V$ and $c + d$ with multiplicity $\dim W$. Hence,</p>

<p>$$ \det(A + B) = \det \left( \frac{1}{x} \left( xA + yB + (x - y)B \right) \right) = \det \left( \frac{1}{x} I +  \frac{x-y}{y}B' \right) \\ = \frac{1}{x}^{\dim V} \left( \frac{1}{x} + \frac{x - y}{y} \right)^{\dim W} = \frac{1}{x}^{\dim V} \frac{1}{y}^{\dim W}.$$</p>
"
"2386830","2386858","<p>If $\varepsilon:J\stackrel{\bullet}{\to}K$ and $\eta: F\stackrel{\bullet}{\to}G$ and the functors $J,F$ are composable then $\varepsilon\eta:JF\stackrel{\bullet}{\to}KG$ with:$$(\varepsilon\eta)_x=\varepsilon_{Gx}\circ J\eta_x=K\eta_x\circ\varepsilon_{Fx}:JFx\to KGx$$</p>

<p>The following diagram commutes:</p>

<p>$$\begin{array}{ccc}
JFx &amp; \stackrel{\varepsilon_{Fx}}{\longrightarrow} &amp; KFx\\
J\eta_{x}\downarrow &amp;  &amp; \downarrow K\eta_{x}\\
JGx &amp; \stackrel{\varepsilon_{Gx}}{\longrightarrow} &amp; KGx
\end{array}$$</p>
"
"2386840","2386869","<p>Given</p>

<p>$$\tag 1 2y'^2 = 2x^2y' - 3xy.$$</p>

<p>Setting $p = \dfrac{dy}{dx}$ in $(1)$ and solving for $y$, we get </p>

<p>$$\tag 2  y = \dfrac{2}{3} (x p - x^{-1} p^2)$$</p>

<p>Differentiating $(2)$</p>

<p>$$\tag 3 y' = p = \dfrac{2}{3}(p + x p' + x^{-2} p^2 -2x^{-1} p p')$$</p>

<p>Factoring $(3)$</p>

<p>$$\tag 4 (x^2 - 2 p)(2 x p' - p) = 0$$</p>

<p>I will assume you can take it from here.</p>
"
"2386860","2386954","<p>Here are some heuristics. As Hans Engler defines, let $k(n)$ be the number of pairs $(a,b)$ with $a&lt;b$ for which $a+b=n$ and $a^2+b^2$ is prime. In other words,
$$
k(n) = \#\{ 1\le a &lt; \tfrac n2 \colon a^2 + (n-a)^2 = 2a^2 - 2an + n^2 \text{ is prime} \}.
$$
Ignoring issues of uniformity in $n$, the <a href=""https://en.wikipedia.org/wiki/Bateman%E2%80%93Horn_conjecture"" rel=""noreferrer"">BatemanâHorn conjecture</a> predicts that the number of prime values of an irreducible polynomial $f(a)$ up to $x$ is asymptotic to
$$
\frac x{\log x} \prod_p \bigg( 1-\frac1p \bigg)^{-1} \bigg( 1-\frac{\sigma(p)}p \bigg),
$$
where $\log$ denotes the natural logarithm and
$$
\sigma(p) = \#\{ 1\le t\le p\colon f(t) \equiv 0 \pmod p \}.
$$</p>

<p>We now calculate $\sigma(p)$ for $f(a) = 2a^2 - 2an + n^2$.
Note that the discriminant of $f$ is $(-2n)^2 - 4\cdot2n^2 = -4n^2$. Therefore if $p$ does not divide $-4n^2$, the number of solutions is given by the Legendre symbol
$$
\sigma(p) = 1 + \bigg (\frac{-4n^2}p\bigg) = 1 + \bigg (\frac{-1}p\bigg) = \begin{cases}
2, &amp;\text{if } p\equiv1\pmod 4, \\
0, &amp;\text{if } p\equiv3\pmod 4.
\end{cases}
$$
Furthermore, we can check by hand that if $p=2$ then $\sigma(p)=0$, while if $p$ divides $n$ then $\sigma(p)=1$. Therefore our prediction becomes
$$
k(n) \approx \frac{n/2}{\log(n/2)} \cdot 2 \prod_{\substack{p\equiv1\pmod 4 \\ p\nmid n}} \frac{p-2}{p-1} \prod_{\substack{p\equiv3\pmod 4 \\ p\nmid n}} \frac p{p-1}.
$$
(We're abusing notation: those two products don't individually converge, but their product converges when the primes are taken in their natural order.)
In principle that constant could be cleverly evaluated to several decimal places. But for the purposes of experiment, perhaps it's valuable to note that $k(n)$ should be approximately $n/\log n$, times some universal constant, times
$$
\prod_{\substack{p\equiv1\pmod 4 \\ p\mid n}} \frac{p-1} {p-2}\prod_{\substack{p\equiv3\pmod 4 \\ p\mid n}} \frac {p-1}p;
$$
and so the data can be normalized by that function of $n$ to test for consistency.</p>
"
"2386865","2386885","<p>Also we need $2a-b=0$.</p>

<p>Indeed, $$\lim \limits_{x \to 0}\frac{a\sin^2x+b \log\cos x}{x^4}=\lim \limits_{x \to 0}\frac{2a\sin x\cos{x}-b \frac{\sin{x}}{\cos{x}}}{4x^3}=\lim \limits_{x \to 0}\frac{2a\cos{x}-b\cdot \frac{1}{\cos{x}}}{4x^2},$$
which gives $2a-b=0$.</p>

<p>Now, $$\lim \limits_{x \to 0}\frac{2a\cos{x}-b\cdot \frac{1}{\cos{x}}}{4x^2}=\lim \limits_{x \to 0}\frac{2a\cos^2{x}-b}{4x^2}=\lim \limits_{x \to 0}\frac{-4a\cos{x}\sin{x}}{8x}=-\frac{a}{2},$$
which gives $a=-1$ and $b=-2$.</p>
"
"2386871","2386874","<p><em>Hint:</em> To solve $(a+b\omega)(x+y\omega)=1$, expand and simplify to get a linear system for $x,y$.</p>

<p>(I assume that $G$ is a subset of $\mathbb C$ and that $\omega\ne1$. Then 
$\omega^3=1$ implies $\omega^2+\omega+1=0$.)</p>

<p><em>Solution:</em></p>

<blockquote class=""spoiler"">
  <p> $1 + 0 \omega= (a+b\omega)(x+y\omega) = (ax-by)+(bx+(a-b)y)\omega$ gives $ax-by=1$, $bx+(a-b)y)=0$, and $x = \dfrac{a - b}{d}$, $y = -\dfrac{b}{d}$, for $d=a^2 - a b + b^2$.</p>
</blockquote>
"
"2386882","2386890","<p>Credit: Thanks fonfonx for pointing out the mistake</p>

<p>The discriminant $D$ is equal to $$4b^4-8b^2ca+4a^3b+4bc^3-4ab^2c$$</p>

<p>Let's factorize the discriminant:</p>

<p>$$4b(b^3-2abc+a^3+c^3-abc)$$</p>

<p>which is equal to </p>

<p>$$4b(b^3+a^3+c^3-3abc)$$</p>

<p>Hence if $b=0$ or $(b^3+a^3+c^3-3abc)=0$, the discriminant is $0$.</p>
"
"2386884","2386889","<p>There is a <a href=""https://math.stackexchange.com/questions/14051/symmetric-polynomials-and-the-newton-identities/14061#14061"">simple <strong>algorithm</strong></a> of Gauss to rewrite a symmetric polynomial $f(x,y)$ as polynomial in the elementary symmetric polynomials $\,s_1\! = x+y,\ \ s_2 = xy.\,$ Namely if $f$ has highest degree term $\ c x^a y^b $ in the lex (dictionary) order  (i.e. $\,(a,b) &gt; (c,d)\, $ if $\,a &gt;c,\,$ or $\,a= c\,$ and $\, b &gt; d)\,$ then cancel the highest term of $\,f\,$ by subtracting $\,cs_1^{a-b} s_2^b,\, $ then recurse on what remains. </p>

<p>Let's perform Gauss's algorithm on the simpler example $\, f = x^3 + y^3.\, $ Since $\,(3,0) &gt; (0,3)\,$ the highest degree monomial is $\ 1\cdot x^\color{#0a0}3y^\color{#c00}0,\, $ so we subtract $\ 1\cdot s_1^{\color{#0a0}3-\color{#c00}0} s_2^\color{#c00}0\, =\, (x+y)^3 $ yielding </p>

<p>$$\ x^3+y^3\ -\ (x+y)^3\, =\ {-}3x^2 y - 3x y^2$$</p>

<p>By $(2,1)&gt;(1,2),\, $ RHS has high term $\,-3x^{\color{#0a0}2} y^\color{#c00}1$ so we subtract $\, {-}3 s_1^{\color{#0a0}2-\color{#c00}1} s_2^\color{#c00}1 =\, -3(x\!+\!y)(xy)$</p>

<p>$$\ x^3+y^3\, -\ (x+y)^3\, +\ 3(x+y)(xy) \ =\ 0$$</p>

<p>So the algorithm terminates, yielding $\ f = s_1^3 - 3s_1 s_2.\ $ </p>

<p>Exactly the same <em>algorithm</em> works for polynomials in any number of variables: for the high term $\,c\, x_1^{\large a_1}\cdots x_k^{\large a_k}\,$ we subtract $\,c\, s_1 ^{\large a_1-a_2} s_2^{\large a_2-a_3}\cdots s_{k-1}^{\large a_{k-1}-a_k}s_k^{\large a_k},\,$ e.g. in your trivariate example, for high term $\,c\, x^a y^b z^c\, $ we subtract $\,c\, s_1^{a-b} s_2^{b-c} s_3^c.\,$ This reduces such problems to rote <em>mechanical</em> computation, i.e. no guesswork is required to solve such problems, only simple polynomial arithmetic. The algorithm yields a constructive interpretation of the <a href=""http://en.wikipedia.org/wiki/Elementary_symmetric_polynomial#The_fundamental_theorem_of_symmetric_polynomials"" rel=""nofollow noreferrer"">Fundamental Theorem of Symmetric Polynomials,</a> that every symmetric polynomial has a unique representation as a polynomial in the elementary symmetric polynomials.</p>

<p>Gauss's algorithm may be viewed as a special case of GrÃ¶bner basis methods (which may be viewed both as a multivariate generalization of the (Euclidean) polynomial division algorithm, as well as a nonlinear generalization of Gaussian elimination for linear systems of equation). Gauss's algorithm is the earliest known use of such  a lexicographic order for term-rewriting (now mechanized by the Grobner basis algorithm and related methods).</p>
"
"2386891","2386958","<p>write everything in einstein notation, and the result follows steadily.
Or, exploit the great flexibility of the concept of differential:
$$
d_{W}\operatorname{tr}((Y-XW)(Y-XW)^T)=\operatorname{tr}(d_{W}[(Y-XW)(Y-XW)^T])=
$$
$$
=\operatorname{tr}\left([d_{W}(Y-XW)][(Y-XW)^T]+[(Y-XW)][d_{W}(Y-XW)^T]\right)=
$$
$$
=\operatorname{tr}\left((-X\,d_{W}W)(Y-XW)^T+(Y-XW)(-X\,d_{W}W)^T\right)=
$$
$$
=\operatorname{tr}\left((-X\,d_{W}W)(Y-XW)^T+(-X\,d_{W}W)(Y-XW)^T\right)=
$$
$$
=2\operatorname{tr}\left((-X\,d_{W}W)(Y-XW)^T\right)=
$$
$$
=2\operatorname{tr}\left((XW-Y)^T(X\,d_{W}W)\right)=
$$
$$
=2\operatorname{tr}\left(d_{W}W^T\, X^T(XW-Y)\right)
$$
so:
$$
d_{W}\operatorname{tr}((Y-XW)(Y-XW)^T)=2\operatorname{tr}\left(d_{W}W^T\, X^T(XW-Y)\right)=\#
$$
using index notation:
$$
\#=2\sum_i\left[d_{W}W^T\, X^T(XW-Y)\right]_{i,i}=
$$
$$
=\sum_i\sum_q\left[d_{W}W^T\right]_{i,q}\left[2X^T(XW-Y)\right]_{q,i}=
$$
$$
=\sum_i\sum_q\left[d_{W}W\right]_{q,i}\left[2X^T(XW-Y)\right]_{q,i}
$$
so:
$$
\frac{d}{dW_{q,i}}\operatorname{tr}((Y-XW)(Y-XW)^T)=\left[2X^T(XW-Y)\right]_{q,i}
$$</p>
"
"2386895","2386901","<p>The integral is <strong>not simple</strong>.</p>

<p>The answer is</p>

<p>$$
\dfrac{2\arctan\left(\left(\sqrt{3}+1\right)\tan\left(\frac{x}{2}\right)+1\right)-2\arctan\left(\left(\sqrt{3}-1\right)\tan\left(\frac{x}{2}\right)-1\right)+\sqrt{2}\left(\ln\left(\left|\tan\left(\frac{x}{2}\right)+\sqrt{2}+1\right|\right)-\ln\left(\left|\tan\left(\frac{x}{2}\right)-\sqrt{2}+1\right|\right)\right)}{3}
$$</p>

<p>which is certainly <strong>not pretty</strong>.</p>

<p>Hence,</p>

<ol>
<li>The solution will be complicated no matter what.</li>
<li><a href=""http://www.integral-calculator.com/#expr=1%2F%28cosx%5E3%20-%20sinx%5E3%29"" rel=""nofollow noreferrer"">Online integral solvers may help you with steps.</a></li>
<li>Interesting insights may be provided here in the answers, but you will surely not get a short solution.</li>
</ol>
"
"2386896","2386904","<p>In order for $f$ to be continuous at $1$, we need to see if
$$\lim_{x\to 1}f(x) \quad\text{and}\quad f(1)$$
both exist and are equal.</p>

<p>To do so, compute the limit from the left, the limit from the right, and $f(1)$. If
$$
\lim_{x\to1^-}f(x) = f(1) = \lim_{x\to1^+}f(x),
$$
then $f$ is continuous at $1$. If one of the equalities doesn't hold, then $f$ is not continuous at $1$.</p>

<p>I'll let you take it from here.</p>
"
"2386906","2386919","<p>Note that any ordering on $G$ induces an ordering on $G\otimes\mathbb{Q}$, by saying $x/n&lt;y/m$ for $n,m\in\mathbb{Z}_+$ iff $mx&lt;ny$.  Conversely, any ordering on $G\otimes\mathbb{Q}$ induces an ordering on $G$ by restriction, and the original order on $G\otimes\mathbb{Q}$ can be recovered from the order on $G$ as above.  So the set of orderings on $G$ is in bijection with the set of orderings on $G\otimes\mathbb{Q}$.</p>

<p>So we may assume $G$ is a $\mathbb{Q}$-vector space.  Now the answer is easy.  If $G$ has dimension $0$ there is only one possible order, and if $G$ has dimension $1$ there are only two possible orders (the usual order on $\mathbb{Q}$ and its reverse).  If $G\cong\mathbb{Q}^2$, it has infinitely many possible orders: for any $q\in\mathbb{Q}$, take the lexicographic order with respect to the basis $(1,0)$ and $(q,1)$.  Explicitly, $(a,b)$ is positive in these orders iff either $a&gt;qb$ or $a=qb$ and $b&gt;0$ (this makes it clear the orders are distinct).  Finally, if $G$ has dimension greater than $2$, choose a two-dimensional subspace $V$ and a complement $W$ so that $G=V\oplus W$.  There are infinitely many orders on $V$, and combining them lexicographically with any order on $W$, we get infinitely many orders on $G$.</p>

<p>Thus a $\mathbb{Q}$-vector space has infinitely many orders iff it has dimension greater than $1$.  It follows that a torsion-free group has infinitely many orders iff it has rank greater than $1$.</p>
"
"2386909","2386916","<p>Although completely true, your estimate that</p>

<p>$$e^{1/3} &lt; 8^{1/3} = 2$$</p>

<p>loses a lot of information. In fact, it's easy to check (by hand, even!) that $e^{1/3} &lt; 1.5$, since $1.5^3 = 2.25 * 1.5 &gt; 3$. This improvement is enough to get that</p>

<p>$$R_5 &lt; 0.0000515$$</p>

<p>Another by-hand estimate shows that $1.4^3 = 2.744 &gt; e$, so we could improve this a touch more, to $$R_5 &lt; 0.0000481.$$</p>
"
"2386910","2386914","<p>In the $r e^i\theta$ form, both $r$ and $\theta$ are assumed to be real numbers, not complex; $r$ is typically taken to be nonnegative as well. </p>

<p>But it IS true that as you've noticed, 
$$
i = e^{\frac{\pi}{2} i},
$$
which looks a little odd. I don't think I've ever seen the thing on the right used to express ""$i$"", but I'm sure it's appeared. </p>

<p>And, of course, it's also true that 
$$
i = i e^{2\pi i}
$$
but probably no one every writes that, either. It's like writing $17$ by writing $17 + 8 - 8$. :) </p>
"
"2386918","2387182","<p>Three Series is not necessary here, but One Series might help. By the monotone convergence theorem,</p>

<p>$$\mathbb E\left(\sum_n|X_n|\mathbf1_{[|X_n|&gt;1]}\right) =\sum_n\mathbb E(|X_n|\mathbf1_{[|X_n|&gt;1]})&lt;\infty$$</p>

<p>and hence $\sum_n|X_n|\mathbf1_{[|X_n|&gt;1]}&lt;\infty$ almost surely. This implies that, with probability one, $|X_n|&gt;1$ for at most finitely many $n$. It thus suffices to show $\sum_n Y_n$ converges, where $Y_n=X_n\mathbf1_{[|X_n|\le1]}$. Are you able to finish it off?</p>
"
"2386923","2389172","<p>One has 
$$y(x)=\int_0^x \bigl(y^2(t)+t\bigr)\&gt;dt\geq {x^2\over2}\qquad(x&gt;0)\tag{1}$$ with equality only if $y(t)\equiv0$, which is not the case, by $(1)$. It follows that $y(1)&gt;{1\over2}$.</p>

<p>From $y'(t)\geq y^2(t)$ we infer
$${y'(t)\over y^2(t)}\geq1\qquad(t\geq1)\ ,$$
and integrating this from $1$ to $x$ gives
$${1\over y(1)}-{1\over y(x)}\geq x-1\ .$$
This implies ${1\over y(x)}\leq1+{1\over y(1)}-x$, hence
$$y(x)\geq{1\over\beta -x}\qquad(x\geq1)$$
for some $\beta&lt;3$.</p>
"
"2386925","2386944","<p>Using the <a href=""https://en.wikipedia.org/wiki/Product_rule#Vector_functions"" rel=""nofollow noreferrer"">product rule for scalair multiplication</a>
$$2aa'=(a^2)' = (\mathbf{a}\cdot \mathbf{a})' = \mathbf{a}\cdot \mathbf{a}' + \mathbf{a}'\cdot \mathbf{a} = 2(\mathbf{a}\cdot \mathbf{a}')$$</p>
"
"2386926","2390264","<p>OK, I think the answer must be given by:
$$
\int_{0}^{2\pi}\int_{0}^{R}r\sqrt{a^2+r^2-2ar\cos{\theta}} dr d{\theta}
$$
Where $a$ is the distance of the point from the center, $R$ is the radius of the disc. Wolfram Alfa won't work this out and I haven't the inclination (or maybe ability) to do it myself.</p>

<p>I have done it numerically though, and the result is shown below (where $R=100$). As you can see when $a=0$, we get 66.66 ($\frac{2R}{3}$) as expected.</p>

<p>Within this range it is well approximated with a quadratic.</p>

<p><a href=""https://i.stack.imgur.com/5U4ke.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5U4ke.png"" alt=""Average distance to a point within a disc""></a></p>
"
"2386929","2386940","<p>Question 9 asks you to to find the root graphically using the Newton-Raphson method. I would physically draw the tangents to the curve, to show how the roots are (approximately) determined in a few iterations. The answer is, of course, ~0.2 and 0.6, depending on where you start the algorithm, but the start is given to you.</p>

<p>Since you don't actually know the function plotted, there is little use in interpolating or approximating the plot. This question seems to me to be primarily about showing that you have understood the way the methods works, rather than about calculating a highly accurate solution.</p>

<p>The same goes for question 10.</p>
"
"2386930","2386951","<p><strong>hint</strong></p>

<p>$$\cos^2 (x)=\frac {1}{1+\tan^2 (x)} $$</p>

<p>put $t=\tan (x) $to get
$$\int \frac {dt}{(1+t)(1+t^2)^2} $$</p>

<p>use partial decomposition and by parts integration for
$$\int \frac {dt}{(1+t^2)^2} =\int \frac {1+t^2-t^2}{(1+t^2)^2}dt$$</p>
"
"2386941","2386947","<p>It is not the case that $x$ is a limit point of $\bar{E}$ iff $x$ is in $\bar{E}$. Consider $E:=\{0\}$. Then $\bar{E}=\{0\}$ and $(\bar{E})'=\emptyset$. Thus $0$ is an element of $\bar{E}$ that is not a limit point of $\bar{E}$.</p>

<p>To solve your question, just use the definition of limit point.
Suppose $x\in E'$. We want to show that $x\in(\bar{E})'$. Fix a neighborhood $U$ of $x$. Since $x$ is a limit point of $E$, we know that $(U\cap E)\setminus\{x\}$ is nonempty. Thus $(U\cap\bar{E})\setminus\{x\}$ is nonempty. We have just shown that $x$ is a limit point of $\bar{E}$. Therefore
$E'\subseteq (\bar{E})'$.</p>
"
"2386946","2386986","<p>Remember that the center of mass (assuming all trees have the same mass) is $\frac{1}{5}(A + B + C + D + E)$ (treating the locations as vectors).</p>

<p>After the first step, we end up with the point $\frac{1}{2}(A + B)$, found by taking their midpoint. Now from here, we travel $\frac{1}{3}$ of the distance to $C$. We take the weighted average to get the new position of
$$\frac{2}{3}\left(\frac{1}{2}(A + B)\right) + \frac{1}{3}C = \frac{1}{3}A +\frac{1}{3}B + \frac{1}{3}C$$
Similarly, after the next step, we end up at
$$\frac{3}{4}\left(\frac{1}{3}A +\frac{1}{3}B + \frac{1}{3}C\right) + \frac{1}{4}D = \frac{1}{4}A + \frac{1}{4}B + \frac{1}{4}C + \frac{1}{4}D$$
Likewise, the final step brings us to
$$\frac{4}{5}\left(\frac{1}{4}A + \frac{1}{4}B + \frac{1}{4}C + \frac{1}{4}D\right) + \frac{1}{5}E = \frac{1}{5}A + \frac{1}{5}B + \frac{1}{5}C + \frac{1}{5}D + \frac{1}{5}E$$
Intuitively, each of the vectors adds less proportional weight each time, while those already used have their contributions slightly diminished each time. This has the effect of balancing out in the end to give the average.</p>
"
"2386952","2387047","<p>Let $\sqrt[3]{m+9}=a$, $-3=b$ and $-\sqrt[3]{m-9}=c$.</p>

<p>Hence, we have
$$a+b+c=0.$$
But $$a^3+b^3+c^3-3abc=a^3+3a^2b+3ab^2+b^3+c^3-3a^2b-3ab^2-3abc=$$
$$=(a+b)^3+c^3-3ab(a+b+c)=(a+b+c)((a+b)^2-(a+b)c+c^2-3ab)=$$
$$=(a+b+c)(a^2+b^2+c^2-ab-ac-bc).$$
We see that $$ a^2+b^2+c^2-ab-ac-bc=\frac{1}{2}((a-b)^2+(a-c)^2+(b-c)^2),$$</p>

<p>which says that $a^2+b^2+c^2-ab-ac-bc=0$ for $a=b=c$ only.</p>

<p>In our case it gives
$$\sqrt[3]{m+9}=-3=-\sqrt[3]{m-9},$$ which is impossible.</p>

<p>Thus, our equation it's
$$a^3+b^3+c^3-3abc=0$$ or
$$m+9-27-(m-9)-9\sqrt[3]{m^2-81}=0$$ or
$$m^2-81=-1,$$
which gives the answer:
$$\{4\sqrt{5},-4\sqrt{5}\}$$</p>
"
"2386957","2386972","<p>Call $\Phi$ the vector space isomorphism sending $x$ to $y$ (abusing notation, I will write $x$ and $y$ also when I mean their equivalence classes). </p>

<p>Since $f(x)=0$, we have $\Phi((f(x))=0$. </p>

<p>On the other hand, if $\Phi$ is a field homomorphism we have $\Phi(f(x))=f(\Phi(x))=f(y)$, so $y$ is a root of $f$. By the uniqueness of the minimal polynomial, this implies $f=g$.  </p>
"
"2386967","2386975","<p>You are mapping from a $5$ element set to a $5$ element set.</p>

<p>1) Since we must have $\varphi(-8)=-5$, we need to assign $4$ elements of the domain. In order to make $\varphi$ one-to-one, each of these $4$ elements must map to a unique element of the codomain. Thus we have $4$ options for $\varphi(-9)$ (since it can't be equal to $-5$), and then $3$ options for $\varphi(-7)$ (since it can't be equal to $-5$ or $\varphi(-9)$), and so forth. In total we have $4\cdot3\cdot2\cdot1=4!$ different functions.</p>

<p>2) I suggest counting how many one-to-one functions there are without any restrictions, and then subtracting that number from the one we obtained in (1). I'll let you work out the details.</p>
"
"2386968","2387056","<p>Very generally, if $(X,\mu)$ is a measure space, and $f:X \rightarrow \mathbb{R}$ is measurable and satisfies $\int_{X} |f(x)|d\mu = 0$, then $f = 0$ almost everywhere.</p>

<p>To see why, note that:</p>

<p>$$\{x \mid f(x) \neq 0\} = \bigcup_{n \in \mathbb{N}} \{x \mid |f(x)| &gt; 1/n\}$$</p>

<p>Letting $A_n = \{x \mid |f(x)| &gt; 1/n\}$, we know $|f| &gt; \frac{1}{n}1_{A_n}$, where $1_{A_n}$ is the function which is $1$ on $A_n$ and $0$ elsewhere hence $0 = \int_X |f| d\mu \geq \int_X \frac{1}{n}1_{A_n}d\mu = \frac{1}{n}\mu(A_n)$. We get $\mu(A_n) = 0$, and therefore $\{x \mid f(x) \neq 0\}$ is the countable union of measure-zero sets and so has measure zero.</p>

<p>--</p>

<p>Now, any positive function on $[a,b]$ which is Riemann integrable is also Lebesgue integrable, and the integrals agree. But you can prove this without needing measure theory and just using the definition of Riemann integrability on $[a,b]$ (though this proof is very similar to the above):</p>

<p>Assuming $f$ is Riemann integrable and $\int_a^b |f(x)|\, dx = 0$, given any $\varepsilon &gt; 0$ we can find a partition $P$ of $[a,b]$ such that the upper sum $U(|f|,P) &lt; \varepsilon$. Then for any real number $\delta &gt; 0$, consider just the intervals $I$ in $P$ such that $\sup_{x \in I}|f(x)| \geq \delta$. If the total length of these intervals is $L$, these intervals contribute at least $L\delta$ to the sum $U(|f|,P)$, hence we have $L &lt; \varepsilon/\delta$.</p>

<p>Now fix $\varepsilon_0$, and pick $\varepsilon = \varepsilon_02^{-n}/n$ and $\delta = 1/n$, and the above paragraph shows that we can cover all points where $|f(x)| \geq 1/n$ by intervals whose total length $L$ is $&lt; \varepsilon_02^{-n}$. Then taking the union of all these intervals over $n \in \mathbb{N}$, we get intervals covering the entire set $\{x \mid |f(x)| &gt; 0\}$ whose total length is $&lt; \varepsilon_0$. Since $\varepsilon_0$ was arbitrary, this completes the proof that this set has measure zero.</p>

<p>--</p>

<p>As a last comment, I'll just say that the continuity assumption actually makes this proof way easier, and there's no need for fancy measure theory: If $f(x_0) \neq 0$ then by continuity there is some $\varepsilon &gt; 0$ and $\delta &gt; 0$ such that $|f(x)| \geq \varepsilon$ for $|x - x_0| \leq \delta$, and so you have $\int_a^{b}|f(x)|\, dx \geq \varepsilon \delta$</p>
"
"2386970","2387186","<p>No, there's no such requirement in any base $p$. Convergence of a sequence in $\mathbb{Q}_p$ and convergence of its image in $\mathbb{Q}_p^{\times}$ can be independent, as demonstrated by:</p>

<p>The Cauchy sequence $(-p)^k$ whose image in $\mathbb{Q}_p^{\times}:\{-1,+1\}$ does not converge.</p>

<p>And the sequence:</p>

<p>$\{2,12,14,60,62\ldots\}$ which does not converge in $\mathbb{Q}_2$ while its image in $\mathbb{Q}_2^{\times}: \{1,3,7,15,31,\ldots\}$ is Cauchy.</p>
"
"2386976","2386993","<p>Note that:</p>

<p>$$n^K\ll2^n$$</p>

<p>and</p>

<p>$$a_{n-3}&gt;n\implies a_{n-2}&gt;2^n\implies a_{n-1}&gt;2^{2^n}\implies a_n&gt;2^{2^{2^n}}$$</p>

<p>It's enough to verify that $a_4&gt;7$ and that</p>

<p>$$a_{n-3}&gt;n\implies a_{n-2}&gt;2^n&gt;n+1$$</p>

<p>which concludes the initial condition for the other induction hypothesis.</p>

<p>Thus,</p>

<p>$$a_n\gg2^{2^{2^n}}\gg2^{2^n}\gg2^{n^K}$$</p>
"
"2386985","2387483","<p>Hint/Answer -</p>

<ol>
<li><p>W.L.O.G assume that $f$ maps the upper half-plane to itself. Take a circle of radius $r$ about $0$, say $C_r$, on which $f$ has no zeroes. The winding number of $f(C_r)$ about $0$ is at most $1$, since it only crosses the real line at most two times. Conclude that $f$ has at most one zero by using the <strong>Argument Principle</strong>.</p></li>
<li><p>If $\infty$ is a removable singularity, use <strong>Liouville's Theorem</strong>. If not, consider the case when it is an essential singularity. Now, <strong>Picard's Theorem</strong> implies that $f$, on a punctured nbd. of $\infty$, must take every value, except possibly one, infinitely many times. This is impossible! Thus, $f$ has a pole at $\infty$.</p></li>
<li><p>Since a constant function cannot possibly be the answer, the only possibility that remains is a linear function.</p></li>
</ol>
"
"2386989","2387030","<p>Maybe looking at the $3 \times 3$ case will be helpful.  Note that
$$
\sum_{i,j} E_{ij} \otimes E_{ij} = 
\left[\begin{array}{ccc|ccc|ccc}
1&amp;0&amp;0&amp;0&amp;1&amp;0&amp;0&amp;0&amp;1\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\
\hline
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\
1&amp;0&amp;0&amp;0&amp;1&amp;0&amp;0&amp;0&amp;1\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\
\hline
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\
1&amp;0&amp;0&amp;0&amp;1&amp;0&amp;0&amp;0&amp;1
\end{array}
\right]
$$
If we take the transpose of each $3 \times 3$ block, we get the desired matrix
$$
\sum_{i,j} E_{ij} \otimes E_{ij}^T = 
\left[\begin{array}{ccc|ccc|ccc}
1&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;1&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;0&amp;0\\
\hline
0&amp;1&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;1&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;0\\
\hline
0&amp;0&amp;1&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1
\end{array}
\right]
$$</p>
"
"2386991","2387007","<p>First show that the Bochner integrable simple functions are dense in $L^p([0,T];X)$ - you can do this easily using the Lebesgue dominated convergence theorem for the Bochner integral. Now you just need to show that $\mathscr{C}([0,T];X)$ is dense in the space of Bochner integrable simple functions: let $I\subseteq[0,T]$ be a measurable set and let $\{\varphi_n\}_{n=1}^{\infty}\subset\mathscr{C}([0,T];\mathbb{R})$ be a sequence of continuous (you can make these smooth if you want to) such that $\varphi_n\to\chi_E$ in $L^p([0,T];X)$. Now for any vector $x\in X$ we have $\varphi_n\cdot x\to \chi_E\cdot x$ as $n\to\infty$, and so by linearity the space of continuous functions is dense in the space of simple functions, and hence is dense in $L^p([0,T];X)$.</p>
"
"2386994","2387014","<p>Hint: Consider the event $X^2 - X&lt;c$. See that this corresponds to $ x_1 \le X  \le x_2$, where $x_1,x_2$ are the roots of $X^2 - X =c$.  You should be able to compute this probability in terms of $F_X$ (or simply use the information that $X$ is uniform...)</p>
"
"2386997","2390773","<p>Here are few things to know:</p>

<ol>
<li><p>If two homeomorphisms (diffeomorphisms) of closed 2-dimensional manifolds are homotopic then they are isotopic (smoothly isotopic); this is due to D.B.A.Epstein, 1966. </p></li>
<li><p>If you do not assume that the surface is closed then the following is a counter example: $f(z)=1/\bar{z}$, the inversion of the punctured complex plane ${\mathbb C}^*$. It is homotopic but not isotopic to the identity map.</p></li>
<li><p>For closed aspherical 3-dimensional manifolds, homotopic homeomorphisms (diffeomorphisms) are isotopic (smoothly isotopic). A reference for this is a bit painful to trace, since it is scattered between several papers (Haken case is due to Waldhausen, I think; hyperbolic case is due to Gabai, etc.), the hardest part of the proof is Perelman's Geometrization Theorem.    </p></li>
<li><p>For general closed 3-dimensional manifolds, homotopy does not imply isotopy, see  J. L. Friedman, D.M. Witt, <a href=""https://www.researchgate.net/profile/John_Friedman2/publication/256364564_Homotopy_is_not_isotopy_for_homeomorphisms_of_3-manifolds/links/00b7d533d7f2b5c668000000.pdf?origin=publication_detail"" rel=""nofollow noreferrer"">Homotopy is not isotopy for homeomorphisms of 3-manifolds</a>,  Topology 25 (1986), no. 1, 35â44. (The same in the smooth category.)  </p></li>
<li><p>Probably the most famous example is in dimension 6, due to Milnor: There are diffeomorphisms of 6-dimensional sphere which are continuously isotopic but not smoothly isotopic.  (Gluing two copies of the 7-ball via such diffeomorphisms results in exotic 7-spheres.)  </p></li>
</ol>
"
"2387010","2387023","<p>Your calculation gives you the number of $3'$s present in $180!$. But each $18$ requires two $3'$s, so the number of $18's$ you can have is $\left \lfloor \dfrac {93}{2} \right\rfloor=46$ .</p>
"
"2387011","2387040","<p>$P(A)=P(B)=18/36=1/2$ because there are $36$ possibilities and $18$ of those have red showing even and $18$ have blue showing odd.</p>

<p>$P(C)$, $P(D)$, and $P(E)$ are correct.</p>

<p>$P(F)=10/36=5/18$ because $(1,2),(2,3),(3,4),(4,5),(5,6),(6,5),(5,4),(4,3),(3,2),(2,1)$ are all of the possibilities of having a difference of $1$ between the two dice.</p>

<p>(a) $P(A\cap B)=9/36$ because the red die can only be $2,3,4$ and the blue die can only be $1,3,5$, so there are $3^2=9$ possibilites of having both $A$ and $B$.</p>

<p>(b) $P(F\cap D)=2/36=1/18$ because, as seen in the enumeration above when calculating $P(F)$, only $(4,5)$ and $(5,4)$ sum to $9$ and have a difference of $1$.</p>

<p>(c) $P(E\cup F) = P(E)+P(F)-P(E\cap F)$. Now $P(E\cap F)=4/36=1/9$ by again looking at the enumeration when calculating $P(F)$.</p>

<p>For conditional probabilities, you want to use the definition
$$ P(X \mid Y) = \frac{P(X\cap Y)}{P(Y)}.$$</p>

<p>(d) $P(F \mid E) = \frac{P(F\cap E)}{P(E)}$. We have already found both $P(E\cap F)$ and $P(E)$, so this one is easy.</p>

<p>(e) $P(A \mid E) = \frac{P(A\cap E)}{P(E)}$. We know $P(E)$, so we need only compute $P(A\cap E)$. I'll let you do this.</p>
"
"2387015","2387029","<p>Just including this as you said showing it isn't a field is out of range.</p>

<p>$\mathbb{Z}[\sqrt{3}]/(2)\cong\mathbb{Z}_2[\sqrt{3}]$.
Let's look at the inverse of a generic element:
$$\frac{1}{a+b\sqrt{3}} = \frac{a-b\sqrt{3}}{a+3b}$$
For this to be integral, we need to have $a+3b\equiv 1\mod 2$.
It follows that $(1+\sqrt{3})$ isn't invertible.</p>

<p>To see this explicitly:
\begin{align*}
(1+\sqrt{3})(a+b\sqrt{3})  &amp;\equiv 1\mod 2 \\
\implies a+3b+(b+a)\sqrt{3}&amp;\equiv 1\mod 2 \\
\implies (a+b)(1+\sqrt{3})&amp;\equiv 1\mod 2
\end{align*}
This can clearly never be true.</p>

<p>So, $\mathbb{Z}[\sqrt{3}]/(2)$ isn't a field, so $(2)$ isn't maximal.</p>
"
"2387017","2387034","<p>The field is <em>locally conservative</em>, meaning that around any point in its domain there is a small neighborhood on which the field is indeed conservative. This is also known as curl-free, for the simple reason that the curl of the field is zero. However, once there is a hole in the domain you open the door for larger-structure non-conservativeness, just like you discovered here.</p>
"
"2387018","2387205","<p>A well known property of linear time-invariant (LTI) systems is that exponentials are eigenfunctions. That is, if the signal $x[n]=e^{i\omega n},n\in\mathbb{Z}, \omega \in \mathbb{R}$, is input to a <em>stable</em> LTI filter $h[n],n\in\mathbb{Z}$, the output will be equal to </p>

<p>$$
\begin{align}
y[n]&amp;=\sum_{k\in\mathbb{Z}}h[k]x[n-k]\\
&amp;=e^{i\omega n}H\left(e^{i \omega}\right),
\end{align}
$$
for all $n\in \mathbb{Z}$, where </p>

<p>$$
\tag{1}
H\left(e^{i \omega}\right)\triangleq \sum_{n\in \mathbb{Z}}h[n]e^{-i\omega n},\omega \in \mathbb{R},
$$</p>

<p>is the, so called, discrete-time Fourier transform (DTFT) of $h[n]$ (assuming that the sum of the RHS of (1) exists).</p>

<p>Now, note that </p>

<p>$$
\cos(\omega n)=\frac{1}{2}e^{i \omega n} + \frac{1}{2} e^{-i \omega n}.
$$</p>

<p>It is easy to generalize the above and show that the output $y[n]$ with an input $x[n]=\cos(\omega n)$ equals</p>

<p>$$
y[n] = \frac{1}{2}H\left(e^{i \omega}\right) e^{i\omega n} + \frac{1}{2}H\left(e^{-i \omega}\right) e^{-i\omega n}
$$</p>

<p>In general the above output is $not$ equal to zero (for all $n \in \mathbb{Z}$) unless it holds $H\left(e^{i \omega}\right) = H\left(e^{-i \omega}\right)=0$. This conclusion, of course, holds also for the case $\omega = \pi$ as in your question.</p>

<p>Remark: Note that the filter $h[n]$ in your question, <em>does not</em> have a DTFT, hence the above analysis does not hold (unstable filters are not interesting both from a practical and mathematical perspective).</p>
"
"2387025","2387046","<p>Here's a proof</p>

<p>I use your notations we have $e_n\rightarrow a$ and I'm going to construct a subsequence of $P_n$ converging to $a$.</p>

<p>first, pick $n_1$ such that $d(a,e_{n_1})&lt;\frac{1}{2}$ then pick $m_1$ such that $d(P_{m_1}, e_{n_1})&lt;1/2$ it follows by the triangle inequality that $d(a,P_{m_1})&lt;1$.</p>

<p>Now by induction I claim there exists $m_k$ such that $m_k&gt;m_{k-1}$,  and $d(a,P_{m_k})&lt;1/k$.</p>

<p>Assume I already picked $m_1,...,m_k$ satisfying the condition we shall pick $m_{k+1}$ and this is how:</p>

<p>First we pick $n_{k+1}$ such that $d(a,e_{n_{k+1}})&lt;1/2(k+1)$, then we have a sub sequence of $P_i$ converging to $e_{n_{k+1}}$ so we have arbitrary large values of $m_{k+1}$ (pick one larger than $m_{k}$) such that $d(e_{n_{k+1}},P_{m_{k+1}})&lt;1/2(k+1)$
Use the triangle inequality and we are done.</p>

<p>Now the sequence $P_{m_k}$ is converging to $a$ and therefore $a\in E$.</p>
"
"2387032","2387037","<p>There are not. A square always has a residue of 0 or 1 mod 4, so the sum of two squares has a residue of 0, 1, or 2.</p>

<p>But among any four consecutive numbers, one has a residue of 3 and so cannot be a sum of two squares.</p>
"
"2387039","2387077","<p>Given the inequality</p>

<p>$$\frac{1}{x}&gt;-4$$</p>

<p>you may indeed divide by $-4$, giving</p>

<p>$$-\frac{1}{4x}&lt;1$$</p>

<p>But you're wrong to say you can ""interchange the places"" of $x$ and $-4$. You cannot multiply by $x$ <em>unless you make an assumption about its sign</em>, for otherwise you don't know what to do with the inequality sign. So you may break your work into two cases:</p>

<ul>
<li><strong>Case One:</strong> $x&gt;0$</li>
</ul>

<p>Then multiplication by $x$ gives $-\frac{1}{4}&lt;x$. But to deduce this inequality we had to assume $x&gt;0$ anyway. So we conclude $x&gt;0$.</p>

<ul>
<li><strong>Case Two:</strong> $x&lt;0$</li>
</ul>

<p>Now multiplication by $x$ gives $-\frac{1}{4}&gt;x$. This is consistent with the assumption $x&lt;0$, so we conclude $x&lt;-\frac{1}{4}$.</p>

<hr>

<p>Therefore the solution to $\frac{1}{x}&gt;-4$ is that <em>either</em> $x&gt;0$ <em>or</em> $x&lt;-\frac{1}{4}$.</p>

<p>The moral of this example is that you cannot multiply or divide by a variable in an inequality unless you know its sign. In particular, you cannot think of a proportional inequality $\frac{a}{b}&gt;\frac{c}{d}$ as if it were a proportional equation $\frac{a}{b}=\frac{c}{d}$, where $a$ and $d$ can trade places -- <em>unless</em> you have information about the signs of $a$ and $d$.</p>
"
"2387041","2387045","<p>$U_1$ isn't $\Bbb R^n\times(-1,\infty)$; it does not contain, say $(-1/2,a)$ for $a\in A$.</p>

<p>$U_1$ deformation retracts to $\Bbb R^n\times\{1\}$ along ""vertical lines"",
and $\Bbb R^n\times\{1\}$ surely is clearly contractible.</p>
"
"2387042","2387044","<p>Hint:</p>

<p>In order to get the largest odd number, you'll want to use something along the lines of</p>

<p>$$\underbrace{1+\dots+1}_9+x=20$$</p>

<p>You want to find what $x$ is.</p>
"
"2387048","2387051","<p>Expectation of a geometric random variable with parameter $p$ is given by $\frac{1}{p}$. So in this case, we would have an expectation of $\frac{1}{p^2+q^2}$ trials. But each trial corresponds to 2 games, so we get an expected value of $\frac{2}{p^2+q^2}$ games played.</p>
"
"2387062","2387070","<p>Hint: what do you know about groups of order $10$?</p>
"
"2387067","2387080","<p>We say a square matrix, $A$ is diagonalizable if there exists invertible matrix $P$ and a diagonal matrix $D$ such that $A=PDP^{-1}$. Matrix $B$ is not a square matrix here. </p>

<p>In solving the system $Bx=0$, the unique solution would be $x=0$. The constraints that the system has to satisfy are $x_1=0$, $x_2=0$, $0=0$. The third constraint is trivially true and $(x_1,x_2)=(0,0)$ is the only solution due to the first two constraint. Notice that matrix $B$ is already in an RREF form. A homogeneous system has infinitely many solution if there is a non-pivot column, which is not the case here. </p>

<p>The columns are independent.</p>
"
"2387076","2389395","<p>I believe that the only mistake is in your application of the chain rule. I find it helpful to break the function into its composite parts and apply the chain rule one step at a time.</p>

<p>Let $f, g, h$ be the functions given by
\begin{align} f(y) &amp;= 4 + y^2 \\ g(y) &amp;= \ln(y) \\ h(y) &amp;= g(y). \end{align}
Then $$k(y) = h(g(f(y)) = \ln(\ln(4+y^2)$$ is the function of interest and $k$ is defined and differentiable for all $y$. Moreover,
$$ k'(y) = h'(g(f(y))g'(f(y))f'(y) = \frac{1}{\ln(4+y^2)}\frac{1}{4+y^2}2y.$$
The derivative $k'$ can be bounded as follows
$$ |k'(y)| \leq \frac{1}{\ln(4)} \frac{2|y|}{4 + y^2},$$
because $\ln$ is monotone increasing. You have already applied the helpful inequality
$$ |ab| \leq \frac{1}{2} (a^2 + b^2),$$
which in our case, where $a=2$ and $b=|y|$, allows for the estimate
$$ |k'(y)| \leq \frac{1}{2 \ln(4)}.$$</p>

<p>Breaking complicated functions into composite parts is especially useful when programming computers. One line per component produces a program which is easy for a human being to debug/verify.</p>
"
"2387078","2387087","<p>Since $B_{1/n}(q)$ is not contained in $X\setminus K$, there must be some element that is in $K\cap B_{1/n}(q)$. Since this is true for each $n$, we get a sequence $(x_n)_n$ satisfying $x_n\in K\cap B_{1/n}(q)$.</p>
"
"2387083","2387143","<p>Can we do it this way as well?</p>

<p>Since $\gamma$ generates $\Bbb F_q^\ast$, which has $q -1$ elements, we have</p>

<p>$\gamma^{q - 1} = 1; \tag 1$</p>

<p>now</p>

<p>$(\gamma^{\frac{q - 1}{2}} - 1)(\gamma^{\frac{q - 1}{2}} + 1) = (\gamma^{\frac{q - 1}{2}})^2 - 1 = \gamma^{q - 1} -1 = 0, \tag 2$</p>

<p>by (1); also,</p>

<p>$\gamma^{\frac{q - 1}{2}} \ne 1; \tag 3$</p>

<p>otherwise, $\langle \gamma \rangle = F_q^\ast \tag 4$</p>

<p>would only have $\frac{q - 1}{2}$ elements, that is, the order of $\Bbb F_q^\ast$ would only be $\frac{q - 1}{2}$, not $q -1$; since $q$ is an odd prime,</p>

<p>$\dfrac{q - 1}{2} \ne q -1, \tag 5$</p>

<p>and so $\vert \Bbb F_q^\ast \vert = \frac{q -1}{2}$ is impossible.  Thus (3) binds and we may cancel the factor of $\gamma^{\frac{q - 1}{2}} - 1$ out of (2), leaving</p>

<p>$\gamma^{\frac{q - 1}{2}} + 1= 0, \tag 6$</p>

<p>or</p>

<p>$\gamma^{\frac{q - 1}{2}} = -1. \tag 7$</p>
"
"2387084","2387105","<p>The problem is that in order to remedy the problems and paradoxes of naive set theory, the mathematicians around the turn of the century realised that you can't let just any collection be a set. You have to have a list of axioms defining specific sets and set operations, and declare that any collection you can reach in a finite number of steps using the axioms is deemed a set, and nothing more. The axiom of choice lets you access some sets you can't with only the others, specifically functions fulfilling certain properties.</p>
"
"2387085","2387095","<p>Yes, your proof is correct. I would not have defined $x_1$; instead, I would just have written that$$\bigl|g_{N+1}\bigl((N+1)x_0\bigr)\bigr|=\bigl|f(x_0)\bigr|.$$But that is a matter of taste.</p>
"
"2387093","2387102","<p>Apply rank-nullity. Let $T(\textbf{x}) = A\textbf{x}$ be the linear map. Since the rank of $A$ is $r$, then by rank-nullity we have $\textbf{dim}(\textrm{im}(T)) + \textbf{dim}(\textrm{ker}(T)) = \textbf{dim}(V)$ i.e $r + k = n \Rightarrow k = n-r.$</p>
"
"2387103","2387255","<p>You've shown each $A_N$ is a subalgebra of $C[0,1]$ that contains the constants. By S-W, $A_N$ is dense in $C[0,1]$ iff $A_N$ separates points. You showed $A_1$ separates points, but for some reason you didn't consider any other values of $N.$</p>

<p>Note that for $N=1,2,3,$ $\cos Nx$ is injective on $[0,1].$ This follows because $\cos x$ is injective on $[0,\pi],$ and for these values of $N,$ $x\to Nx$ maps $[0,1]$ injectively into $[0,\pi].$</p>

<p>Trouble starts with $N=4.$ Note that $\pi/4 \in (0,1).$ For $k=0,1,2,\dots,$ we have</p>

<p>$$\cos 4k(\pi/4+h) = \cos (k\pi + 4kh) = \cos (k\pi - 4kh) = \cos 4k(\pi/4-h)$$</p>

<p>for all $h\in (0,1-\pi/4).$  Thus $A_4$ does not separate points. The same is true for any $N&gt;4$ by similar reasoning.</p>

<p>So the answer to the question is: $N=1,2,3.$</p>
"
"2387104","2387109","<p>So you want to negate the statement, ""For every hour, there is a man who dies.""</p>

<p>Let's see intuitively when this statement is true.  This statement should be true if, <strong>at any given hour</strong>, you can find a man who dies.</p>

<p>So, when is it false then?  Well, it's <strong>definitely</strong> false if you can find a single hour such that <strong>no man</strong> dies during that hour.  Right?  But if <strong>no man dies</strong>, that means <strong>every man lives</strong>.</p>

<p>So it's not enough to find an hour such that a single man lives.  There could still be another man somewhere else who has died.</p>
"
"2387110","2387115","<p>Hint:
Use the real number analogue to gain some intuition: If $|x|&lt;1$, then
$$\frac{1}{1-x}=\sum_{n=0}^\infty x^n.$$</p>

<hr>

<p>Set $B:=I-A$. Then $A=I-B$, so our intuition says that
$$ ``\ A^{-1} = \frac{1}{I-B} = \sum_{n=0}^\infty B^n.\ ""$$
(Quotes in the above expression are there to indicate that it is nonsense. I include the expression anyway because sometimes nonsense can help us prove things.)</p>

<p>Now show that $\sum_{n=0}^\infty \|B^n\|&lt;\infty$. Since the space of bounded operators on a Banach space is complete, this implies that $\sum_{n=0}^\infty B^n$ converges in $X$. Then show that $(I-B)(\sum_{n=0}^\infty B^n)=(\sum_{n=0}^\infty B^n)(I-B)=I$, which will complete the proof that $A$ is invertible and
$$
A^{-1} = \sum_{n=0}^\infty (I-A)^n.
$$</p>
"
"2387112","2387119","<p>Let us set $\phi_a(z) = \frac{z-a}{1-\overline{a}z}$ for $a\in D(0,1)$. I let you check that $\phi_a$ is an holomorphic bijection from $D(0,1)$ to $D(0,1)$.</p>

<p>Now take a complex $z \in D(0,1)$ and set $w=f(z)$. Consider the new application $$g = \phi_w \circ f \circ \phi^{-1}_z.$$ By construction $g : D(0,1) \to D(0,1)$ is holomorphic and $g(0)=0.$ Hence by Schwartz lemma, we have $$|g'(0)| \leq 1.$$ Can you finish the computation ?</p>
"
"2387116","2387125","<p>Yes, you can prove that $X$ and $Z$ have the same distribution using the following facts:</p>

<ol>
<li><p>If $X_n\to X$ in probability then $X_n\to X$ in distribution.</p></li>
<li><p>If $X_n\to X$ in distribution and $X_n\to Z$ in distribution then $X$ and $Z$ have the same distribution.</p></li>
</ol>
"
"2387117","2387167","<p>No, this need not be the case. For instance, it is possible that with probability $1$, there exists $j$ such that $Z_j=M$ and $Z_k=0$ for all $k\neq j$. For an explicit example, let $X$ be a Poisson random variable and define $Z_j:=\mathbf1_{\{X=j\}}$ for each $j$. Then $\sum_{j=0}^\infty Z_j=1$ almost surely, but $\operatorname{esssup}(Z_j)=1$ for all $j$, so in particular it does not converge to zero.</p>

<p>Note that the conclusion you are trying to draw - that $Z_j\to0$ in $L^\infty$ - is extremely strong. This is rarely something we would try to prove in probability theory.</p>

<p>EDIT: As we have discussed in the comments, it <em>is</em> true that $Z_j\to0$ in $L^p$ for any $1\le p&lt;\infty$. To see this, it is sufficient to show that there is a constant $C$ such that $\sum_jZ_j^p\le C$ almost surely, since this would imply $\sum_j\mathbb E[Z_j^p]=\mathbb E\left[\sum_jZ_j^p\right]\le C$ by the monotone convergence theorem, which of course shows that $\mathbb E[Z_j^p]\to0$.</p>

<p>Let $A=\{j\in\mathbb N:Z_j\le1\}$ and $B=\mathbb N\setminus A$. Notice that $Z_j^p\le Z_j$ for all $j\in A$, $Z_j\le M$ for all $j\in B$ almost surely, and $|B|\le\sum_{j\in B}Z_j\le M$. This implies</p>

<p>$$\sum_jZ_j^p=\sum_{j\in A}Z_j^p+\sum_{j\in B}Z_j^p\le\sum_{j\in A}Z_j+M\cdot M^p\le M+M^{p+1}$$</p>

<p>almost surely, completing the proof.</p>
"
"2387120","2387124","<p>Yes, that works.</p>

<p>In logic, the original is:</p>

<p>$\forall x (S(x) \rightarrow \exists y (H(y) \land I(y,x) \land \exists z (P(z) \land L(z,y) \land ((R(x) \land B(x)) \lor (E(x) \land K(x)))))$</p>

<p>If you negate this:</p>

<p>$\neg \forall x (S(x) \rightarrow \exists y (H(y) \land I(y,x) \land \exists z (P(z) \land L(z,y) \land ((R(x) \land B(x)) \lor (E(x) \land K(x))))) \Leftrightarrow$</p>

<p>$\exists x \neg (S(x) \rightarrow \exists y (H(y) \land I(y,x) \land \exists z (P(z) \land L(z,y) \land ((R(x) \land B(x)) \lor (E(x) \land K(x))))) \Leftrightarrow$</p>

<p>$\exists x (S(x) \land \neg  \exists y (H(y) \land I(y,x) \land \exists z (P(z) \land L(z,y) \land ((R(x) \land B(x)) \lor (E(x) \land K(x))))) \Leftrightarrow$</p>

<p>$\exists x (S(x) \land   \forall y \neg(H(y) \land I(y,x) \land \exists z (P(z) \land L(z,y) \land ((R(x) \land B(x)) \lor (E(x) \land K(x))))) \Leftrightarrow$</p>

<p>$\exists x (S(x) \land   \forall y (H(y) \land I(y,x) \rightarrow \neg \exists z (P(z) \land L(z,y) \land ((R(x) \land B(x)) \lor (E(x) \land K(x)))))$</p>

<p>.. which is what your sentence says</p>

<p>If we push the negation further in, we get:</p>

<p>$\exists x (S(x) \land   \forall y (H(y) \land I(y,x) \rightarrow \forall z \neg (P(z) \land L(z,y) \land ((R(x) \land B(x)) \lor (E(x) \land K(x)))))\Leftrightarrow$</p>

<p>$\exists x (S(x) \land   \forall y (H(y) \land I(y,x) \rightarrow \forall z (P(z) \land L(z,y) \rightarrow \neg  ((R(x) \land B(x)) \lor (E(x) \land K(x)))))\Leftrightarrow$</p>

<p>$\exists x (S(x) \land   \forall y (H(y) \land I(y,x) \rightarrow \forall z (P(z) \land L(z,y) \rightarrow (\neg  (R(x) \land B(x) \land \neg (E(x) \land K(x))))\Leftrightarrow$</p>

<p>$\exists x (S(x) \land   \forall y (H(y) \land I(y,x) \rightarrow \forall z (P(z) \land L(z,y) \rightarrow ((\neg R(x) \lor \neg B(x) \land (\neg E(x) \lor \neg K(x))))$</p>

<p>which translates to:</p>

<p>""There is a street in the city where for every house in that street it is true that every person living in that house is not rich or not beautiful, and is also not highly educated or not kind""</p>
"
"2387122","2387507","<p>As angryavian commented, the problem reduces to : find the zero of $$f(\theta)=\frac \theta {\sin\left(\frac\theta 2\right)}- 2.1\qquad \text{or}\qquad g(\theta)=\theta-2.1{\sin\left(\frac\theta 2\right)}$$ which can only be solved using numerical methods.</p>

<p>Let $x=\frac\theta 2$ to make the equation $$x=1.05 \sin(x)$$ and use, for an approximation, the magnificent $$\sin(x) \simeq \frac{16 (\pi -x) x}{5 \pi ^2-4 (\pi -x) x}\qquad (0\leq x\leq\pi)$$ was proposed by Mahabhaskariya of Bhaskara I, a seventh-century Indian mathematician (see <a href=""https://math.stackexchange.com/questions/976462/a-1-400-years-old-approximation-to-the-sine-function-by-mahabhaskariya-of-bhaska"">here</a>).</p>

<p>Skipping the trivial $x=0$, this leads to the quadratic 
$$4 x^2+\left(\frac{84}{5}-4 \pi \right) x+\pi\left(5 \pi -\frac{84  }{5}\right)=0$$ the positive solution of which being $$x=-\frac{21}{10}+\frac{\pi }{2}+\frac{1}{40} \sqrt{7056+3360 \pi -1600 \pi ^2}\approx 0.537445$$ making $$\theta \approx 1.07489$$ while the exact solution would be $\approx 1.07682$.</p>

<p>With this first result, we could use a Taylor expansion around $\theta=\frac \pi 3$ and get $$g(\theta)=\left(\frac{\pi }{3}-\frac{21}{20}\right)+\left(1-\frac{21 \sqrt{3}}{40}\right)
   \left(\theta-\frac{\pi }{3}\right)+\frac{21}{160} \left(\theta-\frac{\pi
   }{3}\right)^2+O\left(\left(\theta-\frac{\pi }{3}\right)^3\right)$$ Ignoring the higher order terms, another quadratic leading to $$\theta \approx 1.07683$$</p>
"
"2387130","2387168","<p><strong>Hint:</strong> Making use of symmetry and the tangent half-angle substitution, we find</p>

<p>$$\begin{align}
\mathcal{I}
&amp;=\int_{0}^{\pi}\mathrm{d}\theta\,\frac{\ln{\left(1+\cos{\left(\theta\right)}\right)}}{\cos{\left(\theta\right)}}\\
&amp;=\int_{0}^{\frac{\pi}{2}}\mathrm{d}\theta\,\frac{\ln{\left(1+\cos{\left(\theta\right)}\right)}}{\cos{\left(\theta\right)}}+\int_{\frac{\pi}{2}}^{\pi}\mathrm{d}\theta\,\frac{\ln{\left(1+\cos{\left(\theta\right)}\right)}}{\cos{\left(\theta\right)}}\\
&amp;=\int_{0}^{\frac{\pi}{2}}\mathrm{d}\theta\,\frac{\ln{\left(1+\cos{\left(\theta\right)}\right)}}{\cos{\left(\theta\right)}}-\int_{0}^{\frac{\pi}{2}}\mathrm{d}\theta\,\frac{\ln{\left(1-\cos{\left(\theta\right)}\right)}}{\cos{\left(\theta\right)}};~~~\small{\left[\theta\mapsto\pi-\theta\right]}\\
&amp;=\int_{0}^{\frac{\pi}{2}}\mathrm{d}\theta\,\frac{\ln{\left(\frac{1+\cos{\left(\theta\right)}}{1-\cos{\left(\theta\right)}}\right)}}{\cos{\left(\theta\right)}}\\
&amp;=\int_{0}^{1}\mathrm{d}t\,\frac{2}{1+t^{2}}\cdot\frac{1+t^{2}}{1-t^{2}}\ln{\left(\frac{1}{t^{2}}\right)};~~~\small{\left[\tan{\left(\frac{\theta}{2}\right)}=t\right]}\\
&amp;=-4\int_{0}^{1}\mathrm{d}t\,\frac{\ln{\left(t\right)}}{1-t^{2}}.\\
\end{align}$$</p>
"
"2387158","2387689","<p>The two notions you outlined are equivalent. I'll outline the proof and leave the details to you, don't hesitate to ask if anything is unclear.</p>

<p>Let's suppose that al vector spaces are over a field $\mathbb{K}$. Let $A$ be an algebra, if you want you may assume that it's a Clifford algebra, but that's inessential. For simplicity of notation I will omit the subscript $\mathbb{K}$ from the tensor products.</p>

<p>First let's elucidate what an $A$-module structure on $V$ is. As in your statement, it is first of all a <em>linear</em> map
\begin{equation}
 \Psi:A \otimes V \rightarrow V,
\end{equation}
with the property that $\Psi(e \otimes v) = v$. Second, note that there is a natural map
\begin{equation}
 \mu:A \otimes A \otimes V \rightarrow A \otimes V,
\end{equation}
given by the extension of the map
\begin{equation}
 (a,a',v) \mapsto (aa',v).
\end{equation}
If we want an $A$-module structure on $V$ we now demand that the following holds
\begin{equation}
 \Psi \circ \mu = \Psi \circ (1 \otimes \Psi).
\end{equation}
Let's call this condition (<strong>I</strong>).
This condition might seem complicated, but it's basically point (ii) in your description.</p>

<hr>

<p>If the pair $(V, \Phi)$ is a representation of $A$ in the sense of wikipedia, then one obtains a map
\begin{equation}
 A \otimes V \rightarrow V,
\end{equation}
by the universal property of the tensor product, since the map $A \times V \rightarrow V, (a,v) \mapsto \Phi(a) v$ is $\mathbb{K}$-bilinear.$^{1}$ One may check that the map obtained in this way satisfies condition (<strong>I</strong>), (you will need (ii)). So a representation in the sense of wikipedia implies a representation in the sense of <a href=""http://www.math.harvard.edu/~ecp/latex/misc/haynes-notes/haynes-notes.pdf"" rel=""nofollow noreferrer"">Haynes</a>.</p>

<hr>

<p>In the other direction, if one has a representation in the sense of Haynes, that is a map $\Psi: A \otimes V \rightarrow V$ that satisfies property (<strong>I</strong>), then one obtains a representation in the sense of wikipedia by simply composing $\Psi$ with the natural map $A \times V \rightarrow A \otimes V$.</p>

<hr>

<p>(1) This is a bit stronger than what you quoted from wikipedia, I'm not sure what article you found on wikipedia, but the condition that I wrote down is definitely the right one.</p>
"
"2387160","2387225","<p>This answer tries to shine some operator-theoretic light on the issue. I do make two key assumptions which can probably be verified by reading the text your are referencing.</p>

<p>Let's consider the operator $(1+a_1B+a_2B^2)$ if we (or Maurice) assume that there exist solutions $\mu_1,\mu_2$ to $a_2=\mu_1 \mu_2$ and $a_1 = -\mu_1 - \mu_2$, then we can write $$(1+a_1B+a_2B^2)=(1-\mu_1B)(1-\mu_2B).$$</p>

<p>If furthermore $\|\mu_i B\| &lt; 1$ (this is an operator norm), then we get that $(I-\mu_i B)$ is an invertible operator and that $(I-\mu_i B)^{-1}=\sum_{k=1}^\infty (\mu_i B)^k$ This is the <a href=""https://en.wikipedia.org/wiki/Neumann_series"" rel=""nofollow noreferrer"">Neumann series</a>, a generalization of the geometric series for operators. Writing this as a fraction is kind of a sloppy notation.</p>

<p>Furthermore, the <a href=""https://en.wikipedia.org/wiki/Resolvent_formalism#Resolvent_identity"" rel=""nofollow noreferrer"">first resolvent identity</a> provides us with $(I-\mu_1 B)^{-1}(I-\mu_2 B)^{-1} = \frac{1}{\mu_1 - \mu_2}(\mu_1(1-\mu_1B)^{-1} - \mu_2(1-\mu_2B)^{-1}).$</p>

<p>To put it all together: If the $\mu_i$s exist and $\|\mu_i B\| &lt; 1$ then $(I-\mu_iB)$ is invertible and we get from </p>

<p>$(1+a_1B+a_2B^2)X_t= (1-\mu_1B)(1-\mu_2B)X_t = \epsilon_t$  that</p>

<p>$$X_t = (I-\mu_1 B)^{-1}(I-\mu_2 B)^{-1}\epsilon_t = \frac{1}{\mu_1 - \mu_2}(\mu_1(1-\mu_1B)^{-1} - \mu_2(1-\mu_2B)^{-1})\epsilon_t =  \frac{1}{\mu_1-\mu_2}[\sum_{s=0}^{\infty}(\mu_1^{s+1}-\mu_2^{s+1})B^s]\epsilon_t.$$</p>

<p>Unfortunately, I cannot provide proof for why $\|\mu_i B\| &lt; 1$ (or equivalently $\frac{1}{\mu_i}\in \rho(B)$) since this depends on the choice/properties of your $a_i$s and is likely related to the stability mentioned in the other answer.</p>
"
"2387185","2387258","<p>I don't think there is a way to evaluate the limit without using the l'Hospital rule in some form. (Using Taylor series is basically the same, because they are obtained by calculating the derivatives.)  </p>

<p>I would calculate the limit by applying l'Hospital once and then using known series expansions:
\begin{align}
&amp;~~~~\lim_{x \to 0}\frac{\sin^2{x}+2\ln\left(\cos{x}\right)}{x^4}
\\
&amp;=\lim_{x \to 0} \frac{ \sin(2x)- 2 \tan(x)}{4x^3}
\\\
&amp;=\lim_{x \to 0} \frac{2x-\frac 8 6 x^3 + \mathcal O(x^5) - 2(x + \frac 1 3 x^3 +\mathcal O(x^5)) } {4 x^3} 
\\
&amp;= \lim_{x \to 0}\frac{-2 x^3 + \mathcal O(x^5)}{4x^3} = -\frac 1 2
\end{align}</p>
"
"2387200","2389023","<p>I didn't get the desired formula due to wrong calculation of the partial derivatives. For $f(t,x)=y(t)x,$ one should have
$$
f_1(t,x)=y'(t)x=-c_1(t)y(t)x\\
f_2(t,x)=y(t)\\
f_{22}(t,x)=0
$$
A direct substitution to (2.30) would then solve the problem. </p>
"
"2387203","2388215","<p>You can find $C_n$ multiplying by $\sin(\frac{m\pi y}{a})$ and integrating from $0$ to $a$: define
$$
V_0(y)=
\begin{cases}
V_0, &amp;  0&lt;y&lt;\frac{a}{2} \\
-V_0, &amp; \frac{a}{2}&lt;y&lt;a
\end{cases}
$$</p>

<p>knowing that
$$
\int_0^a \sin(\frac{n\pi y}{a}) \sin(\frac{m\pi y}{a}) dy=
\begin{cases}
0, &amp;  \text{if $n\neq m$} \\
\frac{a}{2}, &amp; \text{if $n= m$}
\end{cases}
$$
you arrive at
$$
C_n=\frac{2}{a}\int_0^a V_0(y) \sin(\frac{n\pi y}{a}) dy
$$
now you just split the integral from $0$ to $\frac{a}{2}$ and from $\frac{a}{2}$ to $a$ and integrate; $C_n$ will be zero for some $n$ and non-zero for others (e.g. $C_2$).</p>
"
"2387204","2391136","<p>Take any power series on the form $F(q)=1+a_2q^2+a_3q^3+\cdots$ with integer coefficients: ie, the constant term is $1$ and the linear term is zero, otherwise it is a general power series. Your $G(q)$ takes this form, as do the $k$th partial convergents.</p>

<p>Next, rewrite $F(q)$ on the form
$$
F(q)=\prod_{k=2}^{\infty}(1+\alpha_k q^k)=(1+\alpha_2 q^2)(1+\alpha_3 q^3)\cdots
$$
which can always be done and gives $\alpha_i\in\mathbb{Z}$.</p>

<p>Now, for $F(q)=\exp f(q)$, you ask why $f(q)=f_1q+f_2q^2+\cdots$ gives integer coefficients for $f_n$ whenever $n$ is a prime, but not when $n$ is composite.</p>

<p>To see why, write $f(q)=\ln F(q)$ and take the power expansion
$$
f(q) = \ln F(q) = \sum_{k=2}^\infty \ln(1+\alpha_k q^k)
= \sum_{k=2}^\infty \sum_{i=1}^\infty (-1)^{i-1}\frac{\alpha_k^i q^{ki}}{i}.
$$
Contributions to the coefficient of $q^n$ come from pairs $(k,i)$ where $ki=n$. If $n$ is prime, then $k=n$ and $i=1$: the opposite, $k=1$ and $i=n$, does not contribute as $k\ge2$ (or, equivalently, the coefficient $\alpha_1=0$). When $k=n$ and $i=1$ is the only contributing term to the coefficient of $q^n$, the coefficient $f_n=\alpha_n$.</p>

<p>When $n$ is not prime, there may be terms contributing to the coefficient of $q^n$ where $i&gt;1$ and which may therefore be non-integral.</p>
"
"2387211","2387478","<p>I think <a href=""https://en.wikipedia.org/wiki/Descriptive_set_theory"" rel=""nofollow noreferrer"">descriptive set theory</a> books are the way to go; either <a href=""http://www.springer.com/us/book/9780387943749"" rel=""nofollow noreferrer"">Kechris</a> or <a href=""http://www.math.ucla.edu/~ynm/lectures/dst2009/dst2009.pdf"" rel=""nofollow noreferrer"">Moschovakis</a> will serve you well.</p>

<p>Personally, for a first introduction I vastly prefer Kechris; specifically, I would read the beginning few chapters to get a good sense of the context (Polish spaces, Borel sets, some tree combinatorics) and then would read chapters 20-21, diving back to the earlier chapters when needed. </p>

<p>Chapter 6 (""The playful universe"") of Moschovakis goes into more sophisticated material, but is also much heavier.</p>
"
"2387213","2387216","<p>Hint: What is $\frac{d}{dt}\int_{a}^{b} e^{tx}f(x) dx$?</p>
"
"2387224","2387418","<p>$\newcommand{\v}{\operatorname{var}}$You have $X,Y \sim N(0,1).$</p>

<p>You did not say anything about their <b>joint</b> distribution beyond that --- in particular whether they are jointly normally distributed.</p>

<p>If they are independent, then they are jointly normally distributed. In that case you have $\v(X-Y) = \v(X)+\v(Y) = 2,$ so $\operatorname{sd}(X-Y) = \sqrt 2.$</p>

<p>Suppose they are jointly normal but not independent, and the correlation is $\rho.$ Then
\begin{align}
\v(X-Y) &amp; = \v(X)+\v(Y) - 2\operatorname{cov}(X,Y) \\
&amp; = \v(X) + \v(Y) - 2\rho\sqrt{\v(X)} \sqrt{\v(Y) } \\
&amp; = 1 + 1 -2\rho.
\end{align}</p>

<p>Since $-1\le\rho\le1,$ we have $0 \le \v(X-Y) \le 4.$</p>

<p>So $X-Y\sim N(0,2-2\rho).$ Knowing the variance of $X-Y,$ you can find $\rho,$ and knowing $\rho$ you can find the variance of $X-Y.$</p>

<p>The distributions in the graph do not look normal except that the red one may be a crude approximation. Perhaps density estimation based on a small sample from a normally distributed population could have produced them.</p>

<p>It took me a while (20 seconds?) to suspect that the hyphens in the graphic might have been intended to be minus signs.
\begin{align}
\text{minus sign: } &amp; X-Y \\[10pt]
\text{hyphen: } &amp; X \text{ - } Y \\[10pt]
\text{hyphen: } &amp; X \text{-} Y
\end{align}</p>
"
"2387231","2387904","<p>The system has a constant solution $n=1, \phi=u=0$. You want to linearise near this point, by assuming that the system is slightly perturbed, you let
$$n(x, t)=1+\delta{n}(x, t)$$
$$\phi(x, t)=\delta{\phi}(x, t)$$
$$u(x, t)=\delta{u}(x, t)$$
Where $\delta{n}(x, t), \delta{u}(x, t), \delta{\phi}(x, t)$ are all small, and slowly varing functions. Plugging this into the equations and dropping the terms larger that the first order, we get
$$\partial_{t}\delta{n}(x, t)+\partial_{x}\delta{u}(x, t)=0$$
$$\partial_{t}\delta{u}(x, t)-\partial_{x}\delta\phi(x, t)=0$$
$$\epsilon^{2}\partial_{x}^{2}\delta{\phi}(x, t)-\delta{n}(x, t)-\delta\phi(x, t)=0$$
Now, differentiate the last equation with respect to time twice
$$\epsilon^{2}\partial^{2}_{t}\partial_{x}^{2}\delta{\phi}(x, t)-\partial_{t}^{2}\delta{n}(x, t)-\partial_{t}^{2}\delta\phi(x, t)=0$$
From the first equation it then follows
$$\epsilon^{2}\partial^{2}_{t}\partial_{x}^{2}\delta{\phi}(x, t)+\partial_{t}\partial_{x}\delta{u}(x, t)-\partial_{t}^{2}\delta\phi(x, t)=0$$
And from the second one
$$\epsilon^{2}\partial^{2}_{t}\partial_{x}^{2}\delta{\phi}(x, t)+\partial_{x}^{2}\delta{\phi}(x, t)-\partial_{t}^{2}\delta\phi(x, t)=0$$
So
$$(1+\epsilon^{2}\partial_{x}^{2})\partial_{t}^{2}\delta\phi(x, t)-\partial_{x}^{2}\delta{\phi}(x, t)=0$$
The equation is colse to the one you've stated, however the signs do not match and there is an extra $\epsilon^{2}$ factor. Are you sure that there are no mistakes in the original system of equations you've stated?</p>
"
"2387232","2387245","<p>Let $dim(E)=n$ and $A=\{v_1,v_2...v_n\}$ a basis of $E$.</p>

<p>Now let $x \in E$,then  $x=a_1v_n+a_2v_2+...a_nv_n$ for some $a_1...a_n \in F$</p>

<p>Take the set of linear functionals $B=\{f_1,f_2...f_n\}$ where $f_i(v_j)=0$ if $i \neq j$ and $f_i(v_j)=1$ if $i=j$.</p>

<p>We have that  $$0=f_i(x)=a_1f_i(v_1)+a_2f_i(v_2)+...+a_if_i(v_i)+...+a_nf_i(v_n)=a_i,\forall i \in \{1,2....n\},$$</p>

<p>Thus $a_i=0,\forall i \in \{1,2....n\} \Rightarrow x= \bar{0}$</p>
"
"2387236","2387468","<p>Given an arbitrary set $S$, the goal is to prove that $S$ is not an element of itself. The proposed proof proceeds by considering the set $T=\{S\}$, i.e., the set $T$ whose one and only member is the set $S$. Then $T$ is nonempty, because it has a member, namely $S$. So we can apply the axiom of regularity to conclude that $T$ has a member that has no members in common with $T$.  That member of $T$ is $S$, because $S$ is the only member of $T$. So $S$ has no members in common with $T$. Tbat is, no member of $T$ can also be a member of $S$. Well, $S$ is a member of $T$, so $S$ cannot be a member of $S$, q.e.d.</p>
"
"2387238","2387252","<p>A simple solution would be to generate two unit vectors in the plane of the tilted circle. Since you already have the normal $\vec{u}=(u_x,u_y,u_z)$ you could use for instance $\vec{v} = (-u_y,u_x,0)$ and the other $\vec{w}=\vec{u} \times \vec{v} = (-u_x u_z,-u_y u_z,u_x^2+u_y^2)$ and normalise them.</p>

<p>Then use your favourite method to generate a point $(x,y)$ on the unit-circle and transform this to a point on the tilted circle $x \hat{u} + y \hat{v}$.</p>

<p>Adding the comment of Robert Israel. In the case that numerical issues would arise caused by $u_x,u_y$ being close to zero you can take a $\vec{v}=(u_z,0,-u_x)$ or $(0,u_z,-u_y)$ as an alternative. This might occur/become important if you have to generate many such points on different randomly tilted circles.</p>

<p>Outlined above is one of the standard ways of solving the problem, where in a computer code for particular problems that require consistent high numerical accuracy different cases will have to be considered. </p>
"
"2387248","2389180","<p>For 5.10:</p>

<p>Suppose that $f=gh$ in $R[X]$, then in $\bar{R}[X]$ this becomes $\bar{a}\bar{\phi}^m=\bar{g}\bar{h}$. Then, since $\bar{\phi}$ is prime, we see that $\bar{g}=\bar{c}\bar\phi^l$ and $\bar{h}=\bar{c'}\bar{\phi}^k$ with $k+l=m$ and $\bar{c}\bar{c'}=\bar{a}$. We can suppose that $cc'=a$ and write $g=c\phi^l+\pi q$ and $h=c'\phi^k+\pi q'$ for some $q,q'\in R[X]$.</p>

<p>Now we have $$f\equiv a\phi^m+\pi(c\phi^lq'+c'\phi^kq)\bmod \pi^2,$$ so $$ \bar{\psi}=\overline{\left(\frac{f-a\phi^m}{\pi}\right)}=\bar{c}\bar{\phi}^l\bar{q'}+\bar{c'}\bar{\phi}^k\bar{q}.$$ Passing to $\bar{R}[X]/(\bar{\phi})$, the hypothesis for $\bar{\psi}$ shows that $l$ or $k$ is $0$, meaning that, say, $\bar{g}$ is constant. But since $a$ is prime with $\pi$, the dominant coefficient of $g$ is prime with $\pi$, so $\deg g=\deg\bar{g}$. Therefore, $g$ is constant and must be a unit in $R$ since otherwise $f$ couldn't be primitive.  </p>

<p>5.11: You have almost solved it yourself.</p>

<p>If $g\in Z[X]$ has its all complex roots of absolute value strictly less than 1, then their product has absolute value strictly less than $1$ also. But the constant coefficient of $g$ is of absolute value at least 1 and is equal to $(-1)^{\deg g}\times$ product of the roots of $g$. Therefore a contradiction.</p>

<p>You can use the norm if $g$ is irreducible, which is completely legit to assume since any polynomial in $Z[X]$ is divisible by an irreducible polynomial (here we should remember that $Z[X]$ is a UFD). </p>
"
"2387269","2389640","<p>By Stolz-Cesaro,</p>

<p>$$\lim_{n \to \infty} \frac{a_n}{n} = \lim_{n \to \infty} (a_{n+1} - a_n) = 0,$$</p>

<p>and if $a_n \to a$,</p>

<p>$$\lim_{n \to \infty} \frac{S_n}{n} = \lim_{n \to \infty}a_n = a $$</p>

<p>Consequently, to find a counterexample for part (2) we need a sequence where $\lim_{n \to \infty}a_n$ does not exist and $|a_n|$ grows no faster than $n$.</p>

<p>A suitable counterexample is $a_n = \sin \sqrt{n}$. </p>

<p>The limit does not exist, since the subsequence $a_{n^2} = \sin n$ is easily shown not to converge. We also have 
$$|\sin \sqrt{n+1} - \sin \sqrt{n}| \leqslant \sqrt{n+1} - \sqrt{n} = \frac{1}{\sqrt{n+1} + \sqrt{n}} \to 0,$$</p>

<p>as well as,</p>

<p>$$\tag{*}\sum_{k=1}^n \sin \sqrt{k} = O(\sqrt{n}),$$</p>

<p>whence,</p>

<p>$$\lim_{n \to \infty} \frac{1}{n}\sum_{k=1}^n \sin \sqrt{k} = 0$$</p>

<p>The result (*) can be proved by comparing the sum with $\int_1^n \sin \sqrt{x} \, dx = O(\sqrt{n})$. </p>
"
"2387278","2387281","<p>If  $a=b=0$ and the root is $0$.</p>

<p>However, if $(a,b) \neq 0$, then </p>

<p>the discriminant is equal to $-4(a^4+4b^4)&lt;0$, hence the roots are imaginary.</p>
"
"2387296","2387318","<p>By Cayley's theorem every group is isomorphic to a subgroup of  $S_n $.  Then $S_n $ is isomorphic to a subgroup of $A_{n+2}$.  To see this,  map even permutations to themselves,  and for odd permutations map to the even permutation gotten by multiplying by the transposition  $(n+1 \ n+2) $.</p>
"
"2387317","2387323","<p>Let $e$ be the intersection line of planes $P_1$ and $P_2$, and let their normals be $n_1$ and $n_2$.</p>

<p>Then $n_1$ is perpendicular to all lines of $P_1$, in particular, to $e$. Similarly, $n_2$ is perpendicular to $e$.</p>

<p>Hence, <em>the direction</em> of $e$ can be obtained as a vector orthogonal to both $n_1$ and $n_2$, which means (a scalar multiple of) $n_1\times n_2$.</p>

<p>To actually find $e$, we also need a point on $e$ (i.e. we have to identify one point -anyhow- which lies on both planes $P_1$ and $P_2$).</p>
"
"2387327","2388008","<p>Since the degrees in the O-notation terms are all equal to or larger than the degree in their respective equation, it can safely be assumed that the author of the question was implying as $ x \rightarrow \infty $.</p>

<hr>

<p>In the most basic terms, big-O says ""grows no faster than"". So, for $O(x^5)$, the largest degree of any polynomial that satisfies this notation will be $x^5$.</p>

<p>Little-o is to big-O what $&gt;$ is to $\ge$. So, for $ o(x^5) $, the largest degree of any polynomial that satisfies this notation will be $x^4$, assuming we're using integral powers.</p>

<p>If dealing with </p>

<p>There is actually a different notation for each inequality sign:</p>

<ul>
<li>$O(x^5)$ implies degree $\le 5$. </li>
<li>$o(x^5)$ implies degree $&lt; 5$. </li>
<li>$\Theta(x^5)$ implies degree $= 5$. </li>
<li>$\Omega(x^5)$ implies degree $&gt; 5$. </li>
<li>$\omega(x^5)$ implies degree $\ge 5$. </li>
</ul>

<p>If dealing with $ x \rightarrow 0 $, simply use these inequalities bottom to top. This is because as $ x \rightarrow 0 $, $ x^n \rightarrow 0 $, and the notations specifically talk about growth (i.e the original ones talk about decay, so it goes bottom to top).</p>

<p><strong>Correct equations:</strong></p>

<blockquote class=""spoiler"">
  <p> A, D and E.</p>
</blockquote>
"
"2387335","2387342","<p><strong>Step 1.</strong> Define $T_a : L^1(\mathbb{R}) \to L^1(\mathbb{R})$ by </p>

<p>$$ T_a f (x) = f(x-a). $$</p>

<p>Then this function has the following properties:</p>

<ol>
<li>$T_a$ is linear.</li>
<li>$\|T_a f\|_{L^1} \leq \|f\|_{L^1}$ for all $f \in L^1(\mathbb{R})$ and for all $a$.</li>
<li>For each $g \in C_c(\mathbb{R})$, i.e. if $g$ is continuous and compactly supported, we have $ T_a g \to g$ in $L^1$ as $a \to 0$.</li>
</ol>

<p>Probably only the 3rd property requires explanation. Assume that $g$ vanishes outside $[-R, R]$. Then $g$ is uniformly continuous on $[-R, R]$ and hence on $\mathbb{R}$. Then for each $\epsilon &gt; 0$, there exists $\delta \in (0, 1)$ such that $|g(x) - g(y)| &lt; \frac{\epsilon}{2R+2}$ whenever $|x - y| &lt; \delta$. Then for $|a| &lt; \delta$, we have</p>

<p>$$ \| T_a g - g \|_{L^1}
= \int_{-R-1}^{R+1} |g(x) - g(x-a)| \, dx
\leq \int_{-R-1}^{R+1} \frac{\epsilon}{2R+2} \, dx
= \epsilon. $$</p>

<p>This proves the 3rd property. (Or the bounded convergence theorem gives a one-line proof for this property.)</p>

<hr>

<p><strong>Step 2.</strong> In this step, forget everything about the specific definition of $T_a : L^1(\mathbb{R}) \to L^1(\mathbb{R})$ and recall only the 3 properties listed above. Recapitulating, they are</p>

<ol>
<li>$T_a$ is linear.</li>
<li>$\|T_a f\|_{L^1} \leq \|f\|_{L^1}$ for all $f \in L^1(\mathbb{R})$ and for all $a$.</li>
<li>For each $g \in C_c(\mathbb{R})$, we have $ T_a g \to g$ in $L^1$ as $a \to 0$.</li>
</ol>

<p>Then for any $f \in L^1(\mathbb{R})$ and for any $g \in C_c(\mathbb{R})$, we have</p>

<p>\begin{align*}
\| T_a f - f \|_{L^1}
&amp;\leq \| T_a f - T_a g \|_{L^1} + \| T_a g - g \|_{L^1} + \| g - f \|_{L^1} \\
&amp;\leq \| T_a g - g \|_{L^1} + 2\| g - f \|_{L^1}.
\end{align*}</p>

<p>You may recognize this as our good old $3\epsilon$-argument. Then taking $\limsup$ as $a \to 0$ gives</p>

<p>$$ \limsup_{a\to 0} \| T_a f - f \|_{L^1} \leq 2\| g - f \|_{L^1}. $$</p>

<p>But since $C_c(\mathbb{R})$ is dense in $L^1(\mathbb{R})$ in $L^1$-norm, we can send $g \to f$ in $L^1$ and thus the RHS can be made arbitrarily small. Therefore $\limsup_{a\to 0} \| T_a f - f \|_{L^1} = 0$ and hence</p>

<p>$$ \lim_{a\to 0} \| T_a f - f \|_{L^1} = 0. $$</p>
"
"2387336","2387374","<p>Your proof is fine, but I think it's a little easier if you <strong>define</strong> the numbers $F_n$ by the recurrence
$$F_n=F_0F_1\cdots F_{n-1}+2$$
and then <strong>prove</strong> the identity $F_n=2^{2^n}+1$ by induction.</p>

<p>Basis step:
$$F_0=3=2^{2^0}+1$$
Inductive step:
$$F_{n+1}=F_0F_1\cdots F_{n-1}F_n+2$$
$$F_{n+1}-2=(F_0F_1\cdots F_{n-1})F_n=(F_n-2)F_n=\left(2^{2^n}-1\right)\left(2^{2^n}+1\right)=\left(2^{2^n}\right)^2-1=2^{2^{n+1}}-1$$
$$F_{n+1}=2^{2^{n+1}}+1$$</p>
"
"2387338","2387434","<p>You are, more or less, looking at different <em>coordinate</em> systems. </p>

<p>The conjugate prior distribution is for a <em>rate</em> parameter, while the other is the distribution for a <em>time</em> random variable.</p>

<p>$$\bbox[2ex]{\underbrace{\quad \lambda}_{\text{a rate}} \sim \mathcal{Gamma}(n_0,\underbrace{t_0\qquad}_{\text{time interval}})} \\ \text{versus}\\\bbox[2ex]{\underbrace{\qquad\quad X}_{\text{a time interval}} \sim \mathcal{Gamma}(1 ,\underbrace{\nu\quad}_{\text{a rate}})}$$</p>

<p>That is all.</p>
"
"2387341","2387343","<p>The period will be the lowest common multiple of the generators of the ideals making up the union. </p>

<p>However, for the converse, if $T=\{5\}$ then $\chi_T$ is periodic, but in what sense is $T$ a union of ideals? Am I missing something obvious?</p>
"
"2387347","2388609","<p>I've done a lot of searching and reading on the web since posting the question but I don't understand all the concepts in all the articles so I may have overlooked something...</p>

<p>It turns out <strong>my intuition was only ""slightly right""</strong>, but mostly wrong.</p>

<p>There are only two constraints if the 4 points are not considered to have an intrinsic order:</p>

<ol>
<li>The four points must form a <strong><em>convex</strong> quadrilateral</em>.</li>
<li>No three points can lie on the same line. (The degenerate case where the points actually form a triangle. Not illustrated.)</li>
</ol>

<p><a href=""https://i.stack.imgur.com/bnzkc.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bnzkc.gif"" alt=""types of quadrilaterals""></a></p>

<p>I didn't think about this when posting the question, but if the points are considered to have an intrinsic order then there is a constraint that no two edges may cross one another. But this is just one kind of non-convex quadrilateral anyway, so is already covered.</p>

<p>My intuition was that there would be a lot more to constrain acceptable sets of four points than this, so in that I was wrong.</p>

<p>I now believe that for any given set of four points forming a convex quadrilateral there are exactly two squares in 3D space than can map to them via some perspective projection onto the 2D plane, and an infinite number of rectangles. (I did not fully understand every concept in the articles I read to reach this conclusion though, so I may still be in error.)</p>
"
"2387348","2387356","<p>Assuming I've understood your question correctly:</p>

<p>$(A)$ There are $6!$ ways in which Doggo will sit on the left edge, as he goes on the left edge and the remaining 6 can go in any order. Similarly, there are $6!$ ways he can sit on the right edge. So there are $2\times 6!$ ways he sit on the edge. So probability he does not sit on the end is $\frac{7!-2(6!)}{7!}=\frac{7-2}{7}=\frac57$</p>

<p>Alternatively we note that he is equally likely to go in any place, so $5/7$ chance he goes in the middle.</p>

<p>$(B)$ You're correct that the probablility that they will sit next to each other is $\frac{2!\times 6!}{7!}$ but since we're dealing with probabilities the probability that they will not sit next to each other is $1-\frac{2!\times 6!}{7!}=\frac57$ once again.</p>

<p>If ever you get a probability (much) bigger than $1$ you've gone wrong somewhere.</p>
"
"2387351","2387370","<p>Okay, I figured out where I was getting hung up. I was thinking $x$ would be any number and that $n$ meant infinity (I gotta stop doing that!?) </p>

<p>So my thought process was like ""Oh, if $n = 5$ then I have 5 solutions, but if $n = \infty$ then I have infinity solutions so that means there are infinity solutions for $x^n = 1$ this was super flawed :( Honestly, it seems like the question is a little wild.</p>

<p>So, for any kids who might stumble upon this question with the same ????? I had on my face - $n$ is just some number. Consider the easy fact that when $x \in \mathbb{R}, x^n = 1$ has one solution when $n$ is odd, and 2 solutions when $n$ is even (think about this!)</p>

<p>Add in the complex numbers and now you're looking at roots of unity which tells us.... what? ;) Check out those patterns too. if I choose $n = 5$ how many solutions are there for $x^5 = 1$ with $x \in \mathbb{C}$?</p>

<p>Everyone gave great hints, I think I just didn't really know what I needed to know until I knew it.</p>

<p>Feel good knowing that it (hopefully) didn't take you two days of back and forth's with a mathematician to figure it out.</p>
"
"2387353","2387363","<p>Dividing by $y^n$, we see that you only need to prove it for $y=1$.  So you need that:</p>

<p>$$(ax+(1-a))^n\leq ax^n+1-a,\text{ for all }a\in[0,1].$$</p>

<p>Rewrite this as:</p>

<p>$$(1+a(x-1))^n\leq 1+a(x^n-1)$$</p>

<p>You can prove this by induction:</p>

<p>$$\begin{align}(1+a(x-1))^{n-1}&amp;\leq 1+a(x^{n-1}-1)\\
(1+a(x-1))^{n}&amp;\leq\left(1+a(x^{n-1}-1)\right)(1+a(x-1))\\
&amp;=1+a(x^{n-1}-1)+a(x-1)+a^2\left(x^{n}-x^{n-1}-x+1\right)\\
&amp;\leq 1+a(x^n-1)+(a^2-a)(x^{n-1} - 1)(x-1)
\end{align}$$</p>

<p>But $\alpha^2-\alpha\leq 0$  and $(x^{n-1}-1)(x-1)\geq 0$, so you get:</p>

<p>$$(1+a(x-1))^n\leq 1+a(x^n-1)$$</p>
"
"2387357","2387439","<p>If the limit exists then $m=M$ and they are equal to the limit. This is because there is only one sub sequential limit, so the infimum and supremum of the set of subsequential limits coincide.</p>

<p>Note also that $n\left(\frac{2n+1}{2n+2}-1 \right) = \frac{-n}{2n+2}$. So your limit should be $-1/2$.</p>
"
"2387364","2387369","<p>The wonderful thing about math is that you don't have to rely on your feelings to get the right answer.</p>

<p>You have two functions
$$
f(x) = 2\log(x) \\
g(x) = \log(x^2)
$$
Indeed for $x &gt; 0$ you have $f(x) = g(x)$. Both functions aren't defined for $x=0$. The domain of $f$ is all positive real numbers, but the domain of $g$ also contain the negative numbers. So the two functions aren't equal.</p>

<p>It is possible that you entered the functions wrong on your calculator. You might have entered $\log(x^2)$ as $(\log(x))^2$ and in this case you  obviously get something different. So make sure you have the right parentheses.</p>
"
"2387368","2393566","<p>Let 
$$
G_n:=\max\{length(S), \ \text{such that $S$ is a solid sequence and } S\subset\{1,\dots,n\}\}.
$$
Then one has $G_1=1$, $G_2=3$, $G_3=7$.</p>

<p>Your claim is that $G_{2^{m-1}-1}&lt;2^m$. This is false. The example below shows that 
$$
G_{F_n}\ge 2^{n-1}-1,\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad (Ex) 
$$ 
where $F_n$ is the $n$th term in the Fibonacci sequence. In particular,
for $m=5$, we have $15=2^{m-1}-1&gt;F_7=13$, and so 
$$
G_{15}=G_{2^{m-1}-1}\ge G_{F_7}\ge 2^{6}-1&gt;2^m=32.
$$
In particular, the solid sequence of length $63$ with no term higher than $15$ is
$$
(1,13,8,13,5,13,8,13,3,13,8,13,5,13,8,13,2,13,8,13,5,13,8,13,3,13,8,13,5,13,8,13,
$$
$$
1,13,8,13,5,13,8,13,3,13,8,13,5,13,8,13,2,13,8,13,5,13,8,13,3,13,8,13,5,13,8),
$$</p>

<p>Note that $(Ex)$ provides a lower bound (as will do any family of examples), instead of the desired upper bound.</p>

<p>I suspect that this lower bound is also an asymptotic upper bound, but I have no proof.</p>

<p>$\bf{The\ Example}:$
For each $n\ge 1$ we will provide inductively an example of a sequence $S^{(n)}$ with $length(S^{(n)})=2^{n}$, such that 
$(S^{(n)}_1,\dots,S^{(n)}_{2^n-1})$ is a solid sequence, and such that $S^{(n)}\subset\{1,\dots,F_{n+1}\}$.</p>

<p>First terms:</p>

<p>For $n=1$ clearly $S^{(1)}=(1,1)$ satisfies the hypothesis.</p>

<p>For $n=2$ the sequence is $S^{(2)}=(1,2,1,2)$.</p>

<p>For $n=3$ we have $S^{(3)}=(1,3,2,3,1,3,2,3)$ and </p>

<p>for $n=4$ we have $S^{(4)}=(1,5,3,5,2,5,3,5,1,5,3,5,2,5,3,5)$.</p>

<p>The inductive construction is the following:</p>

<p>We set 
$$
S^{(n)}_{2k}:=F_{n+1}\quad\text{and}\quad S^{(n)}_{2k-1}:=S^{(n-1)}_{k}\quad\text{for $k=1,\dots, 2^{n-1}$}.
$$</p>

<p>Now we prove inductively that $(S^{(n)}_1,\dots,S^{(n)}_{2^n-1})$ is a solid sequence.</p>

<p>For this we prove a little more:</p>

<p>(1.) For each $S=S^{(n)}$ and any two consecutive sets $A,B\subset S^{(n)}$, if $\#A&gt;\#B$, then $\sum_{i\in A}S_i&gt; \sum_{i\in B}S_i$.</p>

<p>(2.) For each $S=S^{(n)}$ and any two consecutive sets $A,B\subset S^{(n)}$, if $\#A=\#B+1$, then $\sum_{i\in A}S_i- \sum_{i\in B}S_i&lt;F_{n+2}$.</p>

<p>(3.) For each $S=S^{(n)}$ and any two consecutive sets $A,B\subset S^{(n)}$, if $\#A=\#B$, then $|\sum_{i\in A}S_i- \sum_{i\in B}S_i|&lt;F_{n+1}$.</p>

<p>(4.) For each $S=S^{(n)}$ and any two consecutive sets $A,B\subset S^{(n)}$, if $\#A=\#B$, then $\sum_{i\in A}S_i\ne \sum_{i\in B}S_i$, except when 
$\#A=\#B=2^{n-1}$ (and so $A\cup B=S^{(n)}$). </p>

<p>Here we consider at the same time the two cases in which $A$ lies above $B$ and viceversa.</p>

<p>For $n=1,2,3$ one can verify (1.)--(4.) by hand.</p>

<p>Assume (1.), (2.), (3.), (4.)  are true for $n-1$. </p>

<p>We first prove (3.) and (4.).</p>

<p>If $\#A=\#B =2k$, then (3.) and (4.) are clear form the inductive hypothesis, since both sets have the same number of $F_{n+1}$'s. Once you remove them, 
by inductive hypothesis the sums satisfy the required conditions (one has to be careful with the case $A\cup B= S^{(n)}$).</p>

<p>If $\#A=\#B =2k-1$, then one of them has one term more with value $F_{n+1}$, and the other has one term more of $S^{(n-1)}$.
Assume $A$ has one term more with value $F_{n+1}$ and write $A_{n-1}$ and $B_{n-1}$ for the sets $A$ and $B$ with all the terms $F_{n+1}$ removed. 
Then
$$
0&lt;\sum_{i\in A}S^{(n)}_i-\sum_{i\in B}S^{(n)}_i=F_{n+1}-(\sum_{i\in B_{n-1}}S^{(n-1)}_i-\sum_{i\in A_{n-1}}S^{(n-1)}_i)&lt;F_{n+1},
$$
which proves (3.) and (4.) in this case.
In fact, the equality is clear, and the last inequality follows from the inductive hypothesis item (1.), since $\#A_{n-1}&lt;\#B_{n-1}$. The first inequality follows from the inductive hypothesis item (2.), since $\#B_{n-1}=\#A_{n-1}+1$, and so 
$$
\sum_{i\in B_{n-1}}S^{(n-1)}_i-\sum_{i\in A_{n-1}}S^{(n-1)}_i&lt;F_{(n-1)+2}=F_{n+1}.
$$</p>

<p>Now we prove (1.). </p>

<p>Assume $\#A&gt;\#B$ and define $A_{n-1}$ and $B_{n-1}$ as before. Then 
$$
\sum_{i\in A}S_i-\sum_{i\in B}S_i=F_{n+1}(\#\{F_{n+1}\ \text{ in }A\}-\#\{F_{n+1}\ \text{ in }B\})+ \sum_{i\in A_{n-1}}S^{(n-1)}_i-\sum_{i\in B_{n-1}}S^{(n-1)}_i
$$
and there are two cases:</p>

<p>a) $\#A_{n-1}&gt;\# B_{n-1}$ and $\#\{F_{n+1}\ \text{ in }A\}$ is greater than or equal to $\#\{F_{n+1}\ \text{ in }B\}$. In this case clearly 
$\sum_{i\in A_{n-1}}S^{(n-1)}_i-\sum_{i\in B_{n-1}}S^{(n-1)}_i&gt;0$ by inductive hypothesis item (1.) and so item (1.) for $n$ follows directly.</p>

<p>b) $\#A_{n-1}=\# B_{n-1}$ and $\#\{F_{n+1}\ \text{ in }A\}$ is greater than $\#\{F_{n+1}\ \text{ in }B\}$. In this case, if 
$\sum_{i\in A_{n-1}}S^{(n-1)}_i-\sum_{i\in B_{n-1}}S^{(n-1)}_i&gt;0$, then item (1.) for $n$ follows directly.
Else $0&lt;\sum_{i\in B_{n-1}}S^{(n-1)}_i-\sum_{i\in A_{n-1}}S^{(n-1)}_i&lt;F_n$ by inductive hypothesis item (3.), and so
$$
\sum_{i\in A}S_i-\sum_{i\in B}S_i\ge F_{n+1}-\left( \sum_{i\in B_{n-1}}S^{(n-1)}_i-\sum_{i\in A_{n-1}}S^{(n-1)}_i\right)&gt;F_{n+1}-F_n&gt;0.
$$</p>

<p>Finally we prove (2.).</p>

<p>Assume $\#A=\#B+1$ and define $A_{n-1}$ and $B_{n-1}$ as before. Then 
$$
\sum_{i\in A}S_i-\sum_{i\in B}S_i=F_{n+1}(\#\{F_{n+1}\ \text{ in }A\}-\#\{F_{n+1}\ \text{ in }B\})+ \sum_{i\in A_{n-1}}S^{(n-1)}_i-\sum_{i\in B_{n-1}}S^{(n-1)}_i
$$
and there are two cases:</p>

<p>a) $\#A_{n-1}&gt;\# B_{n-1}$ and $\#\{F_{n+1}\ \text{ in }A\}$ is equal to $\#\{F_{n+1}\ \text{ in }B\}$. In this case clearly 
$\sum_{i\in A_{n-1}}S^{(n-1)}_i-\sum_{i\in B_{n-1}}S^{(n-1)}_i&lt;F_{n+1}&lt;F_{n+2}$ by inductive hypothesis item (2.) and so item (2.) for $n$ follows directly.</p>

<p>b) $\#A_{n-1}=\# B_{n-1}$ and $\#\{F_{n+1}\ \text{ in }A\}$ is $\#\{F_{n+1}\ \text{ in }B\}$ plus one. In this case, 
by inductive hypothesis item (3.) we have 
$$
\sum_{i\in A_{n-1}}S^{(n-1)}_i-\sum_{i\in B_{n-1}}S^{(n-1)}_i&lt; F_n
$$
and so 
$$
\sum_{i\in A}S_i-\sum_{i\in B}S_i = F_{n+1}+\left( \sum_{i\in A_{n-1}}S^{(n-1)}_i-\sum_{i\in B_{n-1}}S^{(n-1)}_i\right)&lt;F_{n+1}+F_n=F_{n+2},
$$
as desired.</p>

<p>Clearly $(1.)$ and $(4.)$ together imply that $(S^{(n)}_1,\dots,S^{(n)}_{2^n-1})$ is a solid sequence.</p>
"
"2387375","2387394","<p>So $E(v(t))$ is $\dfrac{\text{miles}}{\text{gallon}}$, or in calculus terms, $\frac{dm}{dg}$. Your speed is $\frac{dm}{dt}$ (miles per hour). So to get $\frac{dg}{dt}$, the derivative you need, it is $\dfrac{\frac{dm}{dt}}{\frac{dg}{dt}}$, which is $\dfrac{v(t)}{E(v(t))}$.</p>
"
"2387377","2390043","<p>Consider:$$f:\mathbb{R}^2\supset X\rightarrow\mathbb{R}^2,~z=(x,y)\mapsto \zeta=(\xi, \eta)=f(x,y)$$
Let it be $\mathbb{R}$-differentiable.</p>

<p>If you displace a point $z$ from a $z_0$ the linear part of the change in the value of $f$ (what is called the differential of $f$ at $z_0$) goes from zero vector to a vector that is $\mathbb{R}$-linearly related to $z-z_0$, that is $$df_{z_0}(\Delta z) = D_{z_0}\Delta z\tag{1}$$ where $\Delta z=z-z_0$, $D_{z_0}$ is a $\mathbb{R}$-linear operator.</p>

<p>(You can skip this but remember that the differential of $f$ is defined in $\mathbb{R}^2$ and not simply in $X$ as is $f$. The differential is a linear map and among other things its domain must be a linear space and not a subset. Indeed its independent variable usually indicated with $dz$ is a <em>displacement</em> of the independent variable of $f$ and it can be taken greater as you want even if the resultant displaced $z$ goes outside of $X$: that's why some highlights this fact by calling it <em>virtual displacement</em>. When the virtual displacement is such that $z$ remains in $X$ then the <em>real</em> diplacement $\Delta z$ exists and $\Delta z=dz$)</p>

<p><strong>$\mathbb{R}$-conclusion.</strong>
You can easily see in the $\zeta$-plane where $df_{z_0}(\Delta z)$ is sent when $\Delta z$ is $\mathbb{R}$-linearly related to $\Delta z'$ and $\Delta z''$, that is $\Delta z=a\Delta z'+b\Delta z''$ with $a,~b\in\mathbb{R}$, knowing $df_{z_0}(\Delta z')$ and $df_{z_0}(\Delta z'')$ for every $\Delta z'$ and $\Delta z''$, because for $\mathbb{R}$-linearity it must be that $df_{z_0}(a\Delta z' +b\Delta z'') = a\cdot df_{z_0}(\Delta z')+b\cdot df_{z_0}(\Delta z'')$.</p>

<p>Now let $f$ be $\mathbb{C}$-differentiable.</p>

<p>What I've said above before ""$\mathbb{R}$-conclusion"" remains unaltered with the exception of the occurrences of the expression $\mathbb{R}$-linear (included all grammatical variations) that must be substituted for by $\mathbb{C}$-linear.</p>

<p>Now from $(1)$ above, due to the fact each $\mathbb{C}$-linear operator on $\mathbb{C}$-monodimensional spaces is characterized completely by the multiplication by a complex scalar and so by two real parameters: a relative length variation and angular displacement applied to each and every vector, it follows that the vector $df_{z_0}(\Delta z)$ has a relative length variation and angular displacement with respect to $\Delta z$ that are independent from $\Delta z$! (while this was not the case when only $\mathbb{R}$-differentiability was involved).</p>

<p><strong>$\mathbb{C}$-conclusion.</strong>
You can easily see in the $\zeta$-plane where $df_{z_0}(\Delta z)$ is sent when $\Delta z$ is $\mathbb{C}$-linearly related to $\Delta z'$, that is $\Delta z = a\Delta z'$ with $a\in\mathbb{C}$, knowing $df_{z_0}(\Delta z')$ for every $\Delta z'$, because for $\mathbb{C}$-linearity it must be that $df_{z_0}(a\Delta z') = a\cdot df_{z_0}(\Delta z')$.</p>

<p>(That is, if $\Delta z$ has a length $k$ times that of $\Delta z'$ and an angle of $\theta$ w.r.t. $\Delta z'$ then $df_{z_0}(\Delta z)$ will have a length $k$ times that of $df_{z_0}(\Delta z')$ and an angle of $\theta$ w.r.t. $df_{z_0}(\Delta z')$. That's the meaning of $\mathbb{C}$-linearity in $\mathbb{C}$-dimension $1$, that is, $\mathbb{C}$-proportionality)</p>

<p>But in general you cannot see using the same strategy where $df_{z_0}(\Delta z)$ is sent knowing $df_{z_0}(\Delta z')$ and knowing that $\Delta z$ and $\Delta z'$ are only $\mathbb{R}$-linearly related, that is $\Delta z = h(\Delta z')$, where $h$ is an $\mathbb{R}$-linear operator on $\mathbb{C}$. Indeed, this operator is completely determined when it is known what values it takes on two $\mathbb{R}$-linearly independent vectors. Let's say that it is know, for instance, what the values taken on the vectors $1$ and $i$ are (but we can choose other values). Now let's $\mathbb{R}$-decompose $\Delta z'$ w.r.t. $1$ and $i$: $\Delta z'=\frac{\Delta z'+\overline{\Delta z'}}{2}+\frac{\Delta z'-\overline{\Delta z'}}{2} =\frac{\Delta z'+\overline{\Delta z'}}{2}+i\frac{\Delta z'-\overline{\Delta z'}}{2i}$. For this to be a $\mathbb{R}$-decomposition w.r.t. to $1$ and $i$, the coefficients of $1$ and $i$ must be real, and they are if $\overline{\Delta z'}$ is the complex conjugate of $\Delta z'$. Then it follows straighforwardly that $$h(\Delta z') = h(1)\frac{\Delta z'+\overline{\Delta z'}}{2} + h(i)\frac{\Delta z'-\overline{\Delta z'}}{2i} = \frac{h(1)-i\cdot h(i)}{2} \Delta z' + \frac{h(1)+i\cdot h(i)}{2}\overline{\Delta z'}$$</p>

<p>Concluding, it is:</p>

<p>$$df_{z_0}(\Delta z) = df_{z_0}(h(\Delta z')) = \frac{h(1)-i\cdot h(i)}{2} df_{z_0}(\Delta z') + \frac{h(1)+i\cdot h(i)}{2}df_{z_0}(\overline{\Delta z'})$$ that means that knowing a given $\mathbb{R}$-linear relation between $\Delta z$ and $\Delta z'$ you cannot determine the value of $df_{z_0}(\Delta z)$ from just $df_{z_0}(\Delta z')$: you need $df_{z_0}(\overline{\Delta z')}$ as well when $\frac{h(1)+i\cdot h(i)}{2}\ne0$, so in this case $df_{z_0}$ is not $\mathbb{R}$-linear (although it is $\mathbb{C}$-linear).
<strong>End of the $\mathbb{C}$-conclusion.</strong></p>

<p>As you have seen, differences between $\mathbb{R}$- and $\mathbb{C}$-differentiability are to be searched for in the algebraic part of the process not the analytical and recognizing the special character of the $\mathbb{C}$-differentiability from the fact that composition of a $\mathbb{C}$-linear operator (the differential) with a $\mathbb{R}$-linear operator is only a $\mathbb{R}$-linear operator.</p>
"
"2387380","2387453","<p>This is intended to be a non-rigorous approach. Someone else can probably give a rigorous proof based on distributions.</p>

<p>I've found it helpful to keep in mind what the delta function does: 
$\delta (x)=0$ for $x \neq 0$ and $\displaystyle \int_a^b \delta (x)dx = 1$  for $a &lt; 0 &lt; b$ (normalization).</p>

<p>To start, $\delta (h(x))$ will vanish except where $h(x) = 0$. Suppose this happens at $x^*$. We can then write</p>

<p>$$\displaystyle \int_{-\infty}^{\infty} \delta (h(x))dx = \int_{x^* - \epsilon}^{x^* + \epsilon} \delta (h(x))dx$$</p>

<p>When $x \approx x^*$ , $h(x) \approx h'(x^*)(x - x^*)$. Using this with the scaling property, $\delta (ax) = \dfrac{1}{|a|} \delta(x)$,</p>

<p>$$\int_{x^* - \epsilon}^{x^* + \epsilon} \delta (h(x))dx = \int_{x^* - \epsilon}^{x^* + \epsilon} \delta (h'(x^*)(x - x^*))dx = \dfrac{1}{|h'(x^*)|} \int_{x^* - \epsilon}^{x^* + \epsilon} \delta (x - x^*)dx = \dfrac{1}{|h'(x^*)|}$$</p>

<p>The last integral evaluates to $1$ from the normalization property.</p>
"
"2387382","2387392","<p>Basically no: the lying over theorem follows quickly from this fact, so any proof of this fact is essentially a proof of lying over.  Indeed, localizing at $p$, you may assume $A$ is local and $p$ is its maximal ideal.  If $pB$ is proper, then any maximal ideal containing it must lie over $p$.</p>

<p>But if you want a quick proof that does not directly go through lying over, here's one.  We may localize at $p$ and assume $A$ is local with maximal ideal $p$.  If $pB=B$, then there is a finitely generated $A$-subalgebra $B_0\subseteq B$ such that $pB_0=B_0$ (since only finitely many elements of $B$ are involved in writing $1$ as an element of $pB$).  Since $B_0$ is integral over $A$, it is finitely generated as an $A$-module.  But now by Nakayama, $pB_0=B_0$ implies $B_0=0$, which is a contradiction.</p>
"
"2387383","2387387","<p>Checking if the condition of $Av=\lambda v$ holds:</p>

<p>$$\begin{bmatrix} 3 &amp; 0 &amp; 0 \\ 1 &amp; 2 &amp; 0 \\ -4 &amp; 5 &amp;- 1 \end{bmatrix}\begin{bmatrix} 0 \\ 0 \\ 1\end{bmatrix}=\begin{bmatrix} 0 \\ 0 \\ -1\end{bmatrix}=-\begin{bmatrix} 0 \\ 0 \\ 1\end{bmatrix}$$</p>

<p>$$\begin{bmatrix} 3 &amp; 0 &amp; 0 \\ 1 &amp; 2 &amp; 0 \\ -4 &amp; 5 &amp;- 1 \end{bmatrix}\begin{bmatrix} 0 \\ 0.6 \\ 1\end{bmatrix}=\begin{bmatrix} 0 \\ 1.2 \\ 2\end{bmatrix}=2\begin{bmatrix} 0 \\ 0.6 \\ 1\end{bmatrix}$$</p>

<p>They are indeed the right eigen pair.</p>

<p>Remark: In case the confusion is due to obtaining different answer from a book. Eigenvectors are not unique.</p>
"
"2387384","2387389","<p>For (2)</p>

<p>There are $2^{100}$ subsets of a set of $100$ elements (each element is either in or our of the subset). How many of them have fewer than 4 elements? ${{100}\choose{3}}+{{100}\choose{2}}+{{100}\choose{1}}+{{100}\choose{0}}=166751$. So the number of subsets containing at least $4$ elements is $2^{100}-166751$. </p>
"
"2387386","2387420","<p>Once you've chosen representative vectors for the four projective points $p_1,p_2,q_1,q_2$, there will be just one linear transformation sending the representative vectors of the $p_i$'s to those of the $q_i$'s. But there will be a different linear transformation sending your  chosen  representatives of $p_1$ and $p_2$ to $3$ times the chosen representative of $q_1$ and $7$ times the chosen representative of $q_2$. The associated projective transformation will send $p_1$ and $p_2$ to $q_1$ and $q_2$, since the factors $3$ and $7$ don't affect the projective points. But that second linear transformation will not have the same effect as the first on other projective points.</p>
"
"2387402","2387424","<p>It's possible whenever $n \geq 3$.  It's pretty clear that this isn't possible for $n = 1,2$.</p>

<p>Let $\langle x,y \rangle$ denote the ""dot-product"" of the vectors $x$ and $y$.  Then the angle between $x$ and $y$ is $45^\circ$ if and only if $\langle x,y \rangle/(\|x\| \cdot \|y\|) = \cos(45^\circ) = 1/\sqrt{2}$.  If we choose unit vectors $x$ and $y$, then this becomes $\langle x,y \rangle = 1/\sqrt{2}$.</p>

<p>More generally, the angle between vectors is $\cos^{-1}[\langle x,y \rangle/(\|x\| \cdot \|y\|)]$.</p>

<p>In $n$-dimensional space with $n \geq 3$, we can always take
$$
x = (\cos(22.5^\circ),\sin(22.5^\circ),0,0,\dots,0)\\
y = (\cos(22.5^\circ),-\sin(22.5^\circ),0,0,\dots,0)\\
z = (1,0,\alpha,0,\dots,0)
$$
Where $\alpha$ is chosen so that $\langle x,z \rangle/\|z\| = 1/\sqrt{2}$, which is to say that
$$
\alpha = \sqrt{2\cos^2(22.5^\circ) - 1} = \sqrt{\cos(45^\circ)} = \frac{1}{\sqrt[4]{2}}
$$</p>

<hr>

<p>Another viable strategy to find the three vectors: begin with an equilateral triangle in $2$-space, i.e.
$$
(1,0),(-1/2,\sqrt{3}/2),(-1/2,-\sqrt{3}/2)
$$
Then, take
$$
x = (1,0,\alpha)\\
y = (-1/2,\sqrt{3}/2,\alpha)\\
z = (-1/2,-\sqrt{3}/2,\alpha)
$$
Where $\alpha$ is chosen so that
$$
\frac{\langle x,y \rangle}{\|x\|\|y\|} = \frac{1}{\sqrt{2}} \implies\\
\frac{-1/2 + \alpha^2}{1 + \alpha^2} = \frac{1}{\sqrt{2}} \implies\\
-1/\sqrt{2} + \sqrt{2}\alpha^2 = 1 + \alpha^2 \implies\\
(\sqrt{2} - 1)\alpha^2 = \frac{1 + \sqrt{2}}{\sqrt{2}} \implies\\
\alpha^2 = \frac{1 + \sqrt{2}}{\sqrt{2}(\sqrt{2} - 1)} = \frac{4 + 3\sqrt{2}}{2} \implies\\
\alpha = \sqrt{\frac{4 + 3\sqrt{2}}{2}}
$$</p>
"
"2387403","2387417","<p>Yes, that is correct.</p>

<p>There are</p>

<p>$$3 = \frac{3!}{2!(3 - 2)!}$$</p>

<p>disjoint successful outcomes: $(1, 1, n)$, $(1, n, 1)$, and $(n, 1, 1)$. The probability of each is equal to</p>

<p>$$P(1)P(1)(1 - P(1)) = 0.5^2 \cdot 0.5$$</p>

<p>You add $3$ of these together, since any one of them is successful.</p>
"
"2387410","2387441","<p>Remember, when you negate a predicate, you negate ""each part of it"". It's like the negation distributes.</p>

<p>Some rules are: $$\neg\exists=\forall, \neg\forall=\exists,\neg(\lor)=\land, \neg(\land)=\lor,\neg P=\neg P$$
Therefore, distribute the $\neg$
$$\neg(\exists y \forall x ((y &gt; 0) \land (x &lt; y)))\iff \forall y\exists x(\neg((y &gt; 0) \land (x &lt; y))$$</p>

<p>Now, $\neg(y\gt 0)=(y\leq 0)$, ""if its not greater, then it must be less than or equal to""</p>

<p>Similarly, $\neg(x&lt;y)=(x\geq y)$</p>

<p>Finishing out negation off, we get:</p>

<p>$$\iff\forall y \exists x ((y\leq 0)\lor(x\geq y))\text{ notice $\land$ changes to $\lor$}$$</p>
"
"2387412","2387416","<p>Let $N$ be a direct sum of infinitely many copies of $M_1$ and infinitely many copies of $M'$.  Since adding one element to an infinite set doesn't change its cardinality, $M_1\oplus N\cong N$ and $M'\oplus N\cong N$.</p>
"
"2387423","2387461","<p>Let $x+y+z=3u$, $xy+xz+yz=3v^2$ and $xyz=w^3$.</p>

<p>Hence, we need to prove that
$$\sqrt[3]{\frac{27u^2-27uv^2}{w^3}+3}+\sqrt{\frac{v^2}{3u^2-2v^2}}\geq1+\sqrt[3]3,$$
which is $f(w^3)\geq0,$ where $f$ is a decreasing function.</p>

<p>Thus, it's enough to prove our inequality for a maximal value of $w^3$.</p>

<p>Now, $x$, $y$ and $z$ are positive roots of the following equation.
$$(X-x)(X-y)(X-z)=0$$ or
$$X^3-3uX^2+3v^2X-w^3=0$$ or
$$X^3-3uX^2+3v^2X=w^3,$$
which says that the graph of $f(X)= X^3-3uX^2+3v^2X$ and the line $Y=w^3$ </p>

<p>have three common points $(x,f(x))$, $(y,f(y))$ and $(z,f(z)).$</p>

<p>Now, draw the graph of $f$.</p>

<p>Indeed, $f(X)= X^3-3uX^2+3v^2X$, which gives $$f'(X)=3X^2-6uX+3v^2=3(X^2-2uX+v^2)=$$
$$=3\left(X-(u+\sqrt{u^2-v^2})\right)\left(X-(u-\sqrt{u^2-v^2})\right).$$</p>

<p>Thus, $X_{max}=u-\sqrt{u^2-v^2}$, $X_{min}=u+\sqrt{u^2-v^2}$ </p>

<p>and the graph of $f$  goes through origin $(0,0)$.</p>

<p>Draw it, please!   </p>

<p>Let $z\leq y\leq x$, $u$ and $v^2$ be constants and $w^3$ increases. </p>

<p>Hence, $x$, $y$ and $z$ changes and $w^3$ will get a maximal value, </p>

<p>when a line $Y=w^3$ will touch to the graph of $f$ in the maximum point of $f$, </p>

<p>which happens for equality case of two variables (when $z=y=u-\sqrt{u^2-v^2}$ in our case).</p>

<p>Since our inequality is homogeneous, we can assume $y=z=1$ and we need to prove that
$$\sqrt[3]{\frac{x^3+2}{x}}+\sqrt{\frac{2x+1}{x^2+2}}\geq1+\sqrt[3]3$$ or
$$\sqrt[3]{\frac{x^3+2}{x}}-\sqrt[3]3\geq1-\sqrt{\frac{2x+1}{x^2+2}}$$ or
$$\frac{x^3-3x+2}{\sqrt[3]x\left(\sqrt[3]{(x^3+2)^2}+\sqrt[3]{3x(x^3+2)}+\sqrt[3]{9x^2}\right)}\geq\frac{x^2-2x+1}{\sqrt{x^2+2}\left(\sqrt{x^2+2}+\sqrt{2x+1}\right)}$$ or
$$(x+2)\sqrt{x^2+2}\left(\sqrt{x^2+2}+\sqrt{2x+1}\right)\geq\sqrt[3]x\left(\sqrt[3]{(x^3+2)^2}+\sqrt[3]{3x(x^3+2)}+\sqrt[3]{9x^2}\right).$$
Now, by C-S we obtain
$$\sqrt{x^2+2}=\frac{1}{\sqrt3}\sqrt{(1+2)(x^2+2)}\geq\frac{x+2}{\sqrt3}$$ and
$$\sqrt{2x+1}=\frac{1}{\sqrt3}\sqrt{(2+1)(2x+1)}\geq\frac{2\sqrt{x}+1}{\sqrt3}.$$
Also, by AM-GM
$$3x\leq x^3+2.$$
Thus, it's enough to prove that
$$(x+2)^2(x+2\sqrt{x}+3)\geq9\sqrt[3]{x(x^3+2)^2}$$ or
$$(a^2+2)^2(a^2+2a+3)\geq9\sqrt[3]{a^2(a^6+2)^2},\tag1$$
where $a=\sqrt{x}$ and the rest is smooth. </p>

<p>The last inequality follows from the following inequalities.
$$(a^2+2)^5\geq54a(a^6+2)$$ and
$$2(a^2+2)(a^2+2a+3)^3\geq27a(a^6+2).$$
Done!</p>
"
"2387433","2387457","<p>You need to understand what definition of a ""relationship"" is and what it means.</p>

<p>Let's get colloquial.</p>

<p>We can say something simple like ""$a$ and $b$ are related if $b$ is two times $a$"".  So $7$ and $14$ are related and $46$ and $92$ are related but $8$ and $15$ are not related and so on.  Also $14$ and $7$ are not related because the relationship is only one way.  The second number must be twice the first but not vice versa.</p>

<p>So how do we express the idea mathematically?  Not just how to express the idea that <em>one</em> set of $b = 2a$ but the idea of the <em>relationship</em>?</p>

<p>Well, sets.  $X \times Y$ is the set of all pairs $(x,y)$.  So we want all the pairs $(x,y)$ in $\mathbb Z \times \mathbb Z$ where $x$ is related to $y$.</p>

<p>In this case we want $R\subset \mathbb Z \times \mathbb Z$ where $R = \{(a,b)| b = 2*a\}$.</p>

<p>In other words $R$ is the set of all pairs of integers, where the integers are related in a certain way.  In math a ""RELATION"" is a set of ordered pairs where the pair have a certain condition.</p>

<p>In this case the relation is that $R = \{(a,b)| (a,b) \not \in \{(n,n)\}\}$ which can be written as $R = \mathbb Z \times \mathbb Z \setminus \{(n,n)|n \in \mathbb Z\}$.</p>

<p>So $a$ and $b$ are related if $(a,b)$ is NOT in $\{(n,n)|n \in \mathbb Z\}$.</p>

<p>So what is a way of saying that in simple english?</p>

<p>Hint: maybe first would be, how would you describe the relation between $a$ and $b$ if $(a,b)$ WERE in $\{(n,n)|n \in \mathbb Z\}$?  So how would you describe it if they were not?</p>
"
"2387437","2387449","<p>You need to find the min and max of $f$ on $x\in[-3,5]$, which doesn't necessarily have to be at endpoints. They can also lie on critical points, where $f'(x) = 0$ or is undefined. Here $f'(x)=2x$, which is $0$ at $x=0$. So the minimum and maximum could be located at $x=-3$, $x=0$, or $x=5$. Now just check,</p>

<p>$$ f(0) &lt; f(-3) &lt; f(5) $$</p>

<p>so the range $f([-3,5]) = [f(0),\ f(5)] = [3, 28]$ .</p>
"
"2387448","2387454","<p>If you use the binomial law the probability to get 1 once is</p>

<p>$$\mathbb{P}(\textrm{1 appears exactly once}) = {3 \choose 1}0.5^1 (0.3+0.2)^{3-1} = 3 \cdot 0.5 \cdot 0.5^2 = 0.375$$</p>

<p>And you get the same result as the truth table.</p>

<p>The way you applied the binomial law is indeed incorrect. Recall that the formula is $P(X=k)={n \choose k}p^k(1-p)^{n-k}$ if you repeat $n$ times an experiment and you want $k$ successes (with the probability of a success being $p$). $X$ is the random variable counting the number of successes.</p>

<p>In your case $p=0.5$, $k=1$ and $n=3$.</p>
"
"2387459","2391743","<p>""How to interpret it as a function of two variables?"" you asked.</p>

<p>Well indeed it is already a function of two (real) variables or one complex variable.
When it comes to differentiability a function of two real variables (or one complex variable, that is the same thing) can be analyzed under two kinds of differentiabilities: $\mathbb{R}$-differentiability and $\mathbb{C}$-differentiability.</p>

<p>A function of two real variables $f: \mathbb{R}^2\supseteq A\rightarrow\mathbb{R}^2, z \mapsto f(z)$ is $\mathbb{R}$-differentiable at a given point $z_0=(x_0,y_0)$ when its displacement at that point ($\Delta f_{z_0}: A-z_0\rightarrow\mathbb{R}^2,\Delta z\mapsto f(z)-f(z_0)$) under a general displacement of the independent variables $z-z_0$ (usually indicated by $\Delta z$) has main component that is a $\mathbb{R}$-linear operator on $\mathbb{R}^2$ (restricted to a subset of $\mathbb{R}^2$ made up of those $\Delta z$'s that send $z_0$ in a point of the domain $A$ of the function $f$, that is, $A-z_0$):</p>

<p>$$f(z)=f(z_0)+D_{z_0}\Delta z+o(\lvert\Delta z\rvert)\tag{1}$$</p>

<p>This main component (extended by $\mathbb{R}$-linearity to the whole $\mathbb{R}^2$, that is, the said $\mathbb{R}$-linear operator) is called $\mathbb{R}$-differential of the function $f$ at $z_0$: $df_{z_0}: \mathbb{R}^2\rightarrow\mathbb{R}^2, dz\mapsto D_{z_0}dz$, where $dz$ is its independent variable usually indicated in that way and named <em>virtual</em> (in the sense that it is not always admissible for $(1)$ to have sense) <em>displacement</em> to distinguish it from $\Delta z$, the ""admissible"" displacement.</p>

<p>This is a $\mathbb{R}$-linear operator on $\mathbb{R}^2$ and so for its complete determination at most two of its values are needed (in the sense that if it is non-singular two values are needed, else less). If we are given its values at $dz=(1,0)$ and $dz=(0,1)$, let's say $a=df_{z_0}(1,0)$ and $b=df_{z_0}(0,1)$, then we can rewrite $(1)$ as<br>
$$f(z)=f(z_0)+a\Delta x+b\Delta y+o(\lvert\Delta z\rvert)\tag{2}$$</p>

<p>where $\Delta x$ and $\Delta y$ are the $\mathbb{R}$-linear components of $\Delta z$ w.r.t. the chosen vectors $(1,0)$ and $(0,1)$ respectively.
Notice that we can choose equivalently other vectors $\mathbb{R}$-linearly independent.</p>

<p>What's important to remember before turning our attention to the $\mathbb{C}$-differentiability is that in $\mathbb{R}$-differentiability no restrictions have been made about the kind of linear operator.</p>

<p>The definition of $\mathbb{C}$-differentiability of $f$ is the same as that of its $\mathbb{R}$-differentiability with the exception that where you read $\mathbb{R}$-linear it must be now read $\mathbb{C}$-linear. If you do what I say, you'll notice that this happens only at ""$\mathbb{R}$-linear operator on $\mathbb{R}^2$"" that now becomes ""$\mathbb{C}$-linear operator on $\mathbb{R}^2$"". This means that equation $(1)$ is valid, with $D_{z_0}$ being now a $\mathbb{C}$-linear operator.</p>

<p>The $\mathbb{C}$-linear operators on $\mathbb{R}^2$ are the operators ""multiplication of a vector by a complex scalar"" and nothing more: this is because $\mathbb{R}^2$ over $\mathbb{C}$ as a vector space is monodimensional, so there cannot be more $\mathbb{C}$-linear operators on it as there are $\mathbb{C}$-scalars. Each of them is characterized uniquely by a complex scalar and so by two real parameters: the relative increment of the length and the oriented angular displacement of the transformed vector w.r.t. to the vector on which they are applied. Notice that this parameters do not depend on the ""argument"".</p>

<p>You can also check that they are also $\mathbb{R}$-linear as operators on $\mathbb{R}^2$. This means that we can use also equation $(2)$ that was derived by $(1)$ with the assumpion that $D_{z_0}$ were $\mathbb{R}$-linear, because as said $\mathbb{C}$-linearity is a special $\mathbb{R}$-linearity. The purpose is to see what constraints are to be imposed on the chosen values (the vectors $a$ and $b$) of the $\mathbb{R}$-differential to make it a $\mathbb{C}$-differential, that is when it is imposed that the differential must be $\mathbb{C}$-linear instead of $\mathbb{R}$-linear. Then let's try to go from $(2)$ to $(1)$, where $D_{z_0}$ is $\mathbb{C}$-linear.
$$f(z)=f(z_0)+a\Delta x+b\Delta y+o(\lvert\Delta z\rvert)=f(z_0)+a\Delta x+\frac{b}{i}i\Delta y+o(\lvert\Delta z\rvert)=f(z_0)+D_{z_0}\Delta z+o(\lvert\Delta z\rvert)$$</p>

<p>Being $\Delta x+i\Delta y=\Delta z$, it results that must be:
$$\frac{b}{i}=a\tag{3}$$</p>

<p>that is, $b$ must be at $\pi$ radians from $a$ (and $D_{z_0}=a$). This equation is known as Cauchy-Riemann equation.</p>

<p>On the contrary, you can proceed from the other direction. If you know that $D_{z_0}$ is a $\mathbb{C}$-linear operator on $\mathbb{R}^2$ (that is, a complex number) it is a $\mathbb{R}$-linear operator on $\mathbb{R}^2$ that takes each vector to a vector whose length has a relative increment of $A$ and whose angular position has been displaced by $\theta$, where $A$ and $\theta$ characterize completely the $\mathbb{C}$-linear operator, that is $a=A(\cos\theta+i \sin\theta)$ being the transformation of $1$ and $b=A(-\sin\theta+i \cos\theta)$ being the transformation of $i$. So $b$ is displaced by $\pi$ radians from $a$, $a=b/i$. Putting this into $(2)$, you get $(1)$.</p>
"
"2387460","2387471","<p>If $a_n=\frac{1}{n}$, then
$$\lim_{n\to\infty}\frac{|a_{n+1}|}{|a_n|}=\lim_{n\to\infty}\frac{n+1}{n}=1$$ but $\sum_{n=1}^\infty |a_n|=\sum_{n=1}^\infty\frac{1}{n}$ diverges. This example shows that we cannot conclude that $\sum_{n=1}^\infty |a_n|$ converges knowing $\lim_{n\to\infty}\frac{|a_{n+1}|}{|a_n|}$.</p>

<p>If $a_n=\frac{1}{n^2}$, then
$$\lim_{n\to\infty}\frac{|a_{n+1}|}{|a_n|}=\lim_{n\to\infty}\frac{(n+1)^2}{n^2}=\lim_{n\to\infty}\frac{n^2+2n+1}{n^2}=1$$ and
$\sum_{n=1}^\infty |a_n|=\sum_{n=1}^\infty\frac{1}{n^2}$ converges. This example shows that we cannot conclude that $\sum_{n=1}^\infty |a_n|$ diverges knowing that $\lim_{n\to\infty}\frac{|a_{n+1}|}{|a_n|}$.</p>

<p>These two examples together show that no conclusion can be made about the convergence of $\sum_{n=1}^\infty |a_n|$ based upon knowing that $\lim_{n\to\infty}\frac{|a_{n+1}|}{|a_n|}=1$.</p>
"
"2387469","2387807","<p>Using the brute force algorithm I've obtained the following result:</p>

<p><code>(result_id) [number_of_breads_taken] result_representation (reference_to_identical_result)</code></p>

<blockquote>
  <p>(000) [00] 1234 <br>
  (001) [01] 0144 <br>
  (002) [02] 0035 <br>
  (003) [03] 00241 <br>
  (004) [04] 00132 <br>
  (005) [05] 00023 <br>
  (006) [06] 000121 <br>
  (007) [07] 000012 <br>
  (008) [08] 0000011 <br>
  <strong>(009) [09] 00000001</strong> <br>
  (010) [07] 0001101 <br>
  (011) [08] 0000011 (8)<br>
  (012) [05] 001211 <br>
  (013) [06] 000121 (6)<br>
  (014) [06] 001102 <br>
  (015) [07] 000012 (7)<br>
  (016) [06] 0012001 <br>
  (017) [07] 0001101 (10)<br>
  (018) [04] 002301 <br>
  (019) [05] 001211 (12)<br>
  (020) [02] 01331 <br>
  (021) [03] 00241 (3)<br>
  (022) [03] 01222 <br>
  (023) [04] 00132 (4)<br>
  (024) [04] 01113 <br>
  (025) [05] 00023 (5)<br>
  <strong>(026) [05] 01004</strong> <br>
  (027) [05] 011021 <br>
  (028) [06] 000121 (6)<br>
  (029) [06] 0110101 <br>
  (030) [07] 0001101 (10)<br>
  (031) [04] 012111 <br>
  (032) [05] 001211 (12)<br>
  (033) [05] 011021 (27)<br>
  (034) [05] 012002 <br>
  (035) [06] 001102 (14)<br>
  (036) [05] 0121001 <br>
  (037) [06] 0012001 (16)<br>
  (038) [06] 0110101 (29)<br>
  (039) [03] 013201 <br>
  (040) [04] 002301 (18)<br>
  (041) [04] 012111 (31)<br>
  (042) [01] 1125 <br>
  (043) [02] 0035 (2)<br>
  (044) [02] 1016 <br>
  (045) [03] 10051 <br>
  <strong>(046) [04] 100401</strong> <br>
  (047) [02] 11141 <br>
  (048) [03] 00241 (3)<br>
  (049) [03] 10051 (45)<br>
  (050) [03] 11032 <br>
  (051) [04] 00132 (4)<br>
  (052) [04] 110211 <br>
  (053) [05] 001211 (12)<br>
  (054) [05] 110102 <br>
  (055) [06] 001102 (14)<br>
  (056) [05] 1102001 <br>
  (057) [06] 0012001 (16)<br>
  (058) [03] 111301 <br>
  (059) [04] 002301 (18)<br>
  <strong>(060) [04] 100401</strong> (46)<br>
  (061) [04] 110211 (52)<br>
  (062) [01] 12231 <br>
  (063) [02] 01331 (20)<br>
  (064) [02] 11141 (47)<br>
  (065) [02] 12122 <br>
  (066) [03] 01222 (22)<br>
  (067) [03] 11032 (50)<br>
  (068) [03] 12013 <br>
  (069) [04] 01113 (24)<br>
  (070) [04] 120021 <br>
  (071) [05] 011021 (27)<br>
  (072) [05] 1200101 <br>
  (073) [06] 0110101 (29)<br>
  (074) [03] 121111 <br>
  (075) [04] 012111 (31)<br>
  (076) [04] 110211 (52)<br>
  (077) [04] 120021 (70)<br>
  (078) [04] 121002 <br>
  (079) [05] 012002 (34)<br>
  (080) [05] 110102 (54)<br>
  (081) [04] 1211001 <br>
  (082) [05] 0121001 (36)<br>
  (083) [05] 1102001 (56)<br>
  (084) [05] 1200101 (72)<br>
  (085) [02] 122201 <br>
  (086) [03] 013201 (39)<br>
  (087) [03] 111301 (58)<br>
  (088) [03] 121111 (74)<br>
  44 different cases <br>
  9 breads taken at most <br></p>
</blockquote>

<p>Python script:</p>

<pre><code>class Result:
    id = 0
    level = 0
    value = """"
    refId = -1

    def __init__(self):
        return

results = list()


def solve(result, resultats):
    val = result.value
    for i in xrange(0, val.__len__()-1):
        if val[i] != ""0"" and val[i+1] != ""0"":
            a = int(val[i])
            b = int(val[i+1])
            c = 0
            if (i+2) &lt; val.__len__():
                c = int(val[i+2])
            newres = """"
            for k in xrange(0, max(i+2, val.__len__())):
                character = val[min(k, val.__len__())]
                if k == i:
                    character = str(a-1)
                if k == i+1:
                    character = str(b-1)
                if k == i+2:
                    character = str(c+1)
                newres += character
            if i+2 == val.__len__():
                newres += str(c+1)

            res = Result()
            res.value = newres
            res.id = resultats.__len__()
            res.level = result.level+1
            refid = -1
            for x in resultats:
                if x.value == res.value and x.refId == -1:
                    refid = x.id
            res.refId = refid

            resultats.append(res)
            if refid &lt; 0:
                solve(res, resultats)
    return result


res = Result()
res.value = ""1234""
results.append(res)

solve(res, results)

counter = 0
maxBreadsTaken = 0
for x in results:
    if x.refId &lt; 0:
        counter += 1
    if maxBreadsTaken &lt; x.level:
        maxBreadsTaken = x.level

    pream = ""(""+str(x.id).zfill(3)+"") "" + ""["" + str(x.level).zfill(2)+""] ""
    ref = "" ""
    if x.refId &gt;= 0:
        ref += ""(""+str(x.refId)+"")""
    print pream + x.value + ref + ""&lt;br&gt;""

print str(counter) + "" different cases &lt;br&gt;""
print str(maxBreadsTaken) + "" breads taken at most &lt;br&gt;""
</code></pre>
"
"2387482","2387492","<p>There are some textbooks that do take the historical perspective, e.g.
Priestley's <a href=""https://link.springer.com/book/10.1007%2F978-1-4684-9349-8"" rel=""nofollow noreferrer"">Calculus: An Historical Approach</a>.</p>

<p>One problem is that history tends to be messy.  A new idea often comes out as a by-product of an investigation of something else (often something that we don't care much about any more). The original statement may be quite different from the modern version (often much less general, and lacking a lot of the framework of ideas and terminology that only later grew up around it), and the original proof may be much more complicated than modern proofs.  Often different parts of a theorem arise at different times, and are only later put together.</p>

<p>Case in point: you mentioned Cauchy-Schwarz.  This was ""discovered"" by Cauchy (for sums, in 1821), by Bunyakovsky (for integrals, in 1859), and by Schwarz (again for integrals, in 1888, with something like the modern proof).  There is a nice discussion <a href=""https://hsm.stackexchange.com/questions/3498/who-attached-buniakovskys-name-to-the-cauchy-schwarz-inequality"">here</a>.</p>
"
"2387485","2387490","<p>Hint: Dirichlet's Theorem on primes in arithmetic progression.</p>
"
"2387495","2387536","<p>Regardless of <strong>how</strong> you define differentiation wrt $\beta$, the expected rules still apply.</p>

<p>\begin{align}
   RSS(\beta) &amp;= (\mathbf y- \mathbf X\beta)^T (\mathbf y- \mathbf X\beta) \\
   RSS(\beta)'
      &amp;= (\mathbf 0 - \mathbf X)^T (\mathbf y- \mathbf X\beta)
       + (\mathbf y- \mathbf X\beta)^T (\mathbf 0 - \mathbf X) \\
      &amp;= -\mathbf X^T (\mathbf y- \mathbf X\beta)
       - (\mathbf y- \mathbf X\beta)^T \mathbf X \\
\end{align}</p>

<p>Notice that $(\mathbf y- \mathbf X\beta)^T \mathbf X$ is a scalar. Hence</p>

<p>\begin{align}
   (\mathbf y- \mathbf X\beta)^T \mathbf X
   &amp;= \left((\mathbf y- \mathbf X\beta)^T \mathbf X \right)^T \\
   &amp;= \mathbf X^T (\mathbf y- \mathbf X\beta)
\end{align}</p>

<p>In which case
$$RSS(\beta)' = -2\mathbf X^T (\mathbf y- \mathbf X\beta)$$</p>

<p>and equation $(2.5)$ follows.</p>

<p>Let $F : \mathbb R^p \to \mathbb R$ and let $\mathbf u$ be a unit vector in $\mathbb R^p$. Then we can define the partial derivative 
$$F_{\mathbf u}(\beta) = 
  \lim_{\delta \to 0} 
  \frac{1}{\delta}(F(\beta + \delta \mathbf u) - F(\beta))$$</p>

<p>If the value of $F_{\mathbf u}(\beta)$ is independent of the value of $\mathbf u$, then we can define $\dfrac{d}{d\mathbf x}F(\mathbf x) = F_{\mathbf u}(\mathbf x)$</p>
"
"2387497","2387499","<p>The alternating series test cannot be used to determine whether a series converges absolutely. To show that $\sum_n\frac{1}{n\log^2n}$ converges, I suggest using the integral test, since the integral
$$ \int_2^{\infty}\frac{dx}{x\log^2(x)} $$
can be evaluated by setting $u=\log x$.</p>
"
"2387501","2387537","<p>As Robert Israel answered, it does not seem that the antiderivative could be computed even using special functions.</p>

<p>However, considering $$I=\int_0^a\frac{\text{erf}\left(\frac{x}{c}\right)}{\sqrt{1-x^2}}\,dx$$ hoping that $a$ is not too large, you could expand the integrand as a truncated Taylor series and integrate termwise. Otherwise, numerical integration would be required.</p>

<p>You would get something like
$$\frac{\text{erf}\left(\frac{x}{c}\right)}{\sqrt{1-x^2}}=\frac{2 x}{\sqrt{\pi } c}+\frac{\left(3 c^2-2\right) x^3}{3 \sqrt{\pi }
   c^3}+\frac{\left(45 c^4-20 c^2+12\right) x^5}{60 \sqrt{\pi }
   c^5}+\frac{\left(525 c^6-210 c^4+84 c^2-40\right) x^7}{840 \sqrt{\pi }
   c^7}+O\left(x^9\right)$$</p>

<p>Let us try using $a=\frac 12$ for various values of $c$
$$\left(
\begin{array}{ccc}
 c &amp; \text{exact} &amp; \text{approximation} \\
 1 &amp; 0.1450370 &amp; 0.14500970 \\
 2 &amp; 0.0747909 &amp; 0.07477398 \\
 3 &amp; 0.0501538 &amp; 0.05014195 \\
 4 &amp; 0.0376931 &amp; 0.03768400 \\
 5 &amp; 0.0301833 &amp; 0.03017601 \\
 6 &amp; 0.0251659 &amp; 0.02515974 \\
 7 &amp; 0.0215775 &amp; 0.02157225 \\
 8 &amp; 0.0188842 &amp; 0.01887956 \\
 9 &amp; 0.0167883 &amp; 0.01678417
\end{array}
\right)$$</p>

<p>Another solution would be to use, as Robert Israel answered, integration by parts
$$J=\int\frac{\text{erf}\left(\frac{x}{c}\right)}{\sqrt{1-x^2}}\,dx=\sin ^{-1}(x) \text{erf}\left(\frac{x}{c}\right)-\frac{2}{\sqrt{\pi } c}\int { e^{-\frac{x^2}{c^2}} \sin ^{-1}(x)}\,dx$$ $$K=\int { e^{-\frac{x^2}{c^2}} \sin ^{-1}(x)}\,dx=c\int e^{-t^2} \sin ^{-1}(c t)\,dt$$ Using the Taylor expansion
$$\sin ^{-1}(c t)=\sum^{\infty}_{n=0} \frac{(2n)!\,c^{2n+1}}{4^n (n!)^2 (2n+1)} t^{2n+1}\qquad \text{for}\qquad |ct|\leq 1$$ and use $$\int t^{2n+1}e^{-t^2}\,dt=-\frac{1}{2} \Gamma \left(n+1,t^2\right)$$ </p>
"
"2387505","2387511","<p>The number of pairs $p(n)$ is $n(n-1)/2$. This is because from $n$ instances you can select the first instance in $n$ ways and then there is $n-1$ instances left from which you can select the second instance in $n-1$ ways. Then you divide by two because it doesn't matter in which order you select the first and second instance.</p>

<p>You see that this coincide with your recursive formula:</p>

<p>$$\begin{align}
n-1 + p(n-1) &amp;= n-1 + {(n-1)(n-2)\over 2} \\
&amp;= {2(n-1) + (n-1)(n-2)\over 2} \\
&amp; = {(n-1)(2 + (n-2))\over 2} \\
&amp;= {n(n-1)\over2} = p(n)
\end{align}$$</p>

<p>Your formula can of course also be motivated directly as you have $p(n-1)$ pairs using only the first $n-1$ instances and when not using only those you must use the last instance and one of the $n-1$ first, this makes $p(n-1)+(n-1)$ pairs. Using the fact that $p(1) = 0 = 1(1-1)/2$ you can together with the above use induction to prove the closed form formula too.</p>
"
"2387506","2387519","<p>Let $\xi=\sum_i \xi_i$ and $V = \sum_i V(i)\,$, then multiplying the first equation by $c_1$, the second one by $c_2$, and adding together gives:</p>

<p>$$
\sum_i \frac{c_1V(i)+c_2 V(i)\xi_i}{c_1 + c_2 \xi_i} = c_1 n + c_2 \sum_i \xi_i \;\;\iff\;\; V = c_1 n + c_2 \xi
$$</p>

<p>Substituting $\,c_1=(V- c_2 \xi)/n\,$ back into either equation will give a polynomial of degree $N$ to solve for $c_2$ where $N$ is the number of terms in each sum. That's likely to require solving numerically unless there is something particularly ""nice"" about the combination of $N, V(i), \xi_i$.</p>
"
"2387522","2387529","<p>We have that</p>

<p>$$M_y(t) = \mathbb{E}_y[e^{ty}] = \mathbb{E}_x[e^{t(ax+b)}]$$
Note the variable that the expectation is taken over.
We can simplify this to:
$$M_y(t) = \mathbb{E}_x[e^{tax}e^{tb}] = e^{tb}\mathbb{E}_x[e^{(at)x}] = e^{tb}M_x(at)$$</p>

<p>We can take the ""constant"" out due to linearity of $\mathbb{E}$</p>
"
"2387525","2388067","<p>I think of it as two terms. Lifting the disc of water at some arbitrary distance y (&lt;-2) up to the surface of the water (y=-2) and then lifting the whole lot an extra 2 feet.  So the first term is
$\rho \int_{-6}^{-2}  (y+2) \, \pi \, x^2 dy$ and the second term $2 \,\rho \int_{-6}^{-2}   \pi \, x^2 dy$,  this just being 2 feet times the density of water times the volume.
This gives the 25.09 short tons ft in US units </p>
"
"2387530","2387544","<p>You can proceed like this:</p>

<ol>
<li>Because $R$ is an integral domain, the ideal $I = (t) \subseteq R[t]$ is prime (recall that an ideal is prime iff its corresponding quotient ring is an integral domain).</li>
<li>Since $I$ is prime, it is maximal by assumption.</li>
<li>Conclude that $R$ is actually a field.</li>
<li>Note that polynomial rings in one variable over a field are principal ideal domains.</li>
</ol>
"
"2387533","2387543","<p>Yes, they are completely correct. </p>
"
"2387534","2387541","<h2>Originally Answered: Find min-max of $\cos{\theta + \mu\sin{\theta}}$</h2>

<p>The general form of the answer is </p>

<p>$$
-\sqrt{a^2+b^2} \leq a\cos{\theta} + b\sin{\theta} \leq \sqrt{a^2+b^2}
$$</p>

<p>Look for the non calculus solution below.</p>

<h2>Does this help?</h2>

<p><a href=""http://mysite.science.uottawa.ca/cstar050/mat1318a/nov18.pdf"" rel=""nofollow noreferrer"">This article gives a nice, non calculus proof.</a></p>

<h2>The Edited Question (totally different): Find min-max of $\frac{\mu\cos{\theta}}{\cos{\theta + \mu\sin{\theta}}}$</h2>

<p>Dividing numerator and denominator by $\cos{\theta}$ we get,</p>

<p>$$
\frac{\mu}{1+\mu\tan{\theta}}
$$</p>

<h2><strong>Note that the numerator is constant while the denominator has only one variable $\tan{\theta}$ term.</strong></h2>

<h2>Finding a min-max value for the function is futile, because <a href=""https://www.desmos.com/calculator/wxrjmlwoo2"" rel=""nofollow noreferrer"">as this graph shows</a>, the function can achieve any real value and is not bounded.</h2>
"
"2387550","2387558","<p>This is called Newton-Leibniz's Integral rule or sometimes, Leibniz Integral rule.</p>

<p><a href=""https://en.m.wikipedia.org/wiki/Leibniz_integral_rule?wprov=sfla1"" rel=""nofollow noreferrer"">See this for more information</a></p>

<blockquote>
  <p>$$\frac{d}{dt}\int_{\phi(t)}^{\psi(t)} f(t,s) ds = \int_{\phi(t)}^{\psi(t)} \frac{d}{dt}f(t,s) ds+f(t,\psi(t))\frac{d}{dt}\psi(t) -f(t,\phi(t))\frac{d}{dt}\phi(t).$$</p>
</blockquote>
"
"2387567","2387570","<p>you should know that : $$a^b*a^c = a^{b+c}$$
and $$\frac{a^b}{a^c} = a^{b-c}$$
So you can simplify exponents, and to write only with positive ones, it might be useful to remember that $a^{-b} = \frac{1}{a^b}$</p>

<p>PS : it is also important to remember that ${(a^b)}^c = a^{b*c}$ and $a^{\frac {1}{2}} = \sqrt a$</p>
"
"2387569","2387612","<p>The equation is$$\frac{du}{dy}=u+xy$$where $x$ is a ""constant"". </p>

<p>It is separable and using the traditional methods, you should find $$u=c_1\, e^y-x(1+y)$$</p>
"
"2387573","2387617","<p>$x_n \to x$ </p>

<p>$ \iff$ </p>

<p>for each neighborhood $N_x$ of $x$ there is $n_0=n_0(N_x) \in \mathbb N$ such that $x_n \in N_x$ for all $n&gt;n_0$.</p>
"
"2387574","2387579","<p>Let $\displaystyle P = \prod_{k=1}^N p_k$. Then clearly any $a_m$ is removed from the sequence if and only if $a_{m+P}$ is removed from the sequence because for every $1 \leqslant k \leqslant N$: $p_k \mid m \iff p_k \mid m+P$. </p>

<p>Let $S = \# \{ n \in \mathbb{N} : 1 \leqslant (\widehat{T} a)_n \leqslant P \}$. </p>

<p>Then we prove $S = \# \{ n \in \mathbb{N} : A+1 \leqslant (\widehat{T} a)_n \leqslant A+P \}$ by induction on $A$.</p>

<p>Now fix $n$ and let $A+1 = (\widehat{T} a)_n$. Then $(\widehat{T} a)_n + P$ is the least element of the sequence $\widehat{T} a$ lying above the set $\{ n \in \mathbb{N} : A+1 \leqslant (\widehat{T} a)_n \leqslant A+P \}$ and to reach it we skip $S$ terms, so $(\widehat{T} a)_{n+S} = (\widehat{T} a)_n + P$. </p>

<p>Hence $(\widehat{T} a)_{n+S} - (\widehat{T} a)_{n+S-1} = (\widehat{T} a)_n - (\widehat{T} a)_{n-1}$, i.e. $(\widehat{T} a)_n - (\widehat{T} a)_{n-1}$ is periodic with period $S$.</p>
"
"2387580","2387709","<p>As $a$ divides every element of $R$, it divides itself. So $a = ta$ for some $t \in R$. So for every $b \in R$, $ab = tab \implies a(b-tb)=0 \implies b=tb$. So $t$ is the identity of $R$.</p>
"
"2387593","2387607","<p>$||a||^2 = \vec{a}^T*\vec{a} = \vec{a} \cdot \vec{a} = \langle \vec{a}, \vec{a}\rangle\,$ where ""$*$"" stands for <a href=""https://en.wikipedia.org/wiki/Matrix_multiplication#Row_vector_and_column_vector"" rel=""nofollow noreferrer"">matrix multiplication</a>, and ""$\cdot$"" or ""$\langle,\rangle$"" stand for <a href=""https://en.wikipedia.org/wiki/Dot_product#Definition"" rel=""nofollow noreferrer"">dot product</a>. Those would normally be specified in contexts where there is any risk of ambiguity.</p>
"
"2387610","2387614","<p>Yes, what you did is correct. </p>

<p>A consequence of what you did is you have proven that </p>

<p>$$\frac{d}{da}\gamma\|a\|^2=2\gamma a$$</p>
"
"2387611","2387648","<p>In the complex numbers, no.  $|z|$ is defined to be $\sqrt{z*\overline z}$ which is non negative as $z\overline z = \Re(z)^2 + \Im(z)^2$ is always non-negative real.</p>

<p>I suspect you are asking as we <em>extended</em> the reals to the complex (depending upon your philsophy) by allowing square roots of negatives which as a consequence allowed for logarithms of negative numbers, you are asking if we can create another number system that will allow $|z| &lt; 0$.</p>

<p>It's hard for me to imagine how or why we would do so.  There are no numbers that ""want to exist but can't"" because if they existed there modulus would have to be negative.  And as modulus was defined to be non-neg real, it's hard to imagine in what sense we'd define another meaning for it that satisfy any condition associated with it.  Primarily the condition that the modulus represents the quantitative measure of the absolute non-negative size of something.</p>

<p>=====</p>

<p>Okay, more.</p>

<p>In the reals $|x|$ is defined as $|x| = x$ if $x \ge 0$ and $|x| = -x$ if $x &lt; 0$.</p>

<p>In extending to the complex numbers we could have kept that definition as close as possible by saying $|a+ib| = a+ib$ if $a \ge 0$ and $|a+ib| = -a-ib$ if $a &lt; 0$.  Admittedly we wouldn't ever have $|z| &lt; 0$ but we would have $|z| \not \ge 0$.  We could define, god knows why we'd want to but we could, $|a+ib| = a+b$ and the we could have $|z| &lt; 0$.</p>

<p>So we don't we?  Why instead to we replace the simple $|x| = \pm x$ with the scary looking $|a+bi| = \sqrt{(a+bi)(a-bi)} = \sqrt{a^2 + b^2}$?</p>

<p>Well,  <em>BECAUSE</em> $|a+bi|=  \sqrt{a^2 + b^2}$ <strong>IS</strong> greater than or equal to $0$.  That $|z| \ge 0$ is a <em>requirement</em> of the definition of anything that we wish to call a ""modulus"".</p>

<p>Y2H in his/her answer lists some of the requirements for what something we call a ""modulus"" must obey.  Why must we obey it to call it a modulus?  Because if we didn't there wouldn't be any meaning to the word ""modulus"".</p>

<p>(Why is an elephant large, gray and wrinkly?  Because if it were small, white and smooth it would be an aspirin.)</p>

<p>The very first requirement is ... that the ""modulus"" is real, and non-negative.</p>

<p>The modulus is essentially the ""size"" of a number.  And size is positive value (or zero if and only if the number is zero).  That's just.... axiomatic.</p>

<p>There are other conditions.  By <em>definition</em>:</p>

<p>In an algebra, If $a,b \in F$, a field, $|a|$ is called the modulus, we must have:</p>

<p>i)$|a| \in \mathbb R; |a| \ge 0$.</p>

<p>ii) $|a| = 0$ if and only if $a$ is the multiplicative identity of $F$.</p>

<p>iii) $|ab| = |a||b|$</p>

<p>iv) $|a+b| \le |a| + |b|$</p>

<p>In vector spaces, $|x|$ is a norm and we must have (by definition) where $V,W$ are  vectors in a vector space and $a$ is an element of a field:</p>

<p>i) $|V| \ge 0; |V| \in \mathbb R$</p>

<p>ii) $|V| = 0 \iff V = 0$</p>

<p>iii) $|aV| = |a||V|$ where $|a|$ is a modulus of $a$.</p>

<p>iv) $|V + W| \le |V| + |W|$.</p>
"
"2387618","2387821","<p>Suppose $A$ is a $3 \times 3$ matrix with eigenvalues $\lambda$, $\lambda$, and $\mu$.</p>

<ul>
<li><p>If $A$ has three linearly independent eigenvectors $K_{1}$, $K_{2}$, and $K_{3}$, the general solution of $X' = AX$ is
$$
X(t) = e^{\lambda t}(c_{1}K_{1} + c_{2}K_{2}) + c_{3} e^{\mu t}K_{3}.
$$</p></li>
<li><p><a href=""https://en.wikipedia.org/wiki/Jordan_normal_form"" rel=""nofollow noreferrer"">Otherwise</a>, there exist two linearly independent eigenvectors $K_{1}$ and $K_{3}$ of $A$ and a vector $K_{2}$ such that $AK_{2} = K_{1} + \lambda K_{2}$, and the general solution is
$$
X(t) = e^{\lambda t}\bigl[(c_{1} + c_{2}t)K_{1} + c_{2}K_{2}\bigr] + c_{3} e^{\mu t}K_{3}.
$$</p></li>
</ul>
"
"2387621","2387634","<p>$P(Y\leq y)=P((2B-1)X\leq y)=P((2B-1)X\leq y\mid B=0)P(B=0)+P((2B-1)X\leq y\mid B=1)P(B=1)=\dfrac{1}{2}\Big[P(X\geq -y)+P(X\leq y)\Big]=\dfrac{1}{2}\Big[1-\Phi(-y)+\Phi(y)\Big]=\dfrac{1}{2}\Big[2\Phi(y)\Big]=\Phi(y)=P(X\leq y)$. </p>

<p>Since $\Phi(-y)=1-\Phi(y)$, $\Phi$ is cdf of $N(0,1)$</p>

<p>$Cov(X,Y)=E(XY)-E(X)E(Y)=E[(2B-1)X^2]-E(X)E[(2B-1)X]=E(2B-1)\big[E(X^2)-(E(X))^2\big]$</p>

<p>Now note that $E(2B-1)=2E(B)-1=1-1=0$</p>

<p>Hence $Cov(X,Y)=0$</p>
"
"2387623","2387734","<p>Hint: check the similarity of $\Delta ABC$ and $\Delta ABD$</p>

<p>Answer: </p>

<blockquote class=""spoiler"">
  <p> Triangles with One Equal Angle and Two Sides Proportional are Similar. Reference: <a href=""https://proofwiki.org/wiki/Triangles_with_One_Equal_Angle_and_Two_Sides_Proportional_are_Similar"" rel=""nofollow noreferrer"">https://proofwiki.org/wiki/Triangles_with_One_Equal_Angle_and_Two_Sides_Proportional_are_Similar</a></p>
</blockquote>
"
"2387624","2387644","<p>Yes, these properties are all preserved. To see this, note that the extension of the Fourier transform to $L^2$ functions can be written down explicitly in several ways and one of these is
$$
\hat f(\omega) = \lim_{R\to\infty} \int_{-R}^R f(x)e^{-i\omega x}dx
$$
(note that $f$ restricted to $[-R,R]$ is an $L^1$-function and hence, all integrals exist) and the limit is in the sense of $L^2$. Since the properties you mention hold for $L^1$ functions, they hold under the limit and hence, by continuity of the extension, are preserved by the limit.</p>
"
"2387631","2387723","<p>To the first question: in general, we may embed $U(n)$ into $SO(2n)$ in several ways. One way would be
$$
A+iB\mapsto \begin{pmatrix} A &amp; -B \cr B &amp; A \end{pmatrix}.
$$
Here the quotient $SO(2n)/U(n)$ is an irreducible symmetric space.</p>
"
"2387636","2387666","<p>Don't use Bayes' Rule; just apply the definition of conditional probability.</p>

<p>(a) You have it correct: $\mathsf P(\text{Mistaken})~{= \mathsf P(\text{Red $\cap$ Mistaken})+\mathsf P(\text{Green $\cap$ Mistaken})+\mathsf P(\text{Yellow $\cap$ Mistaken})\\
=0.45\cdot 0.05+0.45\cdot 0.04+0.10\cdot0.02 \\ = 0.0425}$</p>

<p>(b) You want to find: $\mathsf P(\text{Red}\mid \text{Mistaken})=\dfrac{\mathsf P(\text{Red $\cap$ Mistaken})}{\mathsf P(\text{Mistaken})}$</p>

<p>Hrmmm.... so...</p>
"
"2387643","2387652","<p>I think you have to assume that the above inequality is true for all possible $x$ and $y$ (otherwise the result isn't true).</p>

<p>Hint: What can you say about $\sum\limits_{x,y}P(X=x\cap Y=y)$?</p>
"
"2387655","2387659","<p>\begin{align}\det(M-\lambda I)&amp;=(\cos t-\lambda)^2+\sin^2 t=\cos^2 t-2\lambda \cos t+\lambda^2+\sin^2t\\&amp;=\lambda^2-(2\cos t)\lambda+1\end{align}Hence, by the sum of roots formula: $$\lambda_1+\lambda_2=2\cos t$$ and therefore $$\lambda_1+\lambda_2=1\iff \cos t=\frac12 \iff t=60^o=\frac{\pi}{3}$$ </p>
"
"2387658","2387665","<p>If a matrix $A$ is symmetric positive definite, then $A^{-1}$ is symmetric positive definite.</p>

<p>Proof:  Let the SVD of $A=UDU^T$, then $A^{-1}=UD^{-1}U^T$, we can see from the eigenvalues that $A^{-1}$ is positive definite.</p>

<p>If a matrix $A$ is positive definite and matrix $B$ is symmetric positive semidefinite, then $A+B$ is positive definite.</p>

<p>Proof:</p>

<p>$$x^T(A+B)x=x^TAx+x^TBx &gt; 0$$</p>
"
"2387664","2387681","<p>You can't hope for an answer in general, because two equations in three variables will generally give a one dimensional set of points. But the answer can sometimes be given in terms of a single parameter. Here it is easy enough to work through:</p>

<p>$$a+b+c=n$$</p>

<p>$$a^2+b^2+c^2+2c(a+b)+2ab=n^2$$</p>

<p>$$2c^2+2c(n-c)+2ab=n^2$$</p>

<p>$$2cn+2ab=n^2$$</p>

<p>$$2c^2n+2abc=cn^2$$</p>

<p>$$abc=\frac{cn(n-2c)}2$$</p>
"
"2387669","2387777","<p>If $C\in C$ then it must satisfy the property behind the colon in the class definition: $\neg(C\in C)$ also written as $C\notin C$, so we have
$$C\in C \implies C\notin C$$</p>

<p>If $C\notin C$ then it satisfies the property so it is an element of $C$. So we also have
$$C\notin C \implies C\in C$$
Combining them yields
$$C\in C \iff C\notin C$$</p>
"
"2387677","2387687","<p>If you consider $$x=\frac{4\sqrt{\frac{y}{2}}+\sqrt{3y^2-y}}{3}$$ and use a Taylor series for infinitely large values of $y$, you should get
$$x=\frac{y}{\sqrt{3}}+\frac{2 \sqrt{2} }{3}\sqrt{y}-\frac{1}{6
   \sqrt{3}}-\frac{1}{72 \sqrt{3}
   y}+O\left(\frac{1}{y^{3/2}}\right)$$ and this matches the results to very high accuracy as shown below
$$\left(
\begin{array}{ccc}
 y &amp; \text{exact} &amp; \text{approximation} \\
 10 &amp; 8.6578861 &amp; 8.6578997 \\
 20 &amp; 15.666746 &amp; 15.666750 \\
 30 &amp; 22.387992 &amp; 22.387994 \\
 40 &amp; 28.960432 &amp; 28.960433 \\
 50 &amp; 35.437794 &amp; 35.437795 \\
 60 &amp; 41.847625 &amp; 41.847625 \\
 70 &amp; 48.206285 &amp; 48.206286 \\
 80 &amp; 54.524436 &amp; 54.524437 \\
 90 &amp; 60.809482 &amp; 60.809482 \\
 100 &amp; 67.066812 &amp; 67.066812
\end{array}
\right)$$</p>

<p>Now, ignoring the terms with negative powers of $y$, you can solve the quadratic and get $$y=\sqrt{3}\, x-\frac{1}{3} \sqrt{20+24 \sqrt{3}\, x}+\frac{3}{2}$$ which, expanded again, would give $$y=\sqrt{3} x-\frac{2 \sqrt{2} }{\sqrt[4]{3}}\sqrt{x}+\frac{3}{2}+O\left(\frac{1}{x^{1/2}}\right)$$</p>

<p>For a sanity check, I replaced in the initial rhs $y$ by this last expression in $x$ and developed it as a Taylor series for infinitely large values of $x$. The obtained result is $$x+\frac{5}{9 \sqrt{2} \sqrt[4]{3}}\frac 1 {\sqrt x}+O\left(\frac{1}{x}\right)$$ </p>
"
"2387679","2388796","<p>I believe I might have found a solution. </p>

<hr>

<p>First, let us define $cov(Y_{i,j})$ - an indicator for the event ""$j$ is blocking the view for $i$"".</p>

<p>We will define $Y_{i,j}$ only for $j&gt;i$ where it makes sense. (Otherwise, $j$ never blocks the view).</p>

<p>Given the assumption $j&gt;i$, with probability $\frac{1}{2}$ from symmetry. 
In the shuffled row of people, $i$ has the same chance to be positioned before $j$, as $j$ has to be position before $i$.</p>

<p>Therefore, $Y_{i,j} = 1$ $ w.p.\frac{1}{2}$,  Otherwise $Y_{i,j} = 0$ $ w.p.\frac{1}{2}$ .</p>

<p>We will notice that $Y_{i} = \sum_{j=i+1}^{n}Y_{i,j} \implies cov(Y_{i},Y_{i+1}) = cov(\sum_{j=i+1}^{n}Y_{i,j},\sum_{j=i+2}^{n}Y_{i+1,j})$ </p>

<p>From bilinearity of the covariance.</p>

<p>$=\sum_{k=i+1}^{n}\sum_{l=i+2}^{n}cov(Y_{i,k},Y_{i+1,l})$</p>

<p>Let us observe $cov(Y_{i,k},Y_{i+1,l}) = \mathop{\mathbb{E}}[Y_{i,k}*Y_{i+1,l}] - \mathop{\mathbb{E}}[Y_{i,l}]*\mathop{\mathbb{E}}[Y_{i+1,l}]$</p>

<p>From properties of indicators $\mathop{\mathbb{E}}[Y_{i,k}]*\mathop{\mathbb{E}}[Y_{i+1,l}] = \frac{1}{2}*\frac{1}{2} = \frac{1}{4}$</p>

<p>For  $Y_{i,k}*Y_{i+1,l}=1 \iff$  $i$ is poistioned before $k$ and $i+1$ is positioned before $l$.</p>

<p>that happens with probability $\frac{1}{2}*\frac{1}{2}$ = $\frac{1}{4}$.</p>

<p>Hence, $cov(Y_{i,k},Y_{i+1,l}) = 0 \implies \sum_{k=i+1}^{n}\sum_{l=i+2}^{n}cov(Y_{i,k},Y_{i+1,l}) = 0 \implies cov(Y_{i},Y_{i+1}) = 0$ </p>
"
"2387680","2387737","<p>It might be easier to just use the explicit description of limits and co-limits. It is a general result in universal algebra in any category of generalized algebras (e.g. rings or groups) that the limit of a diagram $I$ is the subset of the product of the objects of $I$ satisfying the functional equations of the arrows of $I$. Similarly, the co-limit of $I$ is the quotient of the co-product of the objects of $I$, the equivalence relations are given by relations generated by the arrows of $I$.</p>

<p>Apply it to the case of rings and groups. The product (resp. coproduct) of rings is direct product (resp. tensor product), and the product (resp. co-product) of groups is the direct product (resp. free product). However, since the underlying group of  a ring is abelian, for your case free product of the groups in the image of the additive functor is the direct sum. I think it is routine from this point onwards to study how do coherence relations demanded by the arrows of a diagram $I$ behave under this description. Actually, it is immediate that the additive functor preserves limits.</p>

<p>Furthermore, the discussion above also immediately shows the additive functor does not preserve colimits or even finite coproducts as $\mathbb Z\otimes \mathbb Z=\mathbb Z$  as rings while $\mathbb Z\oplus \mathbb Z\neq \mathbb Z$ as groups.</p>
"
"2387688","2387754","<p>If x lands between 0 and 0.75, y will always be greater. $ (3/4)â(1) $</p>

<p>If x lands between 0.75 and 1, y will be greater 50% of the time, as both are uniform. $ (1/4)*(1/2) $</p>

<blockquote>
  <p>$ P(xâ¤y)= (3/4)â(1)+(1/4)(1/2)=7/8 $</p>
  
  <p>$ P(xâ¥y)=1â7/8=1/8 $</p>
</blockquote>

<p>I agree with your calculations.</p>
"
"2387694","2387704","<p>The idea is to construct a set which is none of $u_k$. This is done by assuring that $k \in v \leftrightarrow k\notin u_k$, this means that for any $k$ that $u_k\ne v$.</p>

<p>Note that the proof assumes there is an enumeration $u_k$ to start with and it shows that there is a $v\in 2^{\mathbb N}$ that is not part of the enumeration.</p>

<p>In fact the same argument is used to show that $2^X$ is larger than $X$. Which can be used to create an concrete example. For example assume that $X=\{0,1,2\}$ and suppose that we have an surjective mapping from $X$ to $2^X$ which would mean that we have three sets $u_0$, $u_1$ and $u_2$ which would make up $2^X$. Now take for example:</p>

<p>$$u_0 = \{1,2\}$$
$$u_1 = \{0,1\}$$
$$u_2 = \{0\}$$</p>

<p>Now we construct $v$ from this and we have $0\in v\leftrightarrow 0\notin u_0$, but $0\notin u_0$ so $0\in v$. In the same way we find that $1\notin v$ (because $1\in u_1$) and $2\in v$. So we have $v=\{0,2\}$ which is none of $u_0, u_1, u_2$.</p>
"
"2387697","2387727","<p>Let $P(z)=a_0+a_1z+\cdots+a_nz^n$. Then $f(\theta)=P(e^{i\theta})$. Since $n\in\mathbb N$, $P$ is not constant and therefore, by the maximum modulus principle, the maximum of the restriction of $P$ to the closed unit disk is attainded at the boundary and only at the boundary. In particular $|a_0|=\bigl|P(0)\bigr|&lt;\bigl|P(z)\bigr|$ for some $|z|=1$, and this means that$$(\exists\theta\in\mathbb{R}):|a_0|&lt;\bigl|f(\theta)\bigr|.$$</p>
"
"2387707","2387718","<p>IMO one of the most intuitive proof is to set $$F(s) = \int_{-\infty}^\infty e^{-t^2} e^{-2st}dt, \qquad s \in \mathbb{C}$$
Assuming $s \in \mathbb{R}$ then
$$F(s) =  \int_{-\infty}^\infty e^{s^2} e^{-(t+s)^2}dt =e^{s^2}\int_{-\infty}^\infty  e^{-t^2}dt= e^{s^2} F(0)$$
And to convince yourself that $F(s)-e^{s^2}F(0)=0$ stays true for every $s \in \mathbb{C}$, because both sides are complex analytic functions of $s$ (a non-zero power series $\sum_{n=0}^\infty a_n s^n$ cannot vanish for every $s\in \mathbb{R}$).</p>

<p>Thus $\displaystyle\int_{-\infty}^\infty e^{-t^2}e^{-i \omega t}dt = F(i\omega/2)=e^{-(\omega/2)^2} F(0)$ and $$\int_{-\infty}^\infty\int_{-\infty}^\infty e^{-a^2(t+b)^2}e^{-c^2(r+d)^2}e^{-i k r}e^{-i \omega t}dtdr = \frac{1}{|a|}e^{-(\omega/2)^2/a^2} e^{-i \omega b}\frac{1}{|c|}e^{-(k/2)^2/c^2} e^{-i kd}F(0)^2$$ Finally by inverse Fourier transform we show $F(0) = \sqrt{\pi}$.</p>
"
"2387715","2387748","<p>$$ \sum_{i=0}^{27} \binom{27}{i} (\color{blue}{-3})^{\color{blue}2i}= \sum_{i=0}^{27} \binom{27}{i} (\color{blue}9)^{i}=(1+9)^{27}=10^{27}$$</p>
"
"2387720","2387730","<p>A slight modification to your thinking:</p>

<p>Give up the ""multiply by 10 to get every possible position of the symbol"", but say the first symbol appears on position $i$, $1\le i \le 10$.</p>

<p>For each position $i$, each character before it has $62$ independent choices, and each character after it has $72$ independent choices. The number of choices is</p>

<p>$$\begin{align*}
N &amp;= \sum_{i = 1}^{10} 62^{i-1}\cdot 10 \cdot 72^{10-i}\\
&amp;= \sum_{j = 0}^{9} 62^{j}\cdot 10 \cdot 72^{9-j}\\
&amp;= 10\cdot72^9\sum_{j=0}^{9}\left(\frac{62}{72}\right)^j\\
&amp;= 10\cdot72^9\cdot\frac{1-(62/72)^{10}}{1-62/72}\\
&amp;= 10\cdot \frac{72^{10}-62^{10}}{72-62}\\
&amp;= 72^{10}-62^{10}
\end{align*}$$</p>
"
"2387724","2387744","<p>My first thought, similar to yours, was to consider the sequence $f^n(X)$. Then you have a nested sequence of compact sets, the intersection of which must be non-empty and compact (hence closed). This intersection is an example of a closed set $C$ such that $f(C) = C$.</p>
"
"2387731","2389306","<p>In a comment, you mention ""a tedious process."" I'm guessing that is trying numbers of the form $17n + 9$ in order starting at $n = 0$ and going up until finding one that gives the desired remainder when divided by 13.</p>

<p>Here's a slightly quicker way:</p>

<ol>
<li>Find the smallest positive integer $b_1$ such that $17b_1 \equiv 1 \pmod{13}$. That would be 10.</li>
<li>Find the smallest positive integer $b_2$ such that $13b_2 \equiv 1 \pmod{17}$. This one is easier to find: 4.</li>
<li>Compute $13 \times 17 = 221$.</li>
<li>Next compute $(11 \times 17b_1) + (9 \times 13b_2) = 1870 + 468 = 2338$.</li>
<li>Now, starting with the number from the previous step, repeatedly subtract the number from Step 3 and stop short of a negative number: 2117, 1896, ..., 128. That's your answer. Verify it.</li>
</ol>

<p>Actually, any of the numbers you got from Step 5 will work. In fact, given any integer $n$ whatsoever, we have $2338 + 221n \equiv 11 \pmod{13} \equiv 9 \pmod{17}$ (don't get confused about your directions with negative $n$, though).</p>

<p>This method should work as long as the two moduli are coprime, as 13 and 17 certainly are. This method can also be adapted for three, four moduli, however many more, as long as all the moduli are pairwise coprime.</p>

<p>You can also have a computer do it for you. Go to Wolfram Alpha and type <code>ChineseRemainder[{11, 9}, {13, 17}]</code>.</p>
"
"2387735","2387775","<p>$$\mathsf {Cov}(U,N) ~=~ \mathsf E(UN)-\mathsf E(U)\cdotp\mathsf E(N)$$</p>

<p>Use the Law of Iterated Expectation, Linearity of Expectation, and independence of $(Y_i)$ and $N$, and the identical independent distribution of $(Y_i)$.</p>

<p>For example: $\mathsf E(U) ~{= \mathsf E\Big(\mathsf E(\sum_{k=1}^NY_k\mid N)\Big)\\= \mathsf E\Big(\sum_{k=1}^N\mathsf E(Y_k\mid N)\Big)\\ = \mathsf E(N\,\mathsf E(Y_1))\\= \mathsf E(N)\cdot\mathsf E(Y_1)}$</p>

<p>Then likewise: $\mathsf E(UN) ~{= \mathsf E\Big(\mathsf E(N\,\sum_{k=1}^NY_k\mid N)\Big)\\ \ddots}$</p>
"
"2387743","2387755","<p>Let $\alpha$ be the angle such that $\cos\alpha=3/5,\sin\alpha=4/5$. Consider the points $P_n(\sin(2^n\alpha),\cos(2^n\alpha))$ for $n\geq 1$, all lying on the unit circle. By looking at the triangle formed by $(0,0),P_n,P_m$ we find that the distance between $P_n$ and $P_m$ is
$$2\sin\left(\frac{2^m-2^n}{2}\alpha\right)$$
which (trigonometry exercise) is rational because of our choice of $\alpha$. This gives a countable set as in question (since no three points on a circle are collinear).</p>

<p>To prove there is no larger one, consider any two points $P,Q$ in our set. For any pair of rational numbers $a,b$ there are at most two points in the plane at distance $a$ from $P$ and at distance $b$ from $Q$. Hence there can only be countably many other points in this set. Similar argument (starting with points $P,Q,R$ or so) shows that there are no uncountable such sets even in the Euclidean 3D space, nor are there in higher dimensional ones.</p>

<p>Let me remark that it is an old open question, posed in 1945 by Ulam (see <a href=""https://mathoverflow.net/a/19138/30186"">here</a> for some references) whether there is such a (countable) set of points with pairwise rational distances which is <em>dense</em> in the plane. It's a simple exercise to show that if there was one, then there would be one with no three points collinear as well.</p>
"
"2387745","2387784","<p>If $e_i$ is the $i$th column of an identity matrix, then $Ae_i$ the $i$th column of a matrix $A$. You can write $S$ as
$$
S = \sum_{i=1}^r s_ie_ie_i^T,
$$
where $e_ie_i^T$ is a matrix full of zeros expect the position of $s_i$ in $S$. Substitute it to get
$$
USV^T=U\left(\sum_{i=1}^r s_ie_ie_i^T\right)V^T=
\sum_{i=1}^r s_i (Ue_i)(e_i^TV^T)=
\sum_{i=1}^r s_i (Ue_i)(Ve_i)^T=
\sum_{i=1}^r s_i u_iv_i^T.
$$</p>
"
"2387778","2387886","<p>It is an application of the Fubini's theorem. The step in the picture is assuring that</p>

<p>$$ \sum_{n=1}^{\infty} \mathbb{E}\left[ \left| S_n \mathbf{1}_{\{T = n\}} \right| \right] &lt; \infty. $$</p>

<p>So by the Fubini's theorem we have</p>

<p>$$ \mathbb{E}[S_T]
= \mathbb{E}\left[ \sum_{n=1}^{\infty}  S_n \mathbf{1}_{\{T = n\}} \right]
= \sum_{n=1}^{\infty}  \mathbb{E}\left[ S_n \mathbf{1}_{\{T = n\}} \right]
= \mathbb{E}[X_1]\mathbb{E}[T] $$</p>

<p>with the computational detail of the first equality and the third equality being the same as in your picture.</p>
"
"2387781","2388102","<p>Given the ellipse</p>

<p>$$\frac{x^2}{a^2}+\frac{y^2}{b^2}=1$$</p>

<p>there are several ways to express it in the complex plane. Some are shown in the other answers. Here are two direct expressions for $z(\theta),~\theta\in[0,2\pi]$.</p>

<p>$$
z=a\cos\theta+ib\sin\theta
$$
$$
r=\frac{ab}{\sqrt{a^2\sin^2\theta+b^2\cos^2\theta}}
$$
$$
z=re^{i\theta}
$$</p>
"
"2387787","2387790","<blockquote>
  <p>Can a function map a single X value to multiple Y values?</p>
</blockquote>

<p>By definition, no. A function maps every X value in the valid domain to only a single Y value. </p>

<blockquote>
  <p>""In mathematics, a function is a relation between a set of inputs and a set of permissible outputs with the property that each input is related to <strong>exactly one</strong> output"". </p>
</blockquote>
"
"2387792","2392735","<p>The set of functions created from constants, field operations and allowing exp and log is linearly ordered by big-O. They are called exp-log functions. Take a look at Hardy's ""<a href=""http://www.gutenberg.org/files/38079/38079-pdf.pdf"" rel=""nofollow noreferrer"">Orders of Infinity</a>"", page 24. For more about this, search for ""Hardy fields"".</p>

<p>This class covers most ""typical"" complexities. However, the world of complexity is much richer than any simple description that can be written down. Complexity can be:</p>

<ul>
<li>Extremely slow-growing: There are algorithms whose complexity uses inverse Ackermann (union-find) or $\log^{\ast} n$ (<a href=""https://users.ics.aalto.fi/suomela/dda-2010/lecture-2.pdf"" rel=""nofollow noreferrer"">distributed tricoloring of a tree</a>). Those functions grow slower than $\log(\log(\dots n\dots))$ for any finite number of logs.</li>
<li>Extremely fast-growing: Some problems cannot be solved in time that is $\exp(\exp(\dots n \dots))$, for any finite number of $n$. They are called <a href=""https://en.wikipedia.org/wiki/Nonelementary_problem"" rel=""nofollow noreferrer"">nonelementary</a>. The Ackermann function grows too fast, an according to <a href=""https://cstheory.stackexchange.com/questions/27120/ackermann-function-time-complexity"">this</a> answer there are natural problems with Ackermannian complexity.</li>
<li>Arbitrary: For most sensible functions $F$, it's possible to have an algorithm with complexity $F(n)$, that gets a Turing machine and simulates it for $F(n)$ steps. For example, you can obtain an algorithm with complexity $n^{5+(-1)^n}$. You can also define (artificially) a problem that can be solved in this time.</li>
<li>Ill-defined: Blum's speedup theorem shows that there exists a computable function $f$ such that, if $f$ can be computed in time $X$, then $f$ can computed in time $\log X$. There is no ""best"" algorithm to compute $f$. It's not ""natural"" because it's a diagonalization against possible algorithms, but shows how weird this can get.</li>
</ul>
"
"2387793","2387812","<p>And if $A=\left(\begin{smallmatrix}-1&amp;0\\0&amp;0\end{smallmatrix}\right)$, then the average of its entries is $-\frac14$, whereas a norm never takes values smaller than $0$.</p>
"
"2387796","2387805","<p>The vertices of your triangle are $(a,0,0)$, $(0,b,0)$, and $(0,0,c)$. So, what you have to do is to compute the vector product $\bigl((0,b,0)-(a,0,0)\bigr)\times\bigl((0,0,c)-(a,0,0)\bigr)$, which is $-(bc,ac,ab)$. It's norm is $\sqrt{a^2b^2+b^2c^2+c^2a^2}$ and halving it gives the result that you're after.</p>
"
"2387799","2388261","<p>If you are just dealing with finite lattices (or graphs in general), the term you are looking for is a Hamiltonian path. Infinite lattices (or graphs) will require an extension of the idea of a Hamiltonian path, which depending on its use may be infinite in one direction or two.</p>

<p>Hamiltonian paths that end where they start are called Hamiltonian cycles.</p>
"
"2387810","2390015","<p>I presume that by $\prod\mathbb{Z}$ you mean an infinite product. Since every infinite product contains a countable product, and $\text{Ext}^1(-,\mathbb{Z})$ is right exact, to prove that $\text{Ext}^1(\prod\mathbb{Z},\mathbb{Z})\neq0$ we may as well assume that it's a countable product.</p>

<p>I'll just write $\Pi$ for the product of countably many copies of $\mathbb{Z}$.</p>

<p>By <a href=""https://math.stackexchange.com/questions/320444/why-isnt-an-infinite-direct-product-of-copies-of-bbb-z-a-free-module"">a theorem of Baer</a>, $\Pi$ is not a free abelian group, but if it were, then $\text{Ext}^1(\Pi,\mathbb{Z})$ would be zero. So you can expect any proof to be at least as hard as Baer's theorem.</p>

<p>One proof of Baer's theorem uses a famous <a href=""https://math.stackexchange.com/questions/831845/how-can-one-show-that-rm-hom-bigl-prod-limits-i-geqslant-1-bbb-z-bbb-z"">result of Specker</a> that describes $\text{Hom}(\Pi,\mathbb{Z})$, and in particular shows that it is countable. This can be used to prove your statement.</p>

<p>Although $\text{Hom}(\Pi,\mathbb{Z})$ is countable, $\text{Hom}(\Pi,\mathbb{Z}/2\mathbb{Z})$ is uncountable (in fact, of cardinality $2^{2^{\aleph_0}}$), since $\Pi/2\Pi$ is a vector space of infinite dimension over the field $\mathbb{F}_2$ of two elements, and so has uncountably many homomorphisms to $\mathbb{Z}/2\mathbb{Z}$.</p>

<p>Now apply the functor $\text{Hom}(\Pi,-)$ to the short exact sequence $0\to\mathbb{Z}\stackrel{\times2}{\to}\mathbb{Z}\to\mathbb{Z}/2\mathbb{Z}\to0$, producing an exact sequence
$$\text{Hom}(\Pi,\mathbb{Z})\to\text{Hom}(\Pi,\mathbb{Z}/2\mathbb{Z})\to\text{Ext}^1(\Pi,\mathbb{Z}).$$
Since the first term is countable, but the second term is uncountable, the third term is non-zero.</p>

<p>Probably there are other proofs based on different approaches to Baer's theorem.</p>
"
"2387827","2388139","<p>Let $$A=\begin{bmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{bmatrix}$$</p>

<p>The matrix is PSD, however $A_{1,2}&lt;0$ and hence $e_1^TAe_2 &lt;0$.</p>

<p>For real positive semidefinite matrix, to check positive semidefiniteness, we only verify $x^TAx \geq 0$ rather than $x^TAy \geq 0$.</p>

<p>Hence rather than writing $x^Ta_1M_1y$, I would write $x^Ta_1Mx$.</p>
"
"2387828","2387847","<p>Yes, your original description is basically correct. You can describe the polynomial ring over $R$ having indeterminates $x_1,\ldots ,x_n$ as formal linear combinations of monomials with coefficients from the ring $R$.</p>

<p>But what you describe in 1) and 2) is different from that.</p>

<ol>
<li><p>$R[x_1,x_2]$ ""where $x_1=x_2^2$"" is better known as the quotient ring $R[x_1,x_2]/(x_1-x_2^2)$ of the polynomial ring $R[x_1,x_2]$. This is different from the full set of linear combinations because using the relation, lots of the monomials collapse.</p></li>
<li><p>Is not even well defined, since $x_1$ is just a symbol for an indeterminate, not a real number, or any number $\sin()$ is defined on. A relation between $x_1$ and $x_2$ can only be established using the operation that are available in $R[x_1,x_2]$, and the only two operations available are addition and multiplication.</p></li>
</ol>

<p>A defining feature of polynomial rings is that the <em>lack</em> extra relations like $x_1=x_2^2$. Of course, it is very useful to study such rings, and they are studied <em>as quotients of polynomial rings</em>, but the concept of the big thing that is totally free of any relations is the important thing to establish as a creature of its own. Because polynomial rings are free of relations, they are also called ""free $R$ algebras in commuting indeterminates $x_1,\ldots, x_n$.""</p>
"
"2387832","2387851","<p>Note that $N$ is defined to be the <em>normal</em> subgroup generated by certain elements (here, $ab$ and $b^2$). That means we want to consider the smallest normal subgroup containing these elements and not just the subgroup generated by these elements.</p>

<p>Let us note some things about the quotient $F/N$. It is clearly generated by $a' = aN$ and $b' = bN$. You have $a'b' = 1$ since $ab \in N$. It follows that $a' = b'^{-1}$ so $F/N$ cyclic being generated by $b'$. You also have $b'^2 = 1$ since $b^2 \in N$, so $F/N$ is cyclic of order at most $2$.</p>

<p>Now consider the homomorphism $f: F \to \mathbb{Z}/2\mathbb{Z}$ defined by $f(a) = f(b) = 1 + 2\mathbb{Z}$. Then $f$ is surjective and $ab$ and $b^2$ lie in $\ker(f)$. It follows that $N \subseteq \ker(f)$ and since $\ker(f)$ has index $2$ in $F$, it follows that we actually have $N = \ker(f)$ (as the index of $N$ was at most $2$). We conclude that the index of $N$ in $F$ is $2$.</p>
"
"2387840","2387867","<p>Technically, you need both the statement ""1 is not an integer"" and the statement ""1 is an integer"" to have a logical contradiction.</p>

<p>However, within any 'normal' mathematical context, ""1 is an integer"" is assumed to be true: all working mathematicians use and know ""1"" to refer to a specific integer. So, if within that context you ever derive ""1 is not an integer"" based on some assumption, then you can immediately say that that assumption leads to a contradiction; Having to spell out that ""1 is an integer"" really seems to be rather pedantic. </p>

<p>In formal logic proofs, sure, everything needs to be explicit, but in normal practical mathematical contexts, I would say there is really no need to explicitly get ""1 is an integer"" before you can say you have a contradiction.</p>
"
"2387841","2388219","<p>Under CH (the continuum hypothesis) the answer is yes: theorem 1.2.6.in the <a href=""https://staff.fnwi.uva.nl/j.vanmill/papers/papers1984/handbook.pdf"" rel=""nofollow noreferrer"">introduction to $\beta \omega$</a>  has the following corollary to Paravicenko's theorem under CH:</p>

<blockquote>
  <p>(CH) If $X$ and $Y$ are zero-dimensional, locally compact, non-compact, $\sigma$-compact spaces of weight at most $\mathfrak{c}$, then $\beta X - X$ and $\beta Y - Y$ are homeomorphic.</p>
</blockquote>

<p>Note that $\mathbb{N}$ and $K_0$ are such spaces.</p>

<p>Maybe ideas like these work: we can write $K_0 = \cup_n C_n$ all clopen compact and pairwise disjoint, and map $C_n$ to $n$ and extend to $\beta \mathbb{N}$. This maps $\beta K_0 - K_0$ onto $\beta \mathbb{N} - \mathbb{N}$ (by perfectness, e.g.). It this map were 1-1,  we'd have an absolute homeomorphism, not dependent on CH. </p>
"
"2387846","2388394","<p>First of all, some corrections. The derivative that you found in the end of your post is <strong>NOT</strong> $y'(t)$ &mdash; it's $y'(x)=\frac{dy}{dx}$. And it's slightly wrong &mdash; a negative sign is missing. Do you see why?</p>

<p>Now, what is the maximal possible value of cosine? And what value of an angle, $\theta=\ldots$, has this maximal possible value of cosine? That's the angle you want between the two lines (the given line and the tangent line). Then think geometrically: two lines have this angle of $\theta=\ldots$ between them is the same as saying that the lines are $\ldots$ relative to each other.</p>

<p>Your know the slope of the given line, viz. $m_1=-\frac{1}{\sqrt{3}}$. The slope of the tangent line is $m_2=\frac{dy}{dt}=\cot\left(\frac{1}{2}t+\frac{\pi}{4}\right)$. According to the conclusion in the last paragraph, these slopes must be $\ldots$ &mdash; that's how you can set up an equation to solve for $t$.</p>
"
"2387848","2387862","<p>I think a map is, when you get down to its very essence, just a union of <a href=""https://en.wikipedia.org/wiki/Curve"" rel=""nofollow noreferrer"">curves</a> in the plane or in $3$-space.</p>

<p>Now, if you wish to lend importance to the intersection points of the curves, or other points along the curve that aren't intersections, then the natural interpretation is as a <strong>graph</strong> <a href=""https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)"" rel=""nofollow noreferrer"">in this sense</a>.  If the map is so informative as to tell you the distances between the landmarks you have placed, then it could even be considered a weighted graph.</p>

<p>If your streets are all assumed to be straight (not very reasonable, I suppose, unless you grew up where I did) then you could interpret it as a collection of intersecting lines in geometry (and a graph at the same time, as above.)</p>

<p>The table you gave above records the incidence relation between the streets chosen. You might want to take a look at how <a href=""https://en.wikipedia.org/wiki/Incidence_matrix"" rel=""nofollow noreferrer"">incidence matrices</a> encode the same information. The difference would be that instead of streets on both axes, you'd have streets on one axis and intersection labels on the other.</p>
"
"2387849","2387866","<p>If your $V(t)$ is a ""well fit"" to $v(t)$, I would expect that I can use $V$ instead of $v$ for certain computations. This would imply answer a).</p>

<p>The details however are hidden in the ""well fit"" statement, we have no information on the relationship between $V$ and $v$ beyond that, so
the rational statement is answer c), we cannot say, we need more information.</p>
"
"2387859","2389110","<p>Eq (2) is wrong. The last term should be $h(Z\mid X_G)$. In general </p>

<p>$$h(Y|X_G)=h(X_G + Z|X_G)=h(Z\mid X_G)$$
You can equate this to $h(Z)$ only if $Z,X_G$ are independent.</p>
"
"2387870","2387878","<p>For part <strong>3</strong>) Basically, yes. $AA^{*}$ is self-adjoint (Hermitian). We will use the fact that $AA^{*} = (AA^{*})^{*}$ in the computation below (first equality).</p>

<p>For part <strong>1</strong>)
\begin{equation}
 \langle AA^{*}v, v\rangle = \langle v, AA^{*}v \rangle = \overline{\langle AA^{*}v,v \rangle}.
\end{equation}
Note that the last equality follows from the identity $\langle v, w \rangle = \overline{\langle w, v \rangle}$, which is true for any inner product by definition.</p>

<p>For part <strong>2</strong>) If $AA^{*} = A^{*}A$ then $A$ is called <em>normal</em>, this is  weaker than being self-adjoint. That is, every self-adjoint matrix is normal, but not every normal matrix is self-adjoint. (For an example of a matrix that is normal but not self-adjoint see <a href=""https://math.stackexchange.com/questions/584958/normal-operator-that-is-not-self-adjoint"">this question</a>).</p>
"
"2387877","2387917","<p>For $n=1$ and for $n=2$ it's true.</p>

<p>Let $u_n=3^n-2^n$ and $u_{n-1}=3^{n-1}-2^{n-1}$ for all $n\geq2$.</p>

<p>Thus, $$u_{n+1}=5u_n-6u_{n-1}=5(3^n-2^n)-6(3^{n-1}-2^{n-1})=$$
$$=5(3^n-2^n)-2\cdot3^n+3\cdot2^n=3^{n+1}-2^{n+1}$$
and we are done!</p>
"
"2387879","2391431","<p>Your name suggests that you can read Russian. If so, you can look <a href=""http://www.ega-math.narod.ru/Quant/Tickets.htm"" rel=""nofollow noreferrer"">these</a> articles. In particluar, in the second paper is provided a recurrent formula for $count(n,k)=N_k(n)$ in their notation (they swap $n$ and $k$): </p>

<p>$$N_k(n)=\sum_{l=0}^9 N_{k-1}(n-l),$$</p>

<p>where if $n&lt;9$ then $N_{k-1}(n-l)=0$ for $l&gt;n$;</p>

<p>and a table for $N_k(n)$ for $k\le 4$.</p>

<p>Iâm going to relook these papers for additional results.</p>
"
"2387883","2387941","<p>There's no hypothesis missing.</p>

<p>If $U \subset W$ is open with $\nu^{-1}(S) \subset U$, then $F = W\setminus U$ is closed, and $\nu(F) \cap S = \varnothing$. Since $\nu$ is closed, $\nu(F)$ is a closed set, and hence $V = Z\setminus \nu(F)$ is an open set with $S\subset V$. And $\nu^{-1}(V) \subset U$ since $V = Z\setminus \nu(W\setminus U)$.</p>

<p>In fact, this property characterises closed maps. Let $f \colon W \to Z$ a map such that for every $S \subset Z$ and every open $U \subset W$ with $f^{-1}(S) \subset U$ there is an open $V\subset Z$ with $S \subset V$ and $f^{-1}(V) \subset U$. Then $f$ is a closed map.</p>

<p>For let $F\subset W$ a closed set. Then for $S := Z \setminus f(F)$ we have $f^{-1}(S) \subset U := W\setminus F$, and by assumption there is an open $V\subset Z$ with $S\subset V$ and $f^{-1}(V) \subset U$. But the latter implies $V \cap f(F) = \varnothing$, and hence $V \subset S$. Thus $S = V$ is open, and $f(F) = Z\setminus S$ is closed.</p>
"
"2387884","2387999","<p>As @JMoravitz stated in the comments, your answers to the second problem are correct.</p>

<p>In the first problem, sampling is done with replacement (which is not the best way to check for defective products).  Therefore, we can use the <a href=""https://en.wikipedia.org/wiki/Binomial_distribution"" rel=""nofollow noreferrer"">binomial distribution</a>.</p>

<p>The probability of selecting $k$ good computers and $n - k$ bad computers when $n$ computers are selected with replacement is 
$$\binom{n}{k}p^k(1 - p)^{n - k}$$
where $\binom{n}{k}$ represents the number of orders in which exactly $k$ good computers can be selected in $n$ trials, $p$ is the probability that a good computer is selected, and $1 - p$ is the probability that a defective computer is selected. </p>

<p>Two good computers are selected: Using the formula given above with $n = k = 2$ and $p = 195/200$ yields
$$\binom{2}{2}\left(\frac{195}{200}\right)^2\left(\frac{5}{200}\right)^0 = \left(\frac{39}{40}\right)^2$$</p>

<p>One good and one defective computer are selected:  Using the formula given above with $n = 2$, $k = 1$, and $p = 195/200$ yields</p>

<blockquote class=""spoiler"">
  <p> $$\binom{2}{1}\left(\frac{195}{200}\right)^1\left(\frac{5}{200}\right)^1 = 2\left(\frac{39}{40}\right)\left(\frac{1}{40}\right)$$</p>
</blockquote>

<p>Two defective computers are selected:  Using the formula given above with $n = 2$, $k = 0$, and $p = 195/200$ yields</p>

<blockquote class=""spoiler"">
  <p> $$\binom{2}{0}\left(\frac{195}{200}\right)^0\left(\frac{5}{200}\right)^2 = \left(\frac{1}{40}\right)^2$$</p>
</blockquote>

<p>Since the three events described above are mutually exclusive and exhaustive, their probabilities should add to $1$, which you should check.</p>
"
"2387887","2387912","<p>For every $n\geqslant1$, $$[N_0=n]=[X_n\ne0]\cap\bigcap_{k=1}^{n-1}[X_k=0]$$ hence, on the event $[N_0=n]$, $N_1=n$ on $[X_n\geqslant2]$ and $N_1=n+N'_0$ on $[X_n=1]$, where $$N'_0=\min\{k&gt;n\mid X_k\ne0\}$$
Thus, on each $[N_0=n]$, $$N_1=n+N'_0\mathbf 1_{X_n=1}=N_0+N'_0\mathbf 1_{X_n=1}$$ where $N'_0$ is independent of $N_0$ and distributed as $N_0$. Furthermore, $$P(X_n=1\mid N_0=n)=P(X_n=1\mid X_n\ne0)=\frac{e^{-\lambda}\lambda}{1-e^{-\lambda}}$$ for every $n$, thus call the RHS $p$, and $$E(N_0)=\frac1{P(X_n\ne0)}=\frac1{1-e^{-\lambda}}$$
Finally, $$E(N_1\mid N_0)=N_0+E(N_0)p=N_0+\frac{e^{-\lambda}\lambda}{(1-e^{-\lambda})^2}$$</p>
"
"2387888","2387911","<p>Yes, your answer is correct. I'd remove the ""s.t."" though.</p>
"
"2387889","2387896","<p>The last interpretation is wrong, it should be $$\forall \phi \in E^*, \, x \in \ker \phi.$$ This clearly implies $x=0$; indeed, if $x \neq 0$ we have a functional $x^* \in E^*$ such that $x^*(x)=1$: just extend $x$ to a basis of $E$ and take the dual basis.</p>
"
"2387894","2387909","<p>The conversion from Cartesian coordinates to spherical coordinates is:</p>

<p>$$\begin{align}{ \rho = \sqrt{x^{2}+y^{2}+z^{2}}
\\ \theta = \arctan{\frac{y}{x}}
\\ \phi = \arccos{\frac{z}{\rho}}}\end{align}$$</p>

<p>You've already ascertained that $\theta \in [0,2\pi]$ by rotational symmetry, but you can use the equation of the cone to work out the integration limits of $\rho$ and $\phi$ by expressing them in terms of $z$. Answer below.</p>

<blockquote class=""spoiler"">
  <p> $\rho = \sqrt{z^{2}+z^{2}} = \sqrt{2}z \,;\;\phi = \arccos{\frac{1}{\sqrt{2}}} = \pi/4$. So $\rho \in [\sqrt{2},2\sqrt{2}]$, and as $\phi$ is constant you wouldn't even integrate over it (which makes sense, since you're integrating over a surface, so there should only be two integration variables, which here are $\rho$ and $\theta$).</p>
</blockquote>
"
"2387910","2387939","<blockquote>
  <p>If G is simple that means its only subgroups are e and G itself.</p>
</blockquote>

<p>No, it means that its only <strong>normal</strong> subgroups are $\{e\}$ and $G$ itself.</p>

<p>Fortunately the <em>kernel</em> of $f$ has to be a normal subgroup, so you still know that the kernel is either $\{e\}$ or $G$ itself.</p>

<blockquote>
  <p>But for it to have the identity element, there must be elements $a,bâG$, s.t. $ab=e$ or $ba=e$, which cannot be</p>
</blockquote>

<p>I cannot follow this argument.</p>

<p>Instead, <strong>hint</strong>: Suppose $f$ is a homomorphism with kernel $\{e\}$. Since $G$ is non-abelian, there exist $a,b$ such that $aba^{-1}b^{-1}\ne e$. But what is $f(aba^{-1}b^{-1})$?</p>

<p><em>Bonus question:</em> Which groups other than $S^1$ will this argument also work for?</p>
"
"2387923","2387936","<ul>
<li><p>Show that BDE and EFC are isoceles</p></li>
<li><p>Use it to write $AD+DE+EF+AF=AD+BD+AF+FC=AB+BC=56$</p></li>
</ul>
"
"2387926","2388026","<p>$$\begin{array}{ccccc}
P\left(\frac{X}{Y}\leq z\right) &amp; = &amp; P\left(Y&gt;0\wedge X\leq zY\right) &amp; + &amp; P\left(Y&lt;0\wedge X\geq zY\right)\\
 &amp; = &amp; P\left(Y&gt;0\wedge X\leq zY\right) &amp; + &amp; P\left(Y&lt;0\wedge-X\geq zY\right)\\
 &amp; = &amp; P\left(Y&gt;0\wedge X\leq zY\right) &amp; + &amp; P\left(Y&lt;0\wedge X\leq-zY\right)\\
 &amp; = &amp; P\left(Y&gt;0\wedge X\leq z|Y|\right) &amp; + &amp; P\left(Y&lt;0\wedge X\leq z|Y|\right)\\
 &amp; = &amp; P\left(\frac{X}{|Y|}\leq z\right)
\end{array}$$</p>

<p>Note that only the symmetry of $X$ and $P(Y=0)=0$ are used.</p>

<hr>

<p><strong>edit</strong> (inspired by comment of @Did):</p>

<p>Crucial is of course the equality $P\left(Y&lt;0\wedge X\geq zY\right)=P\left(Y&lt;0\wedge-X\geq zY\right)$ which is actually true because $\langle X,Y\rangle$ and $\langle -X,Y\rangle$ have equal probabilities. </p>

<p>This follows from:$$P(X\leq x\wedge Y\leq y)=P(X\leq x)P(Y\leq y)=P(-X\leq x)P(Y\leq y)=P(-X\leq x\wedge Y\leq y)$$</p>

<p>Here it is used that $X$ and $Y$ are independent and that $X$ and $-X$ have equal distribution.</p>
"
"2387931","2387938","<p>Your claim is not correct. An error in your proof is that $T$ is not a linear map on $W$. Of course you can restrict $T$ to $W$, but for $w \in W$, there is no reason for $T(w)$ to be in $W$, so you have a map from $W$ to $V$.</p>

<p>For a counterxample, consider the map on $\mathbb{C}^2$ given by 
$(z_1,z_2)\mapsto (z_2,0)$. </p>

<p>For more information on what is true, look up Jordan Canonical Form.  </p>
"
"2387932","2388021","<p>I thought it might be useful to present a way forward that admits a general approach.  To that end, we proceed.</p>

<p>Using the binomial theorem we can write </p>

<p>$$\begin{align}
\frac{1}{|\vec x-2\hat e_2|^2}&amp;=\left((\vec x-2\hat e_2)\cdot (\vec x-2\hat e_2)\right)^{-1}\\\\
&amp;=\left(|\vec x|^2 -4\hat e_2\cdot \vec x+4\right)^{-1}\\\\
&amp;=\frac{1}{|\vec x|^2}\left(1-4\frac{\hat e_2\cdot \hat x}{|\vec x|}+4\frac{1}{|\vec x|^2}\right)^{-1}\\\\
&amp;=\frac{1}{|\vec x|^2}\left(1+4\frac{\hat e_2\cdot \hat x}{|\vec x|}+O\left(\frac{1}{|\vec x|^2}\right) \right)\\\\
&amp;=\frac{1}{|\vec x|^2}+4\frac{\hat e_2\cdot \hat x}{|\vec x|^3}+O\left(\frac{1}{|\vec x|^4}\right)\tag1
\end{align}$$</p>

<p>Therefore, for $f=\log(|\vec x|)-\log(|\vec x-2\hat e_2|)$, use of $(1)$ reveals that</p>

<p>$$\begin{align}
\nabla f&amp;=\frac{\vec x}{|\vec x|^2}-\frac{\vec x-2\hat e_2}{|\vec x-2\hat e_2|^2}\\\\
&amp;=\frac{\vec x}{|\vec x|^2}-\left(\vec x-2\hat e_2 \right)\left(\frac{1}{|\vec x|^2}+4\frac{\hat e_2\cdot \hat x}{|\vec x|^3}+O\left(\frac{1}{|\vec x|^4}\right)\right)\\\\
&amp;=\color{blue}{\underbrace{2\frac{\hat e_2}{|\vec x|^2}-4\frac{(\hat e_2\cdot\hat x)\hat x}{|\vec x|^2}}_{=O\left(\frac{1}{|\vec x|^2}\right)}}+O\left(\frac{1}{|\vec x|^3}\right)\\\\
&amp;=O\left(\frac{1}{|\vec x|^2}\right)
\end{align}$$</p>

<p>as was to be shown!</p>
"
"2387937","2387977","<p>A finite-rank operator $A$ can be written as $$A = \sum_{m=1}^N \lambda_m\langle \cdot, f_m\rangle g_m$$ for some orthonormal $\{f_m\}$ and $\{g_m\}$, where $\lambda_m$ are the singular values of $A$. Then, $$\| Ae_n\|^2 = \sum_{m=1}^N \lvert \lambda_m\langle e_n, f_m\rangle\rvert^2$$ by the orthonormality of the $g_m$.Then, the problem inequality becomes $$\sum_{n=1}^{\infty}\sum_{m=1}^N \lvert \lambda_m\langle e_n, f_m\rangle\rvert^2 = \sum_{m=1}^N\lvert \lambda_m\rvert^2\sum_{n=1}^{\infty} \lvert \langle e_n, f_m\rangle\rvert^2\leq \sum_{m=1}^N\lvert \lambda_m\rvert^2\|f_m\|^2 = \sum_{m=1}^N \lvert \lambda_m\rvert^2 &lt; \infty$$ by Bessel's inequality.</p>
"
"2387951","2387973","<p>Let $$f(x) = \left( \frac{x+\log 9}{x-\log 9} \right)^x.$$ We want to compute $\lim_{x\to\infty} \log f(x)$, and then exponentiate the result.
$$
\lim_{x\to\infty} x \log \frac{x+\log 9}{x-\log 9} = \lim_{x\to\infty} \frac{\log \frac{x+\log 9}{x-\log 9}}{\frac{1}{x}} = \lim_{x\to\infty}\frac{(2\log 9) x^2}{x^2 - \log^2 9} = 2\log 9,
$$
where the second equality follows from L'Hospital Rule. The final answer should be $e^{2\log 9} = 81$.</p>
"
"2387954","2388384","<p>For any $z\in(-1,1)$ we have</p>

<p>$$ \int_{0}^{z}e^{-x}\arcsin(x)\,dx = \int_{0}^{z}e^{-x}\sum_{n\geq 0}\frac{\binom{2n}{n}}{(2n+1)4^n}x^{2n+1}\,dx $$
hence the LHS can be represented as a series involving the incomplete $\Gamma$ function.<br>
As an alternative one may exploit the Jacobi-Anger expansion
$$ e^{-z\sin\theta}=\sum_{n\in\mathbb{Z}}J_n(iz)\,e^{in\theta}=I_0(z)+2\sum_{n\geq 1}(-1)^n\left[I_{2n-1}(z)\sin((2n-1)\theta)+I_{2n}(z)\cos(2n\theta)\right] $$
to state
$$\begin{eqnarray*}\int_{0}^{z}e^{-x}\arcsin(x)\,dx &amp;=&amp; \int_{0}^{\arcsin z}e^{-\sin\theta}\theta\cos\theta\,d\theta\\
&amp;=&amp;I_0(1)\left(-1+\sqrt{1-z^2}+z\arcsin z\right)\\&amp;+&amp;\scriptstyle 2\sum_{n\geq 1}(-1)^n\left[I_{2n-1}(1)\int_{0}^{\arcsin z}\theta\cos\theta\sin((2n-1)\theta)\,d\theta+I_{2n}(1)\int_{0}^{\arcsin z}\theta\cos\theta\cos(2n\theta)\,d\theta\right]\end{eqnarray*}$$
which is a typesetting nightmare with better convergence properties, since $0\leq I_n(1)\leq \frac{I_0(1)}{2^n n!}$.<br>
Here $I_n$ is a <a href=""http://mathworld.wolfram.com/ModifiedBesselFunctionoftheFirstKind.html"" rel=""nofollow noreferrer"">modified Bessel function of the first kind</a>.</p>
"
"2387967","2387981","<p>Yes, a complete graph has independence number $\alpha(G)=1$.</p>

<p>Note that an independent set $S$ of size one is the largest set of vertices you can choose without an edge between any two vertices in the set. A set without at least two vertices results in the property being vacuously true. Pick any two distinct vertices in the set $S$ - you can't, there's only one - therefore the property is satisfied. Note that it doesn't matter that the vertex in $S$ has neighbours outside of $S$ - it only matters that there are no two vertices, both in $S$, with an edge between them in the graph.</p>
"
"2387974","2387985","<p>The quarter-circle in the third quadrant centered at the origin is
$x=-a\cos t, y=-a\sin t$.</p>

<p>Your curve is that with $a$ added to
$x$ and $y$.</p>
"
"2387987","2388022","<p>$$S={\iint_{E}{\sqrt{1+4x^2}}dxdy}$$
$$S={\int_{x=0}^1\int_{y=0}^{x}\sqrt{1+4x^2}dydx}$$
$$S={\int_{x=0}^{1}\sqrt{1+4x^2}dx}{\int_{y=0}^xdy}$$
$$S={\int_{x=0}^{1}{x\sqrt{1+4x^2}}dx}$$
By substitution $1+4x^2=u^2$$\implies$ $4xdx=udu$
$$S={\int_{u=1}^{\sqrt5}{u^2\over4}du}$$
$${S} = {(5\sqrt5-1)\over{12}}$$</p>
"
"2387994","2388037","<p>Just an idea for a start.</p>

<p>From given equation we can easily get this equation:$${P_{i+2}+P_{i}\over P_{i+1}} = {P_{i+1}+P_{i-1}\over P_{i}}$$ Since this is valid for all $i$ we have $$  {P_{i+1}+P_{i-1}\over P_{i}}= {P_{3}+P_{1}\over P_{2}} = n+1$$
and thus we have a linear equation:
$$ P_{i+1} = (n+1)P_{i}-P_{i-1}$$</p>
"
"2387997","2388017","<p>Let's make some definitions more clear before we move on to the intuition, so that we can be sure we are talking about the same things. In this context, a <em>path</em> is some possibly curved line between two endpoints. A <em>parameterization</em> of a path are the formulas $x = x(t)$ and $y = y(t)$. You can think of a path as some geometric object and a parameterization as some equations describing the path. The key here is that a path can have several parameterizations; for example, the straight line from $(0, 0)$ to $(1, 1)$ can be parameterized as $x = t, y = t$ or $x=t^2, y=t^2$, for $t \in [0, 1]$.</p>

<p>Now what about line integrals? A line integral is an integral over a <em>path</em>; ordinarily we compute a line integral using a parameterization of a path, but the original line integral won't make mention to a specific parameterization of said path. This is because <em>any parameterization of a path will yield the same line integral</em>. You can try this out with any line integral you will come across right now by using different parameterizations. Indeed, some parameterizations will be easier to calculate the integral than others.</p>

<p>Let's contrast this with what your question is asking. There are some line integrals where <em>only the endpoints</em> of the path of integration actually matter; the path itself can be anything, so long as the endpoints are as specified. This is much different than have different parameterizations (we have whole different paths!). These line integral for which the path of integration ""doesn't matter"" are called conservative, and I'm sure you will cover these types of fields in your course.</p>

<p>As a classic example, think of gravity (say, at the surface of the earth). We know an object falls from point $A$ to point $B$, and the question is how much work gravity has done to this object. Since gravity is a constant unidirectional force, you can convince yourself that no matter what meandering path is taken, gravity will do the same amount of work. For example, if the object goes up, the object must come back down, and so that initial negative work done by gravity is counteracted by the positive work when the object comes back down. Alternatively, if you have taken a physics course, you know that we can calculate the work done by gravity in this example using <em>potential</em> energy. This concept of a potential function is critical to conservative fields.</p>

<p>As a classic non-example, take friction. The longer and more winding the path you take, the more frictive forces act on the object, and the more work is done. Thus path matters here when calculating work (but again, parameterization does not).</p>
"
"2388003","2388013","<p>Since your office is the same, there was no physical change, just a change of basis, hence the change of coordinate you experienced was a <strong>passive</strong> coordinate change.</p>
"
"2388012","2388048","<p>If $\sqrt{n} + \sqrt[3]{n} = m$, then $\sqrt{n} = m - \sqrt[3]{n}$. Squaring both sides yields</p>

<p>$$\begin{equation} \tag{$\ast$} \sqrt[3]{n}^2 - 2m \sqrt[3]{n} + m^2 - n = 0. \end{equation}$$</p>

<p>Now if $n$ is not a cube of a rational number, the polynomial $x^3 - n$ does not have any rational roots, so it is irreducible over $\mathbb{Q}$ (since any factorization would involve a factor of degree one). Thus $\left[ \mathbb{Q} \big[ \sqrt[3]{n} \big] : \mathbb{Q} \right] = 3$, so $(\ast)$ may not hold. Hence $n$ is a cube of a rational number.</p>

<p>Now $\sqrt{n} = m - \sqrt[3]{n}$ is rational, so $n$ is also a square of a rational number, therefore $n = a^6$ for some $a \in \mathbb{Q}$. </p>

<hr>

<p><strong>Edit:</strong> since my solution involves some field theory the reader may be unfamiliar with, I will introduce the necessary part below.</p>

<p>Let $\mathbb{Q}[x]$ stand for the set of all polynomials with rational coeffiecients. </p>

<p><strong>Claim.</strong> Let $n \in \mathbb{Q}$ and $\alpha = \sqrt[3]{n}$. Suppose the polynomial $p(x) = x^3 - n$ is irreducible over $\mathbb{Q}$, that is, whenever $p(x) \equiv u(x) \cdot v(x)$ for some $u(x), v(x) \in \mathbb{Q}[x]$, either $u(x)$ or $v(x)$ is constant. Then there is no nonzero polynomial $q(x) \in \mathbb{Q}[x]$ of degree at most $2$ such that $q(\alpha) = 0$.</p>

<p><strong>Proof.</strong> Suppose that $q(\alpha) = 0$ fo some nonzero polynomial $q(x) \in \mathbb{Q}[x]$ of degree at most $2$. By polynomial division we find $u(x), r(x) \in \mathbb{Q}[x]$ such that $r(x) \equiv 0$ or $\deg r(x) &lt; 2$ and</p>

<p>$$p(x) \equiv u(x) \cdot q(x) + r(x).$$</p>

<p>Substituting $x = \alpha$, we get</p>

<p>$$0 = u(\alpha) \cdot 0 + r(\alpha),$$</p>

<p>i.e. $r(\alpha) = 0$. </p>

<ul>
<li><p>If $r(x) \equiv 0$, we get a contradiction, because $p(x) \equiv u(x) \cdot q(x)$ is a factorization of $p(x)$ over $\mathbb{Q}$ with non-constant factors.</p></li>
<li><p>If $\deg r(x) = 0$ so $r(x)$ is constant nonzero, this is an immediate contradiction.</p></li>
<li><p>If $\deg r(x) = 1$, then $r(\alpha) = 0$ implies $\alpha \in \mathbb{Q}$ so $p(x)$ may be reduced by the factor $x-\alpha$, which is a contradiction.</p></li>
</ul>

<p>So the claim is proved. $\blacksquare$</p>

<p>Now it suffices to note that $(\ast)$ may be written in form $q(\alpha) = 0$, where </p>

<p>$$q(x) = x^2 - 2mx + m^2 - n$$</p>

<p>has rational coefficients and degree $2$.</p>
"
"2388015","2388059","<p>The algebra generated by the coordinate functions is just the set of polynomials that vanish at $0.$ So if $0\notin A,$ you will get density in $C(A)$ by Stone-Weierstrass, otherwise you'll get density in the continuous functions on $A$ that vanish at $0,$ again using S-W.</p>
"
"2388016","2388204","<p>Consider $\mathbb{C}$ as a $\mathbb{Z}/2\mathbb{Z}$-graded ring with $\mathbb{R}$ in degree $0$ and $i\mathbb{R}$ in degree $1$.  Then $1+i$ is not homogeneous, but $(1+i)^2=2i$ is.</p>
"
"2388024","2388202","<p>$\newcommand{\v}{\operatorname{var}} \newcommand{\c}{\operatorname{cov}}$
\begin{align}
&amp; \v(a\hat{\theta}_1 + (1-a)\hat{\theta}_2) \\[6pt]
= {} &amp; \v(a\hat\theta_1) + \v((1-a)\hat\theta_2) + 2\c(a\hat\theta_1,(1-a)\hat\theta_2) \\[6pt]
= {} &amp; a^2 \v(\hat\theta_1) + (1-a)^2\v(\hat\theta_2) + 2a(1-a)\c(\hat\theta_1,\hat\theta_2) \\[4pt]
= {} &amp; a^2\sigma_1^2 + (1-a)^2\sigma_2^2 + 2a(1-a)c.
\end{align}
(A standard exercise asks you to find the value of $a$ that minimizes this.)</p>
"
"2388027","2388145","<p>If you multiply $A^T A$, the $ij$ entry of the result will tell you how many $1$s the $i$-th and $j$-th column have in common positions.</p>
"
"2388031","2388137","<p>If $\mu$ is known, then an unbiased estimator of the population variance $\theta = \sigma^2$ is given by
$$\hat \theta = \frac{\sum_{i=1}^n (X_i - \mu)^2}{n}.$$
With this definition, we have
$$\frac{n\hat\theta}{\theta} = \sum_{i-1}^n \left(\frac{X_i - \mu}{\sigma}\right)^2 = \sum_{i=1}^n Z_i^2 \sim \mathsf{Chisq}(df = n),$$
where $Z_i \stackrel{iid}{\sim}\mathsf{Norm}(0,1),$ and the last
step is the definition of $\mathsf{Chisq}(n).$</p>

<p>With this distribution for $Q = n\hat\theta/\theta,$ one can find
constants $L$ and $U$ such that 
$$P\left(L \le Q = \frac{n\hat\theta}{\theta} \le U\right) = 0.95.$$
Then by manipulating the inequalities, we have 
$P(n\hat\theta/U &lt; \theta &lt; n\hat\theta/L) = 0.95,$
so that a 95% confidence interval for $\theta$ is of the form
$(n\hat\theta/U,\, n\hat\theta/L).$</p>

<p><em>Note:</em> If $\mu$ is <em>unknown</em> and estimated by the sample mean $\bar X,$
then point and interval estimation of $\theta$ are slightly different. In this case, an unbiased estimator of $\theta = \sigma^2$ is
$S^2 = \frac{\sum_{i=1}^n (X_i - \bar X)^2}{n-1}.$
Then $Q^\prime =\frac{(n-1)S^2}{\theta} \sim \mathsf{Chisq}(df = n-1)$ and a $95\%$ CI for $\theta$ is of the form 
$\left(\frac{(n-1)S^2}{U},\, \frac{(n-2)S^2}{L}\right),$ where $L$ and $U$
cut $2.5\%$ from the lower and upper tails (respectively) of $\mathsf{Chisq}(n-1).$
In this case, it is not quite so easy to prove that $Q^\prime \sim 
\mathsf{Chisq}(n-1).$</p>
"
"2388032","2390088","<p>I have solved this problem now, the quadratic BÃ©zier curve will be on some form $f(x) = a + bx + cx^2$, thus the derivative of the curve at the points $(1,1)$ and $(0,0)$ will be its tangents.</p>

<p>These tangents will meet at the control point of the quadratic BÃ©zier curve which is easily determined since from the slopes and points we can determine their equations.</p>
"
"2388041","2389035","<p>See Corollary on page 333 of J.Munkres, Obstructions to the smoothing of piecewise-differentiable homeomorphisms, Bull. Amer. Math. Soc.
Volume 65, Number 5 (1959), 332-334:</p>

<blockquote>
  <p>Every homeomorphism of smooth 3-manifolds can be approximated by diffeomorphisms. </p>
</blockquote>

<p>It is probably explained in more details in </p>

<p>J.Munkres, Obstructions to the smoothing of piecewise-differentiable homeomorphisms. Ann. of Math. (2) 72 (1960) 521--554.</p>
"
"2388066","2388138","<p>Your first step (in the comment) is right, and I assume that, when you wrote ""$x^2\ln a$ is equal to $0$,"" you meant that its third derivative is $0$, which is also true. So now you need to differentiate $x^2\ln x$ three times. By the product rule, the first derivative is $2x\ln x+x$. Two more differentiations will annihilate the $x$ term, so you just need to differentiate $2x\ln x$ twice more. A second differentiation, again using the product rule, gives you $2\ln x+2$, and finally a third differentiation gives $2/x$. The problem says to evaluate this at $x=4$, so you get $1/2$.</p>
"
"2388076","2388098","<p>Every short exact sequence
$$0\to A\to B\to C\to0$$
with $A$ an injective object, splits. Therefore $B\cong A\oplus C$.
If $B$ is also an injective object, then $C$ is a direct summand
of an injective object, and so is a injective object itself.</p>
"
"2388095","2388108","<p>But it's just $\left(\frac{2+(-1)}{2},\frac{0+(-3)}{2}\right)$ or $(0.5,-1.5)$.</p>
"
"2388099","2388135","<p>The answer depends on what exactly your definition of a commutative ring is: whether it's required to have a multiplicative identity $1$ or not.</p>

<p>If $R$ does have a multiplicative identity $1$, then $1 = a + b$ for some $a \in I, b \in J$.  Then $1 = (a + b)^3 = (a + 3b) \cdot a^2 + (3a + b) \cdot b^2 \in I^2 + J^2$, so $I^2 + J^2$ is the entire ring.</p>

<p>If the definition of (commutative) ring you're using does not require a multiplicative identity $1$, then consider the subring of $\mathbb{Z}[x,y]$ generated by $x$ and $y$.  This is the subring of polynomials with a zero constant term.  Let $I := \langle x \rangle$ and $J := \langle y \rangle$.  Then $I^2 + J^2 = \langle x^2, y^2 \rangle$ is the ideal of polynomials such that the $x$, $y$, $xy$ coefficients are all zero.  In particular, $x \notin I^2 + J^2$, so $I^2 + J^2$ is not all of $R$.</p>
"
"2388100","2388196","<p>As pointed out in the comments, the usual way to solve an inhomogeneous solution is to use the <a href=""https://en.wikipedia.org/wiki/Green%27s_function"" rel=""nofollow noreferrer"">Green's function</a> for the differential operator, which depends on the region in which the differential equation is to be solved.</p>

<p>If the region has spherical symmetry, however, one can just look for a particular solution that has the same symmetry: i.e. a function of $r$. One then solves the equation
$$ \nabla^2 v(r) = \frac{1}{r^2} (r^2v')' = c, $$
which has particular solution $v(r)=r^2/6$.</p>

<p>In either case, the homogeneous solution is then included to make the equation satisfy the boundary conditions, as one would with an ordinary differential equation. The boundary conditions can also be incorporated directly using the Green's function using <a href=""https://en.wikipedia.org/wiki/Green%27s_identities"" rel=""nofollow noreferrer"">Green's identities</a>, see also <a href=""https://en.wikipedia.org/wiki/Green%27s_function#Green.27s_functions_for_the_Laplacian"" rel=""nofollow noreferrer"">this section</a> of the Wikipedia article on Green's functions.</p>
"
"2388109","2390084","<p>The conclusion is that numerical stability is subjective, so instability increases as the condition number increases however determining at what point it is considered unstable is subjective.</p>
"
"2388127","2388180","<p>In elementary school, it's hard for kids to ""switch between notation"", as we have done later on in our lives.</p>

<p>We start with $\div$, then we go to $/$, and now we do something like $\frac{a}{b}$.</p>

<p>In elementary, students are not familiar with ""fractions"", or just know the very basics about them. Hence the $\frac{a}{b}$ ratio doesn't make sense ... not until you actually use fractions in middle school and so on.</p>

<p>It's important to realise that $'/'$ the slash, <strong>refers to fraction</strong>. When we write $(a/b)$, it's because we are lazy and do not want to write $\frac{a}{b}$. So technically it's the same thing as fraction.</p>

<p>This is why students in elementary just use the regular $\div$ symbol. It's just a symbol. No fractions or anything complicated just yet. It's the same reason why kids in elementary use $\times$ to indicate multiplication and not the dot, $\cdot$, as we use when we are older. They do not deal with complex numbers, and so these symbols that we use as adults are really after we see the use of math. In elementary, it's very basic, hence basic symbols.</p>

<p>In the end it's because of the type of math we do. Imagine writing something like this:</p>

<p>$$\int^{\dfrac{x}{5}}_{0}(x^3+\frac{x^2}{2}+\ln(\sin(\frac{x}{4})))dx$$</p>

<p>$$\int^{x\div 5}_{0}(x^3+(x^2 \div 5)+\ln(\sin(x\div 4)))dx$$</p>

<p>We even had to add extra parentheses to make it clear that $x^2\div 5$ is one thing. Heavier math = more compact symbols</p>
"
"2388128","2388133","<p>Chinese Remainder Theorem</p>

<p><a href=""https://en.wikipedia.org/wiki/Chinese_remainder_theorem"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Chinese_remainder_theorem</a></p>

<p>There are a few different ways to actually find the desired value.  What works best often depends on what moduli (divisors) you are working with.</p>
"
"2388129","2388237","<p>Hint: Choose $J$ such that $\sum_{j=1}^{J}|x_j|&gt; 1-\epsilon.$ Because $J$ is now fixed, we'll have</p>

<p>$$\sum_{j=1}^{J}|x_{nj}-x_j| &lt; \epsilon$$</p>

<p>for large $n.$ For such $n,$ we then have</p>

<p>$$\sum_{j=1}^{J}|x_{nj}| \ge \sum_{j=1}^{J}|x_j|-\sum_{j=1}^{J}|x_{nj}-x_j| &gt; 1-\epsilon - \epsilon.$$</p>

<p>Thus we will have both $\sum_{j&gt;J}|x_{j}|, \sum_{j&gt;J}|x_{nj}|$ small for large $n.$</p>
"
"2388134","2388195","<p>Suppose that the series $a_k^2$ is divergent, put $S_n=\sum_1^n a_kÂ²$. Then $S_n \to +\infty$; hence we may suppose that $S_n&gt;0$ for $n\geq 1$. Put $b_n=\frac{a_n}{S_n }$. Then as
$$b_n^2=\frac{S_{n}-S_{n-1}}{S_n^2}\leq \int_{S_{n-1}}^{S_n}\frac{dt}{t^2}$$ we see that the series $b_n^2$ is convergent. The hypothesis gives now that the series $u_n=\frac{a_n^2}{S_n}=\frac{S_n-S_{n-1}}{S_n}$ is convergent. Hence $u_n\to 0$, and $\frac{S_{n-1}}{S_n}\to 1$. This imply that $u_n$ is equivalent to $v_n=\frac{S_n-S_{n-1}}{S_{n-1}}$, and hence (the two series are positive) the series $v_n$ is convergent. But
$$v_n=\frac{S_n-S_{n-1}}{S_{n-1}}\geq \int_{S_{n-1}}^{S_n}\frac{dt}{t}$$
and this show that the series $v_n$ is divergent, a contradiction. </p>
"
"2388142","2388163","<p>Given a countably additive function $\rho$ on an elementary family $\mathcal{E}$ such that $\rho(\emptyset)=0$, there exists a unique premeasure $\mu_0$ defined on the algebra $\mathcal{A}(\mathcal{E})$ generated by $\mathcal{E}$. Then define an outer measure $\mu^*$ via
$$
\mu^*(A) := \inf\left\{\sum_1^\infty\mu_0(E_j) : E_j\in\mathcal{A}(\mathcal{E})\ \text{and}\ A \subseteq \bigcup_1^\infty E_j\right\}
$$
and apply CarathÃ©odory's theorem to get a measure $\mu$ whose measurable sets include $\sigma(\mathcal{E})$ and $\mu(E)=\mu^*(E)=\mu_0(E)=\rho(E)$ for every $E\in\mathcal{E}$.</p>

<p>Then you just have to see that $\sigma(\mathcal{E})$ is the collection of Borel sets in the plane. This follows from the fact that every open set is in $\sigma(\mathcal{E})$, as every open set is a countable union of open rectangles.</p>
"
"2388160","2388168","<p>$$x^{12}+1=(x^4+1)(x^8-x^4+1)=0.$$
Thus, $x^{48}+x^{60}=1-1=0$.</p>

<p>In another hand $x^{52}+x^{56}=x^{48}(x^4+x^8)=x^4+x^8=2x^4-1=\pm\sqrt3i$</p>
"
"2388173","2388189","<p>The derivative of $\cos x$ at $x=\pi/2$ is $-1$, and this should match $q_1'(\pi/2)$. Thus, since $q_1'(x)=2a_1x+b_1$, we have
$$2a_1(\pi/2)+b_1=-1.$$</p>

<p>This is the third equation for the first interpolant; the second interpolant will be similar. </p>
"
"2388179","2393014","<p>The second part of your proof is incorrect.
Convexity is not a pointwise property, but a property of the function on a specific set (its domain in most cases). So, saying ""it is also convex for $t=0$"" is inaccurate. </p>

<p>I would simply use definition of convexity to prove this part:</p>

<p>($\Leftarrow$) Let us take $x_1,x_2\in\text{dom}f$. We need to show that for every $\alpha\in[0,1]$
\begin{equation}
\alpha f(x_1)+(1-\alpha)f(x_2)\geq f(\alpha x_1+(1-\alpha)x_2).
\end{equation}
Now, since $g(t)=f(x+vt)$ is convex for all $x\in\text{dom}f$ and all $v$, for every $\alpha\in[0,1]$:
\begin{align}
\alpha g(t_1)+(1-\alpha)g(t_2)&amp;\geq g(\alpha t_1+(1-\alpha)t_2) \\
\alpha f(x+vt_1)+(1-\alpha)f(x+vt_2)&amp;\geq f(x+v(\alpha t_1+(1-\alpha)t_2))
\end{align}
let us take $x=x_1$, $v=x_2-x_1$, $t_1=0$ and $t_2=1$, and assign them to the last inequality:
\begin{equation}
\alpha f(x_1)+(1-\alpha)f(x_2)\geq f(\alpha x_1+(1-\alpha)x_2)
\end{equation}
and therefore $f$ is convex.</p>
"
"2388185","2388192","<p>Let $H$ be the event that heads appears, $T$ the event that the two-headed coin is drawn, $F$ the event that the fair coin is drawn, and $W$ the event that the weighted coin is drawn.</p>

<p>For part (a), we just need to compute $P(H)$. Since $T$, $F$, and $W$ partition the sample space (i.e., they are pairwise disjoint and their union accounts for all possibilities), we can use the law of total probability to get
$$
P(H) = P(H \mid T)P(T) + P(H\mid F)P(F) + P(H\mid W)P(W) = (1)(1/3) + (1/2)(1/3) + (1/3)(1/3) = 33/54.
$$</p>

<p>Now, for part (b), we use Bayes' theorem to compute
$$
P(T \mid H) = \frac{P(H\mid T)P(T)}{P(H)} = \frac{(1)(1/3)}{(33/54)}=6/11.
$$</p>

<hr>

<p>To better understand what is going on with Bayes' theorem, I find it useful to derive the formula above by using the definition of conditional probability:
$$
P(A\mid B) = \frac{P(A\cap B)}{P(B)}.
$$
This definition shows that $P(A\cap B) = P(A\mid B)P(B)$, and so by the symmetry of intersection, $P(A\cap B)=P(B\cap A) = P(B\mid A)P(A)$. This yields the formula
$$
P(A\mid B) = \frac{P(A\cap B)}{P(B)}
= \frac{P(B\mid A)P(A)}{P(B)}.
$$
Thus, in a way, we sort of ""reverse"" our assumption from $B$ to $A$.</p>

<p>Interestingly enough, this is also how the law of total probability is derived. If $X_1,\ldots,X_n$ partition the sample space, then we have
$$
P(A) = P(A\cap X_1) + \cdots + P(A\cap X_n) = P(A\mid X_1)P(X_1)+\cdots +P(A\mid X_n)P(X_n).
$$</p>
"
"2388193","2391360","<p>Your matrix elements for the adjoint are not correct. You are seemingly calculating the matrix elements for $A$ in the basis $(e_i)_i$ which is not orthonormal. Using your scalar product it should then be $w_j^2$ for the adjoint in the $(i,j)$-th place. One has:
$$ \langle A^* e_m , e_k \rangle = \langle e_m,A e_k\rangle = w_m^2 w_k^2 \delta_{m,j}\delta_{k,i}$$</p>

<p>The symmetry becomes perhaps even more apparent if you work in the basis $(u_i)_i$ of unit vectors for your norm. Write:
$$ A = A_{i,j}  = w_i w_j \langle \cdot, u_i \rangle u_j$$
and the adjoint is simply $A^* = A_{j,i}$. Thus $A^* A = w_i^2 w_j^2 
  \langle \cdot, u_i \rangle u_i$ from which you get that the two norms are identical. This is also as one should expect since $A$ has rank 1.</p>
"
"2388194","2388211","<p>You wrote that $m.0=0.m$ is already shown. This is not true. By definition, $m.0=0$, but you have to prove that $0.m=0$. This is easy by induction: $0.0=0$ and if $0.m=0$, then$$0.(m+1)=0.m+0=0+0=0.$$</p>

<p>You can prove the equality $n.m+m=(n+1).m$ by induction on $m$. For $m=0$, this is just $n.0+0=(n+1).0$, but both of them are $0$. On the other hand, if $n.m+m=(n+1).m$, then\begin{align*}n.(m+1)+(m+1)&amp;=n.m+n+m+1=n.m+m+n+1\\&amp;=(n+1).m+(n+1)\\&amp;=(n+1).(m+1).\end{align*}</p>
"
"2388199","2388212","<p><strong>HINTS:</strong></p>

<p>For $1)$, write</p>

<p>$$\sum_{n=0}^\infty p^n\sin(2nx)=\text{Im}\left(\sum_{n=0}^\infty (p\,e^{i2x})^n\right)\tag1$$</p>

<p>Then sum the geometric series on the right-hand side of $(1)$ and take the imaginary part of that result.</p>

<hr>

<p>For $3)$, note that</p>

<p>$$\int_0^\pi \frac{1}{(a+\cos(\theta))^2}\,d\theta=-\frac{d}{da}\int_0^\pi \frac{1}{a+\cos(\theta)}\,d\theta\tag 2$$</p>

<p>The integral on the right-hand side of $(2)$ can be evaluated using the classical <a href=""https://en.wikipedia.org/wiki/Tangent_half-angle_substitution#The_substitution"" rel=""nofollow noreferrer"">tangent half-angle substitution</a> or contour integration.  Finish by taking the derivative with respect to $a$ and multiplying by $-1$.</p>

<hr>

<p>For $4)$ note that </p>

<p>$$\int_0^\infty x^6e^{-x^2}\,dx=-\left.\left(\frac{d^3}{da^3}\int_0^\infty e^{-ax^2}\,dx\right)\right|_{a=1} \tag 3$$</p>

<p>The integral on the right-hand side of $(3)$ is the form of the classical <a href=""https://en.wikipedia.org/wiki/Gaussian_integral#The_integral_of_a_Gaussian_function"" rel=""nofollow noreferrer"">Gaussian integral</a>.  Take its third derivative, evaluate at $a=1$, and multiply by $-1$.</p>
"
"2388203","2389447","<p>I still have not understood your problem completely, so this is only a partial answer. </p>

<p>I believe you are interested in a function $g : I \rightarrow J$, but you do not have an explicit formula for $g$. Here $I$ and $J$ are real intervals. However, it is known that $y = g(x)$ satisfies an equation of the form $$f(x,y) = 0,$$
where $f : I \times J \rightarrow \mathbb{R}$. In this situation, you can certainly try to apply Newton's method. It takes the following form
$$ y_{n+1} = y_n - \frac{f(x,y_n)}{\frac{\partial f}{\partial y}(x,y_n)},$$
where the initial value $y_0$ should be close to the target value $y=g(x)$ to ensure convergence. I want to stress, that the derivative is merely the regular partial derivative of $f$ with respect to its second variable and that the chain rule is not involved here.</p>

<p>It is entirely possible that computing this derivative will be quite expensive in your specific case. It may be worthwhile to apply the secant method instead. It converges at a slower rate than Newton's method, but can be substantially faster because it only requires $1$ rather than $2$ function evaluations per iteration. Ideally, you should combine the secant method with bisection in order to obtain a method which is robust.</p>

<p>You mentioned a specific example, that of 
$$h(x,g(x)) = a(x)g(x).$$
I much prefer to write
$$h(x,y) = a(x)y.$$
With this notation all confusion evaporates and we have
$$\frac{\partial h}{\partial y}(x,y) = a(x).$$
The notation you cited is common in physics texts. I object to it precisely because of the confusion it can generate.</p>

<p>EDIT: It seems prudent to mention the related case of 
$$k(x) = h(x,g(x)).$$
Then if $h = h(x,y)$ and $g = g(x)$ are differentiable, we have that $k=k(x)$  is differentiable, with
\begin{align}
k'(x) &amp;= \frac{\partial h}{\partial x}(x,g(x))\cdot \frac{\partial x}{\partial x}(x) + \frac{\partial h}{\partial y}(x,g(x))\cdot \frac{\partial g}{\partial x} (x) \\
&amp; = \frac{\partial h}{\partial x}(x,g(x)) + \frac{\partial h}{\partial y}(x,g(x))g'(x).
\end{align}</p>
"
"2388205","2388227","<p>How about this order? (Using physics notation, where we place the differential form next to the integral sign)</p>

<p>$$\int_0^{1-b} dx_1 \int_{x_1+b}^{\min(1,x_1+2a)} dx_3 \int_{x_3-a}^{x_1+a} dx_2$$</p>

<p>That seems reasonable to simplify. To get the desired probability, don't forget to normalize by the volume of the simplex $x_1&lt;x_2&lt;x_3$, which is $\frac{1}{3!}$.</p>
"
"2388206","2388214","<p>An exercise in Folland's ""Real Analysis"" states:</p>

<blockquote>
  <p>Suppose $0&lt;p&lt;q&lt;\infty$. Then $L^p\not\subseteq L^q$ iff $X$ contains sets of arbitrarily small positive measure, and $L^q\not\subseteq L^p$ iff $X$ contains sets of arbitrarily large finite measure.</p>
</blockquote>

<p>From this it would appear that you need to not have sets of arbitrarily small positive measure, like for the counting measure on $\mathbb{N}$ to form the $\ell^p$ spaces.</p>
"
"2388213","2388224","<p>Yes! There are several strategies you can use to attempt to normalize the input before being sent of the a softmax function. This is often a desired behavior due to softmax's asymptotes that quickly rise with even a relatively small input. However, there are no ways where you can truly specify a range short of normalizing the inputs along each layer. What you can do is try to keep them small in proportion to keeping them accurate. This is on of the main battles of correctly fitting a neural network.</p>

<p>There are two main approaches you can take to do this. I believe the prior is much more of what you're looking for.</p>

<h2>1. Input Control</h2>

<p>Controlling input to a network involves two things. Controlling the values of the inputs themselves, and controlling the weights that mutate those inputs. </p>

<h3>a) Normalize Your Input</h3>

<p>You are going to want to use some sort of normalization on your initial input to scale your feature vectors appropriately. A standard approach is to scale the inputs to have mean 0 and a variance of 1. Also linear decorrelation/whitening/pca helps a lot.</p>

<h3>b) Use a regularization term in your loss function.</h3>

<p>When adding a regularization term in your loss function, you can allow back prop to train for keeping the weights of your network small as to control the value going into the activation function. There are two main regularization terms for the kernel (weights).</p>

<p>$l1$ regularization:</p>

<p>$new\_loss\_f = old\_loss\_f + kernel$</p>

<p>and $l2$ regularization:</p>

<p>$new\_loss\_f = old\_loss\_f + \frac{1}{2}kernel^2$</p>

<p>For example, cross entropy loss with l2 regularization becomes:</p>

<p>$$\mathcal{L}(X, Y) = -\frac{1}{n} \sum_{i=1}^n y^{(i)} \ln a(x^{(i)}) + \left(1 - y^{(i)}\right) \ln \left(1 - a(x^{(i)})\right) + \frac{1}{2}K^2$$</p>

<p>Thus, your loss function accounts for the size of the kernel, or weights, and punishes them being larger. In optimization (back-prop), the weights will be adjusted accordingly to of course mitigate this additional term.</p>

<h2>2. Batch Normalization</h2>

<p>This is a less common technique, but some choose to use <a href=""https://arxiv.org/abs/1502.03167"" rel=""nofollow noreferrer""> batch normalization</a> to control the inputs to the hidden layers. This is mostly used to speed up training, but does have theoretical control over the inputs to different hidden layers.</p>
"
"2388221","2393099","<blockquote>
  <p><strong>Theorem</strong> .Let $f$ be a complex valued function then </p>
  
  <p>$$f(z) - \overline{f(z)} = 2i \,\mathrm{Im}f(z)$$
  $$f(z) + \overline{f(z)} = 2 \,\mathrm{Re}f(z)$$</p>
</blockquote>

<p>$\textit{proof}$</p>

<p>Rewrite 
$$f(z) = \mathrm{Re}f(z)+ i \mathrm{Im}f(z)$$</p>

<p>Then </p>

<p>\begin{align}f(z) - \overline{f(z)} &amp;= \mathrm{Re}f(z)+ i \mathrm{Im}f(z)-\overline{\mathrm{Re}f(z)+ i \mathrm{Im}f(z)}\\
&amp;=\mathrm{Re}f(z)+ i \mathrm{Im}f(z) - \mathrm{Re}f(z)+ i \mathrm{Im}f(z)\\
&amp;=2i \,\mathrm{Im}f(z)
\end{align}</p>

<p>Similarly </p>

<p>\begin{align}f(z) + \overline{f(z)} &amp;= \mathrm{Re}f(z)+ i \mathrm{Im}f(z)+\overline{\mathrm{Re}f(z)+ i \mathrm{Im}f(z)}\\
&amp;=\mathrm{Re}f(z)+ i \mathrm{Im}f(z) + \mathrm{Re}f(z)- i \mathrm{Im}f(z)\\
&amp;=2\,\mathrm{Re}f(z)
\end{align}</p>

<hr>

<blockquote>
  <p><strong>Corollary</strong>. If $\overline{f(z)} = f(\bar{z})$ then 
  $$f(z) - f(\bar{z}) = 2i \,\mathrm{Im}f(z)$$
  $$f(z) + f(\bar{z}) = 2 \,\mathrm{Re}f(z)$$</p>
</blockquote>

<hr>

<p>Let $z = x+iy$</p>

<p>\begin{align}e^{iy}\mathrm{Ei}(z) - \overline{e^{iy}\mathrm{Ei}(z)} &amp;= 
e^{iy}\mathrm{Ei}(z) - \overline{e^{iy}}\overline{\mathrm{Ei}(z)} \\
&amp;=e^{iy}\mathrm{Ei}(z) - e^{-iy}\overline{\mathrm{Ei}(z)} \\
&amp;= 2i \, \mathrm{Im} \{ e^{iy}\mathrm{Ei}(z)\}
\end{align}</p>

<p>Hence what is remaining is to prove that </p>

<p>$$\overline{\mathrm{Ei}(z)} = \mathrm{Ei}(\bar{z})$$</p>

<p>$\textit{proof}$</p>

<p>\begin{align}\overline{\mathrm{Ei}(z)} &amp;= \overline{\int^\infty_1\frac{e^{-zx}}{x}\,dx} \\
 &amp;= \int^\infty_1\overline{\frac{e^{-zx}}{x}}\,dx\\
 &amp;= \int^\infty_1\frac{e^{-\bar{z}x}}{x}\,dx\\
 &amp;= \mathrm{Ei}(\bar{z})
\end{align}</p>
"
"2388229","2388233","<p>An odd integer greater than $1$ is the sum of two squares if and only if every prime factor of the form $4k+3$ in the factorization occurs with a power with even exponent.</p>

<p>Since the product of two distinct prime numbers of the form $4k+3$ results in a number of the form $4k+1$, such a product is a counterexample.</p>
"
"2388230","2388241","<p>$$\sum_{k=1}^n \frac{1}{(log_x 2^k )*log_x (2^k*2))}=\\
\sum_{k=1}^n \frac{1}{k(log_x 2 )\times(k+1)(log_x 2)}=\\
\frac{1}{(log_x 2)^2}\sum_{k=1}^n \frac{1}{k(k+1)}=\\
\frac{1}{(log_x 2)^2}\sum_{k=1}^n \frac{(k+1)-(k)}{k(k+1)}=\\
\frac{1}{(log_x 2)^2}\sum_{k=1}^n \frac{1}{k}-\frac{1}{k+1}=\\\frac{1}{(log_x 2)^2}(1-\frac{1}{n+1})=\\\to \\
\frac{1}{(log_x 2)^2}(\frac{n}{n+1})=\frac{4n}{n+1}$$ simplify $ \frac{n}{n+1}$
$$\frac{1}{(log_x 2)^2}=4\\(log_x 2)^2=\frac14\\log_x 2=\pm\frac12\to \\
\begin{cases}log_x 2=\frac12 \to &amp; \log_2 x=2 \to x=2^2=4\\log_x 2=-\frac12\to &amp; \log_2 x=-2 \to x=2^{-2}=\frac14 \end{cases}$$</p>
"
"2388245","2388269","<p>$U$ is a surface in $3$-space, whereas $D\subset{\mathbb R}^2$ is the parameter domain of $U$ in the $(s,t)$-plane. In this plane we have the natural ""surface element"" ${\rm d}(s,t)$, that just measures ordinary euclidean area. There is indeed a scaling factor between ${\rm d}(s,t)$ and the corresponding area element ${\rm d}S$ on $U$. This scaling factor appears explicitly in your formula, it is $|{\bf r}_s\times{\bf r}_t|$. In other words: To a tiny rectangle in $D$ of area ${\rm d}(s,t)$ corresponds a tiny parallelogram on $U$ of area $|{\bf r}_s\times{\bf r}_t|\&gt;{\rm d}(s,t)$.</p>

<p>The <em>Jacobian</em>, on the other hand, is the analogous local scaling factor if we  compute a volume integral over a three-dimensional domain $\Omega$ in ""geometric"" three space by parametrizing $\Omega$ using some nice domain $D\subset{\mathbb R}^3$ in an auxiliary parameter space. In this situation we have an essentially bijective $$f:\quad D\to\Omega,\qquad(u,v,w)\mapsto \bigl(x_1(u,v,w), x_2(u,v,w), x_3(u,v,w)\bigr)\ ,$$
and then have to compute the Jacobian $J_f(u,v,w)={\rm det}\bigl(df(u,v,w)\bigr)$.</p>
"
"2388248","2388290","<p>Set $G:=M+N$ (which we know is odd). The operation here - on both jugs - is doubling $\bmod G$.  Looking just at the $M$ jug, you either have<br>
$M\to 2M \quad$ or<br>
$\begin{align}
M\to\  &amp;G-2N \\ 
&amp;= G-2(G-M) \\ 
&amp;= 2M-G
\end{align}$</p>

<p>So the cycle length will divide the order of $2\bmod G$ (the smallest value $k$ such that $2^k\equiv 1 \bmod G$), which itself will divide the <a href=""https://en.wikipedia.org/wiki/Carmichael_function"" rel=""nofollow noreferrer"">Carmichael function</a> $\lambda(G)$.</p>

<p>The minimum cycle occurs when $M=2N$ for example $(M,N)=(6,3)$</p>

<p>If $G$ is prime and  $2$ is a <a href=""https://en.wikipedia.org/wiki/Primitive_root_modulo_n"" rel=""nofollow noreferrer"">primitive root</a> $\bmod G$, and assuming the jugs are labelled, you will have a full cycle of all $G{-}1$ possibilities for every starting configuration.</p>

<p>In the case $G=7$, we have that $2$ is not a primitive root, giving cycles of length $3$:</p>

<p><a href=""https://i.stack.imgur.com/wMRYz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wMRYz.png"" alt=""enter image description here""></a></p>

<p>(here the blue squares are valid starting possibilities as defined, again taking the jugs as labelled)</p>

<hr>

<p>For your example $M=104, N=47$, we have $G=151$ which is prime. The order of $2 \bmod 151$ will divide $151{-}1=150$ by <a href=""https://en.wikipedia.org/wiki/Euler%27s_theorem"" rel=""nofollow noreferrer"">Euler's theorem</a>. We'd need to check the values of $2^i \bmod 151$ to find the cycle length; depending on what tools you have available, it's probably just as easy to run the calculation on $M$.</p>

<p>$104 \to 57 \to 114 \to 77 \to 3 \to 6 \to 12 \to 24 \to 48 \to 96 \to 41 \to 82 \to 13 \to 26 \to 52 \to 104$</p>

<p>So when $M\le 75$ - meaning $M&lt;N$ - the next value is $2M$, otherwise the next value is $2M-151$ as I explained before.</p>

<p>I don't know of any particular shortcut that will give you the minimum values, unless you have a full cycle of $G-1$ (in which case the minimum is $1$).</p>

<hr>

<p>For $G=185$, we have $185=5\times 37,$ then $\lambda(185) = {\rm lcm}( \lambda(37),\lambda(5)) = {\rm lcm}(36,4) = 36$ So we expect a cycle length of (or dividing) $36$ (marked jugs). Also, for initial states with multiples of $37$, we'd expect a cycle length of $4$. </p>

<p>Checking the cycle on $1$, we indeed get a cycle length of $36$ (the second half just being the same as the first but with jug quantities reversed). So there should be $5$ such cycles, since each cycle runs the the full set of congruences $\bmod 37$ (coprime to $37$) and there are $5$ of each such value in the range. And then the $4$-cycle on multiples of $37$ will complete the set, $5\times 36+4=184$.</p>

<p>The minimum value in the cycle will obviously not be a member of any other cycle. Also in the case of $G=185$ you know that $5$ and $37$ will be the smallest member of whichever cycles they are in, since they are prime divisors of $185$ and all quantities in that cycles will be divisible by those starter primes. But in general finding the minimum of every cycle is not quick. I believe it's always prime (or $1$).</p>
"
"2388250","2388409","<p>There are several problems here.</p>

<p>First there is a typographical one: it is silly for you to use the arrows $\color{blue}{\rightarrow}$. What you are asserting is that those functions are <em>equal</em>. So you should be writing $\frac{d}{dx}\int_a^{x^2}f(t)\,dt\color{red}{=}\frac{d}{dx}F(x^2)\color{red}{=}2x\cdot f(x^2)$, etc.</p>

<p>The second error is that the function $x\mapsto \int_a^{x^2}f(t)\,dt$ is not equivalent to the function $x\mapsto\int_a^{x}f(t^2)\,dt$, nor do they differ only by a constant, so it is not clear why you think they ought to have the same derivative.</p>

<p>For example, take $f(t)\equiv 1$, i.e., the function that is equal to $1$ for all values of $t$. Then the value of the first function is $\int_a^{x^2}dt=x^2-a$ while the value of the second function is $\int_a^xdt=x-a$. For this reason, it is wrong to expect that</p>

<p>$$\frac{d}{dx}\int_a^{x^2}f(t)\,dt=\frac{d}{dx}\int_a^{x}f(t^2)\,dt$$</p>

<p>Indeed, in this case, when $f(t)\equiv 1$, we get that the derivative of the first function, with respect to $x$, is the function $x\mapsto 2x$, while the derivative of the second function, with respect to $x$, is the function $x\mapsto 1$.</p>

<hr>

<p>Here is the correct way to do the calculation. Let us set $g(x)=\int_a^{x^2}f(t)\,dt$ and $h(x)=\int_a^{x}f(t^2)\,dt$ and assume that the hypotheses of the FTC are satisfied. </p>

<p>Then indeed $g'(x)=2x\cdot f(x^2)$, using the chain rule.</p>

<p>Moreover, with $f(t^2):=j(t)$, we see $h(x)=\int_a^{x}j(t)\,dt$, so $h'(x)=j(x)=f(x^2)$.</p>

<p><strong>So your final answers, $2x\cdot f(x^2)$ and $f(x^2)$, are indeed correct</strong>; the functions I've called $g$ and $h$ <strong>do</strong> have different derivatives.</p>

<p>Your error is subtle and lies in thinking that both derivatives yield $\frac{d}{dx}F(x^2)$. That is correct for the first calculation, though it would be more precise to write $\frac{d}{dx}(F(x^2)-F(a))$. However, as the user Cauchy says in a comment, the antiderivative of $f(t^2)=j(t)$ is not the same thing as the antiderivative of $f(t)$. The correct statement for the second calculation is $\frac{d}{dx}(J(x)-J(a))$. The FTC says this gives $j(x)=f(x^2)$.</p>
"
"2388252","2388258","<p>No, it is not uniformly continuous. Let $g(y)=\frac{1-y}{1+y}$ and note that $g(y)=f(1,y,0)$. Therefore, if $f$ was uniformly continuous, then $g$ would be uniformly continuous too. But it isn't. Take $\delta&gt;0$. Since $\lim_{y\to-1^+}g(y)=+\infty$, it is easy to find two numbers $x,y\in(-1,-1+\delta)$ such that $\bigl|g(x)-g(y)\bigr|\geqslant1$. Therefore, if you take $\varepsilon=1$,$$(\forall\delta&gt;0)(\exists x,y\in\mathbb{R}\setminus\{-1\}):|x-y|&lt;\delta\wedge\bigl|g(x)-g(y)\bigr|\geqslant\varepsilon.$$This is the negation of the uniform continuity of the function $g$.</p>
"
"2388260","2388263","<p>Hint: $(a^4+a^2b^2+b^4)=(a^4+b^4+2a^2b^2-a^2b^2)=(a^2+b^2-ab)(a^2+b^2+ab)$.</p>
"
"2388267","2388307","<p>All the lines that are not parallel to a coordinate plane intersect all the three coordinate planes.</p>

<p>In the figure you can see the line that passes thorough the points $A=(2,1,0)$ and $ B=(0,2,1)$, that has equation $(x,y,z)=t(2,-1,-1)+(0,2,1)$ and intesects the plane $y=0$ at $C=(4,0,-1)$.</p>

<p><a href=""https://i.stack.imgur.com/XIuZj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XIuZj.jpg"" alt=""enter image description here""></a></p>
"
"2388268","2388401","<p>So, $N$ is the subset of the partition that contains $1$.</p>

<p>For any $A\in \mathcal P$, we indeed have $A\subseteq AN$ because $1\in N$, but this implies that $AN$ must be a subset of the <em>same</em> element, $A$. <br>
[Say, $AN\subseteq B\in \mathcal P$, then $\emptyset\ne A\subseteq B$, so they intersect, hence $A=B$ must hold by the partition.]</p>

<p>It means we have $A=AN$, and similarly $NA=A$ for all $A\in \mathcal P$.</p>

<p>Now, for any $a\in G$, we have a unique $A\in \mathcal P$ containing $a$, let $B\in \mathcal P$ the subset containing $a^{-1}$, and with these,
$$aNa^{-1}\subseteq ANB = AB$$
But, as $1\in AB\cap N$, we have $AB\subseteq N$ by hypothesis.</p>

<hr>

<p><strong>Alternative proof:</strong> Equip $\mathcal P$ with group structure and define the obvious function $G\to \mathcal P$. It will be a homomorphism with kernel $N$.</p>
"
"2388275","2388319","<p>The idea is to show that each coefficient $a_i$ is $0$, starting with $a_0$, and then proceeding to $a_1$, and so on. Since any polynomial has finite degree, this process will have terminated after a finite number of steps, and the claim will be proved.</p>

<p>To show $a_1 = 0$, we do the following: Since $a_0 = 0$, we have that
$$
P(t) = t(a_nt^{n-1}+\dots+a_2t+a_1),
$$
and that $P(t)=0$ for every value of $t$. Let's call $Q(t) = P(t)/t$ whenever $t$ is nonzero. Since $P$ is $0$ for every $t$, $Q$ is necessarily $0$ for every nonzero $t$.</p>

<p>The idea now is to show, by way of contradiction, that $a_1$ is $0$. To accomplish this, the author assumes that $a_1$ is nonzero, and uses this fact to choose some particularly clever value $0&lt;c&lt;1$ such that $|Q(c)| &gt; 0$. This is a contradiction because since $c$ is positive, we must have $Q(c) = 0$.</p>

<p>Assuming you understand the details of the author's proof by contradiction, now you want to show that $a_2$ is $0$. To do this, write
$$
Q(t) = t(a_nt^{n-2} + \dots + a_3t + a_2).
$$
Now, use the same idea we used to show $a_1 = 0$ to show that $a_2 = 0$. Since any polynomial has finite degree, this process of showing that each $a_i = 0$ must terminate after some finite number of steps, and hence we will have shown that $P$ is identically $0$.</p>

<hr>

<p>Since you're having some trouble with the inequalities, I will try to break them down for you, piece by piece. The goal here is to show, first of all, that $a_1 = 0$. Once that's done, we can show that $a_2 = 0$, and so on. </p>

<p>So, suppose that $a_1 \ne 0$. (If it is already $0$, great! We could then show that $a_2$ is $0$, and proceed.) Consider the fraction
$$
F = \frac{|a_1|}{2\,(|a_2| + \dots + |a_n|)}.
$$
Since $a_1\ne 0$, $|a_1|&gt;0$, so $F$ is some positive number. We can, of course, choose another positive number that is less than $F$. We will call this $c$. Thus $c&lt;F$, or, equivalently,
$$
c&lt;\frac{|a_1|}{2\,(|a_2| + \dots + |a_n|)}\iff 2c\,(|a_2| + \dots + |a_n|) &lt; |a_1| \iff c\,(|a_2| + \dots + |a_n|) &lt; \frac{|a_1|}{2}. \tag{1}
$$
Here is a little lemma for you to help with the next inequality we're dealing with: (I won't prove it for you, so consider it an exercise to either prove it or read a proof somewhere.)</p>

<p><strong>Lemma.</strong> If $\alpha$ and $\beta$ are any two real numbers, then
$$
|\alpha + \beta| \ge |\alpha| - |\beta|.
$$
Applying our lemma to $|Q(c)| = |a_1 + \big(Q(c)-a_1\big)|$, we have
$$
|a_1 + \big(Q(c)-a_1\big)| \ge |a_1| - |Q(c)-a_1| = |a_1| - |a_nc^{n-1} + \dots + a_2c|.
$$
By the triangle inequality, $|a_nc^{n-1} + \dots + a_2c| \le |a_nc^{n-1}| + \dots + |a_2c|$, and negatives flip the inequality, so we obtain:
$$
|Q(c)| \ge |a_1| - (|a_nc^{n-1}| + \dots + |a_2c|) = |a_1| - (|a_n|c^{n-1} + \dots + |a_2|c).
$$
Now, since $0 &lt; c &lt; 1$, if $k\ge 1$, then $c^k \le c$, so we get the next inequality:
\begin{align*}
|Q(c)| &amp;\ge |a_1| - (|a_n|c^{n-1} + \dots + |a_2|c) \\
&amp;\ge |a_1| - (|a_n|c + \dots + |a_2|c) \\
&amp;= |a_1| - c(|a_n| + \dots + |a_2|).
\end{align*}
Using the last equivalence in (1), we have
$$
|Q(c)| \ge |a_1| - c(|a_n| + \dots + |a_2|) &gt; |a_1| - \frac{|a_1|}{2} = \frac{|a_1|}{2} &gt; 0.
$$
Now what does this mean? Since $|Q(c)|&gt;0$, it follows that $Q(c)$ is either positive or negative, but not $0$, which is a contradiction because $c$ is positive and $Q(c) = 0$, as we said earlier. Thus $a_1$ must be equal to $0$ after all, and you can proceed in the manner I outlined above.</p>
"
"2388276","2388313","<p>Let us look at how a linear combination of the rows might yield the null vector:
\begin{align}
\sum_i \lambda_i r_i &amp;= 
(\lambda_1, \lambda_2, \lambda_3)
\begin{pmatrix}
1 &amp; 2 &amp; 0 &amp; 4 \\
0 &amp; -1 &amp; 1 &amp; -1 \\
0 &amp; 0 &amp; 1 &amp; 3
\end{pmatrix}
\\
&amp;=
(\lambda_1, 2\lambda_1-\lambda_2, \lambda_2 + \lambda_3, 4\lambda_1 - \lambda_2 + 3\lambda_3)
\\
&amp;= (0,0,0,0)
\end{align}
What will comparison of the components tell you about the coefficients $\lambda_i$? What does this mean for the linear independence of the rows?</p>
"
"2388281","2388378","<p>Consider some real number $x$ in $[0,\frac12]$. For every $n\geqslant1$, $$\color{red}{\frac{x^n}{1-x^n}}\leqslant1$$ Using the expansion $$\log(1-t)=-\sum_{n=1}^\infty\frac{t^n}n$$ one sees that $$\sum_{i=2}^\infty\log(1-x^i)=-\sum_{n=1}^\infty\sum_{i=2}^\infty\frac{x^{in}}n=-\sum_{n=1}^\infty\frac{x^n}n\color{red}{\frac{x^n}{1-x^n}}\geqslant-\sum_{n=1}^\infty\frac{x^n}n=\log(1-x)$$ Exponentiating both sides and multiplying everything by $(1-x)$, one gets $$\prod_{i=1}^\infty(1-x^i)\geqslant(1-x)^2$$ which is equivalent to (a strengthening of) the desired inequality for every $p\geqslant2$.</p>
"
"2388286","2388296","<p>The first implication is done correctly. Well done! A little remark: How do you justify $x \notin B^c \Rightarrow x \in B$?</p>

<p>For the other implication:</p>

<p>How is $x \notin B^c \Rightarrow x \in B^c = \emptyset$? a valid step?  $x \in \emptyset$ is a statement that is always false, as the empty set contains no elements.</p>

<p>My solution for the $\boxed{\Leftarrow}$ implication would be:</p>

<p>Suppose $A \cap B^c \neq \emptyset$. Then, there is an element $x \in A$ and $x \in B^c$. Because $A \subset B$, it follows that $x \in B$ and $x \in B^c$, which is absurd. This yields the desired contradiction $\quad \triangle$</p>
"
"2388293","2388422","<p>You want to see if your <em>observed</em> counts $X = (33, 35, 24, 8)$ of getting $ i =0, 1, 2, 3$ heads in $n=100$ trials are consistent
with $\mathsf{Binom}(3, 1/2),$ which has probabilities $p = (1/8,\, 3/8,\, 3/8,\, 1/8),$ 
respectively of giving those counts. Thus according to the binomial model
the <em>expected</em> counts are $E = np = (100/8,\, 300/8,\, 300/8,\, 100/8).$</p>

<p>Then you need to do the required chi-squared goodness-of-fit test of the null
hypothesis that the correct model is   $\mathsf{Binom}(3, 1/2)$ against
the alternative that this is not the correct model. The test statistic </p>

<p>$$Q = \sum_{i = 0}^3 \frac{(X_i - E_i)^2}{E_i}$$</p>

<p>is approximately distributed as $\mathsf{Chisq}(4-1 = 3),$ rejecting if
$Q &gt; 7.815.$ </p>

<p>The computed value of the test statistic is $Q = 39.75 &gt; 7.815,$
so we reject the null hypothesis, and say that the observed counts
are not consistent with $\mathsf{Binom}(3, 1/8).$ The P-value of this
test is $1.2 \times 10^{-8},$ which is the probability that fair
coins would produce a goodness-of-fit statistic as large as $39.75.$   </p>

<p>Computations in R statistical software are as shown below. Perhaps you
can verify them on a calculator and using printed tables of the chi-squared
distribution.</p>

<pre><code>i = 0:3;  p = dbinom(i, 3, .5)
x = c(33, 36, 24, 8);  n = sum(x);  e = n*p
q = sum((x-e)^2/e);  q
## 39.75248
qchisq(.95, 3)
## 7.814728
1 - pchisq(q, 3)
## 1.202324e-08
</code></pre>

<p>Here is a plot of the density curve of $\mathsf{Chisq}(3),$ with a vertical
dotted line at the critical value 7.815; the observed value 39.75 is far off
the graph to the right.</p>

<p><a href=""https://i.stack.imgur.com/clEcw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/clEcw.png"" alt=""enter image description here""></a></p>

<p><em>Notes:</em> (1) Another way to look at these data (roughly along the lines of the Comment
by @lulu) is that the number of Heads in 300
tosses is $Y = 3(8) + 2(24) + 35 = 107,$ where the assumption is that $Y \sim \mathsf{Binom}(300, 1/2).$
However $P(Y \le 107) = 3.9 \times 10^{-7}.$ So getting only 107 or fewer heads in 300 tosses
of a fair coin is almost impossible. (This P-value is an exact probability,
so it does not precisely match the approximate P-value from the chi-squared test.)</p>

<pre><code>pbinom(107, 300, 1/2)
## 3.908212e-07
</code></pre>

<p>(2) I don't see how an ANOVA test procedure is appropriate for working this problem.</p>
"
"2388294","2388318","<p>$$A=\begin{bmatrix}1 &amp; a &amp; b \\0 &amp; 1 &amp;c\\0 &amp;0 &amp;1\end{bmatrix}$$</p>

<p>Write $A = I + U$, where $U^2 = \begin{bmatrix} a \\ 0 \\ 0 \end{bmatrix} \begin{bmatrix} 0 &amp; 0 &amp; c \end{bmatrix}$ and $U^3=0$.</p>

<p>$$ A^n=(I+U)^n=I+nU+{n\choose 2}U^2 =
\pmatrix{1&amp;na&amp;nb+\frac{n(n-1)}{2}ac\\0&amp;1&amp;nc\\0&amp;0&amp;1}$$</p>
"
"2388302","2388306","<p>Based on how you've setup the problem, we know that the first integer is $x$, the second is $x+1$, the third is $x+2$, and so forth. Since you found $x$ to be $99$, then the third integer is
$$x+2=99+2=101.$$</p>
"
"2388303","2388308","<p>$e^{ax}\cos bx$ is the real part of
$$\exp(ax+ibx)=\sum_{n=0}^\infty (a+bi)^n\frac{x^n}{n!}.$$
I suggest you write $a+bi=re^{it}$ and think about what $(a+bi)^n$
having real part zero means for $n$ and $t$.</p>
"
"2388314","2388329","<p>No. </p>

<p>Take $X:=\mathbb{Q}$ with the standard metric, $Y=[\pi,10]\cap\mathbb{Q}$ and $x=0$. Then $d(x,Y)=inf \{d(x,y) : y \in Y \}=\pi$. But there is no element in $Y$ with $\pi=d(x,y_0)=|y_0|$ (not even in $X$).</p>
"
"2388320","2388330","<p>I don't think it is generally helpful to think of the closure of a set $A$ as the union of $A$ with the set of its limit points. I think of the closure of $A$ as the smallest closed set that contains it.</p>

<p>In your case $\overline B\supseteq B\supseteq A_i$; $\overline B$
is a closed set containing $A_i$. But $\overline{A}_i$ is the smallest
closed set containing $A$. Therefore $\overline B\supseteq\overline{A}_i$.
As this is true for all $i$, $\overline B\supseteq\bigcup_i\overline{A}_i$.</p>
"
"2388333","2388359","<p>First we show that $g_1:=\sup_k f_k$ and $g_2 :=\inf_k f_k$ are measurable. Note that
$$g_1^{-1}(a,\infty] = \bigcup_k f_k^{-1}(a,\infty]
\quad\text{and}\quad
g_2^{-1}[-\infty,a) = \bigcup_k f_k^{-1}[-\infty,a)
$$
are measurable. Therefore the countable supremum and infimum of measurable functions are measurable. Consequently
$$
\limsup_k f_k = \inf_k(\sup_{j\geq k}f_j)
\quad\text{and}\quad
\limsup_k f_k = \sup_k(\inf_{j\geq k}f_j)
$$
are measurable functions.</p>
"
"2388339","2389547","<p>As mentioned in the comments, you can see that:
$$
\Delta_g u= g^{ij}\partial_{ij} u + 
\left( g^{ij}\partial_i\sqrt{|g|}+\partial_i g^{ij} \right)\partial_j u
$$
So we have $$ \Delta_g u = g^{ij}\partial_{ij}u+\partial_i g^{ij}\partial_j u $$
only if $$ g^{ij}\partial_i\sqrt{|g|}\partial_j u =0 $$
Clearly, this is true if $|g|$ is constant.</p>

<p>Just to expand this a bit, we see that it is equivalent to
$$ \frac{1}{2}g^{ij}\sqrt{|g|}\;\text{tr}(g^{-1}\partial_i g)\partial_j u = 0 $$
using the formula for the derivative of a determinant.</p>

<p>Anyone who has an interpretation for this is welcome to comment on it.</p>
"
"2388343","2388348","<p><strong>hint</strong></p>

<p>$y=0$ is a solution.
let us look for another solution.</p>

<p>divide by $y^2$ and put $$z=1/y . $$</p>

<p>it becomes
$$-tz'=t-z$$
$$z'/z=1/t \implies z_h=\lambda t $$
the variation of the constante gives
$$\lambda '(t)=-1/t $$
$$\implies z_p=-t\ln (|t|). $$
$$z=z_h+z_p=\lambda t -t\ln (|t|) $$
and finally
$$y=\frac {1}{\lambda t -t\ln (|t|)} $$</p>
"
"2388347","2388390","<p>Cauchy's estimate should be applied to $g$, instead of $f$. You know that, for some constant $C'$, $g(z)\leqslant C'|z|^{m+n}$, if $|z|\gg0$. But then$$\left|\frac{g^{(N)}(0)}{N!}\right|\leqslant\frac{1}{r^N}r^{m+n},$$if $r$ is large enough. Therefore, making $r\to+\infty$, $g^{(N)}(0)=0$ when $N&gt;m+n$. It follows that$$g(z)=\sum_{k=0}^{m+n}\frac{g^{(k)}(0)}{k!}z^k,$$which is a polynomial function.</p>
"
"2388351","2388379","<p>They simply performed the division of $1$  by $1-\dfrac{z^2}{3!}+\dfrac{z^4}{5!}-\dfrac{z^6}{7!}+\dotsm$ <em>by increasing powers</em>.</p>

<p><em>For those who do not know division by increasing powers:</em></p>

<p>It is a long division in which the dividend and the divisor are ordered by increasing powers. One begins by dividing the lowest degree term of the dividend by the lowest degree term of the divisor, instead of the highest degree terms for Euclidean division. Contrary to the latter, division by increasing powers never stops. It is based on the following general result:</p>

<blockquote>
  <p>Given two polynomials $f,g \in K[X]$ $\:(g(0)\ne 0)$, for any $n\ge 0$, there  exists polynomials $q_n, r_n$ such that
  $$f(X)=q_n(X)g(X)+X^n r_n(X)$$
  and these polynomials are unique (for a given $n$)</p>
</blockquote>

<p>This result can be extended to <em>formal power series</em> (it is this extension which is used here).</p>

<p>A detailed example in my answer to  <a href=""https://math.stackexchange.com/questions/1893913/by-using-a-geometric-series-and-a-factorisation-compute-the-first-three-terms-o/1893977#1893977"">this question</a>.
You can try to perform the division of $1$ by $1+X$ to check that
$$(1+X)^{-1}=1-X+X^2-X^3+\dotsm$$</p>
"
"2388353","2388373","<p>Let $A(a,y_A)$, $B(b,y_B)$ and $C(x,y)$.</p>

<p>Hence, $D\left(\frac{x+b}{2},\frac{y+y_B}{2}\right)$ is a midpoint of $BC$.</p>

<p>Thus, the equation of needed locus is
$$\left(\frac{x+b}{2}-a\right)^2+\left(\frac{y+y_B}{2}-y_A\right)^2=\left(\frac{3}{2}\right)^2,$$
which is equation of the circle:
$$(x-(2a-b))^2+(y-(2y_A-y_B))^2=3^2.$$</p>

<p>Thus, D) is valid because $$\sqrt{(2a-b-b)^2+(2y_A-y_B-y_B)^2}=2\sqrt{(a-b)^2+(y_A-y_b)^2}=2AB=4.$$</p>
"
"2388357","2388405","<p>If the aim is parts of equal area, then a solution is to cut a square portion in the middle (length side $l$): this will leave a square donut with a square opening, to be then cut in two.
In order for the parts to have equal area, the following equation should hold (area of central cut square minus hole area, equal to half the resulting square donut with square opening):
$$ l^2 - 2\pi r^2 = \frac {1}{2}(L^2 - l^2)$$ where $r$ is the radius of the round hole, $L$ the side length of your donut, $l$ the side of the square to be cut in the middle (its center coinciding with the hole center).
The length $l$ turns out to equal $$ \sqrt{\frac{L^2}{3}+\frac{2}{3} \pi r^2} $$ hoping that $r$ is not too large..</p>

<p>In practice, one could also pragmatically cut in four pieces along the diagonals, and serve one part each. Then one can divide the remaining in three parts: being smaller, any error will be reduced.
Even better, cut the excess of a inscribed circumference in the original square: you get a classic donut easy to divide in three exactly. Then you are left with the four corner trimmings: each gets one, and the last is so small, that you are below your measuring uncertainty and the error is negligible.</p>
"
"2388363","2388367","<p>Just to make sure that $\limsup_{n\in\mathbb N}f_n$ is a real function (in the sense that it only takes real values).</p>
"
"2388375","2388408","<p>Sure.  Assuming that I have made no dyslexic typos, the computation looks like</p>

<p>\begin{align}
\overbrace{\int \cos(2x+1)\sin(x)}^{\text{call this $I$}}
&amp;= -\cos(2x+1)\cos(x) - 2\int \sin(2x+1)\cos(x) \\
&amp;= -\cos(2x+1)\cos(x) - 2\sin(2x+1)\sin(x) + 4\int \cos(2x+1)\sin(x) \\
&amp;= -\cos(2x+1)\cos(x) - 2\sin(2x+1)\sin(x) + 4I.
\end{align}
Solving for $I$, we obtain
$$ I = \frac{\cos(2x+1)\cos(x) + 2\sin(2x+1)\sin(x)}{3}. $$
This compares favorably to <a href=""http://www.wolframalpha.com/input/?i=integrate%20cos(2x%2B1)sin(x)"" rel=""nofollow noreferrer"">the solution given by Wolfram|alpha</a>, though note that it has been simplified differently.  A <a href=""https://www.desmos.com/calculator/qwe2tix94y"" rel=""nofollow noreferrer"">comparison of the graphs</a> of the two solutions should be convincing.</p>
"
"2388377","2388449","<p>The way to do this is to fit a cubic spline through the data, and then calculate the derivative.</p>

<p>See <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.CubicSpline.html"" rel=""nofollow noreferrer"">scipy.interpolate.CubicSpline</a></p>

<p>Then use the <code>.Derivative()</code> method to get what you want.</p>
"
"2388388","2388427","<p>The computation is correct, with the justification for residue as follows: </p>

<p>Since $f(z)$ has a simple pole at $z=1$, $\dfrac{1}{z^{n+1}}$ is holomorphic around $z=1$, using the limit formula $Res(\frac{f(z)}{z^{n+1}})=\lim_{z \to 1} \dfrac{f(z)(z-1)}{z^{n+1}}=\dfrac{\lim_{z \to 1} f(z)(z-1)}{\lim_{z \to 1} z^{n+1}} = \dfrac{Res(f,1)}{1^{n+1}} = 1$.</p>
"
"2388391","2388835","<p>Let $[x]$ denote the largest integer not exceeding $x.$ For $n\in \mathbb N$ and for prime $p$ the largest integer $k$ such that $p^k$ divides $n!$ is $$k=\sum_{j=1}^{\infty}[np^{-j}].$$ Note that only finitely many terms in the summation are non-zero, as $[np^{-j}]=0$ when $p^j&gt;n.$ </p>

<p>Consider the $n$ terms in the product $1\times  ... \times n.$ To find $k:\;$  Rather than counting ""$1$"" for each $m\leq n$ that's divisible by $p$ but not by $p^2,$ and counting ""$2$"" for each $m\leq n$ that's divisible by $p^2$ but not by $p^3,$ et cetera, $instead$ we can count ""$1$"" for $every$ $m\leq n$ that's divisible by $p$ (giving $[np^{-1}]$ of them), and count $1$ more for each $m\leq n$ that's divisible by $p^2$ (giving $[np^{-2}]$ of them), et cetera.</p>

<p>The largest power of $13$ that divides $2146!$ is $13^{177}$ because $[2146/13]+[2146/13^2]=165 +[(165+1/13)/13]=165+12,$ and $j\geq 3\implies [2146/13^j]\leq [2146/13^3]=[(165+1/13)/13^2]=0.$ </p>

<p>The largest power of $2$ that divides $2146!$ is greater than  $2^{177}$. So the largest power of $26$ that divides $2146!$ is $26^{177}.$ So in base $26$ the number $2146!$ ends in exactly $177$ zeroes.</p>
"
"2388411","2388416","<p>Consider the ""melting ice cube"" example (I like to have little mnemonics for remembering useful counterexmples):  let $f_n := \frac{1}{n} \chi_{[0,n]}$.  Then $f_n$ converges uniformly to zero, but $\int f_n = 1$ for all $n$, and so
$$ \lim_{n\to \infty} \int f_n = 1 \ne 0 = \int \lim_{n\to\infty} f_n. $$
Note that if $\mu(X)$ is finite, then uniform convergence is good enough to get convergence in $L^1$ (which is, more or less, what you are trying to do).</p>
"
"2388413","2388461","<p>Yes.  To be precise, the following is a theorem of ZFC-Infinity:</p>

<blockquote>
  <p>Suppose there exists a set $N$, a function $S:N\to N$, and an element $0\in N$ such that $(N,S,0)$ satisfies the second-order Peano axioms (that is, the list of properties you wrote).  Then the axiom of infinity is true (that is, there exists a set $I$ such that $\emptyset\in I$ and for all $x\in I$, $x\cup\{x\}\in I$).</p>
</blockquote>

<p>[Note that this statement includes existential quantifiers on $N$, $S$, and $0$ which yours omits, which is necessary in order for it to be expressible as a sentence in the language of set theory.]</p>

<p>It is pretty complicated to write out all the details of the proof, but basically you develop all the usual theory of the natural numbers on the structure $(N,S,0)$ from the second-order Peano axioms to prove it is in bijection with the ordinal $\omega$.  In particular, you prove that you can define functions on $N$ by recursion: given any (class) function $G$ and any $a$, you can define another (class) function $f$ with domain $N$ such that $f(0)=a$ and $f(S(x))=G(f(x))$ for all $x\in N$.</p>

<p>(Proof sketch: define a relation $\leq$ on $N$ by $x\leq y$ iff every subset of $N$ containing $x$ and closed under $S$ also contains $y$.  Now prove by induction on $x$ that there exists a unique function $f_x$ on $\{y\in N:y\leq x\}$ such that $f(0)=a$ and $f(S(y))=G(f(y))$ for all $y&lt;x$.  The union of all these functions $f_x$ is then a function $f$ defined on all of $N$ with the required properties.)</p>

<p>In particular you can define a function $f$ on $N$ such that $f(0)=\emptyset$ and $f(S(x))=x\cup\{x\}$ for all $x$.  By the axiom of replacement, the image of this function $f$ is a set $I$, which then witnesses the axiom of infinity.</p>
"
"2388419","2388791","<p>As commented by paul garrett, this is not true. You can pick $m = 1$,
then $V = H^1(\Omega)$ is certainly closed in $H^1(\Omega)$, but not in $L^2(\Omega)$.</p>

<p>However, one can proof the following:</p>

<blockquote>
  <p>Let $V \subset H^m(\Omega)$ be bounded, closed and convex. Then $V$ is closed in $L^2(\Omega)$.</p>
</blockquote>

<p>The proof is rather straightforward: Let $u_n \to u$ in $L^2(\Omega)$ with $u_n \in V$. Then, $\{u_n\}$ is bounded in $H^m(\Omega)$.
Since $H^m(\Omega)$ is a Hilbert space, a subsequence of $u_n$ (not relabeled) converges weakly to some $\tilde u \in H^m(\Omega)$. It is easily checked that $\tilde u = u$. Further, since $V$ is closed and convex, it is weakly closed in $H^m(\Omega)$ and this yields $u = \tilde u \in V$.</p>

<p>Finally, I would like to mention that you can neither drop the boundedness (see above) or the convexity:
For example, take $m = 1$, $\Omega = (0,1)$ and
$$V = \{ v \in H_0^1(0,1) : |v'| = 1 \text{ a.e. in } (0,1)\}.$$
Then, $V$ is closed and bounded in $H^1(0,1)$ but not in $L^2(0,1)$. If I am not mistaken, the closure in $L^2(0,1)$ should be precisely the closed convex hull of $V$, e.g.
$$\{v \in H_0^1(0,1) : |v'| \le 1 \text{ a.e. in } (0,1)\}.$$</p>
"
"2388426","2388447","<p>Aha! I seem to have found the answer in Milnor's...article (I suppose) ""On the Relationship between Differentiable Manifolds and Combinatorial Manifolds"" (published 1956).</p>

<p>""Two triangulated spaces $(K_1,f_x,X_1)$ and $(K_2,f_2,X_2)$ are <em>isomorphic</em> $K_1$ is isomorphic to $K_2.$ They are <em>combinatorially equivalent</em> if they have isomorphic subdivisions.""</p>

<p>""A <em>triangulation</em> $(K,f)$ of a space $X$ consists of a simplicial complex $K$, together with a homeomorphism $f$ of $|K|$ onto $X$.""</p>

<p>So there it is! :D</p>
"
"2388434","2393078","<p>Write down the forces acting on each piston and add them up. The net force is what is accelerating your piston. This gives you second-order differential equations, which you then can translate to first-order differential equations and solve.</p>

<p>A few hints:</p>

<ul>
<li>Do you assume the blue liquid to be incompressible? If yes, what does this mean for $y_1$ and $y_2$?</li>
<li>How would the forces change if $y_1$ and $y_2$ were different?</li>
<li>If you ignore hydrostatic pressure, whatâs the pressure of the green liquid?</li>
<li>Does it matter where the mass $M$ is attached?</li>
</ul>
"
"2388440","2388459","<p>Yes. Suppose $L=\inf_{[0,t]} f$. If $f(0) = L$, we're done, since $0$ is rational.</p>

<p>Otherwise, for any $\epsilon&gt;0$, there is some $c$ such that $c \in (0,t]$ and $|f(c)-L| &lt; \frac{\epsilon}{2}$. In addition, by left-continuity, you can find some rational $d \in [0,c)$ such that $|f(d) - f(c)| &lt; \frac{\epsilon}{2}$. Hence </p>

<p>$$\epsilon = \frac{\epsilon}{2} + \frac{\epsilon}{2}&gt;|f(c)-L| + |f(d) - f(c)| \geq |f(d)-L|$$</p>

<p>In other words, there's rationals in $[0,t]$ which $f$ maps arbitrarily close to $L$. </p>
"
"2388441","2388527","<p>From a purely geometric point of view, I think you're right: there is no such thing as <strong>the</strong> center of a plane. But in a way that means that you can treat <em>any</em> point of the plane as the center (this idea can be useful in certain Physics problems that have a certain symmetry; e.g. computing the electric field across a parallel plane capacitor), and in a way we can select a point to be the center by imposing a coordinate system on the space.</p>

<p>For example, let's consider 2D space for a minute and consider the line defined by $y = 2x$. As a geometric object, this line also has no center. But with our usual coordinate system, we are tempted to view it as ""centered"" about the origin. We could imagine ""shifting"" the line along itself two units up and one unit to the right, and the ""center"" would now be located at the point $(1,2)$.</p>

<p>Geometrically, of course, we haven't changed anything. That's the symmetry of a line. But I think it's enlightening to see what happens to the <em>equation</em> of the line when we think about performing this ""shift"" operation. To shift the graph of some equation involving $x$ and $y$, we subtract the horizontal and vertical shift amounts from the corresponding variables. In this case, $x \to x-1$ and $y \to y-2$. Thus our line <em>equation</em> becomes
$$ y-2 = 2(x-1) $$
which, of course, simplifies into $y = 2x$ upon algebraic manipulation, which corresponds to the geometric symmetry that shifting a line along itself doesn't really change it. Also note that the equation above is the <em>point-slope form</em> of the equation of a line! And for that matter, when we construct an equation for a line using point-slope form, the point we pick could be thought of as the ""center"".</p>

<p>We can even play this game in 3D with your flat plane example of $z = a$, except, let's rewrite this equation in a way that reflects a coordinate system. I can rewrite it as
$$ z-a = 0(x-h) + 0(y-k) $$
which is something like the point-slope form for a line, but ""centered"" at the point $(h,k,a)$ in space. And you see that, no matter what $h$ and $k$ we choose, the equation trivially simplifies into $z=a$, which corresponds to the plane's symmetry.</p>

<p>So if you want to think of it this way, to ""shift"" a plane (or a line, or any other such symmetrical object) doesn't change the plane <em>itself</em>, but it does change the <em>equation</em> of the plane--although it remains algebraically <em>equivalent</em> to the original equation just as, geometrically, a ""shifted"" plane, though it is shifted, looks identical to the original.</p>
"
"2388443","2388463","<p>$n=\left\lceil\frac1{\sqrt{x-1}}\right\rceil+1$, or $n-1=\left\lceil\frac1{\sqrt{x-1}}\right\rceil$, means
$$n-2&lt;\frac1{\sqrt{x-1}}\le n-1$$
Hence</p>

<p>$\dfrac1{\sqrt{x-1}}\le n-1 &lt; n$</p>

<p>whence squaring and re-arranging should give you the answer.</p>
"
"2388456","2388466","<p>I think the following idea works.</p>

<p>Let $a=1+x$, $b=1+y$ and $c=1+z$.</p>

<p>Hence, we need to solve
$$6+2(x+y+z)=xyz,$$ where $x$, $y$ and $z$ are non-negative integer numbers.</p>

<p>Thus,
$$6+2(x+y)=z(xy-2)\geq xy-2,$$
which gives $$(x-2)(y-2)\leq12.$$</p>

<p>It's not so many cases.</p>
"
"2388457","2388677","<p>The  references  here are  to  the  book <em>Analytic  Combinatorics</em>  by
Flajolet and Sedgewick. Suppose we have</p>

<p>$$F(z) = \frac{1-\sqrt{1-4z}}{2z}$$</p>

<p>and we are interested in the asymptotics of the coefficients of</p>

<p>$$F_k(z) = \frac{1}{1-kzF(z)}.$$</p>

<p>We restrict  to $k\ge 3$  since $k=1$ and  $k=2$ have closed  forms in
terms  of binomial  coefficients  (ordinary  Catalan numbers,  central
binomial coefficients). We obtain</p>

<p>$$\frac{1}{1-k(1-\sqrt{1-4z})/2}
= \frac{1}{1-k/2+k\sqrt{1-4z}/2}
\\ = \frac{1-k/2-k\sqrt{1-4z}/2}{(1-k/2)^2-k^2(1-4z)/4}
= \frac{1-k/2-k\sqrt{1-4z}/2}{1-k+k^2z}
\\ = \frac{1}{1-k} \frac{1-k/2-k\sqrt{1-4z}/2}{1-zk^2/(k-1)}.$$</p>

<p>We thus obtain for the desired coefficient the closed form</p>

<p>$$\frac{k-2}{2k-2} \frac{k^{2n}}{(k-1)^n}
+ \frac{k}{2k-2} [z^n] \frac{\sqrt{1-4z}}{1-zk^2/(k-1)}.$$</p>

<p>We  require the  asymptotics  of the  remaining  coefficient. Now  the
square root term has radius of convergence $1/4$ and the rational term
$(k-1)/k^2.$ Furthermore with $k\ge 3$ we have $1/4 \gt (k-1)/k^2.$ We
are  thus   justified  in  applying  <strong>Theorem   VI.12</strong>  (""elementary
methods"") for our purpose, taking</p>

<p>$$\alpha = 1/4 \quad\text{and}\quad \beta = (k-1)/k^2,
\quad\text{as well as}
\\ a(z) = \sqrt{1-4z} \quad\text{and}\quad
b(z) = \frac{1}{1-zk^2/(k-1)}$$</p>

<p>We get for the asymptotics</p>

<p>$$a(\beta) [z^n] b(z) = \sqrt{1-4(k-1)/k^2} \frac{k^{2n}}{(k-1)^n}.$$</p>

<p>Joining the two terms we find</p>

<p>$$\bbox[5px,border:2px solid #00A000]{
\frac{1}{2}\frac{1}{k-1} 
\left(k-2 + k \sqrt{1-4(k-1)/k^2}\right)
\left(\frac{k^2}{k-1}\right)^n.}$$</p>

<p>The coefficient on the exponential is close to one for $k$ large. What
we see here is  that the two contributions were of  the same order and
we may not omit either one of  them. Consulting the quoted text we see
that  in order  for the  proof to  go through  the value  $\beta$ must
retain its sign (as opposed to merely being the radius of convergence,
which is positive).  This means the  above result also holds for $k\le
-5.$ <P></p>

<p>Remaining case is  $-4\le k\le -1.$ We observe that  $a(z)$ and $b(z)$
have reversed their roles and we require</p>

<p>$$[z^n] \sqrt{1-4z} = [(-z)^n] \sqrt{1+4z} = 
(-1)^n 4^n {1/2\choose n} = (-1)^n \frac{1}{2n} 4^n {-1/2\choose n-1}
\\ = (-1)^n \frac{1}{2n!} 4^n \prod_{j=0}^{n-2} (-1/2-j)
= - \frac{1}{2n!} 2^{n+1} \prod_{j=0}^{n-2} (2j+1)
\\ = - \frac{1}{n!} 2^n \frac{(2n-3)!}{(n-2)! 2^{n-2}}
= - \frac{4}{n} {2n-3\choose n-1}.$$</p>

<p>We get for the asymptotics</p>

<p>$$\frac{k-2}{2k-2} \frac{k^{2n}}{(k-1)^n}
- \frac{k}{2k-2} \frac{1}{1-k^2/(k-1)/4} 
\frac{4}{n} {2n-3\choose n-1}$$</p>

<p>or</p>

<p>$$\bbox[5px,border:2px solid #00A000]{
\frac{k-2}{2k-2} \frac{k^{2n}}{(k-1)^n}
+ \frac{8k}{(k-2)^2} \frac{1}{n} {2n-3\choose n-1}.}$$</p>

<p>Now for the binomial coefficient we have</p>

<p>$$\frac{(2n-3)!}{n! (n-2)!} = 
\frac{(n+1)n(n-1)}{2n(2n-1)(2n-2)} \frac{1}{n+1} {2n\choose n}
\sim \frac{1}{8} \frac{4^n}{n^{3/2} \sqrt{\pi}}.$$</p>

<p>and we have the alternate asymptotic</p>

<p>$$\bbox[5px,border:2px solid #00A000]{
\frac{k-2}{2k-2} \frac{k^{2n}}{(k-1)^n}
+ \frac{k}{(k-2)^2} \frac{4^n}{n^{3/2} \sqrt{\pi}}.}$$</p>

<p>The modulus  of $k^2/(k-1)$ with $k$  in the given range  is less than
$4$  and hence  the  second  term that  originates  with the  binomial
coefficient dominates eventually.</p>
"
"2388469","2388481","<p>The main point is that you can calculate the travel time and response delays and allow for them.  The orbits of the satellites are well known, in fact one of the purposes of them exchanging signals is to measure the orbit.  The delays through the satellite are also known, having been measured over temperature as part of ground testing.  Finally if there is drift of the delay in one satellite, you have redundancy in all the links between the satellites.  It will become clear from the error analysis which the bad actor is and what new value should be used to account for it.</p>
"
"2388470","2388486","<p>Your essential problem is that $b_i\cdot \frac{P_x\#}{p_i}\equiv 1\pmod{p_i}$ determines $b_i$ up to a multiple of $p_i$, so $b_i=\tilde b_i+kp_i$. Hence the possible values of $c_i:=b_i\cdot \frac{P_x\#}{p_i}$ are $\tilde b_i\cdot \frac{P_x\#}{p_i}+kP_x\#$ and so contrary to your argument does <em>not</em> take  different values modulo $P_x\#$.</p>
"
"2388474","2388479","<p>Yes. If $\mathscr{U}\subseteq\mathcal{T}'$ such that $\mathscr{U}$ covers $S$, then as $\mathscr{U}$ is also a $\mathcal{T}$-open cover of $S$ and $S$ is $\mathcal{T}$-compact, we obtain $\mathscr{U}_0\subseteq \mathscr{U}$ finite such that $\mathscr{U}_0$ covers $S$. Thus we have found a finite subcover.</p>

<p>Interestingly enough, we get a dual analogue for Hausdorff - and the two come together nicely.</p>

<blockquote>
  <p>Let $\tau_C$, $\tau_H$, and $\tau$ be topologies on a set $X$ such that $(X,\tau_C)$ is compact and $(X,\tau_H)$ is Hausdorff.</p>
  
  <p>(i) If $\tau\subseteq\tau_C$, then $(X,\tau)$ is compact.</p>
  
  <p>(ii) If $\tau\supseteq\tau_H$, then $(X,\tau)$ is Hausdorff.</p>
  
  <p>(iii) If $\tau_H \subseteq \tau_C$, then $\tau_C=\tau_H$.</p>
</blockquote>

<p>The proof of (i) is above and the proof of (ii) is quite simple. For (iii), suppose $f:(X,\tau_C)\to (X,\tau_H)$ is the identity function on $X$. From $\tau_H\subseteq\tau_C$, we deduce that $f$ is continuous. Then $f$ is a continuous bijection from a compact space into a Hausdorff space, which means that $f^{-1}$ is continuous. Therefore $\tau_C\subseteq\tau_H$.</p>
"
"2388476","2388490","<p>You don't need induction; you can actually show equality with a relatively well-known (though perhaps at first ""tricky"") summation technique:</p>

<p>Well first, what is $1+3+9+\cdots+3^n$? Define that quantity to be $N$, and note that $3N = 3+9+\cdots+3^n+3^{n+1} = N + 3^{n+1}-1$, so $2N = 3^{n+1}-1$, and $N = \frac12(3^{n+1}-1)$.</p>

<p>Now that you know how high you are summing, you get:</p>

<p>$$
S = \sum_{i=1}^Ni = \frac12N(N+1) = \frac18(3^{n+1}-1)(3^{n+1}+1) = \frac18(9^{n+1}-1)
$$</p>

<p>Now, consider $S' = 1^2 + 3^2 + 9^2+\cdots + 3^{2n}$. Note that $9S' = 3^2 + 9^2 + \cdots + 3^{2n} + 3^{2n+2} = S' + 3^{2n+2} - 1$, so $8S' = 9^{n+1} - 1$, and $S' = \frac18(9^{n+1}-1)$.</p>

<p>Since $S' = S$, you are done.</p>
"
"2388489","2398422","<p>Theory of linear equations works exactly the same way no matter the field. Let $k$ a field and $A\in M_n(k)$.</p>

<p>The most relevant result is Kronecker-Capelli theorem:</p>

<blockquote>
  <p>Linear system $Ax = b$ has solution if and only if
  $\operatorname{rank} A = \operatorname{rank}[A|b]$ where $[A|b]$
  denotes augmented system matrix.</p>
</blockquote>

<p>In that case, let $x_0$ be a particular solution of the system, i.e. $Ax_0 = b$. Then the solution set $S$ of homogeneous linear system $Ax = 0$ is vector space of dimension $n-\operatorname{rank} A$ and all solutions of the system $Ax = b$ are given by $$\{x_0+x\mid Ax = 0\} = x_0+S.$$</p>

<p>These results are elementary and will be found in any Linear Algebra textbook. However, it is important to note that ground field $k$ is completely irrelevant in this context, be it $\mathbb R$, $\mathbb C$, $\mathbb Z/p\mathbb Z$ or whatever. It becomes relevant in spectral theory, though.</p>
"
"2388491","2391073","<p>Your posed existence criterion boils down to that the pair $(A,B)$ must be controllable. Namely your second description basically requires the same as the <a href=""https://en.wikipedia.org/wiki/Hautus_lemma"" rel=""nofollow noreferrer"">Hautus lemma</a>.</p>

<p>My answer will be based on an answer of mine to <a href=""https://math.stackexchange.com/questions/2376418"">this related question</a>. This is my own approach onto the problem, so it might not match any known algorithm exactly, but should give a reasonable image of the steps involved in solving this. <a href=""https://arxiv.org/pdf/1305.1370.pdf"" rel=""nofollow noreferrer"">Here</a> is also a paper going into more detail of how solve the problem in a more robust and numerically stable way. That paper also contains a few references to other algorithms.</p>

<p>For convenience I will just define the full state feedback gain as $K\in\mathbb{R}^{m \times n}$ (so no need for the transpose). This means that you are interested in finding a $K$ such that </p>

<p>$$
A - B\,K = \hat{A} = V\,\Lambda\,V^{-1}, \tag{1}
$$</p>

<p>with $\Lambda,V \in \mathbb{R}^{n \times n}$. The right hand side of $(1)$ is the Jordan decomposition of $\hat{A}$, such that the columns of $V$ contain the generalized eigenvectors of $\hat{A}$ and $\Lambda$ is a <a href=""https://en.wikipedia.org/wiki/Jordan_matrix"" rel=""nofollow noreferrer"">Jordan matrix</a>. In order to find a $K$ such that the eigenvalues of $\hat{A}$ match a given set of eigenvalues, including the information about the geometric multiplicity (so $\Lambda$), then you also have to solve for $V$ at the same time. In order to do this $(1)$ can be rewritten as</p>

<p>$$
A\,V - B\,\Omega = V\,\Lambda, \tag{2}
$$</p>

<p>with $\Omega = K\,V$, which also lies in $\mathbb{R}^{m \times n}$. This equation is linear in all unknowns which will make it easier to solve later on. Once $(2)$ is solved for $V$ and $\Omega$, then the full state feedback gain can be found using $K = \Omega\,V^{-1}$. When using the structure of $\Lambda$ it is possible to decouple the dependencies of the columns of $V$ and $\Omega$ to only the columns associated with each Jordan block of $\Lambda$. Namely at the start of each Jordan block you have to solve</p>

<p>$$
A\,V_{\bullet,i} - B\,\Omega_{\bullet,i} = \Lambda_{i,i}\,V_{\bullet,i}, \tag{3}
$$</p>

<p>with $i$ the first index associated with a Jordan block and $X_{\bullet,i}$ denotes the $i$th column of matrix $X$. In order to obtain a nonzero solution an additional constraint has to be added, such as $\|V_{\bullet,i}\| = 1$. Each next equation associated with that Jordan block is</p>

<p>$$
A\,V_{\bullet,i+k} - B\,\Omega_{\bullet,i+k} = V_{\bullet,i+k-1} + \Lambda_{i+k,i+k}\,V_{\bullet,i+k}, \tag{4}
$$</p>

<p>with $k$ from one till the size of the Jordan block. It can be noted that due to the definition of a Jordan matrix then it must be true that $\Lambda_{i+k,i+k} = \Lambda_{i,i}$ for all specified $k$. These equations do not have have a trivial zero solution. However both $(3)$ and $(4)$ can have more unknowns then equations, so some extra constraints or minimization criteria would have to be used. I believe many algorithms differ by choosing different criteria.</p>

<p>It can be noted that the place command of MATLAB also constraints $\Lambda$ to only a Jordan matrix with Jordan blocks all of size one (so exactly diagonal). This implies that it is not possible to assign an eigenvalue with multiplicity greater then the rank of $B$, since the eigenvectors should all be linearly independent of each other, otherwise $V^{-1}$ would not exist. In order to see why this is the case $(3)$ can be rewritten as </p>

<p>$$
(A - \Lambda_{i,i}\,I) V_{\bullet,i} = B\,\Omega_{\bullet,i}. \tag{5}
$$</p>

<p>The right hand side of $(5)$ is just a linear combination of the columns of $B$, so the left hand side should be as well. The number of linearly independent vectors that can be constructed by the right hand side is by definition equal to the rank of $B$. Either $\Lambda_{i,i}$ is not an eigenvalue of $A$, in which case any set of linearly independent $V_{\bullet,i}$ should be transformed by $A - \Lambda_{i,i}\,I$ into another linearly independent set of vectors and vice versa. Or $\Lambda_{i,i}$ is an eigenvalue of $A$, in which case eigenvectors of $A$ corresponding to that eigenvalue will be solutions for $V_{\bullet,i}$ (and $\Omega_{\bullet,i}$ in the null space of $B$). However the number of other solutions will decrease by the same amount because of the properties of the Hautus lemma. So when $A - \Lambda_{i,i}\,I$ loses rank, then any linear combination of its columns also losses the ability to form a column of $B$. Therefore the geometric multiplicity of an eigenvalue can never be greater then the rank of $B$. But this also implies that the number of Jordan blocks associated to the same eigenvalue can also never be greater then the rank of $B$. </p>

<p>If $\Lambda_{i,i}$ is not an eigenvalue of $A$ then one way of reducing the decrease of freedom would be to solve $(3)$ and $(4)$ for the eigenvectors for a given $\Omega$ by rewriting them as</p>

<p>$$
V_{\bullet,i} = (A - \Lambda_{i,i}\,I)^{-1} B\,\Omega_{\bullet,i}, \tag{6}
$$</p>

<p>$$
V_{\bullet,i+k} = (A - \Lambda_{i,i}\,I)^{-1} (B\,\Omega_{\bullet,i+k} + V_{\bullet,i+k-1}). \tag{7}
$$</p>

<p>This should yield a solution as long as $\Omega_{\bullet,i}$ does not lie in the null space of $B$ and its columns associated to $(6)$ using the same eigenvalue are linearly independent.</p>

<hr>

<p>For example given the following system</p>

<p>$$
A = \begin{bmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{bmatrix}, \quad
B = \begin{bmatrix}
0 &amp; 0 \\
1 &amp; 0 \\
0 &amp; 0\\
0 &amp; 1
\end{bmatrix}. \tag{8}
$$</p>

<p>This system basically are two decoupled double integrators, which is controllable. All eigenvalues should be placed at -1. The rank of $B$ is two, so in order to do this there are three choices for the Jordan block sizes, namely 2 and 2, 3 and 1, or only 4. For the first choice the associated $\Lambda$ is given by</p>

<p>$$
\Lambda = \begin{bmatrix}
-1 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; -1 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; -1
\end{bmatrix}.
$$</p>

<p>When solving all $(6)$ and $(7)$ it actually does not really matter what is picked for $\Omega$. As long as it gives a nonsingular $V$ then the feedback gain will always be</p>

<p>$$
K = \begin{bmatrix}
1 &amp; 2 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 2
\end{bmatrix}.
$$</p>

<p>It can be noted that this is actually also a decoupled controller. For the second choice the associated $\Lambda$ is given by</p>

<p>$$
\Lambda = \begin{bmatrix}
-1 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; -1 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; -1
\end{bmatrix}.
$$</p>

<p>When solving all $(6)$ and $(7)$ it does matter what is picked for $\Omega$. I wrote some quick program in MATLAB which, after normalizing $V$, tries to minimize $\|V^\top V - I\|_F$ (a measure of how orthogonal the eigenvectors are). Here $\|X\|_F$ denotes the <a href=""http://mathworld.wolfram.com/FrobeniusNorm.html"" rel=""nofollow noreferrer"">Frobenius norm</a> of $X$. This does seem to have local minima, because it does not always converge to the same solution. One of the solutions gives the following feedback gain</p>

<p>$$
K = \begin{bmatrix}
-0.3628 &amp; 0.6372 &amp; -0.6724 &amp; -0.6724 \\
 2.7621 &amp; 2.7621 &amp;  2.3628 &amp;  3.3628
\end{bmatrix}.
$$</p>

<p>For the third choice the associated $\Lambda$ is given by</p>

<p>$$
\Lambda = \begin{bmatrix}
-1 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; -1 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; -1 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; -1
\end{bmatrix}.
$$</p>

<p>When solving all $(6)$ and $(7)$ it does again matter what is picked for $\Omega$. When using the same optimization criteria as the previous choice for $\Lambda$, then again there seem to be multiple local minima. One of the solutions gives a feedback gain very similar to the gain of from the first choice</p>

<p>$$
K = \begin{bmatrix}
 1.0001 &amp; 2.0000 &amp; 0.0005 &amp; 0.0000 \\
-0.0000 &amp; 0.0000 &amp; 0.9999 &amp; 2.0000
\end{bmatrix}.
$$</p>

<p>This is not the best algorithm, but should give you and indication of what the first steps in general would be and what possibly could be used as criteria in order to optimize your solution with respect to some cost function.</p>
"
"2388493","2388496","<p>Among the upper bounds, there is no least element as you correctly pointed out. A least element of a set is an element that is less than any other element in the set (so no, we can't just choose $c$ or $d$, they aren't the least)</p>

<p>Your definition of complete lattice is slightly wrong tough. You gave de definition for a lattice: every pair of elements has a least upper bound and greatest lower bound). For a complete lattice you demand that every subset has a least upper bound and greatest lower bound. This is a stronger condition.</p>

<p>So in the beginning of your post you actually showed that your poset isn't even a lattice. (let alone a complete lattice)</p>
"
"2388494","2388508","<p>First apply Integration by Parts:</p>

<p>$$\int_0^1f(x)\;dx=-\int_0^1xf'(x)\;dx$$</p>

<p>Here we have used the fact that $xf(x)$ vanishes at both $0$ and $1$.</p>

<p>Now invoke Cauchy-Schwarz.  We have $$\left(\int_0^1f(x)\;dx\right)^2=\left(\int_0^1xf'(x)\;dx\right)^2â¤\int_0^1\left(f'(x)\right)^2\;dx\;\times\;\int_0^1x^2\;dx$$</p>

<p>And your inequality follows at once.</p>
"
"2388495","2388498","<p>The problem with your phrasing is that you just require there to be a single value of $m$ for which the sequence is within $\epsilon$ of $a$. That's easily satisfied by sequences that don't approach the limit appropriately - for example, consider the sequence $a_m = m$.</p>

<p>I claim that $a = \lim_{m \rightarrow \infty} a_m = 3$. For any $\epsilon &gt; 0$, take $m = 3$. Clearly $|a_3 - a| = |3 - 3| = 0 &lt; \epsilon$, so we're all good.</p>

<p>The idea of having $M$ in there is that you need to be able to say that for a given $\epsilon$, then all the terms $a_M, a_{M+1}, \ldots$ are within $\epsilon$ of $a$ - essentially, you can always find an $M$ such that after you cut off the first $M - 1$ terms you've only got numbers in that neighbourhood.</p>
"
"2388497","2388547","<p>I'll show that $\;\bigl\lVert \mathfrak p\bigr\rVert=\bigl\lvert\mathfrak p/\mathfrak p^2\bigr\rvert$ is enough to prove that 
$$\;\bigl\lVert \mathfrak p^2\bigr\rVert=\bigl\lVert \mathfrak p\bigr\rVert^2.$$</p>

<p>Indeed the short exact sequence:
$$0\longrightarrow \mathfrak p/\mathfrak p^2 \longrightarrow R/\mathfrak p^2 \longrightarrow R/\mathfrak p \longrightarrow 0$$
shows that
$$\bigl\lVert \mathfrak p^2\bigr\rVert=\bigl\lvert\mathfrak p/\mathfrak p^2\bigr\rvert\cdot\bigl\lvert  R/\mathfrak p\bigr\rvert=\bigl\lvert\mathfrak p/\mathfrak p^2\bigr\rvert\cdot\bigl\lVert \mathfrak p\bigr\rVert = \bigl\lVert \mathfrak p\bigr\rVert^2.$$
The general case follows by an easy induction, considering the short exact sequence:
$$0\longrightarrow \mathfrak p^i/\mathfrak p^{i+1} \longrightarrow R/\mathfrak p^{i+1} \longrightarrow R/\mathfrak p^i \longrightarrow 0$$</p>
"
"2388502","2388524","<p>$\phi$ is non-negative.</p>

<p>$f(x)=\log(\phi(x))$ satisfies </p>

<p>$$2f(x)+f(2x)=\log\left((x+1)^2\right)$$</p>

<hr>

<p><strong>Since you are talking about analytic continuation</strong></p>

<p>For analytic solutions we can compute the derivatives at $x=0$</p>

<p>$$2f^{(n)}(x)+2^nf^{(n)}(2x)=\frac{d^n}{dx^n}\log\left((x+1)^2\right)$$</p>

<p>Therefore $$f^{(n)}(0)=\frac{1}{2+2^n}\frac{d^n}{dx^n}\log\left((x+1)^2\right)|_{x=0}=\frac{(-1)^nn!}{n\left(1+2^{n-1}\right)}$$</p>

<p>Therefore $$f(x)=\sum_{n=1}^{\infty}\frac{(-1)^n}{n\left(1+2^{n-1}\right)}x^n$$</p>

<p>and </p>

<p>$$\phi(x)=\exp\left(\sum_{n=1}^{\infty}\frac{(-1)^n}{n\left(1+2^{n-1}\right)}x^n\right)$$</p>
"
"2388509","2388565","<p>Let $A_1, A_2$ represent the events for the two parts drawn being valid, when drawn <em>with</em> replacement from a series of 20 items produced by one of three machines. &nbsp; Let $H_{\rm I},H_{\rm II},H_{\rm III}$ be the event that the series originates in the respective machine.</p>

<p>Given that the first sample drawn was valid, updating the probability for the series originating from factory#$\rm I$ is an application for Bayes' Rule.</p>

<p>$$\mathsf P(H_{\rm I}\mid A_1) ~{= \dfrac{\mathsf P(A_1\mid H_{\rm I})\mathsf P(H_{\rm I})}{\mathsf P(A_1\mid H_{\rm I})\mathsf P(H_{\rm I})+\mathsf P(A_1\mid H_{\rm II})\mathsf P(H_{\rm II})+\mathsf P(A_1\mid H_{\rm III})\mathsf P(H_{\rm III})} \\ = \dfrac {15}{15+18+16}\quad=\dfrac{15}{49}}$$</p>

<p>Because when drawing with replacement from the series there is $1/20$ probability for picking the known valid part, and a $19/20$ probability for drawing another part from $H_{\rm I}$ (which are valid with probability $15/20$).</p>

<p>$$\mathsf P(A_2\mid H_{\rm I}, A_1) =\dfrac 1{20}+\dfrac{19}{20}\dfrac{15}{20}$$</p>

<p>And so forth for the other factories. &nbsp; <em>Now</em> apply the law of total (conditional) probability:</p>

<p>$$\mathsf P(A_2\mid A_1)~=~ \sum\limits_{m\in\{{\rm I},{\rm II},{\rm III}\}}\mathsf P(A_2\mid H_m, A_1)\,\mathsf P(H_m\mid A_1)$$</p>
"
"2388513","2389761","<p>I have been doing research with elliptic and related functions and the functional equations they satisfy. What I usually am looking for is multivariable equations. As a simple example: $\sin(x+y)\sin(x-y)=\sin(x)^2-\sin(y)^2$ which is a solution of functional equation $0=f(x-y)f(x+y)-f(x)^2+f(y)^2$ which also has as solutions $f(x)=cx$.</p>

<p>I have many similar examples in <a href=""http://somos.crg4.com/ident04.gp"" rel=""nofollow noreferrer"">Special Algebraic Identities</a> which has many algebraic identities and some of them have interesting solutions when regarded as a functional equation. So the previous algebraic identity appears as
$$\texttt{ id2_3_1_2a = +a*a -b*b -(a-b)*(a+b)}$$
with a $\texttt{[TS]}$ tag. What I don't have listed is a lot more identities with only a single variable because they make uninteresting identities in general. For example, for the $\sin(x)$ alone there are an unlimited number of single variable identites. I want to focus attention on the simplest of such identities. Here is a list of simple functional equations in one variable with interesting solutions:</p>

<p>$$ 0 = f(3x)^2 - f(5x)f(x) + f(3x)f(x) - f(x)^2  \tag{1}$$
$$ 0 = 4f(2x)^2 - 3f(3x)f(x) - f(x)^2 \tag{2}$$
$$ 0 = f(3x)f(2x) - 3f(3x)f(x) + f(2x)f(x) + f(x)^2 \tag{3}$$
$$ 0 = f(4x)f(2x)^2 - f(4x)f(3x)f(x) - f(3x)f(2x)f(x) + f(2x)f(x)^2 \tag{4}$$
$$ 0 = f(5x)f(x)^3 - f(4x)f(2x)^3 + f(3x)^3f(x) \tag{5}$$
$$ 0 = f(5x)(f(4x)-f(3x)-f(2x)+f(x)) - f(4x)(f(3x)+2f(x)) +\\
 f(3x)(3f(2x)+3f(x)) +f(x)(f(x)-4f(2x)) \tag{6}$$</p>

<p>The last functional equation is the most challenging.</p>
"
"2388521","2388797","<p>There are at least three ways to do this, some much better than others:</p>

<p>1) Multiply one term by $1 = \frac gg$</p>

<p>$\frac{a}{\frac{f+hce}{g}}-c=q-g$</p>

<p>$\frac gg\frac{a}{\frac{f+hce}{g}}-c=q-g$</p>

<p>$\frac {ga}{f+hce} - c = q-g$.</p>

<p>You don't have to worry about multiplying other terms be cause you are only multiplying one turn by $1$.</p>

<p>You can develop a ""sloth on a diving board"" intuition that $\frac {x}{\frac yz} = \frac {zx}{y}$.  (The sloth gets tired of hanging off the bottom of the diving board and swings up to the top.)</p>

<p>2) Multiply all times by $\frac {f+hce}g$.</p>

<p>$\frac{a}{\frac{f+hce}{g}}-c=q-g$</p>

<p>$\frac{f+hce}g(\frac{a}{\frac{f+hce}{g}}-c)=\frac {f+hce}g(q-g)$</p>

<p>$a -\frac{c(f+hce)}g = \frac {q(f+hce)}{g} - (f+hce)$</p>

<p>Then multiply again be $g$</p>

<p>$a -\frac{c(f+hce)}g = \frac {q(f+hce)}{g} - (f+hce)$</p>

<p>$g(a -\frac{c(f+hce)}g)= g(\frac {q(f+hce)}{g} - (f+hce))$</p>

<p>$ag - c(f+hce) = q(g+hce) - g(f + hce)$</p>

<p>3) Mutiply both sides by $\frac 1q$</p>

<p>$\frac{a}{\frac{f+hce}{g}}-c=q-g$</p>

<p>$\frac 1g(\frac{a}{\frac{f+hce}{g}}-c)=\frac 1g(q-g)$</p>

<p>$\frac {a}{f+hce} - \frac cg = \frac qg - 1$.</p>

<p>I really recommend 1).</p>
"
"2388523","2388531","<p>Since the OP asked for a proof, this may have way more details than are really important...</p>

<p>Although the short version is that this is just a reordering of terms, there is a little bit of subtlety here because of the subtraction. After all, $a - b$ and $b - a$ are most definitely not the same because subtraction is sensitive to order.</p>

<p>But here, we're ok. Recall that $y - x$ is really shorthand for $y + (-x)$, where $(-x)$ is the additive inverse of $x$ (the thing that satisfies $x + (-x) = 0$). So we have</p>

<p>$$y - x = y + (-x) = (-x) + y$$</p>

<p>because addition is commutative. Now we normally don't bother to write the parentheses here, so this is just $-x + y$ as desired.</p>
"
"2388530","2388540","<p>Use contrapositive. </p>

<p>If $x\equiv 0,2,3 \pmod 4$ then... </p>
"
"2388534","2388551","<p>Simple induction will do; I leave the base case for you ...
\begin{eqnarray*}
f_{n+1}(x+y)&amp;=&amp;f_n(x+y) (x+y-n) \\
&amp;=&amp; \sum_{k=0}^{n} \binom{n}{k}f_k(x)f_{n-k}(y) (\color{green}{x-k}+\color{blue}{y-(n-k)}) \\
&amp;=&amp; \sum_{k=0}^{n} \binom{n}{k}f_{k+1}(x)f_{n-k}(y) +\sum_{k=0}^{n} \binom{n}{k}f_k(x)f_{n-k+1}(y) \\
&amp;=&amp; f_{n+1}(x)+\sum_{k=1}^{n} f_{k+1}(x)f_{n-k}(y) \left( \binom{n}{k}+\binom{n}{k-1}\right)+f_{n+1}(y) \\
&amp;=&amp;\sum_{k=0}^{n+1} \binom{n+1}{k}f_{k}(x)f_{n+1-k}(y)  
\end{eqnarray*}</p>
"
"2388541","2388692","<p>A normed space has the property (that all metric spaces have) that $X$ is separable then all subsets are separable too in their subspace topology. So $X^\ast$ (norm)-separable implies that $B_{X^\ast}$ is separable. The reverse also holds in all normed spaces $Y$ by ""scaling"": if $x \neq 0$ then choose $\alpha = \frac{1}{\|x_n\|}$ so that $\|\alpha x\| =1$. If we then (by separability of the unit ball) find a sequence $d_n$ on the ball $B_Y$ that converges to $\alpha x$, and also a sequence of rationals $q_n \to \frac{1}{\alpha}$. Then $q_n d_n \to \frac{1}{\alpha} (\alpha x) = x$. This essentially shows that if $D$ is countable and dense in $B_Y$ then $\{qd: q \in \mathbb{Q}, d \in D\}$ is (countable and) dense in $Y$. So $B_Y$ separable iff $Y$ separable. So we lose nothing by using the unit ball (here the sphere really). And the Hahn-Banach theorem which links things in $X$ to $X^\ast$, gives us functionals on $B_{X^\ast}$ anyway.</p>

<p>Using the unit sphere makes things easier, because you know the norm of all dense elements, namely 1, which allows for the choice of the $x_n$ (otherwise we'd need to scale there too which makes for a more messy proof). We have to prove something on $X$, so we can find points $x_n$ on which $f_n$ is relatively large: we know that $\|f_n\| = \sup \{|f_n(x)|: x \in B_X \}$, so we can find $x_n \in B_X$ such that $|f(x_n)|$ is as close to $1$ as we like. Here more than $\frac{1}{2}$ is sufficient.</p>

<p>As above a countable dense set in $B_X$ is enough to get one on $X$, using the span with rational coefficients. So try that for the $D = \{x_n: n \in \mathbb{N}\}$ we now have: taking finite sums from $D$ with rational coefficients we can approximate all members of the linear span of $D$ (just approximate real coefficients in $\mathbb{R}$ by members of $\mathbb{Q}$; we use that the field is separable). This $\operatorname{span}_{\mathbb{Q}}(D)$ is still countable (standard set theory argument: finite products of countable sets are countable and a union of countably many countable sets is countable). So $Y = \overline{\operatorname{span}(D)}$ has a countable dense set $\operatorname{span}_{\mathbb{Q}}(D)$. So we'd be done if $Y =X$. So assume it's not.</p>

<p>Then, Hahn-Banach allows us to find a functional $f$ (back to $B_{X^\ast}$ where we know something about the $f_n$) that has norm $1$ and is $0$ on $Y$. (In particular $f$ is such, that it is $0$ on the set $D$ where the $f_n$ are chosen to be large, and so the $f$ is very ""different"" from the $f_n$ and so we cannot approximate it in norm by the $f_n$.) </p>

<p>So we know $f(x_n) = 0$ (as $x_n \in Y$!) and the final part of the proof shows that if $f_n$ is chosen to be close to $f$, the point $x_n$ where $f_n$ is large shows that $f$ should also be at least $\frac{1}{4}$ in that point as well.
This gives the needed contradiction.</p>
"
"2388543","2388726","<p>We choose $t = \frac12s$ so that $W(s)$ is in the form $W(s) = k e^{is}$, which is something that is very familiar and we can handle very well.</p>

<p>The range for $s$ comes directly from the range of $t$:</p>

<p>$$0 \le t \le \pi$$</p>

<p>$$0 \le \frac12s \le \pi$$</p>

<p>$$2 \cdot 0 \le 2 \cdot \frac12s \le 2 \cdot \pi$$</p>

<p>$$0 \le s \le 2 \pi$$</p>
"
"2388544","2388559","<p>Your proof is almost valid, but unfortunately </p>

<blockquote>
  <p>The sequence is bounded so, $\limsup|a_n|^{1/n}=1.$ Let this $=\alpha$</p>
</blockquote>

<p>is wrong for several reasons. First, it wasn't clear what $\alpha$ is; I had to read almost to the end to guess that you mean $\alpha := \limsup |a_n|^{1/n}$ (but that's just a writing issue). More importantly, the logic is not correct. The fact that $a_n$ is bounded does not imply this - for example, $a_n = 2^{-n}$ wouldn't have it. You need to also invoke the fact that the series $\sum_n a_n$ diverges.</p>

<p>To fix this: We have $\alpha \le 1$ because the sequence is bounded. If $\alpha &lt; 1$, then root test would imply convergence of $\sum_n a_n$, so by contraposition we find that $\alpha \ge 1$. Hence $\alpha = 1$ and the rest of your proof is fine.</p>
"
"2388548","2388550","<p><strong>HINT</strong></p>

<p>Notice that you are raising 2 to an even power, and remember that
$$
a^2-1 = (a+1)(a-1)
$$</p>
"
"2388553","2388693","<p>As Sangchul Lee commented, we could use partial fractions to get 
$$\frac{ x^3  \cos(x)}{x^6 + 1}=\sum_{i=1}^6 a_i \frac{\cos(x)}{x-b_i}$$ and then use $$\int_0^\infty \frac{\cos(x)}{x-c}\,dx=-\text{Ci}(-c) \cos (c)-\text{Si}(c) \sin (c)-\frac{1}{2} \pi  \sin (c)$$ (provided that $\Im(c)\neq 0\lor \Re(c)\leq 0$). This would be quite tedious but doable.</p>

<p>To my surprise, a CAS gave 
$$\int_0^\infty \frac{ x^3  \cos(x)}{x^6 + 1} \,dx=\frac{1}{2} \sqrt{\frac{\pi }{3}} G_{1,7}^{4,1}\left(\frac{1}{46656}|
\begin{array}{c}
 \frac{1}{3} \\
 0,\frac{1}{3},\frac{1}{3},\frac{2}{3},\frac{1}{6},\frac{1}{2},\frac{5}{6}
\end{array}
\right)$$ where appears the Meijer G function (see <a href=""https://en.wikipedia.org/wiki/Meijer_G-function"" rel=""nofollow noreferrer"">here</a> and <a href=""http://mathworld.wolfram.com/MeijerG-Function.html"" rel=""nofollow noreferrer"">here</a>).</p>

<p>Its numerical value is $0.118802427933651$.</p>
"
"2388554","2388564","<p>If $f^2$ has bounded imaginary part, then $f^2$ maps $\mathbb{C}$ into a strip $S = \{x + i y : |y| \le M\}$ for some $M$. This strip is conformally equivalent to the disk by a map $g$, so the function </p>

<p>$$g\circ f^2 : \mathbb{C} \to \mathbb{D}$$</p>

<p>is bounded. By Liouville, it is constant. Since $g$ is a bijection, this implies $f^2$ is constant. It's a good exercise in Cauchy-Riemann equations to prove that $f^2$ constant implies $f$ is constant.</p>
"
"2388556","2388569","<p>A glance at <a href=""https://www.gov.uk/expenses-and-benefits-cash-sum-payments/scale-rate-payments"" rel=""nofollow noreferrer"">scale rate</a> and similar pages shows that <em>scale rate</em> is not what you are talking about.  <a href=""https://en.wikipedia.org/wiki/Scale_(ratio)"" rel=""nofollow noreferrer"">Scale ratio</a> is.  In your example, 1 inch = 1.25 feet, so the ratio is 1:15, or 1/15 (because there are 15 inches in 1.25 feet).  </p>

<p>Added 10 hours later: One can also say just plain <em>scale</em>.  ""The scale is 1:15"" is probably more common than ""the scale ratio is 1:15"", and equally correct.</p>
"
"2388567","2391438","<p>The fact that the path graph $P_n$ minimizes algebraic connectivity among all connected graphs with $n$ vertices follows from proposition 4.3 in Fiedler's paper.</p>

<p>This proposition states (among other things) that the algebraic connectivity of a graph $G$ is at least $2 e(G) (1-\cos(\pi/n))$, where $e(G)$ is the edge-connectivity of $G$, i.e. the minimum number of edges that must be removed to disconnect the graph.</p>

<p>This fact is also stated more explicitly (with a reference to Fiedler) as Proposition 1.12 in Belhaiza et al's <em>Variable Neighborhood Search for Extremal Graphs. XI. Bounds on Algebraic Connectivity</em>. </p>
"
"2388568","2388584","<p>If the residue at $z_0$ is $0$, there is indeed a continuous antiderivative in a deleted neighbourhood of $z_0$.  That is, the Laurent series $$f(z) = \sum_{n=-\infty}^\infty a_n (z-z_0)^n$$ (converging for $0 &lt; |z-z_0| &lt; r$, for some $r &gt; 0$) has $a_{-1} = 0$, and an antiderivative is $$F(z) = \sum_{n \ne -1} \frac{a_n}{n+1} (z-z_0)^{n+1}$$</p>
"
"2388572","2388576","<p>This is known as the multinomial coefficient:</p>

<p>$${k\choose t_1, t_2, ... , t_n }=\frac{k!}{t_1! \ldots t_n!}$$</p>

<p>As mentioned in the comments, we must have that $t_1+t_2+\dots +t_n = k$</p>
"
"2388574","2388601","<p>$$\frac{Z_1}{Z_2}=\frac{r_1e^{i \theta_1}}{r_2e^{i \theta_2}}=\frac{r_1}{r_2}e^{i(\theta_1-\theta_2)}=\frac{r_1}{r_2}\left(\cos(\theta_1-\theta_2)+i \sin(\theta_1-\theta_2)\right).$$</p>
"
"2388577","2388654","<p>Noah's answer is excellent but makes things a bit more difficult than necessary since he is proving that $F$ is isomorphic to $\mathbb{R}$, rather than merely that $F$ is complete (and so he is basically also reproducing the proof that every complete ordered field is isomorphic to $\mathbb{R}$).  Here is a quick direct proof that if an ordered field $F$ satisfies the intermediate value theorem, then it is Dedekind-complete.</p>

<p>Suppose $X\subset F$ is a nonempty set that is bounded above but has no least upper bound.  Define a function $f:F\to F$ by $f(x)=1$ if $x$ is an upper bound of $X$ and $f(x)=0$ if $x$ is not an upper bound of $X$.  Let $a\in X$ and let $b$ be an upper bound for $X$.  Then $a-1&lt;b$, $f(a-1)=0$ and $f(b)=1$.  But there is no $c$ between $a-1$ and $b$ such that $f(c)=1/2$.  So, assuming $f$ is continuous, this violates the intermediate value theorem for $F$.</p>

<p>It thus remains only to show that $f$ is continuous.  To show this, it suffices to show that for any $x$, there is an open interval $(c,d)$ containing $x$ such that $f(y)=f(x)$ for all $y\in (c,d)$.  First suppose $f(x)=0$.  Then $x$ is not an upper bound for $X$, so there is some $d\in X$ such that $x&lt;d$.  We then have $f(y)=0$ for all $y\in (-\infty,d)$.  Now suppose $f(x)=1$.  Since $x$ is not the least upper bound of $X$, there is some $c&lt;x$ such that $c$ is also an upper bound of $X$.  We then have $f(y)=1$ for all $y\in (c,\infty)$.</p>
"
"2388580","2388753","<p>Your interpretation is correct - it's a common shorthand in this context to write $g^{-1}$ instead of $D(L_g^{-1}).$ The justification is that when $G$ is a matrix group, the differential of left-multiplication by $g$ is again just left-multiplication by $g$. </p>
"
"2388589","2388621","<p>The Fourier transform of any $L^1$ function <a href=""https://en.wikipedia.org/wiki/Riemann%E2%80%93Lebesgue_lemma"" rel=""nofollow noreferrer"">decays at infinity</a>. Is the function 
$$g(\omega) = \frac{-\hat{f}(\omega)e^{-|\omega|y}}{1+|\omega|}$$
in $L^1$? Yes, it is, for a number of reasons. Even the boundedness of $\hat f$ is enough here, thanks to the exponential term $e^{-|\omega|y}$. Or, even neglecting the exponential term, just having $f\in L^2$ is enough,  since the product 
$$\hat{f}(\omega) \frac{1}{1+|\omega|}$$
is integrable by the Cauchy-Schwarz inequality (both terms are in $L^2$).</p>
"
"2388596","2388771","<p>$f(x)=e^{x-1}$ is convex ($f''(x)&gt;0$ for all real $x$), so its graph lies above the tangent at $x=1$: $e^{x-1}\ge x$ for all real $x$. This means $x\,e^{-x}\le e^{-1}$. Replacing $x$ by $x/n$ and raising to the $n$th power gives $$x^n\,e^{-x}\le n^n\,e^{-n}=C_n$$ for $x&gt;0,$ that's the boundedness.<br>
We have to show only the existence of the improper integral $\int^\infty_1x^n\,e^{-x}\,dx,$ since the existence of $\int^1_0x^n\,e^{-x}\,dx$ is trivial.
But $$\int^b_1x^n\,e^{-x}\,dx=\int^b_1\frac{x^{n+2}\,e^{-x}}{x^2}\,dx\le \int^b_1\frac{C_{n+2}}{x^2}\,dx\le\int^\infty_1\frac{C_{n+2}}{x^2}\,dx=C_{n+2},$$  because $x^{n+2}\,e^{-x}\le C_{n+2},$ as was shown above. The LHS is monotone (the integrand is positive) and bounded, so the limit as $b\rightarrow\infty$ exists.</p>
"
"2388600","2388605","<p>$f(x)=\sum_0^\infty (-1)^n x^{2n}$ is a G.P. with first term $1$ and common ratio $-x^2$.</p>

<p>Therefore you simply need to apply the Formula for infinite sum of a GP.</p>

<p>$$S_{\infty}=\frac{a}{1-r}=\frac{1}{1-(-x^2)}=\frac{1}{1+x^2}$$</p>
"
"2388607","2388608","<p>Yes! Open in open is open. Since $U$ is open in $X'$, then there exists an open subset $V$ of $X$ such that $U=X'\cap V$. The fact that $X'$ is open in $X$ gives that $U$ is a finite intersection of open subsets of $X$. Thus $U$ is open.</p>

<p>Along the same lines, closed in closed is closed.</p>
"
"2388613","2388620","<p>The formula to calculate the surface area of a cone is:</p>

<p>$$SA=\pi r(r+\sqrt{h^2+r^2})$$</p>

<p>We know the $SA$ and $r$. Now solve for $h$.</p>

<p>$$12=\pi(1.3)(1.3+\sqrt{h^2+(1.3)^2})$$</p>

<p>$$\frac{12}{1.3\pi}-1.3=\sqrt{h^2+(1.3)^2}\text{ moved stuff we already know to left side}$$</p>

<p>$$(\frac{12}{1.3\pi}-1.3)^2=h^2\text{ squared both sides}$$</p>

<p>$$\sqrt{\frac{12}{1.3\pi}-1.3}=h\text{ take square root to solve for $h$}$$</p>

<p>$h\approx 1.28$</p>

<p>I'm guessing they made a mistake.</p>
"
"2388623","2391701","<p>$\newcommand{\bbx}[1]{\,\bbox[15px,border:1px groove navy]{\displaystyle{#1}}\,}
 \newcommand{\braces}[1]{\left\lbrace\,{#1}\,\right\rbrace}
 \newcommand{\bracks}[1]{\left\lbrack\,{#1}\,\right\rbrack}
 \newcommand{\dd}{\mathrm{d}}
 \newcommand{\ds}[1]{\displaystyle{#1}}
 \newcommand{\expo}[1]{\,\mathrm{e}^{#1}\,}
 \newcommand{\ic}{\mathrm{i}}
 \newcommand{\mc}[1]{\mathcal{#1}}
 \newcommand{\mrm}[1]{\mathrm{#1}}
 \newcommand{\pars}[1]{\left(\,{#1}\,\right)}
 \newcommand{\partiald}[3][]{\frac{\partial^{#1} #2}{\partial #3^{#1}}}
 \newcommand{\root}[2][]{\,\sqrt[#1]{\,{#2}\,}\,}
 \newcommand{\totald}[3][]{\frac{\mathrm{d}^{#1} #2}{\mathrm{d} #3^{#1}}}
 \newcommand{\verts}[1]{\left\vert\,{#1}\,\right\vert}$
\begin{align}
\sum_{k1 = 0}^{\infty}\ldots\sum_{k_{10} = 0}^{\infty}\bracks{z^{100}}z^{k_{1} + \cdots + k_{10}} &amp; =
\bracks{z^{100}}\pars{\sum_{k = 0}^{\infty}z^{k}}^{10} =
\bracks{z^{100}}\pars{1 - z}^{-10} = {-10 \choose 100}\pars{-1}^{100}
\\[5mm] &amp; =
{-\bracks{-10} + 100 - 1 \choose 100}\pars{-1}^{100} = {109 \choose 100}
\\[5mm] &amp; =
\bbx{109 \choose 9} = 4,263,421,511,271
\end{align}</p>
"
"2388630","2388640","<p>You did nothing wrong.  For instance, the sequence $$0\to\mathbb{Z}\stackrel{1}{\to}\mathbb{Z}\stackrel{0}{\to}\mathbb{Z}\stackrel{1}{\to}\mathbb{Z}\to 0$$ is exact (and, in fact, if you choose the right isomorphisms of your groups with $\mathbb{Z}$, it is your sequence).</p>
"
"2388641","2388644","<p>The sum of two integers is again an integer. </p>
"
"2388643","2388662","<p>In an earlier question, you asked about a Markov chain for which all rows of the transition matrix are the same. Clearly, that is a singular case. So in that <em>particular</em> singular case, the transition matrix would be of no use in approximating the state at the previous step.</p>
"
"2388647","2388655","<p>Theorem $5.1$ says that a normed space is complete iff, whenever $\sum_{n=1}^\infty \|x_n\|$ converges, then $\sum_{n=1}^\infty x_n$ converges (in the norm, of course). Folland's wording is: ""A normed vector space $X$ is complete iff every absolutely convergent series in $X$ converges."" However this is the same as what I've written above, because he defines that a series $\sum_{n=1}^\infty x_n$ <strong>converges</strong> to $x$ if $\sum_{n=1}^N x_n\to x$ as $N\to\infty$, and that a series $\sum_{n=1}^\infty x_n$ is <strong>absolutely convergent</strong> if $\sum_{n=1}^\infty\|x_n\|&lt;\infty$.</p>

<p>To apply this theorem, we assume that $\sum_{k=1}^\infty \|f_k\|_p$ converges and must show that $\sum_{k=1}^\infty f_k$ converges in the $L^p$ norm. This is precisely what Folland does.</p>
"
"2388653","2388683","<p>Consecutive odd numbers differs $2$.</p>

<p>Now suppose $a$ is a odd numbers, then $a-2, a+2$ are also odd numbers.</p>

<p>Now it is given that $$a-2+a+a+2=69\\\Rightarrow 3a=69\\\Rightarrow a=23$$</p>

<p>Then the third number in this sequence is $a+2=25$ (Ans.)</p>
"
"2388658","2388666","<p>Note that the two triangle <strong>are similar by AA test</strong>. Please read about similarity <a href=""https://artofproblemsolving.com/ebooks/intro-geometry-ebook/preview"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Hence,</p>

<p>$$
\frac{x}{22}=\frac{16}{38}
$$</p>

<p>Thus,</p>

<blockquote>
  <p>$$
x \approx 9 
$$</p>
</blockquote>
"
"2388668","2388690","<p>As Nick mentioned, the question is asking to find h when $x=y$ rather than $\lim_{x\to0 and y\to0}$. For example, this could be the case that $x=4$ and $y=4$.$$-$$
This question becomes significantly simpler in polar coordinates: $x=rcos(\theta)$, $y=rsin(\theta)$. $f(x,y) = \frac{sin(x-y)}{(x-y)}$ becomes $f(r,\theta)=\frac{sin(r(cos(\theta)-sin(\theta)))}{r(cos(\theta)-sin(\theta))}$. $\space$Now, $\lim_{x\to0 and y\to0}f(x,y)$ is equivalent to $\lim_{\theta\to\frac{\pi}{4}}$$f(r,\theta)$. Using L'hopital's rule, you get $\frac{-rsin\theta-rcos\theta}{-rsin\theta-rcos\theta} = 1$ at $\theta=\frac{\pi}{4} \space\space\forall{r}$.</p>
"
"2388674","2388675","<p>Note that
$$0\leq (a-b)^2 = a^2 - 2ab + b^2$$
and hence
$$a^2+b^2 \geq 2ab.$$
Therefore, assuming $ab&gt;0$, we have
$$ \frac{a^2+b^2}{2ab} \geq 1.$$</p>
"
"2388680","2389040","<p>$X$ is unitary. Therefore $(A+iB)(A^T-iB^T)=I$. Equating the real parts and imaginary parts on both sides, we see that $MM^T=I$ and $\det M=\pm 1$. Since the set of all $M$s is connected (because $U(2)$ is path-connected) and  $\det M=1$ when $X=I$, we must have $\det M=1$ for all $M$s.</p>

<p>Alternatively, note that
$$
\pmatrix{I&amp;0\\ iI&amp;I}\pmatrix{A&amp;-B\\ B&amp;A}\pmatrix{I&amp;0\\ -iI&amp;I}=\pmatrix{A+iB&amp;-B\\ 0&amp;A-iB}.
$$
Therefore $\det M=\det(A+iB)\det(A-iB)=|\det(X)|^2=1$.</p>
"
"2388682","2388685","<p>Since $\mathbb{P}(\overline{F}\mid\overline{E})=1-a$, it follows that $$\mathbb{P}(F\mid\overline{E})=1-\mathbb{P}(\overline{F}\mid\overline{E})=1-(1-a)=a$$</p>

<p>and so your expression simplifies to $\frac{1}{2}$.</p>
"
"2388689","2388695","<p>Hint: &nbsp;let $b_n=1/a_n$ then $b_{n+1}=(b_n+b_{n-1})/2 \iff (b_{n+1}-b_n)=-(b_n-b_{n-1})/2$. Then:</p>

<p>$$
b_{n+1}-b_n=\frac{-1}{2}(b_n-b_{n-1})=\left(\frac{-1}{2}\right)^2(b_{n-1}-b_{n-2}) = \cdots = \left(\frac{-1}{2}\right)^{n-1}(b_{2}-b_{1})
$$</p>

<p>Next, telescope:</p>

<p>$$
b_{n+1} = b_n + \left(\frac{-1}{2}\right)^{n-1}(b_{2}-b_{1}) = b_{n-1} + \left(\left(\frac{-1}{2}\right)^{n-1}+\left(\frac{-1}{2}\right)^{n-2}\right)(b_2-b_1) = \;\cdots
$$</p>
"
"2388691","2388702","<p>Note that solving $M(a,b,c) = [x,y,z]$, we get 
$$ \eqalign{  a &amp;= \frac{7}{6} x - y + \frac{z}{3}\cr
              b &amp;= -\frac{x}{2}+\frac{y}{2}\cr
              c &amp;= - \frac{x}{3} + \frac{z}{3}\cr} $$
In particular, $6 \mathbb Z^3 \subset M(\mathbb Z^3)$.</p>

<p>Do you see how that answers both parts of the question?</p>
"
"2388697","2388707","<p>The fact that one mapping is not a bijection of the naturals to the rationals doesn't prevent a different mapping from being one.</p>
"
"2388698","2389866","<p>The fundamental theorem of algebra states that every $\mathbb{C}[x]$ polynomial can be broken down into linear $\mathbb{C}[x]$ factors, i.e., $a_nx^n + \cdots + a_0 = a_n(x-b_0)(x - b_1) \cdots (x - b_n)$ for some $b_0,\ldots,b_n$. There are lots of <a href=""https://en.wikipedia.org/wiki/Properties_of_polynomial_roots#Bounds_on_.28complex.29_polynomial_roots"" rel=""nofollow noreferrer"">easy-to-compute ways</a> ways to get a bound on these $b_i$ using only the original $a_j$, for example, Cauchy's bound:
$$|b_i| \leq 1 + \max_{i} \left| {a_i \over a_n}\right|.$$
Since any $\mathbb{Z}[x]$ divisor of $a_nx^n + \cdots + a_0$ must be a $\mathbb{C}[x]$ divisor, it must in particular be a product of some set of $x - b_i$ and one of $a_n$'s integer factors.  This lets us compute an upper bound on the coefficients of such a divisor.</p>

<hr>

<p>This is a somewhat complicated proof, in that it uses more than one named theorem, and especially through the F.T. of algebra, makes some use of calculus. Perhaps there is still a cleaner argument?</p>
"
"2388700","2388711","<p>To show that this quotient cannot be $1$, assume WLOG that the numbers are in increasing order. Then $abcd &lt; 4d \implies abc &lt; 4$. But this has no solution in the positive integers for $a,b,c$ distinct (the smallest possible product of three distinct positive integers is $1\cdot 2 \cdot 3 =6$).</p>

<p>To show that it cannot be 2, the same argument as above leads us to $abc &lt; 8$, which means the only possibility is $a=1, b=2, c=3$; this gives the equation $6d = 2(6 + d)$, which gives $d=3$, so there is no such solution in distinct positive integers for 2. However, replacing the coefficient in fact provides a solution $(1,2,3,6)$ for the quotient to equal 3; their product is $36$ and their sum is $12$.</p>
"
"2388723","2388731","<p>They are not the same equation, but they are <em>equivalent statements</em>. That means that if the first one is true, so is the second, and vice versa. The number of solutions to the system is then the number of solutions to either of the individual equations.</p>

<p>In this case, assuming $x$ and $y$ are meant to be real numbers, then we can see that there are an infinite number of solutions. Indeed, if $x^*$ is some value for $x$, then we can choose $y^* = 1 - \frac{4}{3}x^*$ for $y$, in order to have equality. Since we can do this for any $x^*$, and there are an infinite number of choices (all real numbers), there is an infinite number of solutions.</p>

<p>Note that the number of solutions may be different if you are only interested in rational or integer solutions to the system, but in any case, these are equivalent statements. </p>
"
"2388730","2388799","<p>If I understand correctly, the hypothesis is that for every $\epsilon &gt; 0$ there exists $A$ a measurable subset of $X$ such that $\epsilon &gt; m(A) &gt; 0$. You want to show that there exists a sequence of disjoint measurable subsets $A_n$ such that $m(A_n)&gt;0$ and $m(A_n)\to 0$.  </p>

<p>Consider a sequence of measurable subsets $B_n$ so that $0&lt;m(B_{n+1})&lt;\frac{1}{2}\cdot m(B_n)$ for all $n\ge 0$.  Now take 
$$A_n \colon = B_n \backslash (\cup_{m&gt;n} B_{m})$$</p>

<p>It is easy to see that $m(A_n) &gt; 0$ for all $n$ and $m(A_n)\to 0$. Moreover, the $A_n$'s are pairwise disjoint.</p>
"
"2388735","2388736","<p>In the middle integral: $\cos 0 = 1$. This gives you a $+ \frac{1}{3}$, which integrated in the next step adds $\frac{2\pi}{3}$, as desired.</p>
"
"2388738","2388808","<p>You are lucky because it seems that a very similar pair of sequences is already at OEIS. </p>

<p>The $x$-coordinates are sequence <a href=""https://oeis.org/A174344"" rel=""nofollow noreferrer"">A174344</a> (""List of $x$-coordinates of point moving in clockwise spiral"") and $y$-coordinates (""List of $y$-coordinates of point moving in clockwise spiral"") are sequence <a href=""https://oeis.org/A268038"" rel=""nofollow noreferrer"">A268038</a>.</p>

<p>List of $x$-coordinates of point moving in clockwise spiral. </p>

<blockquote>
  <p>$$0, 1, 1, 0, -1, -1, -1, 0, 1, 2, 2, 2, 2, 1, 0, -1, -2, -2, -2, -2, -2, -1, 0, 1, 2, 3, 3, 3, 3, 3, 3, 2, 1, 0, -1, -2, -3, -3, -3, -3, -3, -3, -3, -2, -1, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, 2, 1, 0, -1, -2, -3, -4, -4, -4, -4, -4, -4, -4, -4, -4, -3, -2, \cdots$$</p>
</blockquote>

<p>List of $y$-coordinates of point moving in clockwise spiral. </p>

<blockquote>
  <p>$$0, 0, -1, -1, -1, 0, 1, 1, 1, 1, 0, -1, -2, -2, -2, -2, -2, -1, 0, 1, 2, 2, 2, 2, 2, 2, 1, 0, -1, -2, -3, -3, -3, -3, -3, -3, -3, -2, -1, 0, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 1, 0, -1, -2, -3, -4, -4, -4, -4, -4, -4, -4, -4, -4, -3, -2, -1, 0, 1, 2, 3, 4, 4, 4, \cdots$$</p>
</blockquote>

<p>In the referred OEIS pages there are formulas to calculate each sequence of coordinates.</p>
"
"2388747","2389317","<p>To make things a little bit simpler, let us assume that the centre of the circle corresponds with the origin of the cartesian reference frame. Then the circle is characterised by
$$
\frac{x^2}{R^2} + \frac{y^2}{R^2} = 1
$$
where $R$ is its radius of the circle and since the point $(x_p,y_p)$ lies on the circle we have $R = \sqrt{x_p^2 + y_p^2}$. </p>

<p>Rotating the circle about the line $y=0$ will result in a projected ellipse with a long axis $R$ that is along the $x$-direction and a shortened axis of length $R \cos\theta$ in the $y$-direction. The projected ellipse is therefore characterised by
$$
\frac{x^2}{R^2} + \frac{y^2}{R^2 \cos^2\theta} = 1
$$
This projected ellipse will in general not pass through the point $(x_p,y_p)$ anymore and we therefore make an isotropic expansion of the ellipse by a factor $f\geq1$. This gives for the scaled ellipse the curve 
$$
\frac{x^2}{f^2 R^2} + \frac{y^2}{f^2 R^2 \cos^2\theta} = 1
$$
Since this has to pass again through the point $(x_p,y_p)$ this gives a simple equation
$$
\frac{x_p^2}{f^2 R^2} + \frac{y_p^2}{f^2 R^2 \cos^2\theta} = 1
$$
which we can easily solve for the scaling factor $f$ and find
$$
f^2 = \frac{x_p^2 \cos^2\theta + y_p^2}{R^2 \cos^2 \theta}
$$
which can be inserted back into the characterisation we obtained for the scaled ellipse and gives the final answer
$$
\frac{x^2 \cos^2\theta}{x_p^2 \cos^2\theta + y_p^2} + \frac{y^2}{x_p^2 \cos^2\theta + y_p^2} = 1
$$
The correctness can easily be confirmed by checking that it is an ellipse that passes through the point $(x_p,y_p)$ and of which the ratio of the long and short axis is indeed $\frac{1}{\cos\theta}$. For an arbitrary centre $(x_o,y_o)$ of the circle this would result in
$$
\frac{(x-x_o)^2 \cos^2\theta}{(x_p-x_o)^2 \cos^2\theta + (y_p-y_o)^2} + \frac{(y-y_o)^2}{(x_p-x_o)^2 \cos^2\theta + (y_p-y_o)^2} = 1
$$</p>
"
"2388751","2388775","<p>To guess a coin correcly, the probability is $\tfrac{1}{2}$. To guess a coin wrongly, the probability is again $\tfrac{1}{2}$. </p>

<p>We are assuming now you distinguish the coins, otherwise it's a very different problem.</p>

<p>So for the three questions, you have to count how many ways you can get 5, 4, or 3 coins right. We get the probabilities $p_5, p_4, p_3$:
$$ p_5 = \frac{1}{2^5}$$
$$ p_4 = {{5}\choose{4}} \frac{1}{2^5} = \frac{5}{2^5}$$
$$ p_3 = {{5}\choose{3}} \frac{1}{2^5} = \frac{10}{2^5}$$
Note that $p_2=p_3, p_1=p_4$ and $p_0=p_5$.</p>
"
"2388755","2388769","<p>Since we speak about <em>intuition</em>, whenever I see a differential equation like the one you posted, my first reaction is to set $y=e^{-x}\,u$. This makes $$u''=x$$ Then, it is clear that an $x^3$ will be coming.</p>

<p>Is this OK for you ?</p>

<p>Suppose now that we face $$yââ + 2yâ +y = P_n(x) \,e^{-x}$$ where $P_n(x)$ is a polynomial of degree $n$, the same procedure would lead to $$u''=P_n(x)$$ and then the solution has to be a polynomial of degree $(n+2)$.</p>
"
"2388757","2388779","<p>This limit holds for any matrix. Any $k \times k$ matrix $A$ cannot have eigenvalues of arbitrary magnitude, so $(n I + A)^{-1}$ is defined for all $n$ in some neighborhood of infinity (and minus infinity). In this neighborhood we can state that 
$$n\left(nI + A\right)^{-1}=n\left(n(I + \frac{1}{n}A)\right)^{-1} = \left(I + \frac{1}{n}A\right)^{-1}.$$ 
So if we define $f(h) = \left(I + hA\right)^{-1}$, we are interested in the limit $$\lim_{h\to 0} f(h).$$ We have seen that $f$ is indeed defined in a neighborhood around $0$ and since taking inverses is continuous we find that $f$ is continuous and thus 
$$\lim_{n \to \infty} n\left(nI + A\right)^{-1} = \lim_{h \to 0} f(h)= f(0) = (I+ 0 \cdot A)^{-1} = I.$$</p>

<hr>

<p>I think of this argument as a sort of generalization of how I would prove that $$\lim_{n \to \infty} \frac{n}{n + a} = 1$$ for any real (or indeed complex) number $a$.</p>
"
"2388761","2388764","<p>$F\circ F=I$, the identity, so $Y=X\circ F$ iff $X=Y\circ F$.
But $\det(X)=\det(Y\circ F)=\det(Y)\det(F)=(-1)(-1)=1$, so $X\in SO(4)$.</p>
"
"2388763","2388810","<p>As a not so obvious substitution set $y=e^{186x}$ then
$$
y''(t)=e^{186x(t)}\,186(x''(t)+186x'(t)^2)=186\,c\,y(t)
$$
which now is a standard linear differential equation of second order.</p>
"
"2388803","2388845","<p>HINTS...To obtain the maximum height, your equation of motion should be $$ma=-mg-kv^2.$$
The expression you have is for the downward motion, not the upward motion.</p>

<p>Use $a=\frac{dv}{dt}$ and get an arctan integral to find the time at max height</p>

<p>To get the terminal velocity put $a=0$ In the downward equation.</p>
"
"2388804","2388823","<p>Let's start at the second line of your approach.</p>

<p>Note that $(A\cap B)\cap(A\cap C^{\complement})=A\cap B\cap A\cap C^{\complement}=A\cap B\cap C^{\complement}=(A\cap B)-C\tag1$</p>

<p>This on base of associativity of $\cap$ and $A\cap A=A$.</p>

<p>However what ""they"" want to see as outcome is $(A\cap B)-(A\cap C)$, which is the same as the RHS of $(1)$ of course.</p>

<p>This can be reached by realizing that $A\cap B\cap A^{\complement}=\varnothing$ so that: $$A\cap B\cap C^{\complement}=(A\cap B\cap A^{\complement})\cup(A\cap B\cap C^{\complement})=(A\cap B)\cap(A^{\complement}\cup C^{\complement})=$$$$(A\cap B)\cap(A\cap C)^{\complement}=(A\cap B)-(A\cap C)$$</p>
"
"2388807","2388820","<p>Var$(3X - Y - 4) = 9$Var$(X) + $Var$(Y)$. Note the variance of $X$ and $Y$ cannot be $0$ because $X$ and $Y$ are not constant RVs.</p>

<p>Use the formula, Var$(X) = E[X^2] - (E[X])^2$ to calculate the variance.</p>
"
"2388818","2388849","<p>The group homomorphism $\mathrm{SU}(2)\times\mathrm{U}(1)\to\mathrm{U}(2)$ given by $(A,\lambda)\mapsto \lambda A$ is not one-to-one, its kernel is $\mathbb{Z}_2$ with nontrivial element $(-I,-1)$.</p>

<p>However, the map $\mathrm{SU}(2)\times\mathrm{U}(1)\to\mathrm{U}(2)$ given by $(A,\lambda)\mapsto[\begin{smallmatrix}\lambda&amp;0\\0&amp;1\end{smallmatrix}]A$ is a diffeomorphism, it's just not a group homomorphism. Indeed, using this copy of $\mathrm{U}(1)$ inside $\mathrm{U}(2)$, we see that $\mathrm{U}(2)$ is the internal semidirect product of $\mathrm{SU}(2)$ and $\mathrm{U}(1)$ (because every element is uniquely a product of two such elements, and $\mathrm{SU}(2)$ is normal).</p>
"
"2388842","2388862","<p>You are mistaken. You cannot build a NOT gate, because NOT is not falsity-preserving ... Try building a NOT gate and you'll see. Notice that aâ§Â¬a
is equivalent to FALSE, not to Â¬a</p>
"
"2388854","2388893","<p>One obvious problem is that</p>

<pre><code>k2X = fX(t(i)+h/2,V(i)+h/2+k1V);
</code></pre>

<p>should read</p>

<pre><code>k2X = fX(t(i)+h/2,V(i)+h/2*k1V);
</code></pre>

<p>multiplication, not addition of $h/2$.</p>

<p>The general critique is that you are using the <strong>Mat</strong>rix <strong>Lab</strong>oratory, so in keeping with its philosophy, use vector valued functions and the built-in vector operations. Also use the provided ODE integrators and make your interfaces similar to them so that switching back and forth is easy.</p>
"
"2388860","2393914","<p>Let $f$ be as stated, and suppose $g = \sum_{n\in J}\langle f,e_n\rangle e_n$. Then
$$
               (f-g)\perp e_n,\;\;\; n\in J.
$$
In particular, $\langle f-g,h\rangle =0$ for all $h \in E_J$, which gives $(f-g)\perp E_J$ and
$$
        \|f-h\|^2 = \|(f-g)+(g-h)\|^2 = \|f-g\|^2+\|g-h\|^2.
$$
Hence,
$$
           \|f-h\|^2 \ge \|f-g\|^2,\;\;\; h\in E_J,
$$
with equality iff $\|g-h\|=0$ or $g=h$. This proves (a).</p>

<p>Part (b) requires finding a,b,c such that
$$
            \int_{-\pi}^{\pi}\{ |t|-ae^{2it}-be^{3it}-ce^{10it} \} e^{-2it}dt =0 \\
      \int_{-\pi}^{\pi}\{ |t|-ae^{2it}-be^{3it}-ce^{10it} \} e^{-3it}dt =0 \\
\int_{-\pi}^{\pi}\{ |t|-ae^{2it}-be^{3it}-ce^{10it} \} e^{-10it}dt =0
$$
This is a system of 3 equations in the 3 unknowns $a,b,c$.</p>
"
"2388863","2388896","<p>You can see this as a ""coincidence"" explained by:</p>

<p>The number of $m$ faces of an $n$-cube is: $$2^{n-m}\binom{n}{m}$$as explained on <a href=""https://en.wikipedia.org/wiki/Hypercube#Elements"" rel=""nofollow noreferrer"">wikipedia</a>.</p>

<p>The probability of throwing exactly $m$ times a $3$ by $n$ throws of a $3$-sided die is: $$\binom{n}{m}\left(\frac13\right)^m\left(\frac23\right)^{n-m}$$ because we are dealing evidently with binomial distribution having parameters $n$ and $p=\frac13$.</p>

<hr>

<p>This equality might inspire to construct some model in which both concepts come together. </p>

<p>That might be done in the other answer(s) to this question :-).</p>
"
"2388865","2389032","<p>According to <a href=""https://en.wikipedia.org/wiki/Leibniz_integral_rule#General_form:_Differentiation_under_the_integral_sign"" rel=""nofollow noreferrer"">Leibniz's integral rule</a>, since $f(a,\theta) = 1/(a+\cos\theta)$ and $f_{a}$ are continuous in both $a$ and $\theta$, we can interchange the integral and derivative as follows.</p>

<p>$$ \begin{aligned} \dfrac{\text{d}}{\text{d}a} \color{red}{\int_{0}^{\pi} \dfrac{1}{a + \cos \theta} \text{ d}\theta} \ &amp; = \int_{0}^{\pi} \dfrac{\partial}{\partial a} \left( \dfrac{1}{a + \cos \theta} \right) \text{ d}\theta \\[5pt] &amp; = - \color{blue}{\int_{0}^{\pi} \dfrac{1}{\left(a + \cos \theta \right)^2} \text{ d}\theta} \end{aligned}$$</p>

<p>Thus, we've simplified $\color{blue}{\text{the}}$ $\color{blue}{\text{integral}}$ $\color{blue}{\text{we're}}$ $\color{blue}{\text{interested}}$ $\color{blue}{\text{in}}$ to the derivative of $\color{red}{\text{an}}$ $\color{red}{\text{integral}}$ $\color{red}{\text{we}}$ $\color{red}{\text{can}}$ $\color{red}{\text{compute}}$. The Weierstrass substitution turns rational functions of trigonometric functions into rational functions we can evaluate more easily. Let $t = \tan \theta/2$ and draw the corresponding triangle with $t$ as the opposite and $1$ as the adjacent.</p>

<p><a href=""https://i.stack.imgur.com/4A7Pl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4A7Pl.png"" alt=""enter image description here""></a></p>

<p>The hypotenuse can be found using Pythagoras' theorem and the double angle formula gives us an expression for $\cos \theta$ in terms of $t$. $$ \implies \cos \theta/2 = 1/\sqrt{1+t^2} \implies \cos \theta = \frac{1-t^2}{1+t^2}$$</p>

<p>The final step to transforming the integral from $\theta$ to $t$ is to deal with the differential. $$\begin{aligned} t = \tan \frac{\theta}{2} \ \implies \ \text{dt} &amp; = \frac{1}{2} \sec^2 \frac{\theta}{2} \text{ d}\theta \\[3pt] &amp; = \frac{1}{2}\left(1 + \tan^2 \frac{\theta}{2}\right) \text{ d}\theta \\[3pt] &amp; = \frac{1}{2}(1+t^2) \text{ d}\theta \end{aligned} $$ $$ \therefore \dfrac{2}{1+t^2} \text{ d}t = \text{d}\theta $$ From here on out, it's a partial fraction decomposition of the integrand and fun stuff.</p>
"
"2388868","2388883","<p>We have $|\frac{dx}{ds}|=|\frac{dx}{ds'}|=1$, so by chain rule $|\frac{dx}{ds'}|=|\frac{dx}{ds}||\frac{ds}{ds'}|$. Hence $|\frac{ds}{ds'}|=1$, implying $\frac{ds}{ds'}=\pm1$ or $s=\pm s'+a$.</p>
"
"2388873","2388917","<p>Generally, an element of $F$ can be described as a product of integer powers of $a$ and $b$, but in the quotient any integer power can be simplified to an exponent of either $0$ or $1$, so the resulting elements all look like alternating products of $a$s and $b$s. Indeed, $F/N$ is generated by $a$ and $ab$, evidently $a$ has order $2$ and $ab$ has infinite order. Notice $a(ab)a^{-1}=ba=(ab)^{-1}$ so in fact we have that $F/N$ is an internal semidirect product $\langle ab\rangle\rtimes\langle a\rangle\cong\mathbb{Z}\rtimes\mathbb{Z}_2$. (I am abusing notation in referring to elements of $F/N$ by their representatives in $F$.)</p>
"
"2388874","2389063","<p>A matrix $A$ for which $A,A^{*}$ commute (where $A^{*}$ is the Hermitian transpose) is called normal. A basic fact about normal matrices is that for all $v \in \mathbb{C}^n$ we have $\| Av \| = \| A^{*} v \|$. The reason is that</p>

<p>$$ \| Av \|^2 = \left&lt; Av, Av \right&gt; = \left&lt; v, A^{*}Av \right&gt; = \left&lt; v, AA^{*}v \right&gt; = \left&lt; A^{*}v, A^{*}v \right&gt; = \| A^{*}v \|^2. $$</p>

<p>In particular, this implies that $Av = 0$ iff $A^{*}v = 0$ so $A$ and $A^{*}$ has the same kernel. Then</p>

<p>$$ \operatorname{Im}(A) = \operatorname{ker}(A^{*})^{\perp} = \operatorname{ker}(A)^{\perp} = \operatorname{Im}(A^{*}) $$</p>

<p>so $A,A^{*}$ also have the same image.</p>
"
"2388877","2389251","<p>Subrings $\,R\subseteq \Bbb Q$ are characterized by the subset of $S$ of primes that become invertible in $\,R,\,$ i.e. $\,R = \Bbb Z[1/p\ :\ p\in S]$ is obtained by adjoining the inverses of all primes in $S$. One easily checks that this is a ring since fractions having denominators being products of such primes are closed under addition and multiplication. </p>

<p>Conversely, let $\,R\,$ be such a proper subring $\,\Bbb Z\subsetneq R\subset \Bbb Q,\,$ and let $\,r = a/b\in R\,$ be a noninteger. Wlog $\,(a,b) = 1\,$ so by Bezout $\,aj+bk = 1\,$ for some $\,j,k\in\Bbb Z.\,$ Thus $\,j(a/b) + k = 1/b\in R.\,$ Hence $\,a/b\in R\iff 1/b\in R,\,$ so $R$ is generated by adjoining inverses of integers to $\,\Bbb Z.\,$ But $n$ is invertible iff all its prime factors are invertible, so  we can restrict to inverses of primes.</p>

<p>The above generalizes to any Bezout domain, i.e. rings like $\,\Bbb Z\,$ where gcds enjoy a Bezout identity. Said in the language of commutative algebra, every overring (of fractions) of a Bezout domain is a localization, i.e. is generated by adjoining inverses of elements.  <a href=""https://math.stackexchange.com/a/194719/242"">See here</a> for further discussion, including literature references.</p>
"
"2388887","2388889","<p>Well, I guess you simply want  the number of triangular matrix entries in an $N\times N$ matrix. .
So without the diagonal it is $N(N-1)/2$ and including  the diagonal it is $N(N+1)/2$.</p>

<p>Using your notation, matrix indices are usually indexed starting with 1, so you get the results via 
$$
\sum_{n=1}^N \sum _{m=n+1}^N  1 = N(N-1)/2
$$
and
$$
\sum_{n=1}^N \sum _{m=n}^N 1 = N(N+1)/2
$$</p>
"
"2388898","2388949","<p><b>Part (a):</b>
<p>
For convenience of notation, let $u = 0.4,\;\;v = 0.8,\;\;w=0.5$.
<p>
\begin{align*}
P(X=0)=\;&amp;(1-u)(1-v)(1-w) = .06\\[4pt]
P(X=1)=\;&amp;u(1-v)(1-w)+(1-u)v(1-w)+(1-u)(1-v)w= .34\\[4pt]
P(X=2)=\;&amp;(1-u)vw+u(1-v)w+uv(1-w)= .44\\[4pt]
P(X=3)=\;&amp;uvw=.16\\[4pt]
\end{align*}
<b>Part (b):</b>
<p>
The mean of $X$ is 
$$0 \cdot P(X=0) + 1\cdot P(X=1)+2\cdot P(X=2)+3\cdot P(X=3) = 1.7\;\text{minutes}$$
<b>Part (c):</b>
<p>
Assume each red light last for $2$ minutes . Then on average, if a given light is red, the waiting time for that light is $1$ minute (half of $2$). 
<p>
Hence, the mean waiting time will be
$$u\cdot 1+v\cdot 1 + w\cdot 1 = 1.7\;\text{minutes}$$</p>
"
"2388899","2389499","<p>The functor $\operatorname{Tor}^\mathbb{Z}_1$ is left exact in each variable (by the long exact sequence for Tor and the fact that $\operatorname{Tor}^\mathbb{Z}_2=0$).  So if $\operatorname{Tor}^\mathbb{Z}_1(U,U)=0$, then $\operatorname{Tor}^\mathbb{Z}_1(A,B)=0$ for any subgroups $A,B\subseteq U$.  Now if $U$ is not torsion free, let $x\in U$ be a nonzero torsion element and let $A$ and $B$ to both be the cyclic subgroup generated by $x$.  Then $\operatorname{Tor}^\mathbb{Z}_1(A,B)\neq 0$ (since $\operatorname{Tor}^\mathbb{Z}_1(\mathbb{Z}/(n),\mathbb{Z}/(n))\cong\mathbb{Z}/(n)$ for all $n&gt;0$), which is a contradiction.  Thus if $\operatorname{Tor}^\mathbb{Z}_1(U,U)=0$, then $U$ is torsion free.</p>

<p>More generally, if $M$ and $N$ are $\mathbb{Z}$-modules, then $\operatorname{Tor}^\mathbb{Z}_1(M,N)\neq 0$ iff there is a prime $p$ such that both $M$ and $N$ have elements of order $p$.  See <a href=""https://math.stackexchange.com/questions/2334066/when-is-operatornametor-1-mathbbz-m-n-neq-0"">When is $\operatorname{Tor}_1 ^\mathbb{Z} (M,N) \neq 0$?</a> for the details.</p>
"
"2388913","2388923","<p>First method is correct, and far easier.</p>

<p>What is missing from your second method are some coefficients. For example, your term for picking A 5 times out of 6 is $0.2^{5} *0.8$. That's not correct. The actual term you need is $\left(\array{6\\5}\right)*0.2^{5} *0.8$. The factor $\left(\array{6\\5}\right)$ accounts for the fact the balls can come in different orders - the non-A ball could be the 1st, 2nd, through to 6th ball you pick. So you need to actually perform the sum:</p>

<p>$$\sum_{n=1}^{6}\left[\left(\array{6\\n}\right) * 0.2^{n} * 0.8^{6-n}\right]$$
Since you only have 6 picks, it's not <em>too</em> bad. But imagine if you want to find the probability you don't pick A in 100 consecutive picks, or 1000.</p>
"
"2388918","2388999","<p>If we assume that for every $A &gt; 0$ the restriction $g \upharpoonright [0, A]$ is bounded, the claim is true.</p>

<p>Let $\varepsilon &gt; 0$ and pick $A &gt; 0$ such that $\left| g(2x) - \frac{1}{2} g(x) \right| \leqslant \varepsilon$ for $x \geqslant 2A$. From the assumpion, there is $B \geqslant 2 \varepsilon$ such that $|g(x)| \leqslant B$ for $A \leqslant x \leqslant 2A$. </p>

<p>Now let $x_0 \in [A, 2A]$ and $x_n = 2^n \cdot x_0$. It's straightforward to prove by induction that </p>

<p>$$|g(x_n)| \leqslant 2 \varepsilon + \frac{B-2\varepsilon}{2^n}.$$ </p>

<p>It remains to pick $N \in \mathbb{N}$ such that</p>

<p>$$\frac{B - 2 \varepsilon}{2^N} \leqslant \varepsilon$$</p>

<p>and note that for each $x \geqslant 2^N \cdot A$ there is a sequence $x_n$ as above such that $x = x_M$ for some $M \geqslant N$, so $|g(x)| \leqslant 3 \varepsilon$. </p>
"
"2388921","2388922","<p>You can do the substitution $t=ax$, so the limit becomes
$$
\lim_{t\to0}\frac{f(t)}{\dfrac{t^2}{a^2}+\dfrac{t}{a}}
=
a^2\lim_{t\to0}\frac{f(t)}{t^2+at}
$$
Now it should be easy.</p>
"
"2388935","2388945","<p>Hint $\cos B=\cos((A+B)-A)$ use the compound angle formula</p>
"
"2388938","2388997","<p>Moise Cohen and Andrew D. Hwang's answers together tell the story. </p>

<p>First, Andrew's: it's possible to have $\pi$, at some point, be rational. But when you look at the point in question, it's not a manifold point (but see below). </p>

<p>Next, Moishe's remark: He's observing that as $d$ goes to zero, your circle and its circumference become more and more like a circle/circumference in the tangent plane, where $\pi$ turns out to be ... well, $\pi$. </p>

<p>So if you want $\pi$ to be rational (and constant) everywhere, it's not going to happen, by Moishe's remark; if you want it to be rational and constant at some point, that's possible (by Andrew's example), but that point will be a non-manifold point. </p>

<p>Regarding ""non-manifold points"": the cone can be thought of as being in continuous 1-1 correspondence with a disk (project along the axial direction), and the <em>disk</em> is certainly a smooth manifold. Can't we just pull back this smoothness structure to make the cone a smooth manifold? The answer is ""Sure...but then the inclusion of the cone into 3-space is not an embedding."" So if you're going to consider the topology and geometry of your surface as being inherited from the ambient space, the arguments above all hold. </p>

<p>By the way: great question!</p>
"
"2388940","2388952","<p>I think part of what is confusing you is that when you say ""polynomial"" you should say ""polynomial in what"". For example, our standard algorithms for prime factorization take ""polynomial time""...in the number itself. But in that field ""polynomial"" means ""polynomial in the number of digits"".</p>

<p>In your case, note $\Delta \leq |V|$, so something $O(|E| |V|)$ is polynomial in $|E|$ and $|V|$. </p>
"
"2388944","2388948","<p>Since there is no restriction on how many balls may be placed in a box, there are $n$ choices for each of the $n$ balls.  Hence, there are $n^n$ ways to distribute $n$ balls into $n$ boxes.  You would obtain $n!$ if each box could contain only one of the $n$ balls.</p>
"
"2388951","2389050","<p>Since the answer is completely yours, I can write one of my favorite theorems from group theory, which essentially proves why $\langle a,b|a^3,b^2,ab=ba\rangle$ and $C_3\times C_2$ are isomorphic. That would be von Dyck's theorem.</p>

<p>So let $G$ be a group with presentation $\langle a_1,\dots, a_n|r_1,\dots, r_m\rangle$ and $H$ some other group. The $r_i$'s are elements of $G$ so they can be written in terms of the generators, i.e. $r_i=a_{i_1}^{\epsilon_1}\cdots a_{i_k}^{\epsilon_k}$ where $\epsilon_j=\pm 1$ for every $i$. </p>

<p>Suppose that there is map $f:\{a_1,\dots,a_n\}\to H$ (<em>$f$ is just a map defined on the generators, not a homomorphism)</em> with the property
$$f(a_{i_1})^{\epsilon_1}\cdots f(a_{i_k})^{\epsilon_k}=1_H$$
<em>Note: If $f$ was a homomorphism that would be just $f(r_i)$, so, what we are assuming is just that $f$ vanishes on each relation of $G$.</em></p>

<p>Then, $f$ can be extended to a homomorphism $\bar{f}:G\to H$ such that $\bar{f}(a_i)=f(a_i)$ for each $i$.</p>

<p>The proof of that is quite obvious, set $\bar{f}(a_i)=f(a_i)\,\forall i$, extend $\bar{f}$ holomorphicaly and voilÃ .</p>

<p>Now that is important for your answer since it proves why $G:=\langle a,b|a^3,b^2,ab=ba\rangle$ and $C_3\times C_2$ are isomorphic. Write $C_3\times C_2$ as $C_6$ and define $f(a)=2$, $f(b)=3$. Then $f$ vanishes on each relation and by von Dyck's theorem you have a homomorphism $\bar{f}:G\to C_6$ which is surjective, since $2$ and $3$ generate $C_6$. Now define $g:C_6\to G$ by setting $g(1)=b\cdots a^{-1}$. If I am not mistaken, $g\circ f=id_G$ and $f\circ g=id_{C_6}$, so they are indeed isomorphic.</p>

<p>The rest of the proof is exactly as in your comment.</p>
"
"2388959","2390055","<p>According to the rules you stated, a gambler </p>

<ul>
<li>loses $\$1$ if the number he or she bets on does not occur</li>
<li>wins $\$1$ if the number he or she bets on occurs exactly once</li>
<li>wins $\$2$ if the number he or she bets on occurs exactly twice </li>
<li>wins $\$5$ if the number he or she bets on occurs exactly three times</li>
</ul>

<p>Hence, 
\begin{align*}
P(V = -1) &amp; = P(X = 0)\\
P(V = 1) &amp; = P(X = 1)\\
P(V = 2) &amp; = P(X = 2)\\
P(V = 5) &amp; = P(X = 3)
\end{align*}
With the exception of the typographical error I mentioned in the comments, you correctly calculated $P(X = 0)$, $P(X = 1)$, $P(X = 2)$, and $P(X = 3)$.  </p>

<p>Another way to calculate the probability that the number the gambler bets on  occurs in exactly $k$ of the $3$ rolls is to use the <a href=""https://en.wikipedia.org/wiki/Binomial_distribution"" rel=""nofollow noreferrer"">binomial distribution</a>.  The probability that the number the gambler bets on occurs exactly $k$ times in $3$ rolls is 
$$P(X = k) = \binom{3}{k}\left(\frac{1}{6}\right)^k\left(\frac{5}{6}\right)^{3 - k}$$
where $\binom{3}{k}$ represents the number of ways exactly $k$ of the dice show the number the gambler bets on, $1/6$ is the probability that number appears, and $5/6$ is the probability that number does not appear.    </p>
"
"2388963","2390179","<p>Since $G$ is $2$-generated, $|G:\Phi(G)| = p^2$, so $M$ has $\Phi(P)$ as a subgroup of index $p$ and exponent $p$.</p>
"
"2388965","2388968","<p>You forgot to invert the variable at $z=0$ and at $z=\infty$.</p>

<p>$f(1/z)=\infty$ for $z=\infty$</p>

<p>and</p>

<p>$f(1/z)=1$ for $z=0$.</p>

<p>Then $g(z)=f(1/z)=1-z$ near $z=0$, which is analytic, with derivative $g'(0)=-1\neq0$, and therefore conformal.</p>
"
"2388966","2388974","<p>Both formulas you wrote are correct, but the first one is the flux of $\mathbf{v}$ through $\Sigma$: $$\text{flux}=\int \int_\Sigma\mathbf{v} \cdot d\mathbf{\Sigma} =\int \int_\Sigma \nabla \times \mathbf{u} \cdot d\mathbf{\Sigma}$$
while the second one is the flux through $\Sigma$ of $\nabla \times \mathbf{v}$</p>
"
"2388971","2389323","<p>If $X:\Omega\to\mathbb R$ is a random variable, and $A\subset\mathbb R$ is a Borel set, then $[X\in A]$ is shorthand for the event $\{\omega\in\Omega:X(\omega)\in A\}$. This is extremely widely used, making it quite surprising that you have studied both Brownian motion and conditional expectation without encountering it.</p>
"
"2388978","2389028","<p>No, it is not closed.</p>

<p>Consider
$$
 y_j=\frac1{\sqrt{j}}, \quad (y_j)\in\ell^\infty.
$$</p>

<p>It is easy to see that $(y_j)\not\in Y$.</p>

<p>Now define $(x_j^n)$ via
$$
 x_j^n = y_j \cdot \mathbb 1_{j\leq n}
$$</p>

<p>Then $(x_j^n)\in Y$, and
$\| (x_j^n)- (y_j)\|_{\ell^\infty} \leq \frac1{\sqrt{n}}$.</p>
"
"2388980","2389015","<p>Your âRigorous proofâ is not correct. Look at the way you chose $N_1$, $N_2$, and $N_3$. Only the sequence $(s_n)_{n\in\mathbb N}$ is mentioned, not the sequence $(t_n)_{n\in\mathbb N}$. You can't get a correct proof without using both of them.</p>

<p><hr />
Here is how I would prove the same statement. Please keep in mind that I always try to avoid to introduce asymmetries when the hypothesis are symmetric. In this case, what is supposed about both sequences is the same and therefore I will avoid to treat them in two different ways.</p>

<p>For each $n\in\mathbb N$, you know that$$s_nt_n-L_1L_2=(s_n-L_1)L_2+(t_n-L_2)L_1+(s_n-L_1)(t_n-L_2)$$and therefore that$$|s_nt_n-L_1L_2|\leqslant|s_n-L_1|.|L_2|+|t_n-L_2|.|L_1|+|s_n-L_1|.|t_n-L_2|.$$Let $\varepsilon&gt;0$ and choose $N_1\in\mathbb N$ such that$$n\geqslant N_1\Longrightarrow|s_n-L_1|&lt;\min\left\{\frac\varepsilon{3\bigl(|L_2|+1\bigr)},\sqrt{\frac\varepsilon3}\right\}.$$Also, choose $N_2\in\mathbb N$ such that$$n\geqslant N_2\Longrightarrow|t_n-L_2|&lt;\min\left\{\frac\varepsilon{3\bigl(|L_1|+1\bigr)},\sqrt{\frac\varepsilon3}\right\}.$$Let $N=\max\{N_1,N_2\}$. Then, if $n\geqslant N$,\begin{multline*}|s_n-L_1|.|L_2|+|t_n-L_2|.|L_1|+|s_n-L_1|.|t_n-L_2|&lt;\\&lt;\frac{\varepsilon|L_2|}{3\bigl(|L_2|+1\bigr)}+\frac{\varepsilon|L_1|}{3\bigl(|L_1|+1\bigr)}+\frac\varepsilon3&lt;\varepsilon.\end{multline*}</p>
"
"2388981","2388982","<p><a href=""https://en.wikipedia.org/wiki/Sturm%27s_theorem"" rel=""noreferrer"">Sturm's theorem</a> allows you to do that.</p>

<p>Small example:</p>

<p>$$p(x)=x^2-1/4$$</p>

<p>We compute $$p'(x)=2x$$</p>

<p>and the opposite of the remainder of the division of $p$ by $p'$ which is, up to a positive constant factor, $$1$$</p>

<p>Now we evaluate those three $(p(x),p'(x),1)$ at $x=0$ to get $(-1/4, 0, 1)$ and at $x=1$ to get $(3/4,2,1)$. The first triple has $1$ change of sign, and the second has $0$ changes of sign. Therefore, there are $1-0=1$ zero of $p$ inside $(0,1]$. </p>
"
"2388986","2388995","<p>You want to find $P(X = x, Y = y)$, where $x,y \in \{R, A, G\}$. (I'm using ""$A$"" for ""amber"" so as to not confuse ""$Y$"" for ""yellow"" or the random variable.) We can use Bayes' theorem to say
$$ P(X = x, Y = y) = P(Y = y \mid X = x)P(X = x). $$</p>

<p>Now, you know what the distribution of $X$ is. So now you only need to calculate the distribution of $Y$ <em>given</em> $X$. For example, suppose that $X = R$. You then know that $Y = R$ with probability $0.95$, $Y = A$ with probability $0.02$ and $Y = G$ with probability $0.03$. We can write this as follows:
$$ \begin{align*}
P(Y = R \mid X = R) &amp;= 0.95; \\
P(Y = A \mid X = R) &amp;= 0.02; \\
P(Y = G \mid X = R) &amp;= 0.03.
\end{align*} $$</p>

<p>Hopefully this is enough so that you can now complete the cases when $X = A$ or $X = G$.</p>
"
"2388994","2389022","<p>Let $U$ be the uncoutable set of all sets of positive integers.</p>

<p>First interpretation: 
As far as I understand, you transform any finite set into $1$ or $0$ depending on the parity of its biggest element. How you transform infinite sets, such as the set of all even numbers $2\mathbb{N}$, for instance, remains unclear.</p>

<p>Second interpretation:
This time you map $U$ onto $V=\{0,1\}^\mathbb{N}$.
$V$ is uncountable.</p>
"
"2389003","2389008","<p>No. Take $A=[0,1/3)$ : we have $A\notin \mathcal{F}_n$ for all $n$, but $A\in \mathcal{B}([0,1])$.</p>

<p>A true statement is :
$$\mathcal{B}([0,1])=\sigma\bigg(\bigcup_{n\in\mathbb{N}} \left\{[k2^{-n},(k+1)2^{-n}) \mid k=0,1,\dotsc,2^n-1\right\}\bigg).$$</p>
"
"2389006","2389698","<p>No. If the Lie algebra (i.e. the first construction) is denoted $\mathfrak{g}$, then the second one will be isomorphic to the associated Carnot-graded Lie algebra $\mathrm{Car}(\mathfrak{g})$. </p>

<p>Since there exist finite-dimensional nilpotent real Lie algebras (in dimension $\ge 5$) $\mathfrak{g}$ that are not isomorphic to their associated Carnot-graded Lie algebra, and since any finite-dimensional nilpotent real Lie algebra is isomorphic to a subalgebra of $\mathfrak{ut}(n,\mathbf{R})$ for some $n$, you get a negative answer to your question (at least for $n$ large enough, probably for all $n\ge 4$ actually). </p>
"
"2389012","2390452","<p>If $a_nb-ba_n\to0$ for all $b\in \mathcal O_2$, then write (using $s_1s_1^*+s_2s_2^*=I$)
$$
\lambda(a_n)-a_n=(s_1a_n-a_ns_1)s_1^*+(s_2a_n-a_ns_2)s_2^*\to0.
$$
Conversely, if $\lambda(a_n)-a_n\to0$, this is 
$$
s_1a_ns_1^*+s_2a_ns_2^*-a_n\to0.
$$
Using that $s_2^*s_1=0$ we get by multiplying by $s_1$ on the right (recall that $s_1,s_2$ are isometries) that $$s_1a_n-a_ns_1\to0.$$Multiplying by $s_1^*$ on the left, $$a_ns_1^*-s_1^*a_n\to0.$$ And similarly with $s_2$. Now we can use this to show that $$a_nb-ba_n\to0$$ for any $b$ that is a polynomial in $s_1,s_1^*,s_2,s_2^*$. And now we can pass to any $b$ by a density argument. </p>
"
"2389019","2389081","<p>Tsemo has sketched a proof (using CW-structures), that there is no way $D^2 \coprod_f S^1$ is a torus.  If you change the problematic statement to a ""wedge of $2h$ circles..."", then I think the question makes sense.</p>

<p>That said attaching a $D^2$ to an $S^1$ need not give a disc - there is no reason the attaching map has to be the identity. It could, for example, be given by the map $z\mapsto z^2$ (interpreting $z\in \mathbb{C}$). Then $D^2\coprod_f S^1$ is homeomorphic to $\mathbb{R}P^2$.</p>
"
"2389039","2389216","<p>There are a few issues you have in your proof. One is that you are very cavalier with regards to when things are zero or not. For instance, in $1)$ you should say that at least one of the $\alpha_i$ is not zero. Similarly at least one of the $\beta_i$ must be non-zero.</p>

<p>One glaring problem is your final line when you say ""If $\alpha_i = \frac{\beta_i}{b_i}$"". This is a huge problem because $\alpha_i$ and $\beta_i$ are scalars and $b_i$ is a vector! </p>

<hr>

<p>I will present a very straightforward proof that avoids all those pesky subscripts. Let $\tilde{B}$ denote the matrix whose columns are the $b_i$. You can reinterpret the second statement in your preliminaries as $$ w = \tilde{B}[w]_B.$$</p>

<p>Suppose we have scalars $\gamma_1,\dots,\gamma_k$ such that $\gamma_1[w_1]_B + \dots + \gamma_k[w_k]_B = 0$. Then:</p>

<p>\begin{align*}
0 &amp;= \tilde{B}0 \\
&amp;=\tilde{B}\left(\gamma_1[w_1]_B + \dots + \gamma_k[w_k]_B\right)\\
&amp;=\gamma_1(\tilde{B}[w_1]_B) + \dots + \gamma_k(\tilde{B}[w_k]_B) \\
&amp;=\gamma_1w_1 + \dots \gamma_k w_k.
\end{align*}  </p>

<p>Because $w_1,\dots,w_k$ are linearly independent, we must have that $\gamma_1, \dots, \gamma_k = 0$. Therefore $[w_1]_B,\dots,[w_k]_B$ are linearly independent.</p>

<hr>

<p>As a side note: We didn't even need to use the fact that $\tilde{B}$ is invertible. Any linear transformation will map dependencies to dependencies. So in this case, if there was a linear dependence among the $[w_i]_B$'s, that would map to a linear dependence among the $w_i$'s which we know does not exist.</p>
"
"2389045","2389069","<p>Recently I researched many complex analysis texts for a seminar and I have to say the following: For a first-time reader, both Ahlfors and Stein-Shakarchi would be too much (except if you are a graduate student and have a good feeling of analysis). Also, Stein's book has a strange sense of ""proving"" for the theorems (in general, that 4 books of Stein are written in a flowing style aiming for graduate students who want to proceed fast), and also there are some things about the structure of that book (the winding number is defined very late, the Jordan curves are not defined at all and the undefined term ""toy curve"" is used instead). I liked these books but I don't recommend them to beginners. </p>

<p>You can go along the <a href=""http://rads.stackoverflow.com/amzn/click/1441972870"" rel=""nofollow noreferrer"">Bak-Newman book</a> or <a href=""http://math.unice.fr/~nivoche/pdf/Brown-Churchill-Complex%20Variables%20and%20Application%208th%20edition.pdf"" rel=""nofollow noreferrer"">Brown-Churchill</a> (which surprisingly is available for free(?)) and has a <a href=""http://www.math.s.chiba-u.ac.jp/~yasuda/ippansug/CVA-sol-7.pdf"" rel=""nofollow noreferrer"">solution manual</a>. The latter book is more intended for physics-mechanics students so many topological aspects are omitted from the text. Also <a href=""http://people.math.sc.edu/girardi/m7034/book/VisualComplexAnalysis-Needham.pdf"" rel=""nofollow noreferrer"">Needham's book</a> is terrific, the only problem being its huge size. The last text is <a href=""https://nnquan.files.wordpress.com/2013/01/giao-trinh-ham-phuc.pdf"" rel=""nofollow noreferrer"">An introduction to complex analysis</a> by Kanishka Perera, Ravi Agarwal, and Sandra Pinelas which is a well-organized readable text with solutions included, aiming for students beginning complex analysis.</p>

<p>Have fun!</p>
"
"2389052","2389542","<p>You need  to define the function in the complex plane, e.g. through:
$$ f(x) = \exp \left( x \log\left( 1 + \frac{1}{x}\right) \right) .$$
Here, the tricky part is the log for which you need to specify a branch cut.
For example, you may say that $\log(z)$ is defined for $z\in {\Bbb C}\setminus {\Bbb R}_-$, so that $f$ becomes defined for $x\notin [-1,0]$. You may choose other branch cuts but for $f$ you will always get some branch cut between $0$ and $-1$. Now $f(x)$ tends to $e$ as $|x|\rightarrow \infty$ so indeed $f$ does have a Laurent expansion which you may find by expansion of $\log$:
$$ f(x) = \exp ( x \sum_{k\geq 1} \frac{(-1)^{k-1}x^{-k}}{k} ) = 
e \cdot \exp \sum_{p\geq 1} \frac{(-x)^{-p}}{p+1} = e \cdot  
\exp \left(-\frac{1}{2x} + \frac{1}{3x^2} \pm ...\right) $$
which you may then expand further... It converges for $|x|&gt;1$.</p>
"
"2389054","2389103","<p>Well, with the exercises of this kind, it seems to be worth to have at least an idea of what precisely is happening, otherwise you are lost in nonsensical symbolic manipulations. So what you have is a function $g$ of two independent variables $x$ and $y,$ let's write it like $z = g(x,y).$ You can identify it with its graph actually as a subset in $\mathbb{R}^3.$ What you are interested most is,loosely speaking, whether there is a function, say $h,$ such that if you perform a change of variables
\begin{eqnarray*}
x &amp; = &amp; a(r,\phi,\psi)\\
y &amp; = &amp; b(r,\phi,\psi)\\
z &amp; = &amp; c(r,\phi,\psi),
\end{eqnarray*}
the graph of $g$ transforms into the graph of $h$ parametrized by two of the three coordinates $r,\phi,\psi.$ The theorem on implicit functions (TIF) tells you that this can be done everywhere the Jacobian matrix $\frac{D(a,b,c)}{D(r,\phi,\psi)}$ is nonsingular.</p>

<p>Now if you put $a(r,\phi,\psi) = r \cos{\phi},$ $b(r,\phi,\psi) = r \sin{\phi}$ and $c(r,\phi,\psi) = \psi$ (this is the so called change of independent variables because the dependent variable transforms like identity), you can figure out (directly or by checking the determinant $\frac{D(a,b,c)}{D(\psi, x, y)}$ and using the TIF) that
\begin{eqnarray}
x &amp; = &amp; r \cos{\phi} \\
y &amp; = &amp; r \sin{\phi}\\
g(x,y) &amp; = &amp; h(r,\phi) (= g(r \cos{\phi}, r \sin{\phi}))
\end{eqnarray}
for certain function $h$ of the independent variables $r,\phi$ (it is better to think like there are NO dependent and independent variables and to view the equations above as implicit relationship between $x,y,r,\phi,\psi.$) Now you want the $g_x$ and $g_y$ in terms of $h_r$ and $h_{\phi}.$ Note that
\begin{equation}
g_x = h_r r_x + h_{\phi} \phi_x
\end{equation}
(yes, in general $r$ and $\phi$ also depend on $x$ and $y,$ a consequence of TIF again) and that
\begin{equation}
g_y = h_r r_y + h_{\phi} \phi_y
\end{equation}</p>

<p>Differentiating the first two of the three equations above with respect to $x$ you have
\begin{eqnarray}
1 &amp; = &amp; r_x \cos{\phi} - (r \sin{\phi}) \cdot \phi_x \\
0 &amp; = &amp; r_x \sin{\phi} + (r \cos{\phi}) \cdot \phi_x
\end{eqnarray}
and eliminating you obtain
\begin{equation}
r_x = \cos{\phi}, \quad \phi_x = - \frac{\sin{\phi}}{r},
\end{equation}
so that in fact
\begin{equation}
g_x = h_r \cos{\phi} - \frac{1}{r} h_{\phi} \sin{\phi}.
\end{equation}
Similarly differentiating the same two equations of the three above with respect to $y$ gives
\begin{eqnarray}
0 &amp; = &amp; r_y \cos{\phi} - (r \sin{\phi}) \cdot \phi_y \\
1 &amp; = &amp; r_y \sin{\phi} + (r \cos{\phi}) \cdot \phi_y
\end{eqnarray}
and eliminating you end up with
\begin{equation}
r_y = \sin{\phi}, \quad \phi_y = \frac{\cos{\phi}}{r},
\end{equation}
so that
\begin{equation}
g_y = h_r \sin{\phi} + \frac{1}{r} h_{\phi} \cos{\phi}.
\end{equation}
Now put everything together and here we go: your equation is equivalent to $r h_r = r$ in the new coordinates.</p>
"
"2389057","2389257","<p>One might recall (from the proof of <a href=""https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion"" rel=""nofollow noreferrer"">Kepler's first law</a>, for instance) that
$$ \rho(\theta) = \frac{\frac{b^2}{a}}{1+\frac{c}{a}\cos\theta} $$
is the polar equation of an ellipse (with semi-axis $b&lt;a$ and $c=\sqrt{a^2-b^2}$) with respect to a focus. The associated cartesian equation clearly is $\frac{(x+c)^2}{a^2}+\frac{y^2}{b^2}=1$. In a similar way the cartesian equation associated to 
$$ \rho(\theta) = \frac{\frac{b^2}{a}}{1-\frac{c}{a}\cos\theta} $$
clearly is $\frac{(x-c)^2}{a^2}+\frac{y^2}{b^2}$. So, if the polar equation is $\rho(\theta)=\frac{A}{1-\frac{1}{\sqrt{2}}\cos\theta}$, we have $a^2=2b^2$ and $\frac{b^2}{a}=A$ and the cartesian equation is given by
$$ \frac{\left(x-\sqrt{2}A\right)^2}{(2A)^2}+\frac{y^2}{(\sqrt{2} A)^2}=1. $$</p>
"
"2389060","2389080","<p>The notation $X\sim \mathcal{D}$ means ""The random variable $X$ has law (is distributed according to) the <em>probability distribution</em> $\mathcal{D}$.</p>

<p>$\mathcal{N}(0,1)$ is <a href=""https://en.wikipedia.org/wiki/Normal_distribution#Notation"" rel=""nofollow noreferrer"">standard notation</a> for the standard normal distribution over $\mathbb{R}$, i.e. the continuous real-valued probability distribution with probability density function $f\colon x\mapsto \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$.</p>

<p>Writing $X\sim\mathcal{N}(0,1)$ thus makes sense. Writing $\sigma\mathcal{N}(0,1)=\mathcal{N}(0,\sigma^2)$, for $\sigma&gt;0$, also makes sense (as a common notation), since $\mathcal{N}(0,\sigma^2)$ is the Gaussian/Normal distribution with mean zero and variance $\sigma^2$. <em>(Note that this is again a notation: you shouldn't think of it as the usual multiplication, more as a common shortcut for ""$\sigma\mathcal{N}(0,1)$ is the distribution of $\sigma X$, when $X\sim\mathcal{N}(0,1)$."" The key is that the notation is common and quite standard.)</em></p>

<p>Writing $x\sim(\mathcal{N}(0,1))^2$, however, does not (unless you want to use it as an <em>ad hoc</em> notation, in which case you need to <em>define</em> the new operation on probability distributions $\mathcal{D}\mapsto\mathcal{D}^2$ which maps a probability distribution to another legit probability distribution). What is $(\mathcal{N}(0,1))^2$? Is it a probability distribution, as it should? </p>

<hr>

<p>In your specific case of the square of a Gaussian random variable, anyway, I'd recommend using the standard notation $X\sim\chi^2$, since by definition this is what the <a href=""https://en.wikipedia.org/wiki/Chi-squared_distribution"" rel=""nofollow noreferrer"">chi-squared distribution</a> is...</p>
"
"2389083","2389089","<p>Your mistake is in finding the general solution of the ODE. Your particular solution is not a solution at all. Instead, a particular solution to $y'=ay + b$ is $-\frac{b}{a}$. Thus $\hat{u}=C e^{i \xi t} - \frac{1}{i\xi} \widehat{\sin(x)}$.</p>

<p>To avoid this pitfall when solving linear differential equations, put all of the terms involving the dependent variable on one side of the equation. The terms on the other side can be treated through superposition, but the terms involving the dependent variable cannot.</p>

<p>That said this particular problem is easier to treat through the method of characteristics. For each fixed $x$, if $v(t;x)=u(t,x-t)$ then $\frac{dv}{dt}(t;x)=\sin(x-t)$ and $v(0;x)=u(0,x)=\sin(x)$, which is easy to solve. (The semicolon is used to denote that the arguments after it are merely parameters, not proper variables.) Then $u(t,x)=v(t;x+t)$.</p>
"
"2389084","2397836","<p>Suppose that the equation is<br>
$$ - (p(x) y'(x))' + q(x) y(x) = \lambda w(x) y(x)$$</p>

<p>defined on $[0,\infty)$.
First of all, at an LP endpoint, a boundary condition is neither
required nor allowed (Zettl [1, Sect. 7.1]).  Second, the classification
of an LP endpoint at $\infty$ depends only on the asymptotic behavior
of $1/p, q, w$ (Zettl [1, Sect. 7.2]), so the LP classification itself
is difficult to escape.</p>

<p>However, a viable approach is to try to prove that the spectrum is
discrete as is. A simple method is presented by Titchmarsh [2, Ch. V]:
First, apply the Liouville transformation (if possible; see, e.g.,
Everitt [3, 4]) to transform the equation to the form $$ - y'' + Q y =
\lambda y .$$ If $Q(x) \to \infty$ when $x \to \infty$, then
the $\infty$ endpoint is LP and the spectrum is discrete.</p>

<p>When the ultimate purpose is to find a series expansion, it may be
desirable not only to prove that the spectrum is
discrete, but also bounded from below. This has been called the BD
property, and is discussed briefly in Zettl [1, Sect. 10.13].
Somewhat surprisingly, there is a simple sharp condition for the BD
property discovered by Molchanov. The Liouville-transformed system 
(i.e., $p=w=1$) has
the BD property if and only if, for some $h, 0 &lt; h &lt; \infty$,
$$\int_t^{t+h}Q(x)\,dx \to \infty$$
when $t \to \infty$.</p>

<p>[1] Zettl, A.: Sturm-Liouville Theory. AMS. 2005.</p>

<p>[2] Titchmarsh, E.C.: Eigenfunction Expansions Associated with Second-order
Differential Equations. Part I. 2nd ed. Oxford at the Clarendon press. 1962.</p>

<p>[3] Everitt, W.N.: Charles Sturm and the Development of Sturm-Liouville
Theory in the Years 1900-1950.
In Amrein et al. (eds.): Sturm-Liouville Theory: Past and Present.
BirkhÃ¤user Verlag. 2005.</p>

<p>[4] Everitt, W.N.: A Catalogue of Sturm-Liouville Differential Equations. In
Amrein et al. (eds.): Sturm-Liouville Theory: Past and Present.
BirkhÃ¤user Verlag. 2005.</p>
"
"2389098","2389402","<p>For $k\geq 2$ here is a way to get them. 
 One class of covering spaces for $S^1\vee S^1$ comes from covering spaces of one of the $S^1$'s and attaching copies of the other $S^1$ at each lifted base point.  That is, a space which is an $S^1$ along with $n\geq 1$ copies of $S^1$ wedged to it at different points.  This space is homotopy equivalent to $n+1$ wedged $S^1$'s by way of collapsing $1$-cells.</p>

<p>For $k=0$, the universal cover of $S^1\vee S^1$ is contractible, so it is homotopy equivalent to the wedge sum of zero $S^1$'s.</p>

<p>For $k=1$, you can take the covering space associated with the subgroup generated by one of the generators.  This looks like a circle with two ""leaves"" of the Cayley graph for $F_2$ attached.  Each leaf is contractible to the basepoint.</p>
"
"2389113","2389196","<p>Indeed.</p>

<p>Because $H\mid XY\sim\mathcal {Bin}(XY, 1/2)$, therefore $\mathsf E(H\mid XY) = \tfrac 12XY$.  </p>

<p>Hence by the Law of Iterated Expextation: $\mathsf E(HXY)~{=\mathsf E(\mathsf E(H\mid XY)XY)\\ = \tfrac 12\mathsf E(X^2Y^2)\\ \ddots}$ </p>
"
"2389114","2389326","<p>Thank you very much to user26857 who explained in a comment that the answer is negative for the following reason:</p>

<p>Assume by contradiction 
$$
xy_1\in\langle x^2y_1,x^2y_2-xy_1,\dots,x^ny_n-xy_1\rangle.
$$ 
Dividing by $x$ we get 
$$
y_1\in\langle xy_1,xy_2-y_1,x^2y_3-y_1,\dots,x^{n-2}y_{n-1}-y_1,x^{n-1}y_n-y_1\rangle.
$$ 
Setting $y_i:=x^{n-i}y_n$ for $1\le i\le n-1$ we get $x^{n-1}y_n\in\langle x^ny_n\rangle$, which is false.</p>
"
"2389115","2389145","<p>A set is open when no boundary points of it belong to it.</p>

<p>A set is closed when all boundary points of it belong to it.</p>

<p>These two conditions can hold true together for a set only when there are no boundary points for it.</p>

<p>The whole space is one such set.</p>

<p>In symbols:</p>

<p>Let's call $\operatorname{Fr}(X)$ the set of the boundary points for the set $X$.</p>

<p>\begin{equation}
\begin{aligned}
&amp;F \textrm{ is closed}\iff\operatorname{Fr}(F)\subset F\\
&amp;G \textrm{ is open}\iff\operatorname{Fr}(G)\cap G=\emptyset\\
&amp;H \textrm{ is clopen}\iff\operatorname{Fr}(H)\subset H\land\operatorname{Fr}(H)\cap H=\emptyset\iff \operatorname{Fr}(H)=\emptyset\\
\end{aligned}
\end{equation}</p>
"
"2389120","2389142","<p>$$\lim_{0^-}f (x)=\lim_{0^-}(2+x)=2$$</p>

<p>$$\lim_{0^+}f (x)=\lim_{0^+}x^3=0$$</p>

<p>$$\implies \lim_{0^-}f (x)\ne \lim_{0^+}f (x) $$</p>

<blockquote>
  <p>and</p>
</blockquote>

<p>$$\lim_{1^-}f (x)=\lim_{1^-}x^3=1$$
$$\lim_{1^+}f (x)=\lim_{1^+}\sin (\frac {\pi x}{2})=1$$
$$\implies \lim_{1^-}f (x)=\lim_{1^+}f (x) $$</p>

<p>Can you conclude.</p>
"
"2389122","2389128","<p>$\Phi[t/x]$ means to take the formula $\Phi$, and replace every occurrence of the variable $x$ with the term $t$. For example, if $\Phi$ is $x^2 \geq 0$, and $t$ is $y-3$, then $\Phi[t/x]$ is $(y-3)^2 \geq 0$.</p>
"
"2389133","2389224","<p>$$
\mathbb{E}(x) = \int_{-\infty}^\infty \frac{1}{4s}\mathrm{sech}^2\left(\frac{x-\mu}{2s}\right)
$$
firstly lets change variables $t = \frac{x-\mu}{2s}\to dt = \frac{1}{2s}dx$
so we have
$$
\int_{-\infty}^\infty x\frac{1}{4s}\mathrm{sech}^2\left(\frac{x-\mu}{2s}\right) = \frac{1}{2}\int_{-\infty}^\infty \left(2st +\mu\right)\mathrm{sech}^2t \,dt
$$ 
we can use the relation
$$
 \frac{d}{dx}\tanh x = \mathrm{sech}^2 x 
$$
this leads to
$$
\frac{1}{2}\int_{-\infty}^\infty \left(2st +\mu\right)\mathrm{sech}^2t \,dt = \frac{1}{2}\int_{-\infty}^\infty \left(2st +\mu\right)\frac{d}{dt}\tanh t\,dt
$$
so we compute
$$
\int_{-\infty}^\infty t\frac{d}{dt}\tanh t = \left[t\tanh (t)\right]_{-\infty}^\infty - \int_{-\infty}^\infty \tanh (t) \,dt
$$
Since we have
$$
\tanh(-x) = -\tanh(x)
$$
i.e odd function with another odd function
$$
-t\tanh(-t) = -t\cdot -\tanh(t) = t\tanh(t)
$$
we have an even function which for symmetric bounds vanishes the term in the brackets.
The integral left is
$$
\int_{-\infty}^\infty \tanh (t) dt
$$
we can use a similar argument
$$
\int_{-\infty}^0 \tanh (t) dt + \int_{0}^\infty\tanh (t) dt
$$
change of variables for the first integral
$$
\int_{\infty}^0 \tanh(-t) (-dt) = -\int_{\infty}^0 \tanh(-t)dt = \int_0^\infty \tanh(-t)dt = -\int_0^\infty \tanh(t)dt 
$$
so 
$$
\int_{-\infty}^\infty \tanh (t) dt = -\int_0^\infty \tanh(t)dt  + \int_{0}^\infty\tanh (t) dt = 0
$$
this leaves us with
$$
\frac{1}{2}\int_{-\infty}^\infty \left(2st +\mu\right)\frac{d}{dt}\tanh t\,dt = s\int_{-\infty}^\infty t\frac{d}{dt}\tanh t\,dt + \frac{1}{2}\mu \int_{-\infty}^\infty \frac{d}{dt}\tanh t\,dt = \frac{1}{2}\mu \left[\tanh(t)\right]_{-\infty}^\infty
$$
using the similar argument again
$$
\frac{1}{2}\mu \left[\tanh(\infty) - \tanh(-\infty)\right] = \frac{1}{2}\mu\left[1 -(-1)\right] = \mu.
$$</p>
"
"2389143","2389156","<p>An effort for an informal explanation: The story is the following: you have a random variable $X$ that is distributed according to some probability density function $f_X(x)$. But you do not know some parameter of this function $f$. For example $f_X(x)=ax+b$ but you do not know $a$. So, right $f(x; a)$ instead (ok?).</p>

<p>Now, you start taking observations from $X$, formally you take a sample of independent random variables $X_1,X_2,\dots, X_n$ and you observe them, in order to get some information about $a$ (which you do not know, remember?). What do I mean by that: $X_1$ denotes the result (which you do not know yet) of your first sample. So, just observe it and write $X_1=x_1$ (where $x_1$ is what you saw, it is a number). To see the difference between $X_1$ and $x_1$: let's say, you forgot what you saw (you wrote it somewhere and you lost the note), then you repeat the observation and you get $X_1=x_1'$. Got it? </p>

<p>So, you take $n$ observations. Now, ask: What is the likelihood that I will observe $(x_1,x_2,\dots,x_n$. Answer: these observations are independent and each one comes from the distribution $f$, so (by the multiplicative law
$$f(x_1,x_2,\dots x_n ; a)=f(x_1; a)f(x_2;a)\dots f(x_n;a)=\prod_{i=1}^nf(x_i;a)$$ and that is your likelihood function, or informally ""the likelihood that you observed what you observed"". Now, maximize this with respect to $a$. That is were your theory takes over.</p>
"
"2389147","2389188","<p>Consider a number of the form $8n+3$. Its trajectory goes: $8n+3, 24n+10, 12n+5, 36n+16, 18n+8, 9n+4$. Thus, the stopping time for $9n+4$ is $5$ less than the stopping time for $8n+3$. The number $9n+4$ has approximately the ratio $9/8$ to the number $8n+3$, so its logarithm is greater by approximately $\log 9/8$, and its stopping time is less by $5$. That's the regularity along downward sloping lines that you're seeing.</p>

<p>Additionally, $f^5(8n+1)=9n+2$, and $f^5(8n+6)=9n+8$</p>

<p>Next, consider a number of the form $8n+1$. Its trajectory goes: $8n+1, 24n+4, 12n+2, 6n+1$. The ratio of $8n+1$ to $6n+1$ is about $4/3$. Thus, in those cases, when the logarithm decreases by about $\log 4/3$, the stopping time decreases by $3$. That's the regularity along the lines with positive slope.</p>

<p>Additionally, $f^3(8n+2)=6n+2$, $f^3(8n+4)=6n+4$, $f^3(8n+5)=6n+4$, and $f^3(8n+6)=6n+5$.</p>

<p>Note that two of those are the same: thus, $3$ steps upstream of every $6n+4$, we find an $8n+4$ <em>and</em> an $8n+5$.</p>

<hr>

<p>These two regularities intermesh because of the Chinese Remainder Theorem. In many cases, one of those $6n+k$ numbers is also an $8n+j$ number, so a step down-and-to-the-left is followed by another step down, to the left and/or to the right.</p>

<hr>

<p>There are still horizontal runs of points to explain, clustered around intersections in the lattice. First, as noted above, $f^3(8n+4)=f^3(8n+5)=6n+4$. Their trajectories converge after $3$ steps, so they have the same stopping time.</p>

<p>Similarly, $16n+8$ and $16n+10$ have the same stopping time, with $f^4(16n+8)=f^4(16n+10)$.</p>

<p>Exploring trajectories modulo various powers of $2$, you can keep finding patterns like these.</p>
"
"2389151","2389165","<p>Your confusion probably lies in the difference between</p>

<p>""<em>a function of <strong>two</strong> variables</em>""</p>

<p>and the fact that the <em>equation</em> of a line in the plane contains the two cartesian coordinates $x$ and $y$.</p>

<p>Any line in the plane has a cartesian equation of the form $ax+by+c=0$ which can be solved for $y$ if the line is non-vertical. In that  case, it is also the graph of ""<em>a function of <strong>one</strong> variable</em>"" $y=f(x)$ with $f(x)$ of the form $mx+q$ in the case of a line.</p>

<p>Note that this is not the same as <em>a function of <strong>two</strong> variables</em>: $z=f(x,y)$.</p>
"
"2389153","2389466","<p>An inequality which can be useful in situations such as this one is derived from the Taylor expansion of the natural exponential function about the point $0$. We have $$ 1 + x \leq \exp(x),$$
for all $x \ge 0$. </p>

<p>Using this inequality, we can estimate
$$ \left( 1 + \frac{h}{N}\right)^N \leq \left( \exp\left(\frac{h}{N}\right) \right)^N = \exp(h).$$</p>

<p>This will certainly allow you to find an $N_0$, such that
$$ \frac{1}{4NL} \left [ \left( 1 + \frac{h}{N}\right)^N - 1 \right] \leq \tau,$$
for all $N \ge N_0$. However, it will not give you the <em>smallest</em> value of $N_0$. </p>

<p>Normally, I would just examine my choices, i.e. $\{1,2,\dotsc,N_0\}$ using either brute force, i.e. one at a time starting from the last, or using bisection.</p>
"
"2389154","2389193","<p>Essentially, you want to solve a differential equation of the form $$\frac{dy}{dx} = \Phi(y).$$ The usual separation of variables gives $$x = \int \frac{dy}{\Phi(y)},$$
so you obtain an explicit solution whenever you are able to calculate the indefinite integral $I(y) = \int \frac{dy}{\Phi(y)}$ and to  invert (at least locally) the expression $x=I(y)$. </p>

<p>Of course, this is not always possible in terms of elementary functions. For instance, take $\Phi(y) = e^{y^2}$, so that $I(y)$ becomes the Gaussian integral $\int e^{-y^2} dy$.</p>
"
"2389160","2389175","<p>If, around $0$,$$f(z)=a_0+a_1z+a_2z^2+a_3z^3+\cdots,$$then$$a_0+a_1z+a_2z^2+a_3z^3+\cdots=a_0+z+a_1z^2+a_2z^4+a_3z^6+\cdots$$From this, you deduce nothing concerning $a_0(=f(0))$, but now you know that $a_1=1$. And you also know that $a_2=a_1=1$. And that $a_3=0$.  And that $a_4=a_2=1$. And so on. The general rule is: $a_n=1$ if $n$ is a power of $2$ and $0$ otherwise. That is$$f(z)=a_0+z+z^2+z^4+z^8+\cdots$$and this series converges if and only if $|z|&lt;1$.</p>
"
"2389162","2389449","<p>There are maps that are conformal at $\infty$. Yours is not. In order to check for conformality introduce a new complex coordinate $\zeta$ via $z={1\over \zeta}$ in the neighborhood of $z=\infty$ as well as in the neighborhood of the assumed value $w=\infty$. In your example this would mean looking at the function
$$g(\zeta):={1\over f(1/\zeta)}=\zeta\cdot e^{-1/\zeta}\qquad(\zeta\ne0)$$
and $g(0):=0$ in the neighborhood of $\zeta=0$.</p>

<p>On the other hand, the function $h(z):=z$ of course is conformal at $z=\infty$, as are Moebius transformations 
$$T(z):={az+b\over cz +d},\qquad ad-bc\ne0\ .$$</p>
"
"2389166","2389192","<p>It suffices to show that the sequence is Cauchy.</p>

<p>Let $a:=f_1$ and $b:=f_2$. Then show by induction that
$$|f_n-f_{n-1}|=\frac{1}{2^n}|a-b|.$$
Use this to prove that the sequence is Cauchy; i.e., that for every $\varepsilon&gt;0$ there is $N\in\mathbb{N}$ such that
$$|f_{n+k}-f_n|&lt;\varepsilon$$
whenever $n\geq N$ and $k\in\mathbb{N}$.</p>
"
"2389181","2389213","<p>This problem is really asking whether or not you understand the statement of the Fundamental Theorem of Calculus. Note that the Fundamental Theorem of Calculus is often phrased in two parts (and not all sources agree on which is ""part 1"" and which is ""part 2"").</p>

<p>Part 1 is often stated: Suppose $f(x)$ is an integrable function. Then
$$ \frac{d}{dx} \int_0^x f(t) dt = f(x).$$
Part 2 is often stated: Suppose $f(x)$ is an integrable function and that $F(x)$ is a function such that $F'(x) = f(x)$. Then
$$ \int_a^b f(x) dx = F(b) - F(a).$$</p>

<hr>

<p>I will also comment on some confusion in the problem. The $dt$ within the integrand serves to indicate what variable is being integrated. In this case, it's the $t$ variable. But the integral is a function of $x$. That is, there is a function in your question that we might call $g(x)$, given by
$$ g(x) = \int_2^x \frac{dt}{t + \ln t}.$$
This function has the interpretation ""give the area under the graph of the function $1/(t + \ln t)$ from $2$ to $x$"". So the $x$ and the $t$ are playing different roles: $t$ is what is being integrated, while $x$ is the variable of the overall function.</p>

<p>It would be very confusing to write
$$ \frac{d}{d{\color{#dd1111} x}} \int_2^{\color{#dd1111}x} \frac{d{\color{#5555ff} x}}{{\color{#5555ff}x} + \ln {\color{#5555ff} x}},$$
as the red $\color{#dd1111}x$ and the blue $\color{#5555ff}x$ have different meanings.</p>

<p>If it is helpful, I wrote a <a href=""http://davidlowryduda.com/an-intuitive-introduction-to-calculus/"" rel=""nofollow noreferrer"">short note on introductory calculus</a> for my students a few years ago.</p>
"
"2389182","2389226","<p>You can show that the set $[\kappa]^n$ of $n$-subsets of $\kappa$ has cardinality $\kappa$; that is, $|[\kappa]^n|=\kappa$. (Put your mouse on the box below if you have no idea how to prove this.)</p>

<blockquote class=""spoiler"">
  <p> It uses the Cantor-Bernstein theorem. You can easily see that there are at least $\kappa$ $n$-subsets of $\kappa$, for example you can consider $\{0,1,\cdots, n-2, \alpha\}$ for $n-1\le \alpha &lt; \kappa$. That is, we have $|[\kappa]^n|\ge \kappa$.
 
 Conversely, there is an obvious surjection from $\kappa^n$, a set of all sequences over $\kappa$ of length $n$, to $[\kappa]^n$. Therefore, by considering the right inverse of this onto function, which is one-to-one, we have $|[\kappa]^n|\le \kappa^n = \kappa$.</p>
</blockquote>

<p>We know that the set $[\kappa]^{&lt;\omega}$ of all finite subsets  of $\kappa$ is $\bigcup_{n\in\mathbb{N}} [\kappa]^n$. Since $[\kappa]^{&lt;\omega}$ has at least $\kappa$ elements (namely $\{\alpha\}$, for each $\alpha&lt;\kappa$) we have $|[\kappa]^{&lt;\omega}|\ge \kappa$. On the other hand,
$$|[\kappa]^{&lt;\omega}|= \left|\bigcup_{n\in\mathbb{N}} [\kappa]^n \right| \le \sum_n |[\kappa]^n|\le \sum_n\kappa = \kappa\cdot\aleph_0 = \kappa$$</p>

<p>so $|[\kappa]^{&lt;\omega}| = \kappa$.</p>
"
"2389183","2389194","<p>The square of the distance of $B$ to $(2-t,1+47,2+t)$ is $D(t)=18t^2-36t+27$. Since $D'(t)=36t-36$, the minimum is attained when $t=1$. Since $D(1)=9$, the distance that you're after is $3$.</p>
"
"2389184","2389506","<p>The statement is correct.  ""Polynomials"" are allowed to refer to individual elements of your algebra as constants, so the unary polynomials for a vector space are not just $ax$ for $a$ a scalar but also $ax+v$ where $a$ is a scalar and $v$ is an element of the vector space.  Your example $C$ does not satisfy $\tau(C)\cap C=\emptyset$ or $\tau(C)\subseteq C$ for all polynomials $\tau$ of this form.</p>
"
"2389186","2389223","<p><strong>Hint</strong> without complex analysis.</p>

<p>Using Feynman's trick of differentiating under the integral. $$I(a)=\int_0^{\pi} \frac{dt}{(5+a\cos t)^2}=-\frac d{da} \int_0^{\pi} \frac{dt}{5+a\cos t}$$ could be an interesting solution.</p>
"
"2389187","2390301","<p>\begin{align}
1+w^h+w^{2h}+\dots + w^{(n-1)h}&amp;=\sum^{n-1}_{j=0}(w^h)^j\\&amp;=\frac{1-(w^h)^n}{1-w^h}\\
&amp;=\frac{1-(w^n)^h}{1-w^h}\\
&amp;=\frac{1-1^h}{1-w^h}=0
\end{align}</p>
"
"2389208","2389212","<p>Any root of a polynomial equation with algebraic coefficients is still algebraic. In other words, the field of algebraic numbers is algebraically closed. Therefore, $\mathbb{A}_1=\mathbb{A}_2=\mathbb{A}_3=\cdots$</p>
"
"2389215","2389248","<p>You probably know this factorisation</p>

<p>$$x^n-1 = (x-1)(x^{n-1} + x^{n-2} + ... + x + 1)$$</p>

<p>There is also a homogeneous version, which you can get by substituting $x=a/b$ and multiplying through by $b^n$:</p>

<p>$$a^n-b^n = (a-b)(a^{n-1} + a^{n-2}b + a^{n-3}b^2 + ... + b^{n-1})$$</p>

<p>In this case we use $a=d$ and $b=-(p-d)$, as well as $n=k$.</p>
"
"2389221","2389245","<p>Define $u=f(t)$ so $du=f' (t) dt$ and $dt=\frac{du}{f'f^{-1}(u)}$. The integrals you've requested are the $p\in\{ 0,\,1\}$ cases of $\int_0^1 \dfrac{(f^{-1}(u)))^p u^k(1-u)^{n-k}}{f'f^{-1}(u)}du$. Given how variable the function $f'f^{-1}$ can be, I doubt some more interesting expression is obtainable in general.</p>
"
"2389222","2389228","<p>In general you can use the <a href=""https://en.wikipedia.org/wiki/Havel%E2%80%93Hakimi_algorithm"" rel=""nofollow noreferrer"">HavelâHakimi algorithm</a> (although testing for an even degree sum is a useful and easy first step).</p>

<p>The process is that you sort the degree sequence descending, as usual, then generate a new sequence by removing a highest-degree node of degree $d$ and subtract one off  each of the following $d$ degree values, to produce a new shorter sequence.</p>

<p>The process can be applied repeatedly until you reach a sequence that is trivial to recognize as possible or impossible. (In principle, for a <a href=""https://en.wikipedia.org/wiki/Degree_(graph_theory)#Degree_sequence"" rel=""nofollow noreferrer"">graphical sequence</a> - one that corresponds to a realizable graph - this process can be continued to finish with all zeroes).</p>

<p>Sample - your case (b): </p>

<p>$\begin{array}{c}
&amp;3&amp;3&amp;3&amp;3&amp;2\\
&amp;&amp;2&amp;2&amp;2&amp;2\\
&amp;&amp;&amp;1&amp;1&amp;2\\
\text{re-sort}&amp;&amp;&amp;2&amp;1&amp;1\\
&amp;&amp;&amp;&amp;0&amp;0&amp;\checkmark \\
\end{array}$</p>

<p>So as you say this degree sequence can be drawn as a graph.</p>
"
"2389229","2389265","<p>Suppose you have $x_{n}$ particles after $n$ timesteps. Provided a particle turning into two or not is independent of any other particle doing so, then the expected number of <em>new</em> particles is $x_{n}p$ by linearity of expectation. So you can work out $E[x_{n+1}]$ as a function of $x_{n}$.</p>

<p>Then provided a particle turning into two or not is independent of past events, then you can use this expression for $E[x_{n+1}]$ as a recurrence relation to give you the closed-form expectation, again using linearity of expectation.</p>

<p>Recurrence relation:</p>

<blockquote class=""spoiler"">
  <p>$E[x_{n+1}] = x_{n} + x_{n}p = x_{n}(1+p)$, and since $E[E[x_{n+1}]]=E[x_{n+1}]$, we have $E[x_{n+1}]=E[E[x_{n+1}]]=E[x_{n}(1+p)]=(1+p)E[x_{n}]$ from linearity of expectation and since $p$ is constant</p>
</blockquote>

<p>Solution</p>

<blockquote class=""spoiler"">
  <p>$E[x_{n+1}] = E[x_{0}](1+p)^{n}$; in your case $x_{0}=1$ so $E[x_{n+1}]=(1+p)^{n}$</p>
</blockquote>
"
"2389230","2389237","<p>The statement $$a \in_M |N'| \Leftrightarrow (a \in M) \land (M \models a \,\epsilon\, |N|)$$ is incorect.  The set $|N'|$ is probably not even an element of $M$, so it is meaningless to write $a \in_M |N'|$.  Rather, what is true is $$a \in |N'| \Leftrightarrow (a \in M) \land (M \models a \,\epsilon\, |N|).$$  So $|N'|$ is the set whose <em>actual</em> elements are the $M$-elements of $|N|$ (that is, the elements of $|N|$ according to the element relation of the structure $(M,\epsilon)$).</p>
"
"2389232","2389270","<p>\begin{eqnarray*}
  n^{2015} +n+1 &amp;=&amp; n^{2015} -n^2+n^2+n+1  \\
   &amp;=&amp; n^2(n^{2013} -1) + n^2+n+1   \\
   &amp;=&amp; n^2(n^3-1)\underbrace{(n^{2010}+n^{2007}+...+n^3+1)}_a +n^2+n+1 \\
   &amp;=&amp; (n^2+n+1)(an^2(n-1)+1) \\
   &amp;=&amp;
\end{eqnarray*}
Since $n^2+n+1\geq 3$ this could be prime only if $an^2(n-1)+1 =1$ and this is only when $n=1$. </p>
"
"2389239","2389304","<p>Consider first how to compute the trace of an operator. Given an orthonormal basis $\{e_1,...,e_n\}$ of a finite dimensional Hilbert space $\mathcal H$, we can compute the trace of an operator $A: \mathcal H \to \mathcal H$ as $$\text{tr}(A)= \sum_{j=1}^n \langle e_j,Ae_j \rangle$$ where $\langle \cdot,\cdot \rangle$ is the inner product on $\mathcal H$. Next think about how we represent $A$ as a matrix. Really what we do is some coordinate isomorphism $\mathcal H \cong \mathbb C^n$ where 
$$e_1 \mapsto \begin{pmatrix} 1 \\0 \\ \vdots \\ 0\end{pmatrix} \hspace{1cm} e_2 \mapsto \begin{pmatrix} 0 \\1 \\ \vdots \\ 0\end{pmatrix} \hspace{0.5cm} ... \hspace{0.5cm} e_n \mapsto \begin{pmatrix} 0 \\0 \\ \vdots \\ n\end{pmatrix} \hspace{1cm}$$ and the columns of $A$ represent it's action on each of the basis vectors. This basis is then orthonormal with respect to the standard inner product on $\mathbb C^n$: $$\Bigg\langle \begin{pmatrix} x_1 \\x_2 \\ \vdots \\ x_n\end{pmatrix},\begin{pmatrix} y_1 \\y_2 \\ \vdots \\ y_n\end{pmatrix} \Bigg\rangle = \begin{pmatrix} \overline{x_1} &amp; \overline{x_2} &amp; ... &amp; \overline{x_n} \end{pmatrix}\begin{pmatrix} y_1 \\y_2 \\ \vdots \\ y_n\end{pmatrix}= \sum_{j=1}^n \overline{x_j} y_j $$ which we can also write as $\langle x,y\rangle= x^\dagger y$ for any $x,y \in \mathbb C^n$. Next we use the fact that the trace is independent of which orthonormal basis was originally chosen. In this formulation of constructing the matrix $A$ we get that $$\text{tr}(A)= \sum_{j=1}^n e_j^\dagger A e_j$$ which exactly corresponds to the sum of the diagonal elements of $A$ in the reference basis. </p>

<p>This idea can be generalized for the partial trace as follows. Let $\{a_1,...,a_m \}$ and $\{ b_1,...,b_n\}$ be orthonormal bases for two Hilbert spaces $\mathcal H_A$ and $\mathcal H_B$ respectively. We choose the reference basis of $\mathcal H_A \otimes \mathcal H_B$ to be the natural choice $\{ a_1 \otimes b_1,a_1 \otimes b_2,a_1 \otimes b_3,...,a_m \otimes b_n\}$ and represent each of these as a column vector $$a_1 \otimes b_1 \mapsto \begin{pmatrix} 1 \\0 \\ \vdots \\ 0\end{pmatrix} \hspace{1cm} a_1 \otimes b_2 \mapsto \begin{pmatrix} 0 \\1 \\ \vdots \\ 0\end{pmatrix} \hspace{0.5cm} ... \hspace{0.5cm} a_m \otimes b_n \mapsto \begin{pmatrix} 0 \\0 \\ \vdots \\ n\end{pmatrix} \hspace{1cm}$$ in $\mathbb C^{mn}$. Then any linear operator $T:\mathcal H_A \otimes \mathcal H_B \to \mathcal H_A \otimes \mathcal H_B$ has a matrix representation as $T: \mathbb C^{mn} \to \mathbb C^{mn}$ as we constructed above. The partial trace can then be computed by performing this sum of inner products with the basis of $\mathcal H_B$ only $$\text{tr}_B(T)= \sum_{j=1}^n (\text{id}_A \otimes b_j^\dagger)T (\text{id}_A \otimes b_j) $$ where $\text{id}_A$ is the $m \times m$ identity matrix and $b_j$ is $j^{th}$ coordinate vector in $\mathbb C^n$. Each $\text{id}_A \otimes b_j$ is therefore a $mn \times m$ matrix and each $\text{id}_A \otimes b_j^\dagger$ is an $m \times mn$ matrix resulting in an operator on $\mathbb C^m$ from which you can reverse the coordinate isomorphism to get back to $\mathcal H_A$. So in summary</p>

<ol>
<li><p>Fix orthonormal bases $\{a_1,..,a_m\}$ and $\{b_1,...,b_n\}$ of $\mathcal H_A$ and $\mathcal H_B$. Write as column vectors in $\mathbb C^m$ and $\mathbb C^n$ respectively.</p></li>
<li><p>Compute $\text{id}_A \otimes b_j$ and $\text{id}_A \otimes b_j^\dagger$ for each $j$ where $\text{id}_A$ is the $m \times m$ identity matrix.</p></li>
<li><p>$\text{tr}_B(T)= \sum_{j=1}^n (\text{id}_A \otimes b_j^\dagger)T (\text{id}_A \otimes b_j) $</p></li>
</ol>
"
"2389240","2390045","<p>We need to prove that
$$\sum_{cyc}\left(\frac{a}{1+(b+c)^2}-a\right)\leq\frac{3(a^2+b^2+c^2)}{a^2+b^2+c^2+12abc}-3$$ or
$$\sum_{cyc}\frac{a(b+c)^2}{1+(b+c)^2}\geq\frac{36abc}{a^2+b^2+c^2+12abc}.$$
Now, by C-S
$$\sum_{cyc}\frac{a(b+c)^2}{1+(b+c)^2}=\sum_{cyc}\frac{a}{1+\frac{1}{(b+c)^2}}=$$
$$=\sum_{cyc}\frac{a^2}{a+\frac{a}{(b+c)^2}}\geq\frac{(a+b+c)^2}{\sum\limits_{cyc}\left(a+\frac{a}{(b+c)^2}\right)}=\frac{9}{3+\sum\limits_{cyc}\frac{a}{(b+c)^2}}.$$
Thus, it remains to prove that
$$a^2+b^2+c^2+12abc\geq4abc\left(3+\sum\limits_{cyc}\frac{a}{(b+c)^2}\right)$$ or
$$a^2+b^2+c^2\geq4abc\sum\limits_{cyc}\frac{a}{(b+c)^2}$$ or
$$\sum_{cyc}\left(\frac{a}{bc}-\frac{4a}{(b+c)^2}\right)\geq0$$ or
$$\sum_{cyc}\frac{a(b-c)^2}{bc(b+c)^2}\geq0.$$
Done!</p>
"
"2389246","2389260","<p>You have to consider two different cases separately: $0&lt;x&lt;1$ and $x&gt;1$. The solution you presented here is only valid for the latter case when $x&gt;1$. But in the case $0&lt;x&lt;1$, logarithm base $x$ is a decreasing function, and thus eliminating it would flip the sign of the inequality &mdash; so that your second line will be $\log_{36}(\cdots)\color{red}{&gt;}x$.</p>

<p>By the way, you seem to have made a similar mistake when finishing your solution too. Solving the quadratic inequality gives you $u&lt;-1$ or $u&gt;\frac{2}{3}$. As you pointed out, the equation $\left(\frac{4}{9}\right)^x=-1$ has no solutions; moreover, the inequality $\left(\frac{4}{9}\right)^x&lt;-1$ has no solutions either. But the other root gives you an inequality
$$u&gt;\frac{2}{3} \quad \Longrightarrow \quad \left(\frac{4}{9}\right)^x&gt;\frac{2}{3} \quad \Longrightarrow \quad x\color{red}{&lt;}\frac{1}{2}.$$
Again, the inequality sign flips because the base is less than $1$. Notice that this solution was for the case $x&gt;1$, so in the end we see that this case has no solutions (the outcome $x&lt;\frac{1}{2}$ contradicts the case condition $x&gt;1$). But you still have the other case to solve.</p>
"
"2389250","2389332","<p>Let $X = Y = Z = S^1 \subseteq \mathbb{C}$ and take arbitrary $A \subseteq S^1$ such that $A = -A$. Define</p>

<p>$$\begin{align*} 
f(z) &amp; = \begin{cases} -z &amp; \text{for } z \in A \\ z &amp; \text{for } z \notin A \end{cases} \\[1ex]
g(z) &amp; = z^2.
\end{align*}$$</p>

<p>So $f$ swaps $z$ with $-z$ if $z \in A$ and  fixes it otherwise.</p>

<p>Then:</p>

<ul>
<li>$X, Y, Z$ are compact metric spaces;</li>
<li>$g$ is a continuous surjection;</li>
<li>$f$ is a bijection;</li>
<li>$g \circ f = g$ is continuous;</li>
<li>for each $w \in S^1$ the preimage $(g \circ f)^{-1}[ \{ w \} ]$ is a $2$-point space $\{ z, -z \}$ where $z^2 = w$. If $z \in A$, then $f$ swaps the two points, otherwise it fixes them. Either way, $f$ is continuous on this fiber.</li>
</ul>

<p>But clearly $f$ does not have to be continuous if we begin with a wild set $A$. </p>
"
"2389252","2389258","<p>As each finite Abelian group is self-dual, it has as many groups of order $d$ as of order $|G|/d$. You have seven subgroups of order two, but only six of order four. There is a subgroup of order four missing. It is
$\left&lt;ab,ac\right&gt;$.</p>
"
"2389268","2389316","<p>Your rephrasing of the original problem lacks some information which is crucial for the result to hold.  Let's go back to the <a href=""https://math.stackexchange.com/questions/2386829/problem-of-rank-trace-determinant-and-eigenvalue/2386886#2386886"">original problem</a> and see how every piece of information is used to get the required result. You are given that $A' + B' = I$ and $A'B' = 0$.</p>

<ol>
<li>The first observation is that by multiplying $A' + B' = I$ by $A'$ from the left and using $A'B' = 0$, we get $A'^2 = A'$. Similarly, by multiplying the identity by $B'$ from the right, we get $B'^2 = B'$. Those conditions tell you that $A',B'$ are projections so they are diagonalizable with possible eigenvalues $0,1$.</li>
<li>Let's assume we are given a projection $P = P^2$. Any vector $v \in \mathbb{F}^n$ decomposes as $v = Pv + (v - Pv)$ where $Pv \in \operatorname{im}(P)$ and $v - Pv \in \ker(P)$ (since $P(v - Pv) = Pv - P^2v = 0$). Hence, $\mathbb{F}^n = \operatorname{ker}(P) + \operatorname{im}(P)$. Since $n = \dim \operatorname{ker}(P) + \dim \operatorname{im}(P)$, this is a direct sum decomposition so $\ker(P) \oplus \operatorname{im}(P) = \mathbb{F}^n$. The image $\operatorname{im}(P)$ is also the eigenspace of $P$ associated to the eigenvalue $1$ while the kernel is the eigenspace of $P$ associated to the eigenvalue $0$. To see why, note that $\ker(P - I) \subseteq \operatorname{im}(P)$ while if $v = Pw \in \operatorname{im}(P)$ then $Pv = P^2w = Pw = v$. Finally, the map $Q = I - P$ also satisfies $Q^2 = Q$ so it is a projection whose image is $\ker(P)$ and whose kernel is $\operatorname{im}(P)$ (check this!).</li>
<li>Consider the space $V$ which is the eigenspace of $A'$ associated to the eigenvalue $1$. By the previous item, $V$ is <strong>also</strong> the kernel of $I - A' = B'$. Similarly, $W$ is <strong>also</strong> the kernel of $I - B' = A'$. In particular, we have $V \oplus W = \mathbb{R}^n$.</li>
<li>Using the above, we see now that if $v \in V$ and $w \in W$ then
$$ (A + B)v = \frac{A'}{x}v + \frac{B'}{y}v = \frac{v}{x} + 0 = \frac{v}{x}, \\
(A + B)w = \frac{A'}{x}w + \frac{B'}{y}w = 0 + \frac{w}{y} = \frac{w}{y}$$
so indeed $V$ is the eigenspace of $A + B$ associated to the eigenvalue $\frac{1}{x}$ and $W$ is the eigenspace of $A + B$ associated to the eigenvalue $\frac{1}{y}$.</li>
<li>Since $\det(A + B)$ is the product of the eigenvalues of $A + B$, we immediately get
$$ \det(A + B) = \frac{1}{x}^{\operatorname{dim} V} \frac{1}{y}^{\operatorname{dim} W}. $$</li>
</ol>
"
"2389272","2389274","<p>Taking logarithm on both sides</p>

<p>$$\ln \left( 1- \frac{U_c}{U_0}\right)=-\frac{t}{T}$$</p>

<p>$$-T\ln \left( 1- \frac{U_c}{U_0}\right)=t$$</p>

<p>Since $a\ln b = \ln b^a$, we have</p>

<p>$$t=\ln \left( 1- \frac{U_c}{U_0}\right)^{-T}$$</p>
"
"2389281","2389297","<p>Let $M=\begin{pmatrix}1 &amp; 4 \\ 4 &amp; 7\end{pmatrix}$. We are looking for
$$ \min_{(x\,y) M (x\, y)^T=225} x^2+y^2 $$
and $M$ is a symmetric matrix with eigenvalues $9,-1$, associated with the eigenvectors $(1,2)^T$ and $(-2,1)^T$. By the <a href=""https://en.wikipedia.org/wiki/Spectral_theorem"" rel=""nofollow noreferrer"">spectral theorem</a>, the above minimum equals
$$ \min_{9x^2-y^2=225} x^2+y^2 = \min_{9x^2-y^2=225} \frac{9x^2-y^2}{9}+\frac{10y^2}{9}=\color{blue}{25} $$
hence the wanted minimum distance equals $\color{blue}{5}$, attained by $\pm\left(\sqrt{5},2\sqrt{5}\right)$.</p>
"
"2389291","2389330","<p>If the symbol $\mathcal{L} \left\{\left( 0, \frac{-1}{750}, \frac{-65}{750}, \frac{1}{30}\right)\right\}$ means the linear space spanned by the vector $\left( 0, \frac{-1}{750}, \frac{-65}{750}, \frac{1}{30}\right)$ than , Yes, you are correct. But I cannot verify your calculations and <a href=""https://www.wolframalpha.com/input/?i=5x-3y%2B2z%2B4t%3D3,4x-2y%2B3z%2B7t%3D1,%203x-6y-z-5t%3D9,7x-3y%2B7z%2B47t%3Da"" rel=""nofollow noreferrer"">Wolfram Alpha</a> gives a different solution: for the system</p>

<p>$$
(x,y,z,t)=\left(0\;,-\frac{7}{5}-\frac{a}{75}\;,-\frac{3}{5}-\frac{13a}{150}\;,\frac{a}{30}\right)
$$</p>
"
"2389303","2389320","<p>I don't know why your notes say what they say. Often notes are written when giving a course, and it is often convenient in introductory courses to talk about non-strict orders, and then when you reach well-orders it is necessary to talk about strict orders, and the definitions become cumbersome.</p>

<p>How do I know all that? Because five years ago I made that same mistake.</p>

<p>So we say that a set $\alpha$ is an ordinal if it is <em>transitive</em> and strictly well-ordered by the $\in$ relation. And for two ordinals $\alpha$ and $\beta$ we write $\alpha&lt;\beta$ if $\alpha\in\beta$.</p>

<p>It is necessary to require that the well-order is strict, since otherwise if we do not assume the Axiom of Regularity, it is possible that there is some $x=\{x\}$, which is then certainly a transitive set which is well-ordered by $\in$. But it is not strictly well-ordered by $\in$, since $x\in x$. And it is also necessary to assume that the set is transitive, since every singleton $x$ such that $x\neq\{x\}$ is strictly well-ordered by $\in$, whereas you want your universe to have exactly one ordinal which is a singleton.</p>
"
"2389309","2389318","<p>As in comment, 
$$\sum _na_n b_n \le \left(\sum_na_n^2\right)^{\frac{1}{2}}\left(\sum_nb_n^2\right)^{\frac{1}{2}}$$</p>
"
"2389312","2389319","<p>Since $f''\leq 0$, your function is concave down.
Let us fix a point $x_0\in\mathbb{R}$. By concavity you have that
$$
f(x) \leq f(x_0) + f'(x_0) (x-x_0),
\qquad \forall x\in\mathbb{R}.
$$
Since $f'(x_0) &lt; 0$, by comparison you have that
$$
\lim_{x\to +\infty} f(x) = -\infty.
$$</p>
"
"2389322","2389325","<p>Take $C_{2017}$, the cycle on $2017$ nodes. All odd cycles have chromatic number $3$.</p>
"
"2389324","2389341","<p>$$\int_x ^{2x} \frac{1}{t} f(t) dt-\int_x^{2x}\frac{1}{t}f(0)dt=\int_x^{2x}\frac{1}{t}\left(f(t)-f(0)\right)dt$$
Since $f$ is continuous at $0$, given $\epsilon \gt 0$, there is $\delta \gt 0$ such that $|x| \lt \delta \implies |f(x)-f(0)| \lt \frac{\epsilon}{\ln 2}$. For $|x| \lt \delta$, since $|t| \lt |x| \lt \delta$, we have $$\left|\int_x^{2x}\frac{1}{t}\left(f(t)-f(0)\right)dt\right| \le \int_x^{2x}\left|\frac{1}{t}\left(f(t)-f(0)\right)\right|dt\lt \frac{\epsilon}{\ln 2}\int_x^{2x}\frac{1}{t}dt=\epsilon$$</p>
"
"2389337","2389359","<p>If you have more rows that columns, then all rows can <strong>NOT</strong> be linearly independent &mdash; but then, it actually does <strong>NOT</strong> matter here. You have the right answer in your second paragraph: if you know that all $N$ columns of the coefficient matrix are linearly independent, and if you know that solutions exist, then it has to be the case of a unique solution, since in this case
$$\operatorname{rank}(\text{coefficient matrix})=\operatorname{rank}(\text{augmented matrix})=\text{number of variables}.$$</p>

<p>The extra equations in this case don't affect the number of solutions. They just happen to be extraneous, or redundant, equations. They are linearly dependent on other equations, but that in and by itself does <strong>NOT</strong> create linear dependency among variables or solutions. Here's a quick example:
$$\left\{\begin{align} x&amp;=5 \\ 2x&amp;=10\end{align}\right.$$
You can see that in this case:
$$\operatorname{rank}(\text{coefficient matrix})=\operatorname{rank}(\text{augmented matrix})=\text{number of variables}=1,$$
so the system has a unique solution, even though $N=1$ and $L=2$. The second equation is just a redundant one, a multiple of the first equation, and it can be eliminated (it will become a row of zeroes when you do Gauss-Jordan reduction).</p>

<p>Also, regarding your statement:</p>

<blockquote>
  <p>&hellip; then the row rank of the matrix can be less than $N$.</p>
</blockquote>

<p>The row and column ranks of a matrix are always equal. So if you already know that the column rank of a matrix is $N$, then its row rank can't possibly be less than $N$.</p>
"
"2389339","2389345","<p>Let $\phi$ be a homomorphism from $G \times H$ to $A$.  Then you get a homomorphism from $G$ to $A$ by sending $g$ to $\phi(g,1_H)$.  You also get another homomorphism from $H$ to $A$ by sending $h$ to $\phi (1_G,h)$.   Thus given the homomorphism $\phi$, you get a pair of homomorphisms, one from $G$ to $A$, and another from $H$ to $A$.  This is your function $$\textrm{hom}(G\times H, A) \rightarrow \textrm{hom}(G,A) \times \textrm{hom}(H,A)$$ which you need to show is a bijection.</p>
"
"2389349","2389356","<p>Do you mean $\|fx\|\leq\|f\|\|x\|$?  That's by the definition
$$
\|f\| := \sup\{|f(x)| : x\in X,\ \|x\|\leq1\}.
$$
It clearly holds if $x=0$, and otherwise consider $\frac{x}{\|x\|}\in X$ and $\|\frac{x}{\|x\|}\| \leq 1$. Then by linearity
$$
\|f\| \geq \left|f\left(\frac{x}{\|x\|}\right)\right| = \frac{|f(x)|}{\|x\|}
$$
and hence
$$
|f(x)|\leq \|f\|\|x\|.
$$</p>
"
"2389353","2389482","<p>A vertex with degree zero is isolated. Then the maximum possible degree value in the remainder of the graph is $N-2$. Thus, whenever the ""remainder of the graph"" exists, such a degree sequence is not possible. So only $N=1$ is feasible.</p>

<p>Drawing graphs is not a bad way to get some intuition into the issue, but the above conflict between maximum and minimum degree vertex should become quickly apparent.</p>

<p>An immediate follow-on result from this is that for any graph of more than one vertex, you will be able to find two vertices with the same degree.</p>
"
"2389368","2389375","<p>Call the points $A$ and $B$, and the third point $C$ --- the one you're looking for. Let $v$ and $w$ be unit vectors in the direction from $A$ to $C$ and from $B$ to $C$ respectively, i.e., </p>

<p>$$
v = (\sin \theta_1, \cos \theta_1)\\
w = (\sin \theta_2, \cos \theta_2)
$$</p>

<p>Then we have
$$
C = A + tv \\
C = B + sw
$$
for some numbers $t$ and $s$, hence
$$
A + tv = B + sw
$$
Taking a dot product with the vector $v' = (-\cos \theta_1, \sin \theta_1)$, which is $v$ rotated 90 degrees counterclockwise, we get
$$
A\cdot v' + t v \cdot v' = B\cdot v' + s w \cdot v'
$$
Since $v \cdot v' = 0$, we get
$$
\frac{(A-B) \cdot v'}{w \cdot v'} = s 
$$
and from this, you can determine $C$. </p>

<p>..and now I realize that all you wanted was $\phi$, which is $\theta_1 - \theta_2$, as @EmilioNovati observed in the comments. But since you'll probably want to work out some more details as well, I'm keeping what follows:</p>

<p>Unwinding all that, </p>

<p>\begin{align}
C 
&amp;= B + sw \\
&amp;= (x_2, y_2) + \frac{(A-B) \cdot v'}{w \cdot v'} w\\
&amp;= (x_2, y_2) + \frac{(x_1 - x_2, y_1 - y_2) \cdot (-\cos \theta_1, \sin \theta_1)}{w \cdot v'} w\\
&amp;= (x_2, y_2) + \frac{(-\cos \theta_1)(x_1 - x_2) + (\sin \theta_1)( y_1 - y_2)}{w \cdot v'} w\\
&amp;= (x_2, y_2) + \frac{(-\cos \theta_1)(x_1 - x_2) + (\sin \theta_1)( y_1 - y_2)}{(\sin \theta_2)(-\cos \theta_1) + (\cos \theta_2)(\sin \theta_1)} w\\
&amp;= (x_2, y_2) + \frac{(-\cos \theta_1)(x_1 - x_2) + (\sin \theta_1)( y_1 - y_2)}{(\sin \theta_2)(-\cos \theta_1) + (\cos \theta_2)(\sin \theta_1)}  (\sin \theta_2, \cos \theta_2)\\
&amp;= (x_2, y_2) + \frac{(-\cos \theta_1)(x_1 - x_2) + (\sin \theta_1)( y_1 - y_2)}{\sin (\theta_1 - \theta_2)}  (\sin \theta_2, \cos \theta_2)
\end{align}</p>

<p>The relationship between $\phi$ and $\ell$ is that if $\theta_1$ and $\theta_2$ are held constant, and so is the direction from $A$ to $B$ (i.e., the direction of the dotted line), then $\phi$ remains constant even though $\ell$ changes. I'm not sure if this is what you meant, but it's the only relation I can think of. </p>
"
"2389370","2389409","<p>At first, why did you suppose that $\inf X$ does exist? Think of $\mathbb{R}$, which is unbounded below.</p>

<p>Now, nack to our proof. Let $E$ be a bounded from above, non-empty subset of $X$. Since we only know that $X$ has the g.l.b. property, we need to construct a set that is bounded from below and, of course is non-empty. So, since $E$ is bounded from above, there exists an $a\in X$ such that:
$$x\leq a\ \forall x\in E$$
Now, consider
$$F:=\{x\in X:x\geq y,\ \forall y\in E\}$$
So, it is clear that $a\in F$, so $\varnothing\neq F\subseteq X$. Moreover, since $E$ is non-empty, there exists a $b\in E$, so, it is clear by $F$'s definition, that:
$$b\leq x,\ \forall x\in F$$
so $F$ is bounded from below. Due to our hypothesis, there exists an $s\in X$ such that:
$$s=\inf F$$
Now, we claim that $s\in X$ is an upper bound of $E$. </p>

<p>Let us suppose that there exists a $x\in E$ such that 
$$x&gt;s$$
Then, by $F$'s definition, there should exist no $y\in F$ such that $$s\leq y\leq x$$
since for every $y\in F$ we have that $y&gt;x$
So, $x$ is a greater than $s$ lesser bound for $F$, which is a contradiction. So $$x\leq s, \forall x\in E$$</p>

<p>Now, if $s\in E$, then it is clear that $s=\sup E$. If $s\not\in E$, let $t$ be another upper bound of $E$ with
$$t&lt;s$$
Then, since $t$ is an upper bound it is clear that $t\in F$, which is a contradiction, since, then:
$$s=\inf F\leq t&lt;s$$
So, $$s=\sup E$$</p>
"
"2389372","2389399","<p>Let $g(x)=f(x)-2x^2$. Then $g'(x)=f'(x)-4x$ and $g''(x)=f''(x) - 4$.</p>

<p>We have $g(0)=g(\frac 1 2)=0$, therefore there is $x_1 \in (0, \frac 1 2)$ such that $g'(x_1)=0$ (why?). Also $g'(0)=0$. Can you take it from here?</p>
"
"2389385","2389411","<p>By induction hypothesis $n^p-n=p\cdot d$</p>

<p>It is easy to see that $p|{p\choose k}$ if $1\leq k \leq p-1$</p>

<p>\begin{eqnarray*}
  (n+1)^p-(n+1) &amp;=&amp; n^p + \underbrace{{p\choose 1}n^{p-1}+...+{p\choose p-1}n}_a+1-n-1 \\
   &amp;=&amp; \underbrace{n^p-n}_{p\cdot d} + p\cdot b \\
 &amp;=&amp; p (d+b)
  \end{eqnarray*}
where $a = p\cdot b$</p>
"
"2389391","2389401","<p>It's more-or-less the binomial theorem.</p>

<p>$$\sum_{k=0}^{n+2}(-d)^k\binom{n+2}k=(1-d)^{n+2}$$
Delete two terms
$$\sum_{k=2}^{n+2}(-d)^k\binom{n+2}k=(1-d)^{n+2}-1+(n+2)d$$
Divide by $(-d)^2$
$$\sum_{j=0}^{n}(-d)^j\binom{n+2}{j+2}=\frac{(1-d)^{n+2}-1+(n+2)d}{d^2}$$</p>
"
"2389392","2389405","<p>Appealing to the half-angle identity, $1-\cos(x)=2\sin^2(x/2)$, reveals </p>

<p>$$\frac{1-\cos(1-\cos(x))}{x^4}=\frac{2\sin^2(\sin^2(x/2))}{x^4}$$</p>

<p>Next, we write</p>

<p>$$\begin{align}
\frac{2\sin^2(\sin^2(x/2))}{x^4}&amp;=2\left(\frac{\sin(\sin^2(x/2))}{x^2}\right)^2\\\\
&amp;=2\left(\frac{\sin(\sin^2(x/2))}{\sin^2(x/2)}\,\frac{\sin^2(x/2)}{x^2}\right)^2\\\\
&amp;=2\,\color{blue}{\underbrace{\left(\frac{\sin^2(\sin^2(x/2))}{\sin^2(x/2)}\right)^2}_{\to 1}}\,\color{red}{\underbrace{\frac1{16}\left(\frac{\sin^2(x/2)}{(x/2)^2}\right)^2}_{\to 1/16}}\\\\
&amp;\to \frac18
\end{align}$$</p>
"
"2389394","2389454","<p>Inspired by Adayah's answer, we can find a function $f$ that is not continuous.</p>

<p>Let $X=Y=[0,1]\times[0,1]$, and $Z=[0,1]$. Define $f$ and $g$ by
$$f(x,y)=\begin{cases} (x,y)\quad y&lt;1/2 \\ (1-x,y)\quad y\ge1/2  \end{cases}$$
$$g(x,y)=y$$</p>

<p>Then $f$ is a bijection and $g$ is a continuous surjection, and $g\circ f$ is continuous since $(g\circ f)(x,y)=y$. The preimage of some $z\in Z$ under $f\circ g$ is the connected set $[0,1]\times \{z\}$, which $f$ acts on either as the identity if $z&lt;1/2$ or by negating the first argument if $z\ge1/2$.</p>
"
"2389397","2389404","<p>The following might help you</p>

<ol>
<li>$$\|A-B\|^2=\langle A-B, A-B\rangle$$</li>
<li>$$\langle A, B \rangle=\|A\|\|B\| \cos \theta$$</li>
</ol>
"
"2389398","2389777","<p>Well, the conditional covariance of two events when given a third is : $$\mathsf {Cov}(X,Y\mid E_i) ~=~ {\mathsf P(X\cap Y\mid E_i)- \mathsf P(X\mid E_i)\,\mathsf P(Y\mid E_i)}$$</p>

<p>So $\dfrac{ \mathsf P(X\mid E_i)\,\mathsf P(Y\mid E_i) }{ \mathsf P(X\cap Y\mid E_i)} ~=~ 1- \dfrac{\mathsf {Cov}(X,Y\mid E_i)}{\mathsf P(X\cap Y\mid E_i)}$</p>

<p>Which ... I don't think has any particular meaning.</p>
"
"2389403","2389415","<p>You are asking about 
$$
(2n)^2 - (2n-1)^2 - \left((2n-2)^2-(2n-3)^2 \right)
$$
which is always 4 by algebra. </p>
"
"2389407","2389416","<p>The problem should be to prove $I-A$ is invertible.  If $A^k=0$, the determinant of $A^k$ is zero, so the determinant of $A$ is also zero, so $A$ is not invertible.  Then the equation to be proved should remind you of the sum of a geometric series.  Just multiply $(I-A)(I+A+A^2+\ldots A^{K-1})$ and most of the terms cancel.</p>
"
"2389420","2389437","<p>Second year MS student here that had to take another semester in order to graduate.</p>

<p>Frustration is normal. Super normal. It's something I've become familiar with the last two years. The big thing, at least for me, is to push on. I also tell my students to ""know when to quit."" What that means is not when to give up, but when to step away, take a break, and come back with a clear head. Sometimes a solution comes after you've slept for the night, had a meal, or played a video game.</p>

<p>Of course, managing the frustration needs to be done healthily. Admittedly, I've wanted to throw things across the room, break cups and plates, even light things on fire (I didn't, thankfully). This behavior has also led to drinking more than I should. Find healthy ways to relieve stress - take a walk, eat a snack, talk to a friend, laugh at a comedy.</p>

<p>Practice makes it better. It's normal to be frustrated. You're not alone.</p>
"
"2389423","2389435","<p>Note that$$\sqrt{\sec x} \tan x = \frac{1}{\sqrt{\cos x}} \cdot \frac{\sin x}{\cos x}= \frac{\sin x}{(\cos x)^{3/2}}$$ So let $u= \cos x$ then $du= -\sin x \, dx$ and thus $$\int \sqrt{\sec x} \tan x \,dx =\int \frac{\sin x}{(\cos x)^{3/2}} \,dx = \int \frac{-1}{u^{3/2}} \, du = \frac{2}{\sqrt u}+C = 2 \sqrt{\sec x}+C$$</p>
"
"2389424","2389444","<p><strong>Definition:</strong> We will say that a function $f : \mathbb{R}\to\mathbb{R}$ is continuous at $a$ if</p>

<ol>
<li>$\lim_{x\to a} f(x)$ exists,</li>
<li>$f(a)$ is defined, and</li>
<li>$\lim_{x\to a} f(x) = f(a)$.</li>
</ol>

<p><strong>Proposition:</strong> The function $f : \mathbb{R}\to\mathbb{R}$ defined by
\begin{equation*}
  f(x) := \begin{cases} x \cos\left( \frac{1}{x} \right) &amp; \text{if $x\ne 0$, and} \\ 0 &amp; \text{otherwise} \end{cases}
\end{equation*}
is continuous at zero.</p>

<p><em>Proof:</em>  From the definition of continuity, it is sufficient to show that $\lim_{x\to 0} f(x)$ exists, and is equal to $f(0) = 0$.  Observe that since $\cos(\theta) \in [-1,1]$ for all real numbers $\theta$, we have
$$ 0 \le |f(x)| = \left| x \cos\left( \frac{1}{x} \right) \right| \le |x| $$
for all $x\ne 0$.  But
$$ \lim_{x\to 0} 0 = 0
\qquad\text{and}\qquad
\lim_{x\to 0} |x| = 0.$$
Therefore, by the Squeeze Theorem, we must conclude that
$$ \lim_{x\to 0} |f(x)| = 0 \implies \lim_{x\to 0} f(x) = 0, $$
which is what we wanted to show.
$$\tag*{$\blacksquare$}$$</p>

<hr>

<p>An alternative proof, using the $\varepsilon-\delta$ definition of continuity:</p>

<p><strong>Definition:</strong>  We will say that a function $f : \mathbb{R} \to \mathbb{R}$ is continuous at $a$ if for all $\varepsilon &gt; 0$ there exists a $\delta &gt; 0$ such that
$$ |x-a| &lt; \delta \implies |f(x) - f(a)| &lt; \varepsilon.$$</p>

<p><strong>Proposition:</strong> The function $f : \mathbb{R}\to\mathbb{R}$ defined by
\begin{equation*}
  f(x) := \begin{cases} x \cos\left( \frac{1}{x} \right) &amp; \text{if $x\ne 0$, and} \\ 0 &amp; \text{otherwise} \end{cases}
\end{equation*}
is continuous at zero.</p>

<p><em>Proof:</em>  Fix $\varepsilon &gt; 0$, and choose $\delta &lt; \varepsilon$.  If $|x-0| = |x| &lt; \delta$, then we have
$$ |f(x) - f(0)|
= \left| x\cos\left(\frac{1}{x}\right) - 0\right|
=  |x| \left| \cos\left(\frac{1}{x}\right)\right|
&lt; \delta \left| \cos\left(\frac{1}{x}\right)\right|.
&lt; \varepsilon \left| \cos\left(\frac{1}{x}\right)\right|.
$$
As above, observe that $\cos(\theta) \in [-1,1]$ for all real $\theta$, from which it follows that
$$ |f(x) - f(0)|
&lt; \varepsilon \left| \cos\left(\frac{1}{x}\right)\right|
\le \varepsilon,
$$
which is what we wanted to show.
$$\tag*{$\blacksquare$}$$</p>
"
"2389425","2389651","<p>Comment becomes an answer</p>

<p>There are prepublication notes by Tim Jameson on $\zeta(3)$ and $\zeta(4)$ <a href=""http://www.maths.lancs.ac.uk/~jameson/polylog.pdf"" rel=""nofollow noreferrer"">http://www.maths.lancs.ac.uk/~jameson/polylog.pdf</a></p>

<p>There is also the paper by Yue and Williams which may help you. <a href=""https://projecteuclid.org/euclid.pjm/1102620561"" rel=""nofollow noreferrer"">https://projecteuclid.org/euclid.pjm/1102620561</a></p>

<p>It would be really interesting if you could work to draw out all the connections in their simplest and clearest form and then publish your work.</p>
"
"2389433","2389452","<p>I would highly recommend Sybilla Beckmann's book ""Mathematics for Elementary Teachers"".</p>

<p>This is a college level book specifically addressing all of your questions:  they ""why's"" of elementary school mathematics.  Included are explanations of the meaning of the 4 basic operations, why the standard algorithms for computing these operations make sense, etc.</p>
"
"2389436","2389690","<p>$\newcommand{\bbx}[1]{\,\bbox[15px,border:1px groove navy]{\displaystyle{#1}}\,}
 \newcommand{\braces}[1]{\left\lbrace\,{#1}\,\right\rbrace}
 \newcommand{\bracks}[1]{\left\lbrack\,{#1}\,\right\rbrack}
 \newcommand{\dd}{\mathrm{d}}
 \newcommand{\ds}[1]{\displaystyle{#1}}
 \newcommand{\expo}[1]{\,\mathrm{e}^{#1}\,}
 \newcommand{\ic}{\mathrm{i}}
 \newcommand{\mc}[1]{\mathcal{#1}}
 \newcommand{\mrm}[1]{\mathrm{#1}}
 \newcommand{\pars}[1]{\left(\,{#1}\,\right)}
 \newcommand{\partiald}[3][]{\frac{\partial^{#1} #2}{\partial #3^{#1}}}
 \newcommand{\root}[2][]{\,\sqrt[#1]{\,{#2}\,}\,}
 \newcommand{\totald}[3][]{\frac{\mathrm{d}^{#1} #2}{\mathrm{d} #3^{#1}}}
 \newcommand{\verts}[1]{\left\vert\,{#1}\,\right\vert}$</p>

<blockquote>
  <p>$\ds{x^{2} + 2x = k + \int_{0}^{1}\verts{t + k}\,\dd t}$.</p>
</blockquote>

<p>\begin{align}
\int_{0}^{1}\verts{t + k}\,\dd t = {} &amp;
\int_{0}^{1}\bracks{t &lt; -k}\pars{-t - k}\,\dd t +
\int_{0}^{1}\bracks{t &gt; -k}\pars{t + k}\,\dd t
\\[5mm] = {} &amp;
\braces{\bracks{0 &lt; -k &lt; 1}\int_{0}^{-k}\pars{-t - k}\,\dd t +
\bracks{-k &gt; 1}\int_{0}^{1}\pars{-t - k}\,\dd t}
\\[2mm] &amp; +
\braces{\bracks{-k &lt; 0}\int_{0}^{1}\pars{t + k}\,\dd t +
\bracks{0  &lt; -k &lt; 1}\int_{0}^{-k}\pars{t + k}\,\dd t}
\\[5mm] = {} &amp;
\braces{\bracks{-1 &lt; k &lt; 0}{k^{2} \over 2} +
\bracks{k &lt; -1}\pars{-\,{1 \over 2} - k}}
\\[2mm] &amp; {} +
\braces{\bracks{k &gt; 0}\pars{{1 \over 2} + k} +
\bracks{-1  &lt; k &lt; 0}\pars{-\,{k^{2} \over 2}}}
\\[5mm] = {} &amp;
\braces{\vphantom{\Large A}
\bracks{k &gt; 0} - \bracks{k &lt; - 1}}\pars{{1 \over 2} + k} =
\left\{\begin{array}{rcr}
\ds{-\,{1 \over 2} - k} &amp; \text{if} &amp; \ds{k &lt; - 1}
\\[2mm]
\ds{{1 \over 2} + k} &amp; \text{if} &amp; \ds{k &gt; 0}
\\[2mm]
\ds{0}&amp;&amp;\text{otherwise}&amp;&amp;
\end{array}\right.
\end{align}</p>

<blockquote>
  <p>We are entitled to study <em>three</em> cases. Namely,</p>
</blockquote>

<ol>
<li>$\ds{x^{2} + 2x + {1 \over 2} = 0\quad\mbox{if}\quad k &lt; -1}$.</li>
<li>$\ds{x^{2} + 2x - \pars{2k + {1 \over 2}} = 0\quad\mbox{if}\quad k &gt; 0}$.</li>
<li>$\ds{x^{2} + 2x - k = 0\quad\mbox{if}\quad k \in \bracks{-1,0}}$.</li>
</ol>
"
"2389446","2390270","<h1>The <em>if</em> case</h1>

<h3>Assumptions</h3>

<ol>
<li>$\lim_{\epsilon' \to 0} \phi(\epsilon') = 0$, i.e. for any $\epsilon&gt;0$ there exists $\epsilon''&gt;0$ such that for $0&lt;\epsilon'&lt;\epsilon''$ we have $0 \leq \phi(\epsilon')&lt;\epsilon$</li>
<li>for any $\epsilon'&gt;0$ there exists $N$ such that for $n&gt;N$ we have $0 \leq |a_nâa| \leq \phi(\epsilon')$</li>
</ol>

<h3>To show</h3>

<p>$\lim_{n \to \infty} a_n = a$, i.e. for any $\epsilon&gt;0$ there exists $N$ such that for $n&gt;N$ we have $|a_nâa| &lt; \epsilon$</p>

<h3>Proof</h3>

<p>Take $\epsilon&gt;0$.</p>

<p>By assumption 1 there exists $\epsilon''&gt;0$ such that for all $0 &lt; \epsilon' &lt; \epsilon''$ we have $0 \leq \phi(\epsilon') &lt; \epsilon$. For such $\epsilon'',$ let $\epsilon'=\epsilon''/2$. Then $0 \leq \phi(\epsilon') &lt; \epsilon.$</p>

<p>By assumption 2 there now exists $N$ such that for $n &gt; N$ we have $0 \leq |a_n-a| \leq \phi(\epsilon')$. But by the previous paragraph we have $\phi(\epsilon') &lt; \epsilon$ so $0 \leq |a_n-a| &lt; \epsilon$.</p>

<p>Thus, for any $\epsilon&gt;0$ there exists $N$ such that for $n &gt; N$ we have $|a_n-a| &lt; \epsilon$.</p>
"
"2389458","2389470","<p>It suffices to show that given any element $[a,b]=ab-ba$, the element $(ab-ba)r\in J$ for all $a,b,r\in R$.</p>

<p>We have $$\begin{align*}(ab-ba)r&amp;=abr-bar\\&amp;=abr-arb+arb-bar\\&amp;=a[b,r]+[ar,b]\end{align*}$$
which is in $J$ because $J$ is a left ideal.</p>
"
"2389469","2389515","<p>The answers to your first two questions are ""no"" in general. E.g. consider
$$
A=\pmatrix{1\\ &amp;-1},\ C=\pmatrix{1&amp;1\\ -1&amp;1}.
$$
Any matrix $B$ that commutes with $A$ must be diagonal. However, if $B$ has two different diagonal entries, then $B$ and $C$ do not commute; if $B$ is a nonzero multiple of $I$ instead, then $ABC$ is not positive definite because the product has zero trace.</p>
"
"2389476","2389654","<p>I kinda overlooked something: Notice that $x - f(x)z \in$ ker$(f) = N$, i.e. $x - f(x)z â¥ z$ as $z â N^{â¥}$. But from this we can already follow that $\langle (x-f(x)z)\,,\,||z||^{-2}z\rangle = ||z||^{-2}\langle (x-f(x)z)\,,\,z\rangle = 0.$</p>
"
"2389485","2389511","<p>$C_0^{\infty}(\mathbb{R})$ is dense in $L^2(\mathbb{R})$ thus exists a sequence $g_n \in C_0^{\infty}(\mathbb{R})$ such that $g_n \rightarrow f$ in $L^2(\mathbb{R})$.</p>

<p>Also $\int|g_n-f|^2=\int g_n^2+\int f^2 \Rightarrow \int g_n^2 \rightarrow -\int f^2$</p>

<p>Also $\int f^2 \leqslant \int |g_n-f|^2+ \int g_n^2$</p>

<p>Taking limits we have that $$2 \int f^2 \leqslant 0 \Rightarrow f^2=0$$  almost everywhere thus $f=0$ almost everywhere.</p>

<p>Now according to the comment below of @zhw there is an easier solution than the first one:</p>

<p>$$|\int g_nf-\int f^2| \leqslant \int|g_n-f||f| \leqslant \sqrt{\int|g_n-f|^2} \sqrt{\int|f|^2} \rightarrow 0$$</p>

<p>Thus $$0=\lim_{n \rightarrow \infty} \int g_nf=\int f^2$$</p>

<p>Thus $f=0$ almost everywhere.</p>
"
"2389500","2389636","<p>I believe $f(z) = e^{-1/z}$ is a counterexample.</p>
"
"2389509","2389570","<p>The assumption that $[E:F]$ is finite is totally irrelevant here.  By definition, $F(A)$ is the smallest subfield of $E$ containing $F$ and $A$.  In particular, this means $F(A)\subseteq E=\langle A\rangle.$</p>
"
"2389512","2389514","<p>You are correct if there exists an optimum (it is sufficient if e.g. the feasible set is compact). The first order conditions are <em>necessary</em> conditions, hence a global optimum must satisfy them. If you have finitely many points that satisfy the necessary conditions it is enough to check all of them.</p>

<p>However, for convex (and some other) problems, the conditions are also <em>sufficient</em> meaning that any such point satisfying the conditions much necessarily be global optimum.</p>
"
"2389521","2390709","<p>It is usually not true that $p_n\in \pi(C_0(X))$ (it may work for carefully chosen $X$, but not in general). </p>

<p>Other than that, your construction works. In fact, $\pi(a)p_n-p_n\pi(a)=0$ for all $n$ and all $a$.</p>
"
"2389525","2389531","<p>Hint: $$\begin{bmatrix}x_1 \\ x_2 \\ x_3 \\ x_4\end{bmatrix} = \begin{bmatrix}x_2+2x_3 \\ x_2 \\ x_3 \\ -x_2+x_3\end{bmatrix} = x_2 \begin{bmatrix}1 \\ 1 \\ 0 \\ -1\end{bmatrix} + x_3 \begin{bmatrix}2 \\ 0 \\ 1 \\ 1\end{bmatrix}.$$
All you need is to find two orthonormal vectors to represent $\begin{bmatrix}1 \\ 1 \\ 0 \\ -1\end{bmatrix}$ and $\begin{bmatrix}2 \\ 0 \\ 1 \\ 1\end{bmatrix}$.</p>
"
"2389527","2389539","<p>If $f\ne 0\in L(D)$, then $\operatorname{div}f + D\ge 0$. However, both $\operatorname{div}f$ and $D$ have degree 0, so this implies that in fact $\operatorname{div}f+D=0$, or $\operatorname{div}f = -D$. Then since $X$ is compact, meromorphic functions are determined up to scaling by their divisors on $X$, so $L(D)=\langle f\rangle$. In particular, this implies that $L(D)$ is one dimensional if it is nonzero. </p>

<p>Summarizing, when $\deg D=0$, if $L(D)\ne 0$, then for any $f\ne 0\in L(D)$, $D=\operatorname{div} 1/f$, and $f$ is a basis for $L(D)$.</p>
"
"2389529","2389536","<p>The ""standard form"" of a line is $Ax + By = C$ where $A$ and $B$ are not all zero. Usually we write lines in the slope intercept form $y = mx +b$. Here's how to go from standard form to slope-intercept form - I show all my students this.</p>

<p>From the standard form, divide by $B$ to allow the coefficient of $y$ to be $1$.</p>

<p>$$ \frac{A}{B}x + y = \frac{C}{B} $$</p>

<p>Now let's subtract $\frac{A}{B}x$ on both sides.</p>

<p>$$y = -\frac{A}{B}x + \frac{C}{B} $$</p>

<p>Now we do a thing called ""equating coefficients."" This process allows us to identify the slope and y-intercept of this line. The coefficient on $x$ is the slope, and the constant term the y-intercept. Hence we may identify $ m = -\frac{A}{B}$ and $b = \frac{C}{B}$.</p>

<p>Of course, you ask about $C$ in particular.  This is a parameter to help define other pieces of the line - namely, intercepts. Changing $C$ changes the location of these intercepts.</p>
"
"2389530","2389543","<p>One has to be careful.  Consider:</p>

<p>$$ f(x) = \cases{0 &amp; if $x \le 0$ or $x &gt; 1$\cr
                 x &amp; if $0 &lt; x \le 1$\cr}$$
$$ f'(x) = \cases{0 &amp; if $x &lt; 0$ or $x &gt; 1$\cr
                  1 &amp; if $0 &lt; x &lt; 1$\cr
           \text{undefined} &amp; if $x = 0$ or $x = 1$\cr} $$
Then $f$ and $f'$ are Lebesgue integrable on $\mathbb R$, but $\int_{\mathbb R} f'(x)\; dx = 1$.</p>

<p>EDIT: On the other hand, it is true that if $f$ is differentiable <strong>everywhere</strong> and $f' \in L^1[a,b]$, then $$f(b) - f(a) = \int_a^b f'(x)\; dx$$
See e.g. Rudin, ""Real and Complex Analysis"", Theorem 8.21.</p>
"
"2389546","2389607","<p>Continuing from $\,n^2\!-\!3\mid 9n\!-\!8\ $ we can avoid quadratic inequalities and checking $n = 1,\ldots,8$</p>

<p>$\!\bmod n^2\!-3\!:\ \color{#c00}{n^2\equiv 3}\,\Rightarrow\, 0 \equiv \color{#c00}{n}(9\color{#c00}{n}\!-\!8)\equiv {27}\!-\!8n\,\Rightarrow\,0\equiv 9n\!-\!8+27\!-\!8n \equiv n\!+\!19\,$ hence $\,n\equiv -19\,$ so $\,0\equiv 8\!-9n\equiv 179\,$ is prime, so its divisor $\,n^2\!-\!3 = \pm1,\pm 179,\,$ so $\,n = 2.$</p>
"
"2389549","2389555","<p>Sure you can. You just have to decide what the mappings between the integers are, and what composition is. For example, one can define the poset category on $\Bbb{Z}$ whose objects are the integers, and has a unique arrow $n\to m$ whenever $n\le m$. You can check that there is a composition defined by transitivity of $\le$ that makes this setup into a category.</p>
"
"2389556","2389883","<p>Rewriting $$\binom{n}k\frac1{n^k}=\frac{n!}{k!\,(n-k)!\,n^k}=\frac 1{k!}\frac{n!}{(n-k)!\,n^k}$$ you then want to show that 
$$\lim_{n\to\infty}\frac{n!}{(n-k)!\,n^k}=1$$ So, consider
$$A=\frac{n!}{(n-k)!\,n^k}\implies \log(A)=\log(n!)-\log(n-k)!)-k \log(n)$$ Now use Stirling approximation $$\log(p!)=p (\log (p)-1)+\frac{1}{2} \left(\log (2 \pi )+\log
   \left({p}\right)\right)+O\left(\frac{1}{p}\right)$$and appply it to the two factorials neglecting the negative powers of $p$. This gives, after some minor simplifications
$$\log(A)=-k+(n-k+\frac 12)\log\left(\frac n {n-k} \right)$$
$$\log\left(\frac n {n-k} \right)=-\log\left(1-\frac kn \right)=\frac{k}{n}+\frac{k^2}{2 n^2}+O\left(\frac{1}{n^3}\right)$$ $$\log(A)\approx -k+\left(n-k+\frac 12\right)\left(\frac{k}{n}+\frac{k^2}{2 n^2}\right)=-\frac{k(k-1)}{2n}+O\left(\frac{1}{n^2}\right)$$ which makes $$\lim_{n\to\infty}\log(A)=0 \implies \lim_{n\to\infty}A=1$$ and then the result.</p>
"
"2389557","2389716","<p>For the scope of this question it appears that we don't have to deal with 6 degrees of freedom, but only two real degrees of freedom corresponding to the two joints $A$ and $B$, plus 1 boolean choice representing a 180Â° rotation around the base, which determines the position of $A$.</p>

<p>Let's go through the text in the paper step by step. $q_1$ is the part happening in the floor plane. There are two ways to line up the plane of the arm with the target wrist position, with a difference of $\pi$ between them. This is the difference between $A$ being right of the vertical axis oder left, so one of the $q_1$ values corresponds to the upper row of images (âfrontâ), the other to the lower (ârearâ).</p>

<p>Now switch from the floor plane to the plane of the arm. You know where $A$ is (from $q_1$ and the lengths) and where $C$ is (from input). So the core question is where is $B$. To that effect, ignore the fixed right angle between $B$ and $C$, and replace it by the hypothenuse of that triangle. So the length between $B$ and $C$ is $d_{34}$ no matter what shape you have between these.</p>

<p>So at this point your task is one of constructing a triangle given three lengths. You have positions $A$ and $C$ and the lengths from these to $B$. Geometrically knowing one length means you know the point is on a certain circle. Knowing two lengths means you have to intersect two circles. There are in general two points of intersection (or none if the target is out of reach). One solution will be above the line $AC$ (left pictures), the other below (right pictures).</p>

<p>So how do you compute the angles of a triangle given its edge lengths? Use the cosine law! Reading <a href=""https://en.wikipedia.org/wiki/Law_of_cosines"" rel=""nofollow noreferrer"">Wikipedia</a> you find</p>

<p>$$c^2=a^2+b^2-2ab\cos\gamma$$</p>

<p>which you can reformulate to</p>

<p>$$\gamma=\pm\arccos\frac{a^2+b^2-c^2}{2ab}$$</p>

<p>In your specific situation that would translate to</p>

<p>$$\beta=\arccos\frac{a_2^2+g^2-d_{34}^2}{2\,a_2\,g}
\qquad
\delta=\arccos\frac{a_2^2+d_{34}^2-g^2}{2\,a_2\,d_{34}}$$</p>

<p>since those are interior angles in $\triangle ABC$ at $A$ res. $B$ and we seem to be using positive angles everywhere. You also have</p>

<p>$$\tan\alpha=\frac{h_z}{h_{xy}}$$</p>

<p>from which you can read $\alpha$ up to a possible term of $\ldots+\pi$, just as in the computation for $q_1$. Pick that so that it matches the actual signs of $h_z$ and $h_{xy}$, use <a href=""https://docs.python.org/3/library/math.html#math.atan2"" rel=""nofollow noreferrer""><code>atan2</code></a> when programming.</p>

<p>The angle $\gamma$ is fixed, defined by the shape of the arm between $B$ and $C$. It should be irrelevant, if the further computation were thinking about $BC$ as a straight line segment, but that isn't the case. So you have to compute an angle in a right-angled triangle again.</p>

<p>$$\gamma=\arctan\frac{d_4}{a_3}$$</p>

<p>From all of these you can compute $q_2$ and $q_3$ as described in the paper.</p>
"
"2389560","2389567","<p>The obvious (and maximal) candidate for the 3d object is
$$\{\,(x,y,z)\in\Bbb R^3\mid (y,z)\in A_1, (x,z)\in A_2, (x,y)\in A_3\,\}$$
obtained by intersecting the maximal sets that give one of the three projections each.
The question is if the projections of this maximal set are as desired. This is the case for the first projection if and only if for each $(y,z)\in A_1$ there exists $x\in\Bbb R$ such that $(x,y)\in A_3$ and $(x,z)\in A_2$. Similarly for the other two projections.</p>

<p>Hofstadter's examples work because already in the vertical bar of the E, there is so much material in the B (its lower line with final arc) that the G is guaranteed to work; and similarly, in the lower bar of the E, there is so much material in the G (its almost straight lower line) that the B is guaranteed to work; and finally the vertical bar of the B and the left end of the G are material enough to guarantee the E to work. So in a way, the trick is that the B and the G are less round than you might normally write them.</p>
"
"2389575","2389595","<p>Let $C_G(N)$ - centralizer of $N$ in $G$ and $|G:C_G(N)| = p^a$</p>

<p>Then $G/C_G(N) \cong A \le Aut(N)$</p>

<p>$p$ is prime, so $|Aut(N)| = p-1$</p>

<p>Hence $p^a | (p-1)$, which is only possible for $a=0$</p>

<p>Therefore $C_G(N) = G$ and $N \le Z(G)$</p>
"
"2389578","2389593","<p>As I understand, from the comments, the goal is to fix $d$ and then require that the additive drop between sites $s_i$ and $s_{i+1}$ be $d$.  </p>

<p>Assume $d,n$ are fixed.  Then all we need to know is the amount of traffic to the first site, call it $s_1$.  Then $s_i=s_1-(i-1)d$ so we compute $$100=\sum_{i=1}^n\left(s_1-(i-1)d\right)=ns_1-\frac {(n-1)(n)}2d$$</p>

<p>It follows that we should take $$\boxed{s_1=\frac {100}n+\frac {n-1}2d}$$</p>

<p>Example:  $n=3,d=20$.  Then $$s_1=\frac {100}3+20=53.333\dots$$
Thus in this case your numbers should be $\{53.333,33.333,13.333\}$.  Note that the numbers you gave do not add to $100$.</p>
"
"2389585","2389601","<p>There's the <a href=""https://en.wikipedia.org/wiki/Zariski_topology"" rel=""nofollow noreferrer"">Zariski topology</a>, in which the set of prime ideals in a commutative ring is made into a topological space (with the ideals as points), so any of the many theorems involving this notion serves as an example that connects commutative ring theory and general topology.</p>
"
"2389587","2390668","<p>Hint: try the naive approach to proving that a $k[t]$-module homomorphism of $(t)$ to itself is also a $k[[t]]$-module homomorphism. When you evaluate a product $fg$ in $k[[t]]$, the $n$-th coefficient in the result is unchanged if you replace the formal power series $f = a_0 + a_1x + a_2x^2 + ...$ by the polynomial $a_0 + a_1x + a_2x^2 + ... + a_nx^n$</p>
"
"2389592","2389596","<p>Certainly not.  For instance, let $(a_n)$ be any decreasing sequence.  Then every subsequence is also decreasing.</p>
"
"2389598","2389602","<p>The rank of any $m \times n$ matrix is at most $\min\{m,n\}$. Additionally, the rank of the product of two matrices $AB$ is bounded above by the ranks of $A$ and $B$. Putting these together, we have that $A^TA$ has rank at most $n$, which is strictly less than $m$; since it is an $m \times m$ matrix, it does not have full rank and cannot be inverted.</p>
"
"2389599","2389610","<p>For expectation - yes due to linearity of expectation. As for variance, you can do the same since the each toss (or each event) are independent. Note that it does not matter whether you toss each coin 'one at a time' or the order you do it in due to independence, of course.</p>

<hr>

<p>Formally, if you let $X_1$ and $X_2$ be the indicator variables for each toss of the first coin ($1$ for heads, $0$ for tails) and the same with $Y_1$ and $Y_2$ for the tosses of the second coin, we have:</p>

<p>$$N=X_1+X_2+Y_1+Y_2$$</p>

<p>Then</p>

<p>$$E(N)=E(X_1+X_2+Y_1+Y_2)$$</p>

<p>$$=E(X_1)+E(X_2)+E(Y_1)+E(Y_2)$$</p>

<p>due to linearity of expectation.</p>

<p>Also, as $X_1,X_2,Y_1,Y_2$ are independent events, we have</p>

<p>$$Var(N)=Var(X_1+X_2+Y_1+Y_2)$$</p>

<p>$$=Var(X_1)+Var(X_2)+Var(Y_1)+Var(Y_2)$$</p>

<hr>

<p>The general rules being used here are:</p>

<p>$$E(aX+bY)=aE(X)+bE(Y)\tag{linearity of expectation}$$</p>

<p>and if $X$ and $Y$ are independent,</p>

<p>$$Var(aX+bY)=a^2Var(X)+b^2Var(Y)$$</p>
"
"2389605","2389627","<p>See <a href=""https://en.wikipedia.org/wiki/Binomial_transform"" rel=""nofollow noreferrer"">binomial transform</a> for more details. Here's a proof using (exponential) generating series.</p>

<p>Let</p>

<p>$$ B(x) = \sum_{n = 0}^\infty b_n \frac{x^n}{n!} $$</p>

<p>be the exponential generating series for $(b_n)$.</p>

<p>Then</p>

<p>\begin{align}
e^x B(x) &amp;= \left( \sum_{i = 0}^\infty \frac{x^i}{i!} \right)\left( \sum_{j = 0}^\infty b_j \frac{x^j}{j!} \right) \\
&amp;= \sum_{n = 0}^\infty \left( \sum_{i + j = n} b_j \frac{x^i}{i!}\frac{x^j}{j!} \right) \\
&amp;= \sum_{n = 0}^\infty \left( \sum_{k = 0}^n b_k \frac{x^{n - k}}{(n - k)!}\frac{x^{k}}{k!} \right) \\
&amp;= \sum_{n = 0}^\infty \left( \sum_{k = 0}^n b_k \frac{n!}{(n - k)!k!} \right) \frac{x^{n}}{n!} \\
&amp;= \sum_{n = 0}^\infty a_n \frac{x^n}{n!} = A(x).
\end{align}</p>

<p>Where $A(x)$ is the exponential generating series for $(a_n)$. That is,</p>

<p>$$ \color{purple}{A(x) = e^x B(x)} $$</p>

<p>and conversely,</p>

<p>$$ B(x) = e^{-x} A(x). \tag{$*$} $$</p>

<p>The product $e^{-x} B(x)$ can be written as</p>

<p>\begin{align}
e^{-x} A(x) &amp;= \left( \sum_{i = 0}^\infty (-1)^{i}\frac{x^i}{i!} \right)\left( \sum_{j = 0}^\infty a_j \frac{x^j}{j!} \right) \\
&amp;= \sum_{n = 0}^\infty \left( \sum_{i + j = n} (-1)^i a_j \frac{x^i}{i!}\frac{x^j}{j!} \right) \\
&amp;= \sum_{n = 0}^\infty \left( \sum_{k = 0}^n (-1)^{n - k}a_k \frac{x^{n - k}}{(n - k)!}\frac{x^{k}}{k!} \right) \\
&amp;= \sum_{n = 0}^\infty \left( (-1)^n \sum_{k = 0}^n (-1)^{k}a_k \frac{n!}{(n - k)!k!} \right) \frac{x^{n}}{n!} \\
&amp;= B(x) = \sum_{n = 0}^\infty b_n \frac{x^n}{n!} \tag{by ($*$)}
\end{align}</p>

<p>Comparing the coefficient of $x^n/n!$ on both sides in $(*)$ gives us</p>

<p>$$ b_n = (-1)^n \sum_{k = 0}^n (-1)^{k}a_k \frac{n!}{(n - k)!k!} $$</p>

<p>or equivalently,</p>

<p>$$ \color{blue}{(-1)^n b_n = \sum_{k = 0}^n \binom{n}{k}(-1)^{k}a_k.} $$</p>
"
"2389606","2389634","<p>For $k\ge 2$, you have that $I_k$ is a closed interval and $f(x)=1/x$ is a continuous function on it, so it attains its $\min$ and its $\max$ on this interval (which Theorem is it?). Hence, for $k\ge 2$ \begin{align}\sup_{I_k}f(x)=\max_{I_k}f(x)&amp;=f(x_{k-1})\\  \inf_{I_k}f(x)=\min_{I_k}f(x)&amp;=f(x_{k})\end{align} Now, in $I_1=[0,x_1]$, you have that $\lim_{x\to 0} f(x)=+\infty$, so $$\sup_{I_1}f(x)=+\infty$$ On the other hand $f(0)=0$, and $f(x)=1/x&gt;0$ for any other $x$, so $$\inf_{I_1}f(x)=0$$</p>

<hr>

<p>Edit: To elaborate on $I_1$: $f$ has exactly one value on $x=0$, which is $f(0)=0$. However, $\lim_{x\to 0+} f(x)=+\infty$, which is different than $f(0)$. This does not mean that $f$ has two values for $x=0$. It means that $f$ is discontinuous at $x=0$, since $$0=f(0)\neq \lim_{x\to0+}f(x)=+\infty$$ Now, as $x$ tends to $0$, $f(x)$ increases all the time without bounds. In other words, name a number, then there is an $x$ (very small, meaning very close to $0$) so that $f(x)=1/x$ is bigger than this number you named. This shows that $$\sup_{I_1}f(x)=\lim_{x\to 0}f(x)=+\infty$$ Note: here you have indeed a $\sup$ and not a maximum (since it is attained asymptotically). Now, for the $\inf$ things are simpler. $f$ takes the value $0$ (exactly, not asysmptotically, or anything the like, but exactly), at $x=0$ and is positive for any other value in $I_1$. Hence, $f(0)=0$ is indeed the least value of $f$ in $I_1$, which establishes that $$\inf_{I_1}f(x)=\min_{I_1}f(x)=f(0)=0$$</p>
"
"2389619","2389847","<p>I think the trick here is to work backwards from your answer.</p>

<p>If you expand $f(n,k)$ out you get</p>

<p>$$f(n,k)=\sum_{r=0}^{n-1}(-1)^r\binom{n}{r}k^{n-r}+(-1)^n\binom{n}{n}k\tag{1}$$</p>

<p>I've added the $\binom{n}{n}$ in the last term for consistency with the following interpretation.</p>

<blockquote>
  <p>Hint: This looks very much like an inclusion-exclusion formula.</p>
</blockquote>

<p>If you don't want the rest then please read no further.</p>

<hr>

<p>Define our objects that we want to count as <em>coloured convex $n$-gons with vertices labeled $1$ to $n$</em>. Then define sets $A_{i,j}$ to be <em>those containing coloured $n$-gons with adjacent vertices $i$ and $j$ equal colours</em>.</p>

<p>So we have the general set intersection as the number of colourings of an $n$-gon with $r$ identical coloured pairs of adjacent vertices</p>

<p>$$|A_{i_1,j_1}\cap\cdots \cap A_{i_r,j_r}|=\begin{cases}k^{n-r}&amp; r\lt n\\ k&amp; r=n\end{cases}$$</p>

<p>and by the inclusion-exclusion principle $f(n,k)$ counts coloured $n$-gons belonging to none of those sets i.e. It counts coloured $n$-gons with no two adjacent equal colours.</p>

<p>Another interpretation, still thinking in terms of inclusion-exclusion, is to weight each of the $r$ edges of neighbouring identical coloured vertices with a $-1$. Then the weight of <em>this</em> n-gon is the product of these weights $(-1)^r$ and the right hand summation $(1)$ counts any particular $n$-gon with $r$ identical adjacent pairs of vertices according to each intersection of sets it belongs to, this gives the sum</p>

<p>$$\binom{r}{0}-\binom{r}{1}+\cdots +(-1)^r\binom{r}{r}=\begin{cases}0&amp; r\gt 0\\ 1 &amp;r=0\end{cases}$$</p>
"
"2389622","2390529","<p>In the paper <a href=""https://www.jstor.org/stable/43965918?seq=1#page_scan_tab_contents"" rel=""nofollow noreferrer"">""on hyperspace of compact subsets of $k$-spaces""</a> by Momir Stanojevic, this question is discussed. </p>

<p>It refers to a Russian paper by Popov that gives examples of $X$ that are $k$-spaces such that $\mathcal{K}(X)$ is not a $k$-space, but the author also gives a machine for producing examples:</p>

<p>Let $X_1$ and $X_2$ be $k$-spaces such that $X_1 \times X_2$ is not a $k$-space.
(e.g. see Engelking 3.3.29): $X_1 = \mathbb{R}\setminus\{\frac{1}{n}: n =2,3,4\ldots\}$, $X_2 = \mathbb{R}/\mathbb{N}$ (the positive integers in $\mathbb{R}$ indentified to a point, in the quotient topology). For the argument, see Engelking. We could use other examples, if you happen to know them.</p>

<p>Define $X = X_1 \oplus X_2$, their disjoint topological sum.
Then $X$ is a $k$-space, clearly.</p>

<p>But the subset $C = \{\{x_1,x_2\}: x_1 \in X_1, x_2 \in X_2\}$ is closed in $\mathcal{K}(X)$ and homeomorphic to $X_1 \times X_2$, so not a $k$-space. (check the homeomorphism and the closedness!). </p>

<p>As a closed subspace of a $k$-space is a $k$-space, this implies that $\mathcal{K}(X)$ is <em>not</em> a $k$-space. So $X$ being a $k$-space is not enough.</p>

<p>In fact from standard facts he proves that all powers $X^n$ $n=1,2,\ldots$ have to be $k$-spaces too (here the square $X^2$ fails to be a $k$-space). This one often sees: hyperspaces of $X$ behave like $X^\omega$ in many ways.</p>

<p>Stanojevic also considers iterated hyperspaces $\mathcal{K}^{(1)}(X) = \mathcal{K}(X)$ and $\mathcal{K}^{(n+1)}(X)= \mathcal{K}(\mathcal{K}^{(n)(X)})$.
There is a natural map of unions from $\mathcal{K}^{(n+1)}(X)$ to $\mathcal{K}^{(n)}(X)$ for all $n$ and the inverse limit of the inverse system so obtained, is called $\mathcal{K}^{(\omega)}(X)$.</p>

<p>Then theorem 7 in the quoted paper says that $\mathcal{K}(X)$ is a $k$-space iff for all $n \ge 2$, $\mathcal{K}^{(n)}(X)$ is a $k$-space iff $\mathcal{K}^{(\omega)}(X)$ is a $k$-space. So if such a hyperspace is a $k$-space it gets preserved by the $\mathcal{K}$-operation itself.</p>

<p>An interesting conjecture based on the above (Stanojevic does not ask this, but it seems reasonable in light of his results): </p>

<blockquote>
  <p>If $X^\mathbb{N}$ is a $k$-space does it follow that $\mathcal{K}(X)$ is one too?</p>
</blockquote>

<p>We need at least all the finite powers to be a $k$-space.</p>
"
"2389623","2389631","<p>Using the second method (lower half-plane) you have
$$
I=-2\pi i\left(\frac{1}{(-i+2i)(-i+3i)}+\frac{1}{(-2i+i)(-2i+3i)}+\frac{1}{(-3i+i)(-3i+2i)}\right)=0
$$
so you have $0$ with both methods.
(I'm assuming the integral is over reals)</p>
"
"2389644","2393096","<p>We will  show that  $f(x) &gt; 0$ for $x&gt;1$ and $f(x) &lt; 0$ for $0&lt;x&lt;1$.</p>

<p>Let's start with the first one,   $f(x) &gt; 0$ for $x&gt;1$.</p>

<p>For real $p &gt; 2$, we can use Bernoulli's inequality (which holds for real exponents!):</p>

<p>$(1 + w)^r \ge 1 + r w $,  for real  $w &gt; -1$ and real $r &gt; 1$.</p>

<p>This gives for the first exponent in $f(x)$:</p>

<p>$(x+1)^{p-1} = x^{p-1} (1+1/x)^{p-1} \geq x^{p-1} (1 + \frac{p-1}{x})$</p>

<p>Inserting into $f(x)$ gives </p>

<p>$$f(x) = (x - 1)(x + 1)^{p - 1} - x^p + 1 \\\ge (x - 1)x^{p-1} (1 + \frac{p-1}{x}) - x^p + 1 \\= x^{p-1} (p-2) - x^{p-2} (p-1) + 1 = h(x)$$</p>

<p>with equality  for $x=1$. Now note that $h(x=1) = 0$. Further,</p>

<p>$h'(x) = (p-2)(p-1)x^{p-3} (x-1)$</p>

<p>so for $x&gt;1$, $h'(x) &gt; 0$ everywhere. Hence $h(x) \geq 0$ with equality  for $x=1$. This establishes that for $x\geq 1$, $f(x) \geq 0$ with equality  for $x=1$. </p>

<p>For the second case $0&lt;x&lt;1$, we will see now that this can be reduced to the first case $x&gt;1$, so indeed there is nothing to do.</p>

<p>Let $y =\sqrt x$. Then the function  becomes:</p>

<p>$$
f(y = \sqrt x) = y^p \left[ (y - \frac1y)(y + \frac1y)^{p - 1} - y^p + \dfrac{1}{y^p} \right]
$$</p>

<p>We have shown above that for $y&gt;1$, $f(y) &gt;0$. I.e. we have that the following inequality holds:</p>

<p>$$
g(y) =  (y - \frac1y)(y + \frac1y)^{p - 1} - y^p + \dfrac{1}{y^p}  &gt; 0 \qquad \bf\text{[ineq. 1]}
$$</p>

<p>Now, we want to show that for $0&lt;y&lt;1$, $f(y) &lt;0$. Here, we need  to show</p>

<p>$$
g(y) =(-y + \frac1y)(y + \frac1y)^{p-1} -[- y^p + \dfrac{1}{y^p} ]&gt; 0
$$
Now replacing $y$ by $1/y$ we get $y &gt; 1$ and, in the replaced variable,<br>
$$
\tilde g(y) =  (y - \frac1y)(y + \frac1y)^{p - 1} - y^p + \dfrac{1}{y^p} &gt; 0
$$
So this is the same condition as before in ineq. 1, which has already been proven.</p>

<p>$\qquad \qquad \Box$</p>
"
"2389645","2389646","<p>Hint: $\lim_{x\rightarrow 0^{-}}{x}=0$.</p>

<p>You first evaluate the limit and then take the floor value.</p>
"
"2389656","2389662","<p>You have discovered a variant of the standard functional iteration or fixed point iteration. The standard iteration is $$x_{n+1} = f(x_n).$$ You are using $$x_{n+1} = g(x_n),$$ where $$g(x) = (x + f(x))/2.$$ Both iterations will converge, if $f : I \rightarrow I$ is a contraction, i.e., there exists $L \in [0,1)$, such that
$$ |f(x) - f(y)| \leq L | x-y|$$
for all $x, y \in I$. </p>

<p>It would be interesting to know if there are cases where your iteration converges faster than the standard iteration.</p>

<p>As for efficiency, I recommend that you use the functional iteration to get close to the root, then apply the secant method which converges substantially faster. Ideally, you would merge the secant method with the bisection method to obtain a method which is both fast and robust because it maintains a bracket around the root.</p>
"
"2389667","2389744","<p>Firstly, you can't just push the parts of a (partial) derivative $\partial u/\partial x$ around as you like: $\partial u/\partial x$ is one quantity, that happens to be composed of a number of symbols. This is clearer if you write it as $u_x$ or $u_{,x}$.</p>

<p>The next problem is that while for total derivatives or functions of one variable
$$ \frac{dy}{dx} \frac{dx}{dy} = 1, $$
the same is not the case for partial derivatives, as the following example will illustrate: let
$$ u = x+y, \qquad v=x-y. $$
Then inverting gives
$$ x = \frac{u+v}{2} \qquad y = \frac{u-v}{2}. $$
So,
$$ \frac{\partial u}{\partial x} = 1 \qquad \frac{\partial u}{\partial y} = 1 \\
\frac{\partial x}{\partial u} = \frac{1}{2} \qquad \frac{\partial y}{\partial u} = \frac{1}{2},
$$
and therefore
$$ \frac{\partial u}{\partial x} \frac{\partial x}{\partial u} = \frac{1}{2} \qquad \frac{\partial u}{\partial y} \frac{\partial y}{\partial u} = \frac{1}{2}, $$
so the sum is $1$, but the products are not $1$ individually.</p>

<p>A further instructive example is the even simpler-looking example
$$ u = x + ay, \qquad v = y. $$
Then
$$ x = u-av \qquad y = v. $$
So $ \frac{\partial v}{\partial x} = 0 $, but
$$ \frac{\partial x}{\partial v} = -a. $$
This tells us two interesting things: firstly, what we already know about the product not being $1$. And secondly, that although $y=v$, $\partial x/\partial y = 0 $ ($x$ and $y$ are meant to be independent to start with, after all!), so $ \frac{\partial x}{\partial v} \neq \frac{\partial x}{\partial y} $. What does this mean? It means that the partial derivative very much depends on what is being held constant, in addition to what is allowed to vary.</p>

<p>In the case of $\frac{\partial x}{\partial y}$, the coordinates other than $y$ (namely $x$) is being held constant, so of course this gives zero. But in $\frac{\partial x}{\partial v}$, $u$ is being held constant, and since this is not the same as $x$, the answer is different. This is one of the most difficult, and most important, things to understand about partial derivatives. There's a nice illustration of this on <a href=""https://books.google.co.uk/books?id=VWTNCwAAQBAJ&amp;lpg=PA189&amp;pg=PA190#v=onepage&amp;q&amp;f=false"" rel=""nofollow noreferrer"">p.190 of Penrose's <em>Road to Reality</em></a>.</p>

<p>The correct approach is to apply the chain rule for partial derivatives, namely that if $f$ is a function of $x,y,z$, which in turn are functions of $u$ and other variables, then
$$ \frac{\partial f}{\partial u} = \frac{\partial x}{\partial u}\frac{\partial f}{\partial x} + \frac{\partial y}{\partial u}\frac{\partial f}{\partial y} + \frac{\partial z}{\partial u}\frac{\partial f}{\partial z}. $$
If then $f=u$, the result follows.</p>
"
"2389674","2390645","<p>Assuming that the plane passes through the origin, this problem comes down to finding the intersection of the plane with the nappe of the cone defined by $\mathbf v$ and $\mathbf r$. This intersection will consist of up to two rays. Finding unit vectors in the directions of these rays is easy, from which vectors you can get the corresponding rotation angles or otherwise construct the rotation using your favorite method.  </p>

<p>The first thing to do is to check whether or not there is a non-trivial intersection. This can be done by comparing the angles between the rotation axis $\mathbf r$, and $\mathbf v$ and the plane represented by $\mathbf n$, respectively, or, more conveniently, their cosines. Since weâre working with unit vectors, there will be an intersection when $$(\mathbf v\cdot\mathbf r)^2\le1-(\mathbf n\cdot\mathbf r)^2.$$</p>

<p>You can solve the cone-plane intersection problem directly, but I find it easier to transform everything so that the cone is aligned with the $z$-axis. A rotation will do the trick, but I think itâs faster and easier to use a Householder reflection instead. So, compute the transformation matrix $$H=I-2{\mathbf a\mathbf a^T\over\mathbf a^T\mathbf a}$$ where $\mathbf a=\mathbf r-(0,0,1)^T$. This is a reflection in the plane that bisects the angle between $\mathbf v$ and $\mathbf r$. The advantage of using this transformation instead of a rotation is that $H=H^T=H^{-1}=H^{-T}$, so the same matrix can be used to transform back to the original coordinate system and to transform the normal vector $\mathbf n$. (Thatâs a covariant vector, so if points are transformed as $M\mathbf p$, it transforms as $M^{-T}\mathbf n$.) In practice, you might not need to construct this matrix, since you can directly compute $H\mathbf p=\mathbf p-2{\mathbf p\cdot\mathbf a\over\mathbf a\cdot\mathbf a}\mathbf a$ and only need to map a few vectors.  </p>

<p>Set $\mathbf v'=H\mathbf v$ and $\mathbf n'=H\mathbf n$. The vectors that weâre looking for are the intersections of the transformed plane with the circle $x'^2+y'^2=1-(z_{\mathbf v}')^2$, $z'=z_{\mathbf v}'$ (i.e., the circle that is the intersection of the transformed cone nappe with the plane $z'=z_{\mathbf v}'$). Note, by the way, that $z_{\mathbf v}'=\mathbf v\cdot\mathbf r$. Following the method described <a href=""https://math.stackexchange.com/a/2387489/265466"">here</a>, reduce this to a two-dimensional problem by computing the intersection of the transformed target plane with $z'=z_{\mathbf v}'$ and project onto the $x'$-$y'$ plane. For the present problem, this amounts to multiplying the third coordinate of $\mathbf n'$ by $z_{\mathbf v}'$, i.e., $\mathbf l=\operatorname{diag}(1,1,z_{\mathbf v}')\,\mathbf n'$. Proceeding as in the linked answer, find the intersection of this line with the circle $x'^2+y'^2=1-(z_{\mathbf v}')^2$ and project back onto the $z'=z_{\mathbf v}'$ plane to get unit vectors $\mathbf w_1'$ and $\mathbf w_2'$ (which might be identical).  </p>

<p>With these vectors in hand, you can proceed in several ways to derive the corresponding rotations. The rotation angle required to take $\mathbf v$ to $\mathbf w_i=H\mathbf w_i'$ is just the negation of the angle between the projections of $\mathbf v'$ and $\mathbf w_i'$ onto the $x'$-$y'$ plane (negated because $H$ is orientation-reversing), and once you have this angle, you can use well-known formulas to construct an appropriate rotation matrix or quaternion. You could also construct a rotation matrix $R'$ in the transformed frame, which will just be a rotation about the $z'$-axis, and then transform it back to the original coordinate system: $R=HR'H$. (This is another instance in which the identity $H=H^T=H^{-1}=H^{-T}$ comes in handy.)  </p>

<p>For numerical stability, you might want to align the rotation axis with one of the other coordinate axes. The best choice in that regard is the standard basis vector $\mathbf e_i$ which maximizes $\|\mathbf r-\mathbf e_i\|$. Getting back to the above solution is a simple matter of permuting coordinates after applying $H$, but be sure you keep track of orientation changes if youâre going to use the computed rotation angles directly.</p>
"
"2389678","2389696","<p><strong>For the part that you were missing/asking for Question 2:</strong></p>

<p>Since $B$ is symmetric $B=U^tDU$ for some diagonal matrix $D$ and some orthogonal matrix $U$.</p>

<p>If $E$ is invariant for $B$ then $U(E)$ is invariant for $D$. Therefore $D$ has an eigenvector $u$ there. It follows that $U^tu$ is an eigenvector for $B$ and belongs to $E$. Therefore is is also an eigenvector for $A$.</p>

<p>It is interesting that this part didn't need that $A$ was symmetric. But you used it to ascertain that $A$ does have an eigenvector. Therefore in question 2 one only needs to know that $A$ has an eigenvector.</p>

<p><strong>Question 1:</strong></p>

<p>Assume all eigenvalues of $A$ are zero and $E$ is its kernel. Then $B(E)\subset E$. Therefore there is an eigenvector of $B$ in $E$ and that would be a common eigenvector for $A$ and $B$. It would correspond to the eigenvalues $0$ for $A$ and perhaps to some other eigenvalue for $B$.</p>

<p>Assume now that not all eigenvalues of $A$ are zero. Let $r\neq0$ be one non-zero eigenvalue of $A$ and $E$ its eigenspace.</p>

<p>Then for all $v\in E$, $ABv=BA^2v=r^2Bv$. If $Bv=0$ for some non-zero $v\in E$ we are done, $v$ is eigenvector of $B$ and of $A$. </p>

<p>Assume then that $Bv\neq0$ for all $0\neq v\in E$. Therefore $r^2$ is eigenvalue of $A$. Now, for $0\neq v\in E$, $AB^2v=BA^2Bv=B^2A^4v=r^4B^2v$. Again, if $B^2v=0$ then $Bv$ is a common eigenvector of $A$ and $B$, of eigenvalue $r^2$ for $A$ and $0$ for $B$. If you assume that $B^2v\neq0$ for all such $v$, then $r^4$ is an eigevalue of $A$.</p>

<p>Continuing in this way we would find either a common eigevector of $A$ and $B$, or a sequence $r,r^2,r^4,...$ of eigenvalues of $A$. Since there are only finitely many eigenvalues we must have $r^m=r^s$ for some $m\neq s$. Since $|r|\neq1$ we must have $r=0$. But we assumed $r\neq0$. Therefore, in this process we should always find a common eigenvalue.</p>
"
"2389679","2389863","<p>The topological def'n is that $f:X\to Y$ is continuous iff $f^{-1}V$ is open in $X$ whenever $V$ is open in $Y.$ Regardless of whether the topologies can be defined in terms of sequences. There are many consequences of the continuity of $f$ that also imply the continuity of $f,$ and could be taken as alternate, equivalent def'ns. One of these is :  $$f(Cl_X(A))\subset Cl_Y(f(A)) \;\text {for all } A\subset X.$$</p>

<p>(i). Suppose $f^{-1}V$ is open in $X$ whenever $V$ is open in $Y$.  Then for $A\subset X,$ the set $C=Y \backslash Cl_Y(f(A))$ is open in $Y,$ so $f^{-1}C$ is open in $X,$ and $(f^{-1}C)\cap A=\phi.$ </p>

<p>Now for any open set $P,$ if $P\cap Q=\phi$ then $P\cap \bar Q=\phi.$ So $(f^{-1}C)\cap \bar A=\phi.$ Therefore  $f(\bar A)\subset Y \backslash C=Cl_Y(f(A)).$</p>

<p>(ii). I will leave the converse of (i) to you.</p>

<p>By (i), if $f:E\to F$ is a continuous closed map and $A\subset E$ then    $$f(\bar A)\subset \overline {f(A)}\subset \overline {f(\bar A)} \;\text { by continuity of } f$$ $$\text { and }\quad  f(\bar A)= \overline {f(\bar A)}\; \text { by closedness of  } f$$ $$\text {so }\quad \overline {f(A)}=f(\bar A).$$</p>
"
"2389686","2389821","<p>As you correctly surmise, you can of course not just pick any constant to instantiate an existential with. Indeed, after having instantiated the universal with $m$, we can of course not guarantee that the 'something' that has property $P$ is that very $m$.  And indeed in general we cannot instantiate the existential with any constant (or any variable-free term) that we are already using elsewhere. </p>

<p>This is why we have to pick a constant that we are <em>not</em> using elsewhere when instantiating an existential: it is our way of saying: ""well, we know <em>something</em> has such-and-such a property, but we don't know who or what it is, so let's just give it a name, so we can at least refer to it by a name"".</p>

<p>Finally, note that just because we use a new constant does not mean that it has to be different from any of the other constants already in use. That is, later on we may realize that $n=m$. But to be safe, and keep all possibilities open, we have to use a new constant.</p>
"
"2389691","2389788","<p>It suffices to show that $f$ is a closed map. Let $A$ be a closed subset $X$. Then $A$ is countably compact, so $f(A)$ is countably compact too. Therefore it is a closed subset of a first countable $T_2$ space $Y$. Indeed, assume the converse, there exists a point $y\in\overline{f(A)}\setminus f(A)$. Let $U_n$ be a countable base at the point $y$
such that $U_{n+1}\subset U_n$ for each $n$. For each $n$ pick a point $y_n\in U_n\cap f(A)$. The latter condition implies that the sequence $\{y_n\}$ converges to the point $y$. On the other hand, the set $f(A)$ is countably compact, so the set $\{y_n\}$ has a cluster point $yâ\in f(A)$. But since the space $Y$ is $T_2$ and the sequence $\{y_n\}$ converges to the point $y$, we should have $yâ=y$. But $yâ\ne y$ because $yâ\in f(A)$, whereas $y\not\in f(A)$, a contradiction.</p>
"
"2389708","2389778","<p>I understtod ""simply ordered"" as ""linearly ordered"". Then Claim a is a corollary of Claim b, and the latter follows from the next proposition from Engelkingâs âGeneral topologyâ</p>

<p><a href=""https://i.stack.imgur.com/80LW6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/80LW6.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/UzsTA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UzsTA.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/aBMrw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aBMrw.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/xLhSD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xLhSD.png"" alt=""enter image description here""></a></p>
"
"2389709","2389730","<p>Given $\varepsilon$, which is probably to be thought of as a small number, this expression (call it $T$) is the largest value of $t$ (perhaps that's time) for which the probability that $\tilde S(t) &lt; \Psi(t)$ is less than $\varepsilon$, i.e. small.</p>

<p>That is, whenever $t &lt; T$ you know that the probability that $\tilde S(t) &lt; \Psi(t)$ is small. For larger values of $t$ you don't have that guarantee.</p>

<p>(Strictly speaking, you might have an equality rather than an inequality at that value of $t$.)</p>
"
"2389719","2389908","<p><strong>Imbalance.</strong> I will begin by addressing your misgivings about grossly
different sample sizes.</p>

<p>A 'balanced' experiment in which the numbers $n_1$ and $n_2$ of subjects
in the two groups are equal is most efficient. ""A chain is only a strong as it's
weakest link."" For you, group A with $n_1 = 984$ subjects is the weak link, compared
with group B with $n_2 = 1974.$ So, <em>very</em> roughly speaking, you would
have about as much 'power' to distinguish between groups if $n_2 \approx 984.$
But a somewhat suboptimal design hardly makes the data worthless.</p>

<p><strong>Test for difference of two proportions.</strong> This situation requires a standard test for comparing two proportions. You want to
test $H_0: p_1 = p_2$ against $H_a: p_1 \ne p_2,$ where $p_1$ and $p_2$
are the proportions of positive responses in the <em>populations</em>
represented by groups A and B, respectively. You can find versions of this
test based on normal approximations to the binomial distributions in most
elementary texts. With sample sizes as large as yours, the normal
approximation should work fine.</p>

<p>Versions of the test vary slightly from one textbook
to another. I will show you results from Minitab 17 software for one of the versions
and also for Fisher's Exact Test.
You have about $X_1 = 0.107(984) = 105$ positive results in Group A and 
$X_2 = 0.062(1974) = 122$ positive results in Group B. (These numbers
are necessary input to the Minitab procedure; because of rounded proportions
I cannot recover these counts exactly, but close enough for a valid test.)</p>

<pre><code>Test and CI for Two Proportions 

Sample    X     N  Sample p
1       108   984  0.109756
2       122  1974  0.061803

Difference = p (1) - p (2)
Estimate for difference:  0.0479527
95% CI for difference:  (0.0257200, 0.0701853)
Test for difference = 0 (vs â  0):  Z = 4.23  P-Value = 0.000

Fisherâs exact test: P-Value = 0.000
</code></pre>

<p>The very small P-values (below 0.0005) indicate that you can reject $H_0$
at any reasonable significance level. That is, it would be extremely
unlikely to get sample proportions as far apart as observed here if the populations
A and B had equal proportions of positive results.</p>

<p>[Also notice the confidence interval given in the output. It does not contain $0.$
This is an indication that the two groups are dissimilar as to proportion of
positive results.]</p>

<p>So the bottom line is that, although the design is technically somewhat inefficient, you have plenty of information in these data to reach a
solid statistical conclusion.</p>

<p>I hope you can find a version of this test comparing two proportions in
your textbook. The result is sufficiently strong that it shouldn't
make any difference which version you use. </p>
"
"2389726","2389731","<p>Yes. If you take out one axiom (the <a href=""https://en.wikipedia.org/wiki/Axiom_of_regularity"" rel=""noreferrer"">Axiom of Regularity</a>, which roughly disallows sets to be nested inside themselves), then ZFC is perfectly happy with the existence of sets $x$ such that $x = \{x\}$. Such sets $x$ are usually known as <a href=""https://en.wikipedia.org/wiki/Urelement#Quine_atoms"" rel=""noreferrer""><strong>Quine atoms</strong></a>.</p>

<p>In fact there are many well-known set theories that explicitly allow the existence of Quine atoms, sometimes as a matter of principle --- <a href=""https://en.wikipedia.org/wiki/New_Foundations"" rel=""noreferrer"">New Foundations</a>, for instance.</p>

<p>Of course it will never be the case for <strong><em>all</em></strong> $x$ that $x = \{x\}$. This would give a contradiction. Specifically, we can prove that $\varnothing \ne \{\varnothing\}$. If you wanted to change this, you would have to change the very definition of membership or equality of sets.</p>

<p>Given your intended computer programming application, I will also mention that there are programming languages which treat $x$ and $\{x\}$ as the same thing. This is not a problem, there is no contradiction, because ""sets"" in these languages are much more restricted than the sets of set theory. Typically, $\{\varnothing\}$ will not be an allowed set, and in general there is only a single level of nesting (i.e., no sets-of-sets).</p>
"
"2389735","2389797","<p>Let's assume you have a population of size $N$ with values $x_1,\ldots,x_N$, mean $\bar x=\frac{1}{N}\sum_{i=1}^N x_i$ and variance $\sigma^2=\frac{1}{N}\sum_{i=1}^N(x_i-\bar x)^2$. (Note that I use lower case $x_i$ to indicate these are not random, but fixed values.)</p>

<p>Now, let's take a random sample $Y_1,\ldots,Y_n$ of $n$ elements (without replacement), with all such subsets equally likely. (Now I use capital $Y$ to indicate these are random.)</p>

<p>Now, $\bar Y=\frac{1}{n}\sum_{i=1}^n Y_i$ and let $V=\sum_{i=1}^n (Y_i-\bar Y)^2$ so that the sample variance would be $V/n$ (like the expression for $\sigma^2$). If we write $V$ out in terms of $(Y_i-\bar x)^2$ and $(Y_i-\bar x)(Y_j-\bar x)$, we get
$$
\begin{split}
V
=&amp; \sum_{i=1}^n (Y_i-\bar Y)^2
= \sum_{i=1}^n \left[(Y_i-\bar x)-(\bar Y-\bar x)\right]^2 \\
=&amp; \sum_{i=1}^n \left[(Y_i-\bar x)^2-2(Y_i-\bar x)(\bar Y-\bar x)+(\bar Y-\bar x)^2 \right] \\
=&amp; \sum_{i=1}^n (Y_i-\bar x)^2 - n(\bar Y-\bar x)^2 \\
=&amp; \left(1-\frac{1}{n} \right) \sum_{i=1}^n (Y_i-\bar x)^2
  -\frac{2}{n}\sum_{1\le i&lt;j\le n} (Y_i-\bar x)(Y_j-\bar x)
\end{split}
$$
where in the last step we use that
$$
\left(\sum_{i=1}^n (Y_i-\bar x)\right)^2
= \sum_{i=1}^n (Y_i-\bar x)^2 + 2\sum_{1\le i&lt;j\le n} (Y_i-\bar x)(Y_j-\bar x).
$$</p>

<p>We know that $\text{E}[(Y_i-\bar x)^2]=\sigma^2$: this is just taking the average of $(Y_i-\bar x)^2$ for $Y_i$ sampled from $x_1,\ldots,N$.</p>

<p>For $i&lt;j$, we can compute $\text{E}[(Y_i-\bar x)(Y_j-\bar x)]$ by using that this is the same as the average of $(x_i-\bar x)(x_j-\bar x)$ for all $1\le i&lt;j\le N$. Since $\sum_{i=1}^N (x_i-\bar x)=0$, we get
$$
0 = \sum_{1\le i,j\le N} (x_i-\bar x)(x_j-\bar x)
= \sum_{i=1}^N (x_i-\bar x)^2 + 2\sum_{1\le i&lt;j\le N} (x_i-\bar x)(x_j-\bar x)
$$
which for $i&lt;j$ makes
$$
\text{E}\left[(Y_i-\bar x)(Y_j-\bar x)\right]
= -\frac{\sigma^2}{N-1}.
$$
Combining these results, we get
$$
\text{E}[V] = (n-1)\sigma^2 + \frac{n-1}{N-1}\sigma^2
= \frac{(n-1)N}{N-1}\sigma^2
$$
giving an unbiased estimator
$$
\hat\sigma^2 = \frac{N-1}{N(n-1)}V
= \frac{N-1}{N(n-1)} \sum_{i=1}^n (Y_i-\bar Y)^2.
$$</p>

<p>As $N\rightarrow\infty$, you get the familiar $s^2$ estimator which corresponds to independent sampling from a distribution, while $n=N$ gives just $\sigma^2$ as it should when the $x_i$ are known for the whole population.</p>
"
"2389747","2389766","<p>Let $(x_n)_n$ be a sequence in $X$.</p>

<p>If $x_n\to x$, then $x$ is a cluster point of $(x_n)$. To see that this is the only one, suppose $y$ is another cluster point of $(x_n)$ and fix a countable base $\{U_n \mid n\in\mathbb{N}\}$ of neighborhoods of $y$ such that $U_{n+1}\subseteq U_n$ for each $n$. Choose $n_1$ such that $x_{n_1}\in U_1$, and having found $n_1&lt;\cdots&lt;n_k$, choose $n_{k+1}&gt;n_k$ such that $x_{n_{k+1}}\in U_{k+1}$. This defines a subsequence $(x_{n_k})_k$ of $(x_n)$ such that $x_{n_k}\to y$. Then we also have $x_{n_k}\to x$, so the fact that $X$ is Hausdorff implies that $y=x$. Therefore $(x_n)$ has only one cluster point.</p>

<p>Conversely suppose that $x$ is the only cluster point of $(x_n)_n$ and assume by contradiction that $x_n\not\to x$. Then there is neighborhood $U$ of $x$ such that $\mathbb{N}_1:=\{n\in\mathbb{N} \mid x_n\not\in U\}$ is an infinite set. Consider the subsequence $(x_n)_{n\in\mathbb{N}_1}$ of $(x_n)_{n\in\mathbb{N}}$. Since $X$ is countably compact, this subsequence must have a cluster point $y$. Hence $y$ is a cluster point of the original sequence. As $x$ is not a cluster point of $(x_n)_{n\in\mathbb{N}_1}$, we have $x\ne y$. This contradicts the assumption that there is only one cluster point. We conclude $x_n\to x$.</p>
"
"2389749","2390837","<p>The following result is quite elementary, but the algebraic techniques will be reused.</p>

<p>Lemma 1: For all real numbers $a$ and $b$,<br>
$\tag 1 | a + b | \le |a| + |b|$
Moreover, if both $a$ and $b$ are nonzero, then equality holds exactly when both the numbers have the same sign.</p>

<p>Proof<br>
Use the $|c| = \sqrt {c^2}$ formulation for absolute value and after squaring both sides, (1) is deduced from algebra and elementary arithmetic laws.</p>

<p>Lemma 2: For all real numbers $x_1 \text{, } y_1$ and $ x_2 \text{, } y_2$</p>

<p>$\tag 2 \left(x_1^2 + y_1^2\right)\left(x_2^2 + y_2^2\right) - (x_1 x_2 + y_1 y_2)^2 = (x_1 y_2 - x_2 y_1)^2$<br></p>

<p>Proof: Equation (1) is an algebraic identity. QED</p>

<p>Using the Pythagorean distance norm for vectors, the triangle inequality emerges,</p>

<p>Proposition 3: For any two nonzero vectors $\vec v$ and $\vec w$ in $\mathbb{R}^2$,</p>

<p>$\tag 3 \|\vec v + \vec w \| \le \| \vec v \| + \| \vec w\|$</p>

<p>Moreover, equality occurs if and only if there exists a unique $\lambda \gt 0$ such that $\vec v = \lambda \vec w$.</p>

<p>Proof<br>
To prove (3), set $\vec v = (x_1,y_1)^t$ and $\vec w = (x_2,y_2)^t$, and apply Lemma 2 with some algebraic technique.<br></p>

<p>For the second part, begin by assuming both sides of (3) are equal. Then, by Lemma 2, it must be true that<br>
$\tag 4 x_1 y_2 - x_2 y_1 = 0$</p>

<p>Since both $\vec v$ and $\vec w$ are nonzero vectors, if say $x_1 = 0$, then $x_2 = 0$ and Lemma 1 can be used. So we can assume that the numbers  $x_1$, $y_1$ and $x_2$, $y_2$ are all nonzero. But then $y_1/y_2 = x_1/x_2$ and by setting $\lambda = y_1/y_2$, (4) will hold true. It can be easily checked that $\lambda$ must be positive for (3) to become an equality.<br></p>

<p>To show that (3) becomes an equality when $\vec v = \lambda \vec w$ for positive $\lambda$ is straightforward. QED</p>

<p>Since the distance between points is invariant under translations, the OP's question can be equivalently recast as follows,</p>

<p>Proposition 4: Let $\vec u$ be a nonzero vector in $\mathbb{R}^2$. Then the line segment connecting $\vec 0$ with $\vec u$ is equal to</p>

<p>$\tag 4 S=\{ \vec s \in \mathbb{R}^2 : \|\vec u\| = \|\vec s\|  + \| \vec u - \vec s\|   \}$</p>

<p>Proof</p>

<p>The line segment is equal to $\{ \alpha \vec u : 0 \le \alpha \le 1 \}$. Apply Proposition 3 and some more algebra to wrap up this proof. QED</p>

<hr>

<p>Observe that the question was answered without using trigonometry or facts about the dot product. But the 'dot product algebra' is encountered in the above derivations, and is a natural way to 'discover' the definition of the inner product.</p>
"
"2389754","2389772","<p>Yes, that's the content of <a href=""https://en.wikipedia.org/wiki/Carath%C3%A9odory%27s_theorem_(convex_hull)"" rel=""nofollow noreferrer"">CarathÃ©odory's theorem</a>.</p>

<blockquote>
  <p>If $S \subseteq \mathbb R^d$ and $x \in \operatorname{conv}(S)$, then there are $x_1, \dots, x_{d+1} \in S$ such that $x \in \operatorname{conv}(x_1, \dots, x_{d+1})$.</p>
</blockquote>
"
"2389762","2389994","<p>For a general category $C$, there is in general no way to define a funtor $C\to C^{op}$. For this problem, you really need to use the fact that your arrows are relations between sets to construct such a functor; it will exist because every relation $R:X\to Y$ has an ""opposite relation"" $R^{o}:Y\to X$ defined by $yR^{o}x\Leftrightarrow xRy$. You can check that this defines a functor $\mathbf{Rel}\to \mathbf{Rel}^{op}$ (which is the identity on objects). You can also check that it is bijective; in fact, it is its own inverse (or rather, its dual is). This is makes it an example of <a href=""https://ncatlab.org/nlab/show/dagger+category"" rel=""nofollow noreferrer"">dagger category</a>. </p>
"
"2389765","2389767","<p>Given $\epsilon &gt; 0$, for large enough $N$, more than $1-\epsilon$ of the integers in $[1,N]$ will have at least one $0$.  That is, the fraction of $d$-digit integers with no $0$ is $(9/10)^{d-1}$, which goes to $0$ as $d \to \infty$.</p>
"
"2389785","2390030","<p>Given an algebra $A$, an automorphism $\phi$ is a linear operator satisfying $\phi(xy)=\phi(x)\phi(y)$. There is a more general notion called an <strong>isotopy</strong> $(\alpha,\beta,\gamma)$ such that $xy=z\Rightarrow\alpha(x)\beta(y)=\gamma(z)$. We will call this $\mathrm{Iso}_1(\mathbb{O})$ in the case of the octonions $\mathbb{O}$, and a priori restrict to $\alpha,\beta,\gamma\in\mathrm{SO}(8)$. (If I recall correctly, they are all in $\mathrm{SO}(8)$ up to some scalars $\lambda,\mu,\eta$ such that $\lambda\mu\eta=1$ I guess I'll leave this as an exercise for the enterprising). Observe this is a subgroup of $\mathrm{SO}(8)^3$, because given any pair $(\alpha_1,\beta_1,\gamma_1)$ and $(\alpha_2,\beta_2,\gamma_2)$ we have</p>

<p>$$\begin{array}{ll} (\gamma_1\gamma_2)(xy) &amp; =\gamma_1(\gamma_2(xy)) \\ &amp; =\gamma_1(\alpha_2(x)\beta_2(y)) \\ &amp; =\alpha_1(\alpha_1(x))\beta_1(\beta_2(y)) \\ &amp; =(\alpha_1\alpha_2)(x)\,(\beta_1\beta_2)(y) \end{array}$$</p>

<p>and thus $(\alpha_1\alpha_2,\beta_1\beta_2,\gamma_1\gamma_2)\in\mathrm{Iso}_1(\mathbb{O})$ as well. (I will let you check inverses.)</p>

<p>Replacing $z$ with $\bar{z}$ in the equation $xy=z$, we see the result is equivalent to both $(xy)z=1$ and $x(yz)=1$ so we may unambiguously write $xyz=1$. An equivalent form of isotopies is then the group of all $(\alpha,\beta,\gamma)\subset\mathrm{SO}(8)^3$ such that $xyz=1\Rightarrow\alpha(x)\beta(y)\gamma(z)=1$. Call it $\mathrm{Iso}_2(\mathbb{O})$.</p>

<p>These are isomorphic $\mathrm{Iso}_1(\mathbb{O})\cong\mathrm{Iso}_2(\mathbb{O})$ via $(\alpha,\beta,\gamma)\leftrightarrow(\alpha,\beta,{}^C\gamma)$, where $C(z):=\overline{z}$ is octonion conjugation and ${}^C\gamma:=C\circ\gamma\circ C^{-1}$ is group-theoretic conjugation, because $xyz=1\Leftrightarrow xy=\bar{z}$ and $\alpha(x)\beta(y)\gamma(z)=1\Leftrightarrow\alpha(x)\beta(y)=\overline{\gamma(z)}$ (replace $z$ with $\overline{z}$ in latter of each iff).</p>

<p>There is also a third manifestation. For real vector spaces, a duality is a nondegenerate bilinear pairing $V_1\times V_2\to\mathbb{R}$. A <strong>triality</strong> is a trilinear mapping $V_1\times V_2\times V_3\to\mathbb{R}$ for which fixing one argument yields a duality of the other two vector spaces. Without loss of generality, $V_1,V_2,V_3$ are the same inner product space and dualizing yields an equivalent multiplication operation $V\otimes V\to V$; the corresponding condition is that it makes $V$ a division algebra.</p>

<p>In the case of $\mathbb{O}$, multiplication gives the triality $t(x,y,z)=\langle xy,z\rangle$ (up to conjugation on $z$ this is the same as the more symmetric form $\langle xy,\bar{z}\rangle=\mathrm{Re}(xyz)$). The symmetries of the triality $t$ are all $(\alpha,\beta,\gamma)\in\mathrm{SO}(8)^3$ such that $t(\alpha(x),\beta(y),\gamma(z))=t(x,y,z)$. Call this $\mathrm{Iso}_3(\mathbb{O})$. Again I don't recall offhand why, but this is isomorphic to the other two isotopy groups. (Another exercise for readers.)</p>

<p>In any case, let's just look at $\mathrm{Iso}_1(\mathbb{O})$. One of the <strong>Moufang identities</strong>, $(ax)(ya)=a(xy)a$, is the same as saying $(L_a,R_a,B_a)\in\mathrm{Iso}_1(\mathbb{O})$. Since $B_a$s generate $\mathrm{SO}(8)$, the group homomorphism $\mathrm{Iso}_1(\mathbb{O})\to\mathrm{SO}(8)$ given by $(\alpha,\beta,\gamma)\mapsto\gamma$ is onto. The kernel is $\mathbb{Z}_2$ with nontrivial element $(-I,-I,I)$ (another exercise for readers), so the map is $2$-to-$1$. With some work one can show it is connected and conclude $\mathrm{Iso}_1(\mathbb{O})=\mathrm{Spin}(8)$, but that is actually not necessary for your goal.</p>

<p>There is an action of $S_3$ on $\mathrm{Iso}_2(\mathbb{O})$ by automorphisms (in fact this gives all of $\mathrm{Out}(\mathrm{Spin}(8))\cong S_3$, but again you don't need this). One of the $3$-cycles acts by $(\alpha,\beta,\gamma)\mapsto(\gamma,\alpha,\beta)$, and one of the $2$-cycles acts by $(\alpha,\beta,\gamma)\mapsto({}^C\beta,{}^C\alpha,{}^C\gamma)$. This is because $xyz=1\Leftrightarrow zxy=1$ (apply $z$ on left and $\bar{z}$ on right), and conjugating $xyz=1\Leftrightarrow \alpha(x)\beta(y)\gamma(z)=1$ yields $$\bar{z}\bar{y}\bar{z}=1\Leftrightarrow\overline{\gamma(z)}\,\,\overline{\beta(y)}\,\,\overline{\alpha(x)}=1$$</p>

<p>(then replace $\bar{z},\bar{y},\bar{x}$ with $x,y,z$ respectively). Thus, $S_3$ acts by permuting the coordinates, and conjugating each coordinate by $C$ in the case of odd permutations (i.e. transpositions). This fact about outer automorphisms being $S_3$ is also called <strong>triality</strong>.</p>

<p>The set of all $L_a$s under one such automorphism gets turned into the set of all $B_a$s, and therefore the $L_a$s are just as much a generating set as  the $B_a$s are. Interestingly, if you restrict to pure imaginary $a$s, then $L_a^2=-1$ yields a representation of $\mathcal{C}\ell(\mathrm{Im}(\mathbb{O}))$ on $\mathbb{O}$, hence an action of $\mathrm{Spin}(7)$ on $\mathbb{R}^8$. This doesn't match the usual action of it on $\mathbb{R}\oplus\mathbb{R}^7$ (corresponding to the $B_a$s since they generate reflections which preserve the real axis), this new action acts transitively on $S^7\subset\mathbb{O}$ (with point-stabilizer $G_2=\mathrm{Aut}(\mathbb{O})$). Using $R_a$s instead of $L_a$s gives yet a third representation of $\mathrm{Spin}(7)$ on $\mathbb{R}^8$.</p>

<p>If we call these three representations $V_1,V_2,V_3$ then the aforementioned Moufang identity (dualized) tells us that the triality $V_1\otimes V_2\otimes V_3\to\mathbb{R}$ is a morphism of $\mathrm{Spin}(7)$-reps, where $\mathbb{R}$ is the trivial rep. (Note that dualizing a map $A\otimes B\to C$ means having a corresponding map $A\to B^{\ast}\otimes C$ or vice-versa, and similarly for more spaces being tensored. An inner product is equivalent to an isomorphism $B\cong B^{\ast}$.)</p>

<p>Finally, since I always wondered, here is a way to think about how to deduce the Moufang identities from scratch. Suppose we've already established alternativity (which isn't too hard using the alternator) and that octonions are not associative. These cover all possible identities between three variables $a,b,c$ in which each variable only shows up once. But what about the next step, where there are four terms and one of the variables shows up twice? Write out all possible nonassociative monomials with two $a$s, a $b$ and a $c$.</p>

<p>With some quick detective work we can rule some out (from $\mathbb{O}$ being nonassociative), prove others (from $\mathbb{O}$ being alternative), and conclude many of them are equivalent (by conjugating equations and replacing variables with their conjugates). The resulting identities left to check are the Moufang identities (IIRC).</p>

<p>To check e.g. $(ab)(ca)=a(bc)a$, it's useful to invoke a series of reductions to simplify the problem: wlog $b$ is a unit imaginary, wlog $c$ is a unit imaginary, wlog $b$ and $c$ are orthogonal, decompose $a=a_{\|}+a_{\perp}$ with respect to the quaternionic subalgebra $\mathrm{span}\{1,b,c,bc\}$, wlog $a_{\perp}\ne0$, $a$ may be normalized so $|a_{\perp}|=1$ and therefore $\{b,c,a_{\perp}\}$ act like $\{i,j,\ell\}$ and a single check of $\mathbb{O}$s multiplication table (following from the Cayley-Dixson laws) is enough to finish.</p>

<p>Some sources off the top of my head,</p>

<ul>
<li><a href=""https://arxiv.org/abs/math/0105155"" rel=""nofollow noreferrer""><em>The Octonions</em></a> by John Baez</li>
<li><a href=""https://services.math.duke.edu/~bryant/Spinors.pdf"" rel=""nofollow noreferrer""><em>Remarks on Spinors in Low Dimension</em></a> by Robert Bryant</li>
<li><a href=""https://mathoverflow.net/questions/116666/triality-of-spin8""><em>Triality of Spin(8)</em></a>, MathOverflow thread</li>
</ul>
"
"2389791","2389953","<p>We can use spherical coordinates with the positive x-axis as the pole. Let $r$ be the distance from the origin, $\phi$ be the (polar) angle from the positive x-axis, and $\theta$ be the (azumithal) angle in the yz-plane (note that the integral will be symmetric in $\theta$) .Our function to integrate is thus simply $r$. The region of integration is a sphere of radius $\frac 12$ whose center is at $x=\frac 12$ on the x-axis. We can write the integral using the volume differential $dV=r^2\sin(\phi)\ d\phi\ d\theta\ dr$.</p>

<p>$$I=\int_0^1\int_0^{2\pi}\int_0^{\phi_0(r)}r^3\sin(\phi)\ d\phi\ d\theta\ dr$$</p>

<p>Where $\phi_0(r)$ is the  $\phi$ coordinate of the boundary of the sphere for fixed $r$. We can find it using the law of cosines, drawing a triangle whose vertices are the origin, the center of the sphere, and a point on the surface of the sphere a distance $r$ from the origin.</p>

<p><a href=""https://i.stack.imgur.com/21GpG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/21GpG.png"" alt=""""></a></p>

<p>$$\left(\frac 12\right)^2=r^2+\left(\frac 12\right)^2-2r\left(\frac 12\right)\cos(\phi_0(r))$$</p>

<p>$$\phi_0(r)=\arccos(r)$$</p>

<p>Separating the integral:</p>

<p>$$I=2\pi\int_0^1r^3dr\int_0^{\arccos(r)}\sin(\phi)d\phi$$</p>

<p>From here, the calculus is reasonably straightforward.</p>

<p>$$I=2\pi\int_0^1r^3[-\cos(\phi)]_{\phi=0}^{\phi=\arccos(r)}dr$$</p>

<p>$$I=2\pi\int_0^1r^3\left[1-r\right]dr$$</p>

<p>$$I=2\pi\int_0^1\left[r^3-r^4\right]dr$$</p>

<p>$$I=2\pi\left[\frac 14r^4-\frac 15r^5\right]_{r=0}^{r=1}$$</p>

<p>$$I=\frac \pi{10}$$</p>
"
"2389803","2389814","<p>Yes. Let $a_n=1/\ln n$ for $n&gt;1.$ For any $t&gt;0$ there are only finitely many $n\in \mathbb N$ for which $(\ln n)^t\geq n. $ So $(a_n)^t&gt;1/n$ for all but finitely many $n\in \mathbb N$.</p>
"
"2389806","2389967","<p>Once again this is not true. There are many counterexamples. As j.p. pointed out in comments, the direct product of a dihededral group of order $14$ and a cyclic group of order $2^4 \cdot 7 \cdot 17$ does not have a unique such subgroup: it has $7$ of them.</p>
"
"2389808","2389813","<p>Man, minutes after posting the question it finally hits me like a ton of bricks. $\mathbb{F}_{p}$ splits $x^p-x$, which is monic. So $x^p-x=\prod_{i=0}^{p-1}(x-i)$. And this immediately gives us the other result too.</p>
"
"2389811","2389831","<p>$\require{begingroup}\begingroup
\newcommand{\degs}{^\circ}
\newcommand{\cut}{\setminus}
\newcommand{\AND}{\ \ {\rm{\small{AND}}}\ \ }
\newcommand{\OR}{\ \ {\rm{\small{OR}}}\ \ }
\newcommand{\NOT}{\ \ {\rm{\small{NOT}}}\ \ }
\newcommand{\Implies}{\Rightarrow}
\newcommand{\If}{\Leftarrow}
\newcommand{\Iff}{\Leftrightarrow}
\newcommand{\x}{\times}
\newcommand{\R}{\mathbb{R}} \newcommand{\C}{\mathbb{C}} \newcommand{\N}{\mathbb{N}} \newcommand{\Z}{\mathbb{Z}} \newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\operatorname{\rm{\small{E}}}}
\renewcommand{\Re}{\operatorname{Re}} \renewcommand{\Im}{\operatorname{Im}}
\newcommand{\dash}{\textrm{-}}
\newcommand{\der}{\partial}
\newcommand{\del}{\nabla}
\newcommand{\inv}{{\sim}}
\newcommand{\eps}{\varepsilon}
\newcommand{\indent}{\ \ \ \ \ \ } \newcommand{\dedent}{\!\!\!\!\!\!\!\!\!}$
I think it's actually okay that there are some unknowns.</p>

<p>The reason is there are <em>many</em> possible $g(x,y)$ functions that will solve this problem, i.e. that will satisfy the condition that the resulting vector field is conservative.</p>

<p>You correctly stated that
$$ \frac{\der M}{\der y} = \frac{\der N}{\der x} $$
which simplifies to
$$ 3 = g_x $$
Integrating both sides with respect to $x$ yields
$$ g(x,y) = 3x + h(y) $$
where $h(y)$ is an arbitrary function.</p>

<p>At this point you might be tempted to try to somehow solve for $h(y)$ (I was, anyway), but it turns out this $h(y)$ is not something to <em>solve</em> for, it rather describes the form of all the possible $g(x,y)$ solutions: i.e. that as long as $g(x,y)$ has the form
$$ g(x,y) = 3x + h(y) $$
for some function $h(y)$, then $g$ is a valid solution to the problem. If you don't believe me, try plugging in the above expression for $g(x,y)$ and solving for the potential function $f$ of the vector field $\vec{\mathbf{F}}$ in terms of $h(y)$. You'll see that it doesn't matter what $h(y)$ is, we can always find $f$.</p>

<p>$\endgroup$</p>
"
"2389820","2389937","<p>Do not confuse  $\quad
\frac{d}{dx}(\sinh^{-1}x)={1\over \sqrt{1+x^2}} \quad
\text{with} \quad
\frac{d}{dx}(\sin^{-1}x)={1\over \sqrt{1-x^2}}$</p>

<p>They are typos in what you wrote (corrected below) :</p>

<p><a href=""https://i.stack.imgur.com/okWxu.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/okWxu.gif"" alt=""enter image description here""></a></p>
"
"2389824","2389873","<p>We have a $4\times 3$ grid, with four ""seats"" in each cell. &nbsp; Each of the twelve people must attend one group in each of the four sessions. &nbsp; Let's fit in the first person.</p>

<p>$$\sf(A\;\_\;\_\;\_)(A\;\_\;\_\;\_)(A\;\_\;\_\;\_)(A\;\_\;\_\;\_)\\ (\_\;\_\;\_\;\_)(\_\;\_\;\_\;\_)(\_\;\_\;\_\;\_)(\_\;\_\;\_\;\_)\\ (\_\;\_\;\_\;\_)(\_\;\_\;\_\;\_)(\_\;\_\;\_\;\_)(\_\;\_\;\_\;\_)$$</p>

<p>The only way any person can share at least one session with each of the other eleven people is if there is <em>only</em> one other person with which two sessions are shared. Thusly we can fill in people who share sessions with $\sf A$, and let the repeat be $\sf B$.</p>

<p>$$\sf(ABCD)(AEFG)(AHIJ)(AKLB)\\ (\_\;\_\;\_\;\_)(\_\;\_\;\_\;\_)(\_\;\_\;\_\;\_)(\_\;\_\;\_\;\_)\\ (\_\;\_\;\_\;\_)(\_\;\_\;\_\;\_)(\_\;\_\;\_\;\_)(\_\;\_\;\_\;\_)$$</p>

<p>Likewise, the only way <em>that</em> person can share at least one session with each of the other eleven people is if the only person with whom two sessions are shared is the first person.</p>

<p>$$\sf(ABCD)(AEFG)(AHIJ)(AKLB)\\ (\_\;\_\;\_\;\_)(B\_\;\_\;\_)(B\_\;\_\;\_)(\_\;\_\;\_\;\_)\\ (\_\;\_\;\_\;\_)(\_\;\_\;\_\;\_)(\_\;\_\;\_\;\_)(\_\;\_\;\_\;\_)$$</p>

<p>Now $\sf C$ cannot share any session but the first with $\sf A$ or $\sf B$. </p>

<p>$$\sf(ABCD)(AEFG)(AHIJ)(AKLB)\\ (\_\;\_\;\_\;\_)(B\_\;\_\;\_)(B\_\;\_\;\_)(\_\;\_\;\_\;\_)\\ (\_\;\_\;\_\;\_)(C\_\;\_\;\_)(C\_\;\_\;\_)(C\;\_\;\_\;\_)$$</p>

<p>Do the same for $\sf D$ who cannot share ... oh, dear...</p>
"
"2389827","2389832","<p>Yes, your proof is perfectly correct. We can simplify the proof a little bit, especially the transitivity part, if we observe that $a+2d=c+2b$ is equivalent to $a-2b=c-2d$, as this way each side of the condition uses entries from the same pair.</p>
"
"2389836","2389839","<p>$$\mathrm{Nm}(1+\sqrt{d}) = (1 + \sqrt{d})(1 - \sqrt{d}) = 1-d$$</p>

<p>For this element to even be  a <em>unit</em>, let alone a fundamental one, you must have $1 - d = \pm 1$.</p>
"
"2389841","2389849","<p>The total number of ways to draw $3$ of $10$ balls is $\binom{10}{3}=\frac{10!}{3!7!}=\frac{10\cdot 9\cdot 8}{3\cdot 2\cdot 1}=120$. That's your denominator. Your numerator for the first problem is the number of ways to draw $3$ of the $4$ white balls. Your numerator for the second one is the number of ways to draw $1$ of the $4$ white balls and $2$ of the $6$ black ones.</p>

<p>Does that help?</p>
"
"2389848","2389857","<p>When $c=-1$ then the curve reduces to $(y^2-1)(x^2-1)=0.$ That means either $y= 1$ or $y=-1$ or $x=1$ or $x=-1.$ So the graph consists of two vertical lines and two horizontal lines.</p>

<p>When $y=1$ or $y=-1$ then the expression you've got for $y'$ becomes $0,$ as you'd expect given that those are two horizontal lines. When $x=1$ or $x=-1,$ then the denominator in the expression you've got for $y'$ becomes $0$ and the derivative is undefined, as you'd expect for two vertical lines.</p>
"
"2389851","2389869","<p>The curve $f(t) = e^{mt} [\cos(t), \sin(t)]$ does indeed have nonconstant speed
$e^{mt} \sqrt{m^2+1}$.  In order to make the speed constant, you could take $t = T(s)$ where
$$\dfrac{dT}{ds} = e^{-mT}$$   The solutions to this differential equation are 
$$ T(s) = \frac{1}{m} \ln(a + m s) $$
for arbitrary constant $a$.  Thus you could parametrize your curve as
$$ F(s) = (a + m s) \left[\cos\left(\frac{\ln(a+ms)}{m}\right), \; \sin\left(\frac{\ln(a+ms)}{m}\right)\right] $$</p>
"
"2389853","2389890","<p>1) Statement is false as stated:
$$\Gamma\left(6\right)	=	5!=120$$
$$6^{3}	=	216$$</p>

<p>2) Proposition: $\Gamma\left(x\right)\geq x^{3}$
  for all real $x\geq7$. </p>

<p>Proof:
$$\frac{\Gamma\left(x\right)}{x^{3}}=\frac{\left(x-1\right)\Gamma\left(x-1\right)}{x^{3}}=...=\frac{\left(x-1\right)\left(x-2\right)\left(x-3\right)\left(x-4\right)}{x^{3}}\Gamma\left(x-4\right)$$
 Then, since $\Gamma\left(x-4\right)\geq1$
  for $x\geq5$: $$\frac{\Gamma\left(x\right)}{x^{3}}\geq\frac{\left(x-1\right)\left(x-2\right)\left(x-3\right)\left(x-4\right)}{x^{3}},\textrm{ }\forall x\geq5$$</p>

<p>Next: $$\frac{\left(x-1\right)\left(x-2\right)\left(x-3\right)\left(x-4\right)}{x^{3}}=\left(1-\frac{1}{x}\right)\left(1-\frac{2}{x}\right)\left(1-\frac{3}{x}\right)\left(x-4\right)$$
 Then, $x\geq7$ implies $-\frac{1}{x}\geq-\frac{1}{7}$.</p>

<p>Thus:</p>

<p>$$\left(1-\frac{1}{x}\right)\left(1-\frac{2}{x}\right)\left(1-\frac{3}{x}\right)\left(x-4\right)\geq\left(1-\frac{1}{7}\right)\left(1-\frac{2}{7}\right)\left(1-\frac{3}{7}\right)\left(7-4\right)=\frac{360}{343}\geq1$$</p>

<p>Thus, $x\geq7$ implies $$\frac{\Gamma\left(x\right)}{x^{3}}\geq1$$.</p>

<p>Q.E.D.</p>
"
"2389859","2389868","<p>It's $$(2a+1)(2c+1)=10,$$ $$(2b-1)(2c+1)=-12$$ and $$(2a+1)(2b-1)=-30.$$</p>

<p>The rest is smooth:
$$(2a+1)^2(2c+1)^2(2b-1)^2=3600,$$
which gives
$$(2a+1)(2c+1)(2b-1)=60$$ or $$(2a+1)(2c+1)(2b-1)=-60.$$ 
If $(2a+1)(2c+1)(2b-1)=60$ then we obtain: </p>

<p>$2a+1=\frac{60}{-12}$, $2c+1=\frac{60}{-30}$ and $2b-1=\frac{60}{10}$, which is
$a=-3$, $b=\frac{7}{2}$, $c=-\frac{3}{2}$.</p>

<p>If $(2a+1)(2c+1)(2b-1)=-60$ then the work is similar</p>

<p>and we get the answer:
$$\left\{\left(-3,\frac{7}{2},-\frac{3}{2}\right),\left(2,-\frac{5}{2},\frac{1}{2}\right)\right\}$$</p>
"
"2389862","2389864","<p>You don't need to actually know the error function to do this problem. Note that $$\nabla f(x, y) = (-2xe^{-x^4}, e^{-y^2})$$ using the fundamental theorem of calculus. Then, $$\nabla f(-1, 1) = (2/e, 1/e)$$ which, along with the fact that $f(-1, 1) = 0$, tells us that the tangent plane will be $z = \frac{2}{e}x+\frac{1}{e}y+\frac{1}{e}$, which could equivalently be expressed as $$2x+y-ez = -1$$</p>
"
"2389865","2389910","<p><strong>Lemma</strong></p>

<blockquote>
  <p>If $Y$ is a normal space and $\{O_1 ,O_2\}$ is an open cover of $Y$, then there is a closed cover $\{F_1, F_2\}$ of $Y$ such that $F_i \subseteq O_i$ for $i=1,2$.</p>
</blockquote>

<p>This is part of a proof that point-finite open covers of normal spaces have so-called closed shrinkings.</p>

<p>Proof of the lemma: $C_1  =Y\setminus O_1$, and $C_2 = Y\setminus O_2$. Then $C_1$ and $C_2$ are disjoint closed sets in $Y$ so they have disjoint open neighbourhoods $N_1$ resp. $N_2$. Then for $i=1,2$, $F_i = Y \setminus N_i$ are as required.</p>

<p>Now your question follows almost immediately: if $X$ is metric and $K \subseteq U_2 \cup U_2$ is compact, then $Y = U_1 \cup U_2$ is a normal space (all I need of a metric space is that it is hereditarily normal), and so we find $F_i \subseteq Y$ as required. Then $K_i = K \cap F_i$ are compact (this is certainly true in $Y$ but compactness is absolute) and as the $F_i$ cover $Y$, these sets are as required.</p>
"
"2389871","2389878","<p>Strictly speaking, the typical definition of $\int_\Omega f\, d\mu$ assumes that $f$ is a measurable function on $\Omega$, so it must be defined everywhere.  For instance, Tao's Definition 1.4.37 defines the integral of a measurable function $f:X\to[0,+\infty]$, which in particular is a function on $X$ and so is defined at every point of $X$.</p>

<p>However, if $f$ is defined only almost everywhere, you can just extend it to be defined on all the points where it isn't and get a function which is defined everywhere to integrate.  It doesn't matter how you define it on those other points, since they are a set of measure $0$ and so do not affect the integral.  As a result, it is common to abuse notation slightly and speak of the integral of a function that is only defined almost everywhere.</p>
"
"2389877","2389879","<p>Not every inner product space is a Hilbert space. Every finite-dimensional one is but there are infinite-dimensional ones which aren't complete, say
the space of continuous functions on $[0,1]$ with $(f,g)=\int_0^1 f(x)g(x)\,dx$. Some authors call inner-product spaces ""pre-Hilbert spaces""
for this reason (the completion is a Hilbert space).</p>
"
"2389889","2389891","<p>How about this? Take groups $H_1$ and $H_2$ with irreps $U_1$ and $U_2$.
Let $G=H_1\times H_2$. Regard $U_i$ as a $G$-representation by pulling
back the $H_i$-action to $G$ via the projection $G\to H_i$. I reckon
then $U_1$, $U_2$ and $U_1\otimes U_2$ will all be irreps of $G$.</p>
"
"2389895","2389909","<p>You can express the CDF of $Y$ quite easily,</p>

<p>$$ F_Y(y) = \mathbb{P}(Y \leq y) = \prod\limits_i^n\mathbb{P}(X_i \leq y) = (\mathbb{P}(X_i \leq y))^n$$</p>

<p>This is true because $max(X_1,..,X_n) \leq y \Longleftrightarrow X_i \leq y$ for all $i$. </p>
"
"2389900","2389902","<p>It's a geometric series</p>

<p>For $-1&lt;\frac{1}{x+1}&lt;1$ it converges to $\frac{\frac{1}{x+1}}{1-\frac{1}{x+1}}=\frac{1}{x}.$</p>

<p>The range of $x$ is $$(-\infty,-2)\cup(0,+\infty)$$
because $\frac{1}{x+1}&lt;1$ it's
$$\frac{1}{x+1}-1&lt;0$$ or
$$\frac{x}{x+1}&gt;0,$$
which gives $x&gt;0$ or $x&lt;-1$;</p>

<p>$$\frac{1}{x+1}&gt;-1$$ it's
$$\frac{1}{x+1}+1&gt;0$$ or
$$\frac{x+2}{x+1}&gt;0,$$
which gives $x&lt;-2$ or $x&gt;-1$ and which the previous result we get:</p>

<p>$x&lt;-2$ or $x&gt;0$.</p>
"
"2389901","2390182","<p>First (3): $A$ being a subset of $P(X)$ ($A\subseteq P(X)$), means that every element of $A$ is a subset of $X$. So in this case we should check if $\emptyset$ and $\{\emptyset\}$ are subsets of $X$. Wel surely $\emptyset $ is, so what about $\{\emptyset\}$. Well, $$x\in\{\emptyset\}\implies x=\emptyset\in X.$$ 
So, $\emptyset $ and $\{\emptyset\}$ are both subsets of $X$, i.e. elements of $P(X)$. Thus: $\big\{\emptyset,\{\emptyset\}\big\}\subseteq P(X)$.  </p>

<p>Now (4): $$x\in \{1,\emptyset\}\implies \big( x=1\in X \vee x=\emptyset\in X\big)\implies x\in X.$$ Thus $\{1,\emptyset\}\subseteq X$.</p>

<p>You were right about (1) and (2); they are both wrong.</p>
"
"2389904","2389933","<blockquote>
  <p>I've only ever done an extremely small amount of probability in high school, but I was told that if something has a probability of 0, it's impossible! </p>
</blockquote>

<p>You should have been told that <em>if an event is impossible, then it has a probability of zero</em>. &nbsp; That's not the same thing.</p>

<p>(If it is a duck, then it is a bird; but if it is a bird, then it <em>may or may not</em> be a duck.)</p>

<p>Impossible events do have a probability of zero, but there are <em>also</em> events which, while possible, have zero measure. &nbsp; (At least, theoretically.)</p>

<p>For example: the lifespans of lightbulbs may be modelled as being continuous random variables with exponential distribution. &nbsp; So two lightbulbs being independently tested are assigned zero probability of failing at <em>exactly</em> the same time; but it <em>is</em> possible.</p>

<p>(Though, in practice, there is a limit on precision of our time measurements.)</p>
"
"2389914","2389922","<p><a href=""https://i.stack.imgur.com/CptX2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CptX2.jpg"" alt=""""></a></p>

<p>Let C be the center point, A the middle of bottom side of the smaller square and B the lower right hand corner of the smaller square. </p>

<p>Length of CA is $x + \frac{1}{\sqrt{2}}$.</p>

<p>Length of AB is $\frac{x}{2}$.</p>

<p>CB is a radius of the circle and also the hypotenuse of the right triangle CAB.</p>

<p>So we have</p>

<p>$1 = \sqrt{(x + \frac{1}{\sqrt{2}})^2 + (\frac{x}{2})^2 }$</p>

<p>Solving for x, and taking the positive root, you have:</p>

<p>$x=\frac{\sqrt{2}}{5}$</p>
"
"2389916","2390146","<p>Note that $M$ is orientable as a hypersurface. The conditions $K_M&gt;0$ and Gauss Bonnet theorem imply that $M$ is diffeomorphic to a sphere. Then it is a general classical result of Almgrem that all minimal immersion $f : \mathbb S^2 \to \mathbb S^3$ must be totally geodesic. The proof can be found in <a href=""https://www.jstor.org/stable/1970520?seq=1#page_scan_tab_contents"" rel=""nofollow noreferrer"">here</a>. The proof is roughly the following:</p>

<p>Let $n$ be a normal vector fields of the immersion and define </p>

<p>$$\Phi = f_{uu} \cdot n+ \sqrt{-1} f_{uv} \cdot n,$$</p>

<p>where $(u,v)$ is a local conformal coordinates of $\mathbb S^2$. Then it is shown that $\partial_{\bar z} \Phi = 0$ (where $z = u+ \sqrt{-1}v$) and $\Phi$ defines a quadratic differential. Since the space of holomorphic quadratic differential on $\mathbb S^2$ is trivial, we obtain $\Phi =0$. </p>

<p>On the other hand, we have </p>

<p>$$ |k_1 - k_2| = 2|f_u|^{-2} |\Phi|^2,$$</p>

<p>where $k_1, k_2$ are the principle curvatures. As $k_1 + k_2  =0$ (it's minimal), we have $k_1 = k_2 = 0$ and so the immersion is totally geodesic. </p>
"
"2389917","2389924","<p>$$a_{n+2}-2a_{n+1}=3a_{n+1}-6a_n.$$
Let $b_n=a_{n+1}-2a_n.$</p>

<p>Thus, $b_0=11-6=5$ and $b_{n+1}=3b_n$, which gives $b_n=5\cdot3^n$.</p>

<p>Hence, $a_{n+1}-2a_n=5\cdot3^n$.</p>

<p>Now, $$a_1-2a_0=5\cdot3^0$$
$$\frac{1}{2}a_2-a_1=\frac{1}{2}\cdot5\cdot3^1=5\cdot\left(\frac{3}{2}\right)^1$$
$$\frac{1}{2^2}a_3-\frac{1}{2}a_2=5\cdot\left(\frac{3}{2}\right)^2$$
$$.$$
$$.$$
$$.$$
$$\frac{1}{2^{n-1}}a_n-\frac{1}{2^{n-2}}a_{n-1}=5\cdot\left(\frac{3}{2}\right)^{n-1}$$ and after summing of these equalities we obtain: 
$$\frac{1}{2^{n-1}}a_n-2a_0=5+5\left(\frac{3}{2}\right)^1+...+5\left(\frac{3}{2}\right)^{n-1}=\frac{5\left(\left(\frac{3}{2}\right)^n-1\right)}{\frac{3}{2}-1}=10\left(\left(\frac{3}{2}\right)^n-1\right),$$
which gives
$$a_n=5\cdot3^n-2^{n+1}.$$
Done!</p>
"
"2389931","2389936","<p>Let $U=A\cap S^{n-1}$. If a polynomial $P$ vanishes on $U$ the, since it is homogeneous, it vanishes in $\mathbb{R}^+U$. But this is an open subset of $\mathbb{R}^n$. If $P$ vanishes on an open subset of $\mathbb{R}^n$, it is the null polynomial.</p>
"
"2389932","2390678","<p>Defining $\overline{A}  =\bigcap\{C \subseteq X \text {closed}: A \subseteq C\}$
(the smallest closed set containing $A$ as a subset):</p>

<p>Let $x \in \overline{A}$ and let $U$ be an open neighbourhood of $x$. If $U \cap A = \emptyset$ then $A \subseteq X\setminus U$, and the latter set is closed, so $\overline{A} \subseteq X\setminus U$, in this definition. Contradiction, as witnessed by $x$. So $A \cap U \neq \emptyset$.</p>

<p>Suppose $x$ has the neighbourhood intersection property. Let $C$ be a closed set that contains $A$. If $x \notin C$, then $X \setminus C$ is a neighbourhood of $x$ that misses $A$, which cannot be. So $x \in C$, and as $C$ was arbitrary closed around $A$,  $x \in \overline{A}$.</p>

<p>Pretty direct I'd say. We need small proofs by contradiction, but this is inherent to the statement, I think. To show $U \cap A \neq \emptyset$, we either have to find a concrete point that witnesses it or assume the opposit to make progress.</p>
"
"2389935","2389941","<p>Yes: these are called <em>universal properties</em>. See Wikipedia.</p>
"
"2389944","2389966","<p><a href=""https://i.stack.imgur.com/jhBL4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jhBL4.jpg"" alt=""Double cover""></a> $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $</p>
"
"2389951","2389974","<p>If $x^2=a+b\,x,$ we have $x^3=a\,x+b\,x^2=a\,x+b\,(a+b\,x)=a\,b+(a+b^2)\,x$. If $x=2^{1/3},$ we'd have $2^{1/3}=\frac{2-a\,b}{a+b^2},$ a rational number. Let $n$ be the smallest positive integer so that both $n\,2^{1/3}$ and $n\,2^{2/3}$ are integers. Then, $m=n(2^{1/3}-1)=n\,2^{1/3}-n$ is an integer $&lt;n$, and $m\,2^{1/3}=n\,2^{2/3}-n\,2^{1/3}$ and $m\,2^{2/3}=2\,n-n\,2^{2/3}$ are integers, contradicting the minimality of $n$.</p>
"
"2389963","2389981","<p>Suppose $a \in \mathbb{Z}$ is such that $p\mid (a^2+1)$, where $p$ is prime, and $p \equiv -1 \pmod 4$.
<p>
Since $p\mid (a^2+1)$, it follows that $p \not\mid a$, hence $\gcd(a,p)=1$.
\begin{align*}
\text{Then}\;\;&amp;p\mid (a^2+1)\\[4pt]
\implies\;&amp;a^2\equiv -1\pmod p\\[4pt]
\implies\;&amp;(a^2)^\frac{p-1}{2}\equiv -1\pmod p
&amp;&amp;\text{[since $\small{\frac{p-1}{2}}$ is odd]}\\[4pt]
\implies\;&amp;a^{p-1}\equiv -1\pmod p\\[4pt]
\implies\;&amp;1\equiv -1\pmod p
&amp;&amp;\text{[by Fermat's little Theorem]}\\[4pt]
\implies\;&amp;2\equiv 0\pmod p\\[4pt]
\end{align*}
contradiction.
<p>
Hmmm . . .
<p>
I didn't see the end of your question. It looks like you want a different kind of proof. In any case, the proof I gave above, using Fermat's little Theorem, is an easy way to prove the claim.</p>
"
"2389973","2389977","<p>It is:
$$\tan(\alpha+\beta)=\frac{\frac13+\frac12}{1-\frac16}=1 \Rightarrow \alpha+\beta=45^0.$$</p>
"
"2389985","2390004","<p>If the $X_i$ are independent (and probably they are in this context) then:$$\text{Var}\overline{X}_n=\text{Var}\left(\frac1{n}\sum_{i=1}^nX_i\right)=\frac1{n^2}\text{Var}\left(\sum_{i=1}^nX_i\right)=\frac1{n^2}\sum_{i=1}^n\text{Var}X_i=\frac{\sigma^2}{n}$$</p>

<p>The third equation is based on independence.</p>

<p>Likewise we find $\text{Var}\overline{X}_{5n}=\frac{\sigma^2}{5n}$ so that: $$\frac{\text{Var}\overline{X}_n}{\text{Var}\overline{X}_{5n}}=5$$</p>
"
"2389999","2390085","<p>As with Batting Averages, it gets tricky when there is rounding (which there usually is).  For example if $2$ out of $7$ vote for a certain thing, and the other $5$ vote against it,  then you'd have $28.571429\dots \%$ and $71.428571\dots\%$ which you would probably round to something like $28.57,\,71.43$.  But if you used those numbers you'd look at the fraction $\frac {2857}{10000}$ and, as that is in least terms, you'd deduce that you needed $10000$ voters.</p>

<p>One way to handle this is to use <a href=""https://en.wikipedia.org/wiki/Continued_fraction"" rel=""nofollow noreferrer"">Continued Fractions</a>.  For example, the Continued Fractions expansion for $\frac {2857}{10000}$ has the terms $[0;3,1,1,1428]$ and of course $\frac 27$ is one of the convergents.</p>

<p>To illustrate using your values: starting from $.296$ we'd look at the convergents to the continued fraction for $\frac {296}{1000}=\frac {37}{125}$.  They are:  $$\{0, \frac 13, \frac 27, \frac 3{10}, \frac {5}{17}, \frac {8}{27}, \frac {37}{125}\}$$</p>

<p>Now...$\frac 5{17} \approx .29411\dots$ so that's unlikely.  However $\frac 8{27}=0.296296296296\dots $ so if you are rounding to the tenths place in the percent then this would work, so I'd argue that the answer here could well be $\boxed {27}$. </p>
"
"2390003","2390028","<p>We begin by computing the derivatives of $w$ with respect to $s$:\begin{align}
\frac{dw}{ds}&amp;=\frac{dy}{dt}\,\frac{dt}{ds}=2\,s\,\frac{dy}{dt},\\
\frac{d^2w}{ds^2}&amp;=2\,\frac{dy}{dt}+4\,s^2\,\frac{d^2y}{dt^2}=u+4\,s\,w^{3/2}.
\end{align}
The first equation of the new system will be $w'=s\,u$. To get the second one we compute $u'$:
\begin{align}
u'&amp;=-\frac{1}{s^2}\,w'+\frac{1}{s}\,\frac{d^2w}{ds^2}\\
&amp;=-\frac{1}{s}\,u+\frac{1}{s}\bigl(u+4\,s\,w^{3/2}\bigr)\\
&amp;=4\,s\,w^{3/2}.
\end{align}</p>
"
"2390010","2390020","<p>Yes, for both questions. In particular, the determinant of a (square) matrix is $0$ if and only if the matrix has no inverse.</p>
"
"2390016","2390021","<p>Does not exist! Try $y\rightarrow+\infty$.</p>

<p>For non-negative variables we can use AM-GM:
$$x^2y=\frac{1}{2}x\cdot x\cdot2y\leq\frac{1}{2}\left(\frac{x+x+2y}{3}\right)^3=32.$$
The equality occurs for $x=4$ and $y=2$, which says that $32$ is a maximal value.</p>
"
"2390026","2390051","<p>Rigorously, you said?</p>

<p>Justify the following claims:</p>

<ol>
<li>Because $K=\Bbb{Q}(\theta,i)$ any automorphism $\tau\in Gal(K/\Bbb{Q})$ is uniquely determined, if we know $\tau(\theta)$ and $\tau(i)$.</li>
<li>Because $[K:\Bbb{Q}]=8\cdot2=16$ we see that the Galois group has sixteen elements.</li>
<li>There are $8$ alternatives for $\tau(\theta)$ and two for $\tau(i)$. Hence all the combinations must occur exactly once. Hence the automorphism $\sigma$ that you described exists.</li>
<li>We have $\sigma(\zeta)=-\zeta$. Hint: write $\zeta$ in terms of $i$ and $\sqrt2$.</li>
<li>Show that $\sigma$ has order eight.</li>
<li>Let $H=\langle \sigma\rangle$. Show that $[Gal(K/\Bbb{Q}):H]=2$. Hence $\operatorname{Inv}(H)$ is a quadratic extension of $\Bbb{Q}$.</li>
<li>Show that $\Bbb{Q}(i)\subseteq \operatorname{Inv}(H)$, and conclude that we must have equality here.</li>
</ol>
"
"2390034","2390052","<p>I think you are right; in this context what is meant is specifically a local minimum which is not the global minimum, and a good shorter way to refer to that is ""non-global minimum"". </p>

<p>I don't think it should worry you that this phrase is much less common than ""local minimum"", since in most other contexts ""local minimum"", i.e. something which might be the global minimum but also might not, is what is meant. You're using a more unusual term because you are expressing a more unusual concept, that is all.</p>
"
"2390036","2390048","<p>Hint: If you have a hypothetical counter-example, can you show you have two colours which are not connected?</p>
"
"2390038","2390060","<p>This seems to be OK. Another, more general approach, is to look at this and noting that $x^4+1$ is the <a href=""https://en.wikipedia.org/wiki/Cyclotomic_polynomial"" rel=""nofollow noreferrer"">cyclotomic polynomial</a> $\Phi_8(x)$, hence $x^4+1=(x-\zeta)(x-\zeta^3)(x-\zeta^5)(x-\zeta^7)$, where $\zeta \in \mathbb{C}$ is a <em>primitive</em> root of unity, with $\zeta^8 =1$. The splitting field over $\mathbb{Q}$ of your polynomial is $\mathbb{Q}(\zeta)$, which is indeed $\mathbb{Q}(i,\sqrt{2})$.</p>
"
"2390040","2390063","<p>Only one theorem applied.</p>

<blockquote>
  <p><strong>Theorem.</strong> If $\lim\limits_{x\rightarrow c}f(x)=L$ and $\lim\limits_{x\rightarrow c}g(x)=M$, then $\lim\limits_{x\rightarrow c}[f(x)g(x)]=LM$ </p>
</blockquote>

<p>$$
\because \qquad \lim_{x\rightarrow0}\frac{f(x)}{\sin(2x)}=2\quad \text{and}\quad \lim_{x\rightarrow0}\left[(\sqrt{x+4}-2ï¼\cdot g(x)\right]=5
$$</p>

<p>$$
\therefore\qquad \lim_{x\rightarrow0}\left\{\frac{f(x)}{\sin(2x)}\cdot \left[(\sqrt{x+4}-2ï¼\cdot g(x)\right]\right\}=2\times5=10
$$</p>

<p>Rearranging:
\begin{align}
\frac{f(x)}{\sin(2x)}\cdot \left[(\sqrt{x+4}-2ï¼\cdot g(x)\right]
&amp;=\left[f(x)g(x)\right]\cdot\frac{\sqrt{x+4}-2}{\sin(2x)}\\
&amp;=\left[f(x)g(x)\right]\cdot\frac{2x}{\sin(2x)}\cdot \frac{1}{2(\sqrt{x+4}+2)}
\end{align}</p>

<p>$$
\Rightarrow\qquad
\lim_{x\rightarrow0}\left\{\left[f(x)g(x)\right]\cdot\frac{2x}{\sin(2x)}\cdot \frac{1}{2(\sqrt{x+4}+2)}\right\}
=10
$$</p>

<p>$$
\because\qquad
\lim_{x\rightarrow0}\frac{\sin(2x)}{2x}=1\quad
\text{and}
\quad \lim_{x\rightarrow0}\frac{2(\sqrt{x+4}+2)}{1}=8
$$</p>

<p>$$
\therefore\qquad
\lim_{x\rightarrow0}
\left\{
\left[\left[f(x)g(x)\right]\cdot\frac{\sin(2x)}{2x}\cdot \frac{1}{2(\sqrt{x+4}+2)}\right]
\cdot
\left[
\frac{2x}{\sin(2x)}
\right]
\cdot
\left[
\frac{2(\sqrt{x+4}+2)}{1}
\right]
\right\}\\=10\cdot 1\cdot8=80
$$
$$
\Rightarrow\qquad
\lim_{x\rightarrow0}\left[f(x)g(x)\right]=80
$$</p>

<p><strong>Tips.</strong> Treat the <em>block</em> of functions as <em>one</em> function.</p>
"
"2390042","2390104","<p>After discarging neglectible terms, the numerator is asymptotic to $2^{8n-3}$, while the denominator is asymptotic to $2^{2f(n)}(-2^{-1}+2^3)$.</p>

<p>By identification of the base $2$ logarithms,</p>

<p>$$8n-3=2f(n)+\log_2\frac{15}2.$$</p>

<hr>

<p>It is improper to speak of <em>the</em> sequence, as any sequence asymptotic to this $f(n)$ can do.</p>
"
"2390057","2390066","<p>Think of the eigenvalues of $aJ_n$, which are simply the eigenvalues of $J_n$ (which you seem to know already) times $a$. Then think of the eigenvalues of $I_n + aJ_n$, which are quite easy to derive, since its eigenvectors are the same. Really, if $aJ_n\cdot\vec v=\lambda\vec v$, then $(I_n+aJ_n)\cdot\vec v=?$</p>

<p>When none of the eigenvalues is 0, the matrix is invertible.</p>
"
"2390061","2390067","<p>The assertion âThere exists a neighbourhood of $a$ on which $f$ is boundedâ means that there exists a neighbourhood $N$ of $a$ such that the set $f(N)=\{f(z)\,|\,z\in N\cap\text{domain of }f\}$ is bounded.</p>

<p>For instance, if $f\colon\mathbb{C}\setminus\{0\}\longrightarrow\mathbb C$ is defined by $f(z)=z$, then $0$ is a removable singularity and $f\bigl(D(0,1)\bigr)=D(0,1)\setminus\{0\}$, which is bounded.</p>

<p>By the way, poles are <em>not</em> removable singularities.</p>
"
"2390065","2390072","<p>The union of $f$ is a set that contains all the sets $a$ as well as all the sets $\{a,b\}$. The union of this, again, is therefore a set that contains, among other things, all the $b$'s. You need to use comprehension to extract the $b$'s alone.</p>

<p>It should've been $$ran(f)=\{b\in \cup\cup f\mid (\exists a)((a,b)\in f)\}$$</p>
"
"2390083","2390508","<p>You have $g(x_e) = x_e$, so the quantity $|g(x_e) - x_a|$ is really the absolute value of the error $T-A$, i.e., the difference between your target $T = x_e$ and your approximation $A = x_a$. </p>

<p>The signed residual is $r = g(x_e) - g(x_a) = x_e - g(x_a)$, the difference between the targeted value of $g$, i.e., $g(x_e)$ and current value of $g$, i.e., $g(x_a)$.</p>

<p>I feel that the difference between the error and the residual is more easily understood in the context of a general non-linear equation $$f(x) = 0.$$ Let $\xi$ denote the solution of interest. Let $x$ be an approximation of $\xi$. Then the error is $e = \xi-x$, whereas the signed residual is $r = -f(x)$.</p>

<p>A favorite example of mine is the case where $R(\theta)$ denotes the range of an artillery shell when the gun is fired using elevation $\theta$. Solving the equation $$R(\theta) = d$$ corresponds to computing an elevation $\theta$ which will land the shell on top of a target at distance $d$ down range. If $\hat{\theta}$ is the computed elevation/firing solution, then the error is $\theta - \hat{\theta}$, but the (signed) residual is $r = d - R(\hat{\theta})$.</p>

<p>In particular, this example shows the relevance of using a signed residual rather than just the absolute value. After all, it is relevant if the shot went too far or fell short of the target.</p>

<p>My choice of sign when defining the error or the residual is entirely one of convenience. It cuts down on the number of minus signs that I have to write, when applying, say, Taylor's theorem and similar results.</p>
"
"2390093","2390100","<p><em>This is too long for a comment.</em></p>

<p>$$\int \frac{\log(x)}{x+\epsilon}=\text{Li}_2\left(-\frac{x}{\epsilon }\right)+\log (x) \log \left(1+\frac{x}{\epsilon
   }\right)$$ where appears the polylogarithm function.
$$\int_0 ^1 \frac{\log(x)}{x+\epsilon}=\text{Li}_2\left(-\frac{1}{\epsilon }\right)\qquad \text{if} \qquad \Re(\epsilon )&gt;0\lor \Re(\epsilon )\leq -1\lor \epsilon \notin \mathbb{R}$$ the development of which being exactly your rhs.</p>

<p><strong>Edit</strong></p>

<p>Looking at <a href=""http://dlmf.nist.gov/25.12"" rel=""nofollow noreferrer"">DLMF</a>, equation $25.12.4$ gives $$\text{Li}_2\left(\frac{1}{z}\right)+\text{Li}_2(z)=-\frac{\pi^2}6-\frac 12\log^2(-z)$$ and, by definition (equation $25.12.1$) $$\text{Li}_2(z)=\sum_{n=1}^\infty \frac{z^n}{n^2}$$ making 
$$\text{Li}_2\left(-\frac{1}{z}\right)=-\frac{\pi^2}6-\frac 12\log^2(z)-\sum_{n=1}^\infty (-1)^n\frac{z^n}{n^2}$$ which is your expansion.</p>

<p>You also could find <a href=""http://functions.wolfram.com/ZetaFunctionsandPolylogarithms/PolyLog/17/ShowAll.html"" rel=""nofollow noreferrer"">here</a> other interesting relations between $\text{Li}_2(z)$ and $\text{Li}_2\left(\frac{1}{z}\right)$.</p>
"
"2390102","2390120","<p>Just start out with some examples. Suppose that $s = 0$. Then what is:
$$
\bigcap_{t \geq 0} (\cos t - 2, \cos t + 2)
$$</p>

<p>Well let's think about some examples.</p>

<ul>
<li>If $t = 0$, then we have: $(-1, 3)$.</li>
<li>If $t = \pi/2$, then we have: $(-2, 2)$.</li>
<li>If $t = \pi$, then we have: $(-3, 1)$.</li>
</ul>

<p>Draw out these intervals. Where do they all overlap as we keep varying our choice of $t$? It should be easy to see that:</p>

<p>$$
\bigcap_{t \geq 0} (\cos t - 2, \cos t + 2) = (-1, 1)
$$</p>

<p>Now repeat this procedure for a different choice of $s$.</p>

<hr>

<p>EDIT: Here's a full proof.</p>

<blockquote>
  <p><strong>Claim:</strong> $\bigcup_{s \in \mathbb R} \bigcap_{t \geq s} A_t = (-1, 1)$</p>
</blockquote>

<ul>
<li><p>$\boxed{\subseteq}:$ Choose any $x \in\bigcup_{s \in \mathbb R} \bigcap_{t \geq s} A_t$. Then there is some $s_0 \in \mathbb R$ such that $x \in \bigcap_{t \geq s_0} A_t$. Now consider $t_{\textsf{even}} = 2|\lceil s_0 \rceil| \pi$ and $t_{\textsf{odd}} = (2|\lceil s_0 \rceil| + 1) \pi$, which are both certainly greater than or equal to $s_0$. Observe that:
$$
A_{t_\textsf{even}} = (-1, 3) \\
A_{t_\textsf{odd}} = (-3, 1)
$$
Thus, we conclude that:
$$
x \in \bigcap_{t \geq s_0} A_t \subseteq (A_{t_\textsf{even}} \cap A_{t_\textsf{odd}}) = (-1, 3) \cap (-3, 1) = (-1, 1)
$$
as desired.</p></li>
<li><p>$\boxed{\supseteq}:$ Choose any $x \in (-1, 1)$. To prove our claim, it is enough to show that $x \in \bigcap_{t \geq 0} A_t$, since this is a subset of $\bigcup_{s \in \mathbb R} \bigcap_{t \geq s} A_t$. To this end, choose any $t \geq 0$. We need to show that $x \in A_t$. Indeed, notice that:
\begin{align*}
x
&amp;&gt; -1 &amp;\text{since } x \in (-1, 1) \\
&amp;= 1 - 2 \\
&amp;\geq \cos t - 2 &amp;\text{since } \cos t \in [-1, 1] 
\end{align*}
Likewise:
\begin{align*}
x
&amp;&lt; 1 &amp;\text{since } x \in (-1, 1) \\
&amp;= -1 + 2 \\
&amp;\leq \cos t + 2 &amp;\text{since } \cos t \in [-1, 1] 
\end{align*}
Thus, we conclude that $x \in (\cos t - 2, \cos t + 2) = A_t$, as desired.</p></li>
</ul>
"
"2390108","2390129","<p>Here's one way to do it . . .
<p>
For $i \in W$, let $r(i),c(i)$ denote the row and column indices, respectively, for the matrix entry corresponding to the index $i$ (with row and column indices starting at $0$).
<p>
Then $r(i)$ and $c(i)$ are given by
\begin{align*}
r(i)&amp;=\left\lfloor\frac{-1+\sqrt{1+8i}}{2}\right\rfloor\\[4pt]
c(i)&amp;=i-\left(\frac{r(i)^2 + r(i)}{2}\right)\\[4pt]
\end{align*}
<p>
For example, based on the above formulas,
$$(r(7),c(7)) = (3,1)$$
which matches your specification.</p>
"
"2390110","2390170","<p>Let $\eta=\frac{1-i}{\sqrt{2}}=e^{-\pi i/4}\in S^1$. The given integral is </p>

<p>$$ \text{Im}\int_{0}^{+\infty} 4y^{4n+3}e^{-\sqrt{2}\eta y}\,dy \stackrel{y\mapsto\frac{z}{\eta\sqrt{2}}}{=}\frac{1}{4^n}\text{Im}\int_{0}^{(1+i)\infty}z^{4n+3}e^{-z}\,dz\tag{1}$$
or:
$$ \frac{1}{4^n}\text{Im}\lim_{R\to +\infty}\left[\int_{0}^{R}z^{4n+3}e^{-z}\,dz+\int_{R}^{R(1+i)}z^{4n+3}e^{-z}\,dz\right]\tag{2}$$
where $\int_{0}^{R}z^{4n+3}e^{-z}\,dz$ is real and $\int_{R}^{R(1+i)}z^{4n+3}e^{-z}\,dz$ is bounded by:
$$\left|\int_{R}^{R(1+i)}z^{4n+3}e^{-z}\,dz\right|\leq R e^{-R}\left(R\sqrt{2}\right)^{4n+3}\tag{3}$$
which goes to zero as $R\to +\infty$. It follows that the original integral equals zero as well.</p>

<p>As mentioned in the comments, this is a famous integral: have a look at <a href=""https://mathoverflow.net/questions/31295/let-a-function-f-have-all-moments-zero-what-conditions-force-f-to-be-identicall"">this MO thread</a>.</p>
"
"2390112","2390130","<p>The sequence may not have a limit at all, which would lead to a problem of what it means to write a probability for an event whose defining formula is not even defined for some outcomes. On the other hand the limes superior always exists (though it may be $\infty$) avoiding that problem. </p>
"
"2390124","2390145","<p>Yes. Let $(x_0,y_0)$ be an arbitrary point in the plane and consider
$$ g(t) = f(x_0+at, y_0+bt) $$
Now compute $g'(t)$ by the chain rule. Together with your assumption that yields $g'(t) = g(t)$.</p>
"
"2390127","2390413","<p>If we have $k,a&gt;n$ we would probably just have to do the exponentiation, rather than reduce the exponent using <a href=""https://en.wikipedia.org/wiki/Euler%27s_theorem"" rel=""nofollow noreferrer"">Euler</a>/<a href=""https://en.wikipedia.org/wiki/Carmichael_function"" rel=""nofollow noreferrer"">Carmichael</a> etc. - but it might be worth checking that (although probably not for an coded system). In particular if $a$ is composite with no small factors it may not be worth the time to determine its factors, since the exponentiation calculation is not very time-consuming anyway.</p>

<p>Assuming that any reduced exponent has been determined, calculate $k^{n-1} \bmod a$ using <a href=""https://en.wikipedia.org/wiki/Exponentiation_by_squaring"" rel=""nofollow noreferrer"">exponentiation by squaring</a>, then multiply by $k$.</p>

<p>Note that you may need to handle intermediate values as large as $a^2$, if using simple multiplication.</p>
"
"2390131","2390366","<p>Your ""compound"" process $M_Y$ at a <em>fixed</em> $t\ge 0$ is the composition of two mappings $\psi_t(\omega):=(\omega,Y_t(\omega))$ and $\varphi(\omega,u):=M_u(\omega)$. The former is an $\mathcal F / \mathcal F\otimes\mathcal B$ measurable mapping of $\Omega$ to $\Omega\times[0,\infty)$. (Where $\mathcal B$ denotes the Borel subsets of $[0,\infty)$.) This is because $\mathcal F\otimes\mathcal B$ is generated by rectangles of the form $F\times B$, $F\in\mathcal F, B\in\mathcal B$, and $\psi_t^{-1}(F\times B)=F\cap Y_t^{-1}(B)$.
To finish you need to know that the latter mapping $\varphi$ is $\mathcal F\otimes\mathcal B / \mathcal R$ measurable. (Here I use $\mathcal R$ to denote the Borel subsets of $\Bbb R$.) This situation is referred to as $M$ being a ""measurable process"" and is an additional hypothesis. For example, if each random variable $M_u$ is $\mathcal F$ measurable and $u\mapsto M_u(\omega)$ is right continuous for each $\omega$, then $M$ is a measurable process.</p>

<p>If both $\psi_t$ and $\varphi$ are measurable as indicated, then the composite function $M_{Y_t}=\varphi\circ\psi$ is $\mathcal F/\mathcal R$ measurable.</p>
"
"2390138","2390156","<p>There are infinitely many self-complementary graphs. A way to construct some is the following:</p>

<p>Choose the self-complementary path-graph $P_3=(\{1,2,3,4\},E)$ and a number $n\in \Bbb N$. Construct the graph $P_3^n$ from $P_3$ where each vertex $v\in \{1,2,3,4\}$ is replaced by $n$ vertices $v_1,...,v_n$ and there is an edge between vertices $v_i$ and $w_j$ if and only if $vw\in E$ or $v=w\in\{1,3\}$. This graph is self-complementary.</p>

<p><img src=""https://i.stack.imgur.com/4Bx5S.png"" width=""400"" /></p>

<p>It is like replacing any edge by a complete bipartite graph and every second vertex by a complete (or edge-less) graph. Of course, for $n\geq 3$ this will be not planar. By the way, the right graph in the picture is planar.</p>
"
"2390157","2390171","<p>Essentially your function is</p>

<p>$$f(n)=\begin{cases}
\quad\dfrac n2\qquad \text{if $n$ is even}\\
-\dfrac {n+1}{2}\quad \text{if $n$ is odd}
\end{cases}$$</p>

<p><strong>Injectivity:</strong></p>

<p>Assume $f(a)=f(b)$. Then we have two cases:</p>

<p>If $f(a),f(b)$ are positive, $\frac{a}{2}=\frac{b}{2} \implies a=b$.</p>

<p>If $f(a),f(b)$ are negative, $-\frac {a+1}{2}=-\frac {b+1}{2}\implies a=b$.</p>

<p><strong>Surjectivity:</strong></p>

<p>I'll let you finish this one off. Take some $x\in\mathbb{Z}$. If $x$ is positive, then ... and if $x$ is negative, then ... .</p>
"
"2390161","2390169","<p>The matrix $A$ is unitary if and only if its inverse is $\overline A^t$. And if $\overline A^t$ is unitary, then its conjugate (which is $A^t$) is unitary too.</p>
"
"2390165","2390207","<p>One could do as follows as well, if some calculus were allowed.</p>

<p>Consider the function $ f(a,b) = a^2 + b^2 + \frac{1}{a^2} + \frac{b}{a}$. 
As we are looking for a lower bound, we could calculate $ \frac{\partial f}{\partial b} = 2b + \frac{1}{a}$ and, upon setting to zero, look along the domain restriction $$ b = -\frac{1}{2a}$$ only.
Substituting yields the expression $$a^2 + \frac{3}{4a^2} = \frac{2a^2 + \frac{3}{2a^2}}{2} $$ after which we can apply the AM-GM inequality $$\frac {x+y}{2} \geq \sqrt {x y}$$ upon setting $x=2a^2$ and $y = \frac{3}{2a^2} $, concluding that the given expression is equal or greater than $$ \sqrt{  2a^2 \frac{3}{2a^2}} = \sqrt{3}$$</p>
"
"2390172","2390177","<p>1, 1,1/2, 1,1/2,1/3, 1,1/2,1/3,1/4, ...</p>
"
"2390195","2390954","<p>For fun, here's a simple trigonograph:</p>

<p><a href=""https://i.stack.imgur.com/MFcRP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MFcRP.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>$$0 &lt;\theta &lt; \frac{\pi}{2}\quad\text{and}\quad 1 &lt; n \in \mathbb{N} \qquad\implies\qquad \tan\theta &gt; n \tan\frac{\theta}{n}$$</p>
</blockquote>
"
"2390203","2390297","<p>Here's a cubic polynomial to play with.
$$
f(x) = ax^3 + bx^2 +cx +d.
$$</p>

<p>You want $d = f(0)$ to be the value of the bananas at the start.</p>

<p>You want the derivative of $f$ at $0$ to be $0$ so that the curve is flat there. That tells you $c=0$. </p>

<p>You want $f(t)=0$ for the known value of $t$ when the bananas are free. That gives you one linear equation relating the unknowns $a$ and $b$ and the known value of $d$:
$$
t^3a + t^2b + d = 0.
$$</p>

<p>You have one more thing you can specify. I thought about using the slope (steepness) at $t$. Call that $-k$ since you want a slope down to the right.</p>

<p>That tells you
$$
3t^2a + 2tb +k = 0.
$$</p>

<p>Now solve for $a$ and $b$:</p>

<p>$$
a = \frac{2d-kt}{t^3}
$$
$$
b =  \frac{kt-3d}{t^2}
$$
This is easy to play with in Excel. Then you can change the values of $d$, $t$ and $k$ and see what happens.</p>

<p>I put these into Excel with the values $d=10$, $t=12$ and $k=2$ and got this picture:</p>

<p><a href=""https://i.stack.imgur.com/p0ZLD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p0ZLD.png"" alt=""enter image description here""></a></p>

<p>Warning: If you try with $k$ too large to get a steeper descent at $t$ then $b$ will become positive and $f(x)$ will become larger than $d$ for a while. That suggests that the best (steepest) value for $k$ is $3d/t$, which will make $b=0$. If I wanted to spend more time on this I'd just assume that from the start. I leave that to you.</p>

<p><strong>Edit:</strong> In retrospect, this is much easier. (Often in mathematics you think ""that's obvious"" - but only after you solved the problem a hard way.)</p>

<p>Given the value $d$ when the bananas are new and the time $t$ when they're free, 
put the inflection point of the cubic at $x=0$:
$$
f(x) = ax^3 +d .
$$
Then  $f(0) = d$. Setting $a = -d/t^3$ makes $f(t) = 0$.</p>
"
"2390212","2390218","<p>Because the definite integral is calculating the area between the curve and the horizontal axis. It isn't just counting points.</p>

<p>In the case of $f(x)=1$ on $[-1,1]$, the region under the curve is a rectangle with length $2$ and height $1$. This gives an area of $2$.</p>
"
"2390225","2390238","<p>$1=-2yy'$, which gives $y'=-\frac{1}{2y}$.</p>

<p>Thus, a slop of the  normal it's $2\cdot\frac{\sqrt3}{2}=\sqrt3$.</p>

<p>Thus, the equation of the normal it's $y-\frac{\sqrt3}{2}=\sqrt3(x-0)$ or $y=\sqrt3x+\frac{\sqrt3}{2}$.</p>

<p>An equation of the tangent it's indeed, $$y-\frac{\sqrt3}{2}=-\frac{1}{\sqrt3}x$$ or
$$\frac{1}{\sqrt3}x+y-\frac{\sqrt3}{2}=0,$$ 
which gives a vector normal $\frac{1}{\sqrt3}\vec{i}+\vec{j}$.</p>
"
"2390233","2390246","<p>If the second limit exists (finite or infinite), then also the first one exists and the two limits are equal. It is a particular case of the <a href=""https://en.wikipedia.org/wiki/Stolz%E2%80%93Ces%C3%A0ro_theorem"" rel=""nofollow noreferrer"">Cesaro-Stolz theorem</a>.</p>
"
"2390239","2390240","<p>The notation $\pi$ here is a parameter in the interval $[0,1]$ representing the probability of success of a Bernoulli trial. It is not referring to the constant $3.14\ldots$.</p>
"
"2390244","2390249","<p>As you have proven by yourself,
$$
 0.999999999999999999999\dot9 = 1.
$$
There are no (big) logical mistakes in your post.</p>

<p>Because $0.99\dots$ is equal to $1$, it also cannot be another fraction.</p>
"
"2390245","2390261","<p>The framework of Fourier-series with trigonometric basis functions only works in $L^2$, ie. the space of square integrable functions. This means that you can write a function as a Fourier-series iff its square is integrable.</p>

<p>However, $\tan(x)^2\notin L^2$. Take $x$ near $\frac \pi 2$: There, to the first order, $\sin x \approx 1$, and $\cos x\approx -x+\pi/2$. Therefore, locally $\tan(x)^2=\left(\frac{\sin x}{\cos x}\right)^2\approx \frac 1{(-x+\pi/2)^2}$. The singularity on the r.h.s. is obviously not square integrable, and so $\tan(x)^2$ isn't either.</p>
"
"2390253","2390275","<p>You are correct, indeed
$$
\frac{d}{dt}\left[\sum_x e^{tX}p(x)\right]
= \sum_x \frac{d}{dt}\left[e^{tX}p(x)\right]
= \sum_x \frac{d}{dt}\left[e^{tX}\right]p(x)
$$
as you suggest since $p(x)$ is constant with respect to $t$. Ross does not need to simplify it for some reason.</p>
"
"2390260","2390381","<p><strong>If that helps</strong>:</p>

<p>Using complex numbers, let </p>

<p>$$z=\frac1{q+s\text{ cis}(\phi-\omega)}.$$</p>

<p>Then</p>

<p>$$k_\phi=\frac1{2\Re(z)},\\
\rho_\phi=\phi+2\angle z.$$</p>
"
"2390263","2390266","<p>You can also set $x-1 = \sqrt{8} \cos t$ and $y+1 = \sqrt{8} \sin t$ this will verify the equation and the parametrization becomes $c(t) = \langle 1 + \sqrt{8} \cos t, -1 + \sqrt{8} \sin t\rangle$</p>
"
"2390271","2390315","<p>You have several problems with this question.  One is that replacing functions with values that are asymptotically equal is not justified.  Consider $f(x)=x^2, g(x)=x^2+\frac 12, h(x)=x^2+\cos x$.  They are all asymptotically $x^2$ but comparing them depends on the non-leading terms.  Second, $\operatorname{li}(n)$ is a much better approximation to $\pi(n)$ than $\frac n{\ln (n)}$.  Skewes' number depends on the comparison between $\pi(n)$ and $\operatorname{li}(n)$.  $\frac n{\log n}$ is not involved.  Your final inequality is false for $n \gt 3.847$  Finally, you seem to expect that $10^{19}$ or Skewes' number is large enough for asymptotics to have taken hold.  It would not be hard to construct an example where this fails.  We could ask for what $n$ does $10^{-6}\log (\log(n))$ exceed $1$.  It obviously does eventually, but for <em>much</em> larger values than we are talking about here.</p>
"
"2390286","2390289","<p>It looks like all of your arrays are sorted. This means that it is very quick to search for a specific element in any array. </p>

<p>It can be done in $O(\log n)$ using a <a href=""https://en.wikipedia.org/wiki/Binary_search_algorithm"" rel=""nofollow noreferrer"">binary search</a>. </p>

<hr>

<p>Suppose you want to find $437$ in your first array, which has length $16$. First, you should check whether $437$ is in the first half of the array or the second half of the array.</p>

<ul>
<li><p>Compare $437$ (the entry you wish to find) with $499$ (which is the $8^\text{th}$ entry). Since $437$ is less than $499$, the entry you want to find is in the first half of the array.</p></li>
<li><p>Compare $437$ (the entry you wish to find) with $251$ (which is the $4^\text{th}$ entry). Since $437$ is greater than $251$, the entry you want to find is in the second half of the first half of the array. </p></li>
<li><p>Compare $437$ (the entry you wish to find) with $375$ (which is the $6^\text{th}$ entry). Since $437$ is greater than $375$, the entry you want to find is in the second half of the second half of the first half of the array. </p></li>
</ul>

<p>At this point, you know that $437$ appears after the $6^\text{th}$ entry and before the $8^\text{th}$, so you can deduce that it must be the $7^\text{th}$ entry. </p>

<hr>

<p>Using a binary search allows you to avoid looping over the entire array.</p>
"
"2390295","2390346","<p>$$
\begin{array}{c|c|c}
\text{Letters}  &amp; \text{Calculation} &amp; \text{# Possibilities}\\
\hline\\
AAASS &amp;  \displaystyle\binom{5}{3} &amp; 10 \\\\
AAASB &amp;  \displaystyle\binom{5}{3} \cdot 2 &amp; 20\\\\
AAABB &amp;  \displaystyle\binom{5}{3} &amp; 10\\\\
AASSS &amp;  \displaystyle\binom{5}{2} &amp; 10\\\\
AASSB &amp;  \displaystyle\binom{5}{2} \cdot \binom{3}{2} &amp; 30\\\\
AASBB &amp;  \displaystyle\binom{5}{2} \cdot 3 &amp; 30\\\\
ASSSB &amp;  5\cdot \displaystyle\binom{4}{3} &amp; 20\\\\
ASSBB &amp;  5\cdot \displaystyle\binom{4}{2} &amp; 30\\\\
SSSBB &amp;  \displaystyle\binom{5}{3} &amp; 10\\\\
\end{array}
$$</p>

<hr>

<p>This gives a total of $$\boxed{170}$$</p>
"
"2390304","2390356","<p>Since $A$ has rank $n$ its columns form a basis for $\mathbb{R}^n$. Assuming $u\neq 0$, there exists a nonzero vector $x\in\mathbb{R}^n$ such that $u = Ax$. Thus</p>

<p>$$
A - ue^T = A - Axe^T = A(I - xe^T),
$$</p>

<p>where $e$ is the $n$-vector of all ones. Since $A$ has rank $n$,</p>

<p>$$
\operatorname{rank}(A-ue^T) = \operatorname{rank}(A(I - xe^T)) = \operatorname{rank}(I - xe^T)
$$</p>

<p>So you have to show that $\operatorname{rank}(I - xe^T) \geq n - 1$, or equivalently that the nullspace of $I - xe^T$ has at most dimension one. To do so, suppose that $v$ is a nonzero vector in $\mathbb{R}^n$ that satisfies the equation</p>

<p>$$
(I - xe^T)v = 0 \iff v_i = x_i\sum_{j=1}^n v_j,~~i = 1,\ldots,n
$$</p>

<p>which is possible if and only if $e^Tx = 1$. Thus, if $e^Tx = 1$, then the nullspace of $I-xe^T$ is $\operatorname{span}\{x\}$ which has dimension one, otherwise, it is the trivial nullspace $\{0\}$ which has dimension zero.</p>
"
"2390309","2390336","<p>$$\lim_{x\to 0}{[\frac{f(x)\cdot{g(x)}-1}{\sin(x)}]}=\\\lim_{x\to 0}{[\frac{f(x)\cdot{g(x)}-1-g(x)+g(x)}{\sin(x)}]}=\\\lim_{x\to 0}{[\frac{(f(x)-1){g(x)}+g(x)-1}{\sin(x)}]}$$divide into cases
$$=\\\lim_{x\to 0}{[\frac{(f(x)-1){g(x)}}{\sin(x)}]}+\lim_{x\to 0}{[\frac{g(x)-1}{\sin(x)}]}=\\\underbrace{\lim_{x\to 0}{g(x)}}_{w.r.t \space  \lim_{x\to 0}{\frac{g(x)-1}{\sin(x)}}=3 \implies g(0) \to 1}\lim_{x\to 0}{[\frac{(f(x)-1)}{\sin(x)}]}+\lim_{x\to 0}{[\frac{g(x)-1}{\sin(x)}]}=1.2+3=5$$ for the second one </p>

<p>$$\lim_{x\to 0}{\frac{x\cdot{f(x)}-g(x)\cdot{\sin(x)}-x+\sin(x)}{\sin^2(x)}}=\\\lim_{x\to 0}{\frac{x({f(x)}-1)-(g(x)-1){\sin(x)}}{\sin^2(x)}}=\\
\lim_{x\to 0}{\frac{x({f(x)}-1)}{\sin^2(x)}}-\lim_{x\to 0}{\frac{(g(x)-1){\sin(x)}}{\sin^2(x)}}=\\\text{simplify   and you know },\lim_{x\to 0}\frac{\sin x}{x}=1\\
\lim_{x\to 0}{\frac{({f(x)}-1)}{\sin(x)}\cdot\frac{x}{\sin x}}-\lim_{x\to 0}{\frac{(g(x)-1)}{\sin(x)}}=\\2.1-3=-1$$</p>
"
"2390312","2390367","<p>We have $\operatorname{Gal}(E/F_1) \cong H_1$ and $E^{H_2} = F_2$ (which does not depend on whether we consider $H_2$ as a subgroup of $H_1$ or $\operatorname{Gal}(E/F)$), so under the Galois correspondence for $E/F_1$, $F_2$ corresponds to $H_2$. Thus $H_2 \unlhd H_1$ iff $F_2/F_1$ is Galois.</p>
"
"2390316","2390327","<p>$$|2[1-(-\frac{1}2)^n]-2|&lt;0.001\\
|-2(\frac{-1}2)^n|&lt;0.001\\
+2|(\frac{-1}2)^n|&lt;0.001\\\to \text{abs function properties  } |\frac{-1}{2^n}|=\frac{1}{2^n}\\
+2.\frac{1}{2^n}&lt;\frac{1}{1000}\\
\frac{2^n}{2}&gt;1000\\
2^{n-1}&gt;1000\\\text{note that  } 2^{10}=\color{red} {1024&gt;1000}\\\to \\n-1\geq 10\\n \geq 11$$</p>
"
"2390320","2390337","<p>This pretty much correct.  One step you might explain a bit more carefully is when you say ""there is an infinite number of $a_n$'s terms in $(a_0-\epsilon, a_0+\epsilon)$"".  What you know is that there exists some $x_j\in (a_0-\epsilon,a_0+\epsilon)$.  You have also said earlier that there are infinitely many $a_n$ which are within any given $\epsilon$ of $x_j$.  However, you have to be a bit careful how you apply this, since you can't use the same $\epsilon$: you need your $a_n$'s to be contained in $(a_0-\epsilon,a_0+\epsilon)$, not in $(x_j-\epsilon,x_j+\epsilon)$.  What you can say is that since $x_j\in (a_0-\epsilon,a_0+\epsilon)$ there is some $\epsilon'&gt;0$ such that $(x_j-\epsilon',x_j+\epsilon')\subseteq (a_0-\epsilon,a_0+\epsilon)$, and then since there are infinitely many $a_n$ in $(x_j-\epsilon',x_j+\epsilon')$ you get what you want.</p>
"
"2390325","2390329","<p>I would write $$x=\frac{1}{2}(x+Tx)+\frac{1}{2}(x-Tx)$$</p>
"
"2390349","2390357","<p>This solution assumes that the balls are chosen without replacement. In other words, I'll assume that the second ball is chosen from a pool of $79$ possibilities once the first ball is removed. </p>

<hr>

<p>Suppose you've picked the first ball already. </p>

<p>There are $79$ remaining balls. Of these, $15$ are the same color as the first, and the rest are not. This means that the probability of the balls being the same color is $$\boxed{\frac{15}{79}\,}$$ </p>
"
"2390364","2390384","<p>Why do you think your reasonning is flawed ?</p>

<p>If all prime factors where superior to $89$, they would be at least $97$. Counting them with their multiplicity, if there was only one such factor it would be $8881$, which contradicts the given fact that $8881$ is not prime. If there are at least two (possibly equal) factors $a$ and $b$, then $ab\leq 8881$ but $ab\geq 97*97&gt;8881$, contradiction.</p>

<p>Your reasonning could be better worded but it is correct.</p>
"
"2390368","2390376","<p>Your second premise is incorrect - distinguishability isn't a factor in choosing combinations or permutations; rather, these choices correspond to caring about <em>order</em> or not.</p>

<p>Both of you treat the balls of the same color as distinguishable (and rightly so). Note here that your fraction is essentially the same as your teacher's - multiply by $\frac{2!}{2!}$. This is because, where your teacher counts the number of ways to choose ""pairs"" of balls, you count the number of ways to choose ""ordered pairs"" of balls. Since this term to account for order is included in both the numerator and denominator, they end up canceling out and you end up with the same result. For counting problems like this, both approaches will lead you to the correct answer.</p>
"
"2390369","2392697","<p>The authors mean the weak* topology on $X'$, i.e., the topology induced by the family of seminorms $p_x(x^*) = |\langle x^*, x\rangle|$, $x\in X$. So, for every  finite dimensional space $M$ the map $K\cap M\to X'$ must be continuous with respect to the standard topology on $M$, restricted to $K\cap M$, and the weak* topology on $X'$. </p>

<p>Another way to express the above: for every $x\in X$, the scalar function $z\mapsto \langle Az, x\rangle$  is continuous on $K\cap M$. You can see this fact being used on page 85, soon after the definition.</p>
"
"2390379","2390535","<p>Note that $1-1/\sqrt n &gt; 1/2$ for $n&gt;4.$ For such $n$ the expression is at least $e^n\cdot (1/2)^n = (e/2)^n \to \infty.$</p>
"
"2390390","2390806","<p>The point youâve labeled $c$ is the intersection of the line with the perpendicular plane to this line through $t$. The points $t$ and $t'$ are then related via the formula $$t'=t+(c-t)+(c-t)=2c-t.$$  </p>

<p>Since most of the other answers so far bring differential calculus to bear, Iâm going to give you a more geometrical approach. The direction vector of the line is $(4,-3,16)$, so the equation of the perpendicular plane through t is $$4x-3y+16z=4t_x-3t_y+16t_z.$$ Eliminate $\lambda$ from ${x+1\over4}={y+1\over-3}={z-15\over16}=\lambda$ to get a pair of linear equations that describe two planes that contain the line, e.g., $3x+4y+7=0$ and $4x-z+19=0$. The point $c$ lies at the intersection of these three planes, i.e., is the solution to the system of the three equations, which you can find using your favorite method. (Back-substitution might be reasonably fast here because two of the equations involve only two variables each.) Once you have $c$, use the formula from the beginning of this answer to compute $t'$.  </p>

<hr>

<p><a href=""https://math.stackexchange.com/a/2390821/265466"">Hardmathâs answer</a> reminded me of another geometric solution. From an equation of a line of the form ${x-x_0\over a}={y-y_0\over b}={z-z_0\over c}$, you can derive the parametric form $(x_0,y_0,z_0)+\lambda(a,b,c)$, or, more succinctly, $p_0+\lambda v$. The vector $v$ gives the direction of the line. As shown in your diagram, the point $c$ is the orthogonal projection of $t$ onto the line, which can be computed using a standard formula: $$c=p_0+{(t-p_0)\cdot d\over d\cdot d}d=p_0+{t\cdot d-p_0\cdot d\over d\cdot d}d.$$ The reflected point $t'$ is then obtained via the formula at top. Plugging in the values in your example, we have $$\begin{align}c &amp;= (-1,-1,15)+{4t_x-3t_y+16t_z-\left((-1)(2)+(-1)(-3)+(15)(16)\right)\over 4^2+(-3)^2+16^2}(4,-3,16) \\ &amp;=(-1,-1,15)+{4t_x-3t_y+16t_z-239\over16+9+256}(4,-3,16).\end{align}$$ The fraction in the second line should look familiar: itâs $\lambda$ from Hardmathâs answer.</p>
"
"2390397","2390642","<p>No, this need not be the case. Let $H=\ell^2$ and $\Omega=[0,\infty)$ with the Lebesgue measure. Let $X=(\mathbf1_{[n,n+1]})_{n\in\mathbb N}$, and let $(e_n)$ be the standard basis. Then $X_n=\mathbf1_{[n,n+1]}$ which is integrable, but $\|X\|_H=1$ $P$-almost everywhere, so $\int_0^\infty\|X\|_Hdx=\infty$.</p>

<p>EDIT: Originally the question asked for a $\sigma$-finite measure space, not finite. The same idea still works for a counterexample. Instead, let $\Omega=[0,1]$ with the Lebesgue measure and suppose $(a_n)_{n\in\mathbb N}$ is a strictly increasing sequence in $[0,1]$ with $a_0=0$ and $\lim_{n\to\infty}a_n=1$. Let $X=(\frac1{a_{n+1}-a_n}\mathbf1_{[a_n,a_{n+1}]})_{n\in\mathbb N}$. Then $\int_0^1X_ndx=1$ for all $n\in\mathbb N$ and $\int_0^1\|X\|_Hdx=\infty$.</p>
"
"2390405","2390412","<p>One may write
$$
\begin{align}
\lim_{h\to 0^+} h \sum_{n=1}^\infty \frac {\sin^{2} (nh)} {(nh)^{2}}&amp;=\lim_{h\to 0^+} h \left[\lim_{M\to \infty}\sum_{n=1}^M \frac {\sin^{2} (nh)} {(nh)^{2}}\right]
\\&amp;=\lim_{h\to 0^+} h \left[\lim_{M\to \infty}\frac1{h^2}\sum_{n=1}^M \frac {\sin^{2} (nh)} {(n)^{2}}\right]
\\&amp;=\lim_{h\to 0^+} h\left[\frac1{h^2}\lim_{M\to \infty}\sum_{n=1}^M \frac {\sin^{2} (nh)} {(n)^{2}}\right]\quad (\text{convergence of the series*})
\\&amp;= \lim_{h\to 0} \frac{1}{h} \sum_{n=1}^\infty \frac {\sin^{2} (nh)} {(n)^{2}}
\end{align}
$$ as desired. </p>

<p>$\text{*}$Notice the uniform convergence
$$
\left| \sum_{n=1}^\infty \frac {\sin^{2} (nh)} {(n)^{2}}\right|\le  \sum_{n=1}^\infty \frac {1} {(n)^{2}}&lt;\infty.
$$</p>
"
"2390406","2390435","<p>Riemann sums compute definite integrals.  You have asked for an antiderivative, an indefinite integral, so there is more to this than just computing the sum.  The <a href=""https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus"" rel=""nofollow noreferrer"">fundamental theorem of calculus</a> tells us two things.  The second is that for an integrable function $f$, having antiderivative $F$, $$\int_a^b f(x) \,\mathrm{d}x = F(b) - F(a)  \text{.}$$  Note that a given $f$ has infinitely many choices of $F$, all of which work.  (The theorem is actually a little more specific about intervals of definition.  All the functions we will encounter are sufficiently well behaved on the entire real line, so I have suppressed these details.)</p>

<p>You seem to have found that the definite integral $$\int_a^b 2x \,\mathrm{d}x = b^2 - a^2  \text{.}$$  Note that this is a function of $b$ minus a function of $a$ and both these functions are the ""square my input"" function, $F(x) = x^2$.  So by comparing terms in this particular definite integral with the result in the fundamental theorem, we find an antiderivative.  Since we have used a non-reductive step, we should check that our result is valid: $\frac{\mathrm{d}}{\mathrm{d}x} x^2 = 2x$, as desired.</p>

<p>But can we be a little more rigorous?  Yes.  The first part of the fundamental theorem tells us $$ F(x) = \int_a^x f(x) \,\mathrm{d}x  \text{.}$$  (The notation here is awful.  We have used ""$x$"" in both a bound and an unbound context.  To not write intentionally ambiguous expressions, we should write instead $ F(x) = \int_a^x f(t) \,\mathrm{d}t $.  But more than a century of convention of misusing $x$ in both places means you should get used to seeing and parsing such expressions.)  But we've already done this calculation.  We were just unfortunate in calling the variable $x$ by the name ""$b$"" previously.  That is, $$F(x) = \int_a^x 2t \,\mathrm{d}t = x^2 - a^2  \text{.}$$  Note that we have recovered the $x^2$ as above, but have an additional ""${}-a^2$"".  But $a$ is some constant.  If we had chosen any other $a$, this would only translate the graph of $F$ vertically, which would change its derivative, $f$, not at all.  So we are free to make any choice of $a$ and we will get one of the many antiderivatives of $f$.  We could just as easily have written $F(x) = x^2 + C$ for $C = -a^2$.  While $C = -a^2$ is always non-positive for real $a$, the expression $x^2 + C$ is a valid antiderivative of $f$ for any choice of $C$ from the reals.  Consequently, a more general antiderivative of $f$ is the function $x^2 + C$.</p>
"
"2390409","2390432","<p>Let's look at the a more general case:  a function of the form $f(x^0 - |\mathbf{x}|)/|\mathbf{x}|$.  For the sake of compactness, I'll switch to spherical polar coordinates where $|\mathbf{x}| = r$.  We therefore have
$$
\begin{multline}
\left[\left(\frac{\partial}{\partial x^0}\right)^2 - \nabla^2 \right] \frac{f(x^0-|\mathbf{x}|)}{|\mathbf{x}|} \\= \frac{f''(x^0 - r)}{r} - f(x^0 - r) \nabla^2 \left( \frac{1}{r} \right) - \frac{1}{r} \nabla^2 f(x^0 - r) - 2 \left[ \nabla \left(\frac{1}{r}\right) \right] \cdot \left[\nabla (f(x^0 - r)) \right]
\end{multline}
$$In spherical coordinates, we have
$$
\frac{1}{r} \nabla^2 f(x^0 - r) = \frac{1}{r} \left[ \frac{1}{r^2} \frac{\partial}{\partial r} \left( r^2 \frac{\partial f(x^0 - r)}{\partial r} \right)  \right] = \frac{1}{r} f''(x^0 - r) - \frac{2}{r^2} f'(x^0 - r)
$$
and 
$$
2 \left[ \nabla \left(\frac{1}{r}\right) \right] \cdot \left[\nabla (f(x^0 - r)) \right] = 2 \left( - \frac{1}{r^2}  \hat{r} \right) \cdot \left( - f'(x^0 - r) \hat{r} \right) = \frac{2}{r^2} f'(x^0 - r).
$$
(If you wish to check these, note that
$$
\frac{\partial f(x^0 - r)}{\partial r} = - f'(x^0 - r) \qquad \frac{\partial^2 f(x^0 - r)}{\partial r^2} = f''(x^0 - r)
$$
due to the chain rule.) Putting all of these together, we find that
$$
\frac{f''(x^0 - r)}{r}  - \frac{1}{r} \nabla^2 f(x^0 - r) - 2 \left[ \nabla \left(\frac{1}{r}\right) \right] \cdot \left[\nabla (f(x^0 - r)) \right] = 0,
$$
and so for any function $f$, we have
$$
\left[\left(\frac{\partial}{\partial x^0}\right)^2 - \nabla^2 \right] \frac{f(x^0-|\mathbf{x}|)}{|\mathbf{x}|} = - f(x^0 - r) \nabla^2 \left( \frac{1}{r} \right) = 4 \pi f(x^0 - |\mathbf{x}|) \delta^3(\mathbf{x}).
$$
The above derivation works for any function of $x^0 - r$ alone, including $\delta(x^0 - r)$.</p>

<p>The reason this works for any function, by the way, is that the general solution for the wave equation in three dimensions under the assumption of spherical symmetry (and excluding the origin) is
$$
u(r, t) = \frac{1}{r} \left[ f(r - t) + g(r + t) \right]
$$
for any two functions $f$ and $g$.</p>
"
"2390414","2390421","<p>It is true that the boundary of the surface in the plane projects a circle of radius $1$ onto the $xy$ plane. The surface on the plane itself is not a circle nor not even a disk, to my imagination it would be an ellipse. As such it's area is not $\pi$ but rather $\sqrt{5}\pi$. This factor shows up because of tilting.</p>

<p>We are looking at the surface $S$ with $(x-1)^2+y^2 \leq 1$ and $z=2x$. We wish to calculate,</p>

<p>$$\iint_{S} dS$$</p>

<p>Because $z=2x$ we have $z_x=2$ and $z_y=0$, hence $\sqrt{1+z_x^2+z_y^2}=\sqrt{5}$ and $dS=\sqrt{5} dA$, so:</p>

<p>$$\iint_{S} dS=\sqrt{5} \iint_{(x-1)^2+y^2\ \leq 1}  dA$$</p>

<p>$$=\sqrt{5} \pi$$</p>
"
"2390415","2391754","<p>As Lee Mosher points out, this fails to even be a surface. The reason is that a typical open neighborhood of a point in the interior of an ""$a$"" or ""$b$"" edge looks like three open half-disks glued together along their diameter. But this space cannot be homeomorphic to the plane. Indeed, removal of a point inside the common diameter yields a space homotopic to three intervals with corresponding endpoints identified, and this in turn is homotopic to a wedge of two circles. On the other hand, removing a point from inside the plane results in a space homotopic to a circle.</p>
"
"2390417","2390519","<p>You can take $$k=a+b-2\sqrt{ab+1}$$ 
For example if $ab+1=n^2$, then you get $$ak+1=a^2+n^2-1-2an+1=(a-n)^2$$and $$bk+1=n^2-1+b^2-2bn+1=(b-n)^2$$</p>
"
"2390419","2390445","<p>Say $x= \tan A$, $y= \tan B$ and $z = \tan C$. It is
well-known that $x+y+z=xyz$. Then we have $$E = {x^2+y^2\over
x^4+y^4} +{y^2+z^2\over y^4+z^4} + {z^2+x^2\over
z^4+x^4}$$
By Cauchy inequality and then by $AM-GM$ we have
$${x^2+y^2\over x^4+y^4} \leq {2\over x^2+y^2}\leq {1\over xy}$$
So $$ E \leq {1\over xy} +{1\over yz}+{1\over zx} = 1$$
We get equality exactly when $x=y=z$, thus $3x=x^3 \Longrightarrow  x=\sqrt{3}$.  </p>
"
"2390423","2390434","<p>When $x = -1$, we have
$$ (-1)^2 - (-1)y + y - 3 = 1 + y + y - 3 = -2 + 2y = 0
\implies y = 1.
$$
Thus we are seeking $\frac{\mathrm{d}y}{\mathrm{d}x}$ when $(x,y) = (-1,1)$.  Via implicit differentiation, we obtain
\begin{align}
x^2 - xy + y - 3 = 0 &amp;\implies 2x - \left( x\frac{\mathrm{d}y}{\mathrm{d}x} + y\right) + \frac{\mathrm{d}y}{\mathrm{d}x} = 0 \\
&amp;\implies 2x - x\frac{\mathrm{d}x}{\mathrm{d}y} - y + \frac{\mathrm{d}x}{\mathrm{d}y} = 0 \\
&amp;\implies (1-x)\frac{\mathrm{d}x}{\mathrm{d}y} = y-2x \\
&amp;\implies \frac{\mathrm{d}x}{\mathrm{d}y} = \frac{y-2x}{1-x}.\tag{$\ast$}
\end{align}
Substituting the point $(x,y) = (-1,1)$ into ($\ast$), we conclude that
$$ \frac{\mathrm{d}y}{\mathrm{d}x} = \frac{1-2(-1)}{1-(-1)} = \frac{1+2}{1+1} = \frac{3}{2}. $$
This gives the slope of the tangent line.  To obtain the equation for the tangent line, remember that a line with slope $m$ passing through a point $(x_0,y_0)$ has equation
$$ y-y_0 = m(x-x_0).$$
Therefore an equation for the line tangent to the given curve through the point $(-1,1)$ is given by
$$y - 1 = \frac{3}{2} (x+1). $$</p>
"
"2390426","2390442","<p>$$\begin{eqnarray*}\arcsin(1)-\arcsin\left(\frac{n}{n+4}\right) &amp;=&amp; \int_{\frac{n}{n+4}}^{1}\frac{dz}{\sqrt{1-z^2}}\\&amp;\stackrel{z\mapsto 1-x}{=}&amp;\int_{0}^{\frac{4}{n+4}}\frac{dx}{\sqrt{x(2-x)}}\\&amp;\stackrel{x\to u^2}{=}&amp;2\int_{0}^{\frac{2}{\sqrt{n+4}}}\frac{du}{\sqrt{2-u^2}}&amp;\end{eqnarray*}$$
leads to $\frac{\pi}{2}-\arcsin\left(\frac{n}{n+4}\right)\sim\sqrt{\frac{8}{n+4}}$ in a more efficient way, but your solution is just fine.</p>
"
"2390443","2390448","<p>Radians are units used for measuring angles.  One presumes that you are trying to find the point on the unit circle that corresponds to an angle of $a$ radians.  </p>

<p>That is, draw ray originating at the origin that makes an angle of $a$ radians to the positive $x$-axis, measuring anti-clockwise.  This ray will intersect the circle of radius $r$ at some point.  The coordinates of that point will be
$$ (x_a, y_a) = (r \cos(a), r\sin(a)).$$</p>

<p>This follows from the definition of the sine and cosine functions.  To wit, if we draw the ray described above, it will intersect the unit circle, i.e. the circle of radius 1 centered at the origin which is given by the equation
$$ x^2 + y^2 = 1,$$
at some point.  By definition, we say that the cosine of $a$ is the $x$-coordinate of that point, and that the sine of $a$ is the $y$-coordinate of that point.</p>

<p><a href=""https://i.stack.imgur.com/snoUq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/snoUq.png"" alt=""enter image description here""></a></p>

<p>To get the point on the circle with radius $r$, dilate by a factor of $r$.  This will multiply every length by $r$.  In particular, the $x$-coordinate of the intersection goes from $\cos(a)$ to $r \cos(a)$, and the $y$-coordinate of the intersection goes from $\sin(a)$ to $r \sin(a)$.</p>
"
"2390444","2390460","<p>Let us devise a purely combinatorial approach: assume to have a parliament with $n$ people in the right wing, $n$ people in the left wing. In how many ways can we form a committee with $n$ people and elect a chief of the commitee from the left wing? The first approach is to select $i$ people from the left wing, $n-i$ people from the right wing, then the chief among the selected $i$ people from the left wing. This leads to $\sum_{i=0}^{n}i\binom{n}{i}\binom{n}{n-i}=\sum_{i=0}^{n}i\binom{n}{i}^2$. The other approach is to select the chief from the left wing first ($n$ ways for doing that), then select $n-1$ people from the remaining $2n-1$ in the parliament. Conclusion:
$$ \sum_{i=0}^{n}i\binom{n}{i}^2 = n\binom{2n-1}{n-1} = n\binom{2n-1}{n}. $$</p>
"
"2390456","2390473","<p>If $v$ is an eigenvector for $\lambda$, then $\text{Re}(M^n v) = M^n \text{Re}(v)$ and $\text{Im}(M^n v) = M^n \text{Im}(v)$.  If $\|M^n v\| &gt; N$, then at least one of these has norm $&gt; N/2$.  Thus for $b = 0$ and at least one of $x_0 = \text{Re}(v)$ and $x_0 = \text{Im}(v)$, the iteration doesn't converge.</p>
"
"2390464","2390478","<p>Here is a picture that attempts to describe the situation:</p>

<p><a href=""https://i.stack.imgur.com/dfufp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dfufp.png"" alt=""enter image description here""></a></p>

<p>The sun will be obscured by the peak if the angle of elevation to the peak $\beta$ is larger than the angle of elevation to the sun, which we know is $38^{\circ}$.  Otherwise, the sun will not be obscured.</p>

<p>We are given that the distance from You to $A$ is 2 miles, and that the difference in altitude from you to the peak is $7200-4000 = 3200$ feet.  Hence
$$ \tan(\beta) = \frac{\text{opp}}{\text{hyp}} = \frac{3200\text{ ft}}{2\text{ mi}} = \frac{3200\text{ ft}}{2\text{ mi}} \cdot \frac{1\text{ mi}}{5280\text{ ft}} \approx 0.30 $$
(note that the units cancel each other out---the tangent is a unitless quantity).  Then
$$ \beta \approx \arctan(0.30) \approx 17^\circ.$$
For those familiar with the unit circle and the difficulties of defining inverses of periodic functions, note that we are working exclusively in the first quadrant, thus the angle we want really is $\arctan(0.30)$, rather than this plus some integer multiple of $90^{\circ}$.</p>

<p>Therefore, since the angle to the peak (approx. $17^{\circ}$) is less than the angle to the sun ($38^{\circ}$), the peak will not obstruct the sun.</p>
"
"2390471","2390474","<p>Here's how I'd approach this particular problem. I'll solve for $k$ monkeys afterwards.</p>

<p>You have $5!$ ways for the monkeys to be arranged in a line without restriction.</p>

<p>There are $8$ ways that $A$ and $B$ can be positioned next to each other; there are $4$ pairs of adjacent spaces, and either $A$ or $B$ can be on the left.</p>

<p>For each of these cases, there are $3! = 6$ ways to arrange the other three monkeys.</p>

<p>So the answer is $5! - 8 \cdot 3! = 72$ ways.</p>

<p>Now, just apply to $k$ monkeys using the same argument:</p>

<p>$$P(k) = k! - 2(k-1)(k-2)! = k! - 2(k-1)!.$$</p>

<p><em>(Hat tip to user471297 for the last simplification.)</em></p>
"
"2390487","2390494","<p>HINT: write your term in the form
$$\exp\left(\frac{\ln(n^2+2n+3)}{n}\right)$$</p>
"
"2390495","2390571","<ol>
<li>If the maximum occurs at only one of the extreme points.</li>
<li>No.  Consider a triangle in the plane with vertices $[0,0], [1,0], [1,1]$ and $F([x,y]) = x$.</li>
<li>No.  Convex combinations of maximizers are also maximizers.</li>
</ol>
"
"2390502","2390518","<p>Let your two permutations be $\vec{u} = (U_1, U_2, \ldots, U_n)$ and $\vec{v} = (V_1, V_2, \ldots, V_n)$.  Then what you're asking for is</p>

<p>$$E \left( \sum_{i=1}^n |U_i - V_i| \right)$$</p>

<p>and by linearity of expectation this is</p>

<p>$$\sum_{i=1}^n E (|U_i - V_i|).$$</p>

<p>Now for any given $i$, $U_i$ is uniformly distributed on $\{1, 2, \ldots n\}$; $V_i$ has the same distribution; and the two are independent.  (This is because they come from different permutations!  $U_1$ and $U_2$, for example, are not independent.)</p>

<p>Now, let's find $E(|U_i - V_i|)$.  Clearly $P(U_i = j, V_i = k) = 1/n^2$ for any $j, k$ with $1 \le j, k \le n$.  So you're looking for</p>

<p>$$ E(|U_i - V_i|) = {1 \over n^2} \sum_{j=1}^n \sum_{k=1}^n |j-k| $$</p>

<p>but we can just include all the terms where $j &gt; k$, if we include them <em>twice</em>  -- the $(j, k)$ term wil be the same as the $(k, j)$ term. This gives</p>

<p>$$ E(|U_i - V_i|) = {2 \over n^2} \sum_{j=1}^n \sum_{k=1}^{j-1} (j-k). $$</p>

<p>Now the inner sum is just $1 + 2 + \ldots + (j-1) = j(j-1)/2$, so you get</p>

<p>$$ E(|U_i - V_i|) = {2 \over n^2} \sum_{j=1}^n {j(j-1) \over 2} = {1 \over n^2} \sum_{j=1}^n (j^2 - j). $$</p>

<p>By well-known formulas for the sum of the first $n$ integers and the first $n$ squares this is</p>

<p>$$ {1 \over n^2} \left( {n(n+1)(2n+1) \over 6} - {n(n+1) \over 2} \right) = {1 \over n^2} {n^3 - n \over 3}. $$</p>

<p>At last the expectation you wanted is $n$ times this, or $(n^2 - 1)/3$.</p>
"
"2390506","2390526","<p>1) How do we solve:</p>

<p>$k \text{ in minutes} \le x \text { in microseconds } &lt; (k+1) \text { in minutes}$</p>

<p>We want to know what is $(k+1) \text { in minutes} - x \text{ in microseconds}$</p>

<p>$k*6000 \le x  &lt; (k+1)*6000$</p>

<p>$x = k*6000 + x\%6000$ where $a \%b = r =$ the positive remainder dividing $a$ by $b$.  </p>

<p>(The notation $a \mod b = r$ is not actually correct as $a \mod b$ is not a single number.  $r \equiv a\mod b$ does indeed mean that $r = a - kb$ for some integer $k$ but it doesn't mean that $0\le r &lt; b$.   $k$ may be any integer and $r \equiv a \mod b$ means that $r$ may be one of <em>many</em> possible number of the form $r = a \pm kb$.  So I will use the notation $a \%b$ to mean specifically the smallest non-negative such option.) </p>

<p>So we simply want $y = (k+1)*6000 - x = (k+1)*6000 - (k*6000 + x\% 6000) = 6000 - x\%6000$.</p>

<p>That's all.</p>

<p>2) What did you do wrong.</p>

<p>Basically you assumed $R = X\%Y &gt; \frac Y2$.  </p>

<p>You are correct that $X + Y\%R \equiv \text { top of next minute} \mod R$ but that doesn't mean $X + Y\%R = \text { top of next minute}$.</p>

<p>It means $x + y\%R + j*R = \text {top of next minute}$ for some integer $j$.  If $R &gt; \frac Y2$ then $j = 0$ but otherwise ... not.</p>

<p>Which is good reason why we shouldn't treat $a \mod b$ as though it were a single specific number.</p>
"
"2390511","2390514","<p>$$\left(f(g(x))\right)'=f'(g(x))g'(x).$$
Thus, $\left(\ln{y}\right)'=\frac{1}{y}\cdot{y'}$.</p>

<p>But I like the following way.
$$(x^{\sin{x}})'=\left(e^{\sin{x}\ln{x}}\right)'=x^{\sin{x}}\left(\cos{x}\ln{x}+\frac{\sin{x}}{x}\right).$$</p>
"
"2390532","2390539","<p>It isn't, though physicists might pretend that it is.  If you add the important qualifications that $\psi(x,t)$ is square-integrable for fixed $t$, and that the Hamiltonian is self-adjoint, you need infinite series in the case of discrete spectrum, and integrals in the case of continuous spectrum.</p>
"
"2390538","2390745","<p>For a different approach, choose $x\in X$ so that $X = \cup_{n \in \mathbb{N}} \overline B_x(n).$ Now consider, for fixed $n\in \mathbb N,$ the fact that the compact ball $\overline B_x(n)$ is totally bounded. This means that, for each $m\in \mathbb N$, there is a $\textit{finite}$ set of points $x_{m,n}\in \overline B_x(n)$ such that if $y\in \overline B_x(n)$ then there is an $m\in \mathbb N$ and $x_{m,n}$ such that $d(y,x_{m,n})&lt;1/m,$ which implies that $\left \{ x_{m,n} \right \}_{m\in \mathbb N}$ is a countable dense subset of $\overline B_x(n)$ and therefore that $\left \{ x_{m,n} \right \}_{m,n\in \mathbb N}$ is a countable dense subset of $X$. The result follows by the equivalence separable $\Leftrightarrow $ second countable. </p>
"
"2390543","2390560","<p>If $x_n \to x$, consider $f_n = f \mathbb{1}_{(-\infty, x_n)}$. 
Take $M=\max{x_n}$ and define $h=|f| \mathbb{1}_{(-\infty, M)}$. </p>

<p>Clearly $|f_n(x)| \leq h(x)$ for all $n$ and all $x$. Besides $h$ is integrable because $F(M)$ is finite.</p>

<p>Since $f_n \to f \mathbb{1}_{(-\infty, x)}$ pointwise you can use the dominated convergence theorem to say that</p>

<p>$$\int_{\mathbb{R}}f_n \mathrm{d}\lambda \to \int_{\mathbb{R}}f\mathbb{1}_{(-\infty, x)}\mathrm{d}\lambda$$</p>

<p>which is exactly $F(x_n) \to F(x)$.</p>

<p><strong>Note</strong></p>

<p>I think the argument $|F(x+h)-F(x)| = |\int_x^{x+h}f(t)\mathrm{d}t| \leq |h|\sup_{[x,x+h]}f$ is incorrect, because $f$ can be unbounded. For example take $f(x)=1/\sqrt{x} \cdot 1_{(0,1]}(x)$.</p>
"
"2390545","2390556","<p><strong>Idea:</strong></p>

<p>Let 
$$A=\begin{bmatrix} -x &amp; 1 &amp; \dots &amp; 1 \\ 1 &amp; -x &amp; \dots &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; 1 &amp; \dots &amp; -x \end{bmatrix}.$$
Then
$$A\begin{bmatrix}1\\1\\ \vdots \\1\end{bmatrix}=((n-1)-x)\begin{bmatrix}1\\1\\ \vdots \\1\end{bmatrix}.$$
Thus $(n-1)-x$ is an eigen value of $A$ so it divides the determinant. </p>

<p>Moreover we can think of $\text{det} A$ as an $n^{\text{th}}$ degree polynomial in $x$. An obvious root is $x=-1$ (because $\text{det} A\big|_{x=-1}=0$). Thus you have another factor of the determinant. </p>

<p>So far we have
$$\text{det} A=(x+1)(x-(n-1))p(x),$$
where $p(x)$ is some polynomial of degree $n-2$.</p>

<p>If $x \neq -1, n-1$ then try showing that the columns/rows are linearly independent so  $\text{det} A \neq 0$. Thus the only roots of the determinant polynomial are $-1,n-1$. Now think about how to deal with multiplicity of the root $x=-1$ (hint: when $x=-1$, the $\text{rank}(A)=n-1$.)</p>
"
"2390553","2390568","<p>How about if we allow a different measure space?<br>
Let the measure space be $\Omega = \overline{\mathbb R}^\infty$, the map $f =$ left shift on $\Omega$, map $\psi : \Omega \to \overline{\mathbb R}$ selects the first coordinate.  The measure on $\Omega$ is made from the joint distributions of the random variables $\psi_n$.  In fact there is a canonical measure-preserving map $X \to \Omega$ onto a subset of (outer) measure $1$ by $x \mapsto (\psi_n)_n$.  The difference from what is asked is that the left shift acts on $\Omega$ not on $X$.</p>
"
"2390555","2390624","<p>I don't think there is any problem because $f$ has compact support: when $x\in K$, a compact subset of $\mathbb{R}^d$, the function $y \mapsto f_{x_i x_j}(x-y)$ vanishes outside the compact set $K - \text{supp}(f)$. It means that
$$|\phi(y)f_{x_i x_j}(x-y)| \le |\phi(y)|\mathbb{1}_{K - \text{supp}(f)}(y) \|f_{x_i x_j}\|_\infty  \in L^1(\mathbb{R}^d)$$
Thus, the integrated function is dominated by an integrable function uniformly when $x\in K$. The continuity follows by dominated convergence because $f\in\cal{C}^2$.</p>

<p>Of course, it depends on what you know about the fundamental solution, but you told us you know it is locally integrable and this is sufficient.</p>

<p>Remark: one often uses $\mathbb{R}^d$ instead of $\mathbb{R}^n$ in PDEs because it makes $n$ available for sequences, as you do with $x_n$.</p>
"
"2390565","2391268","<p>Few points to consider before actually working out the solution.</p>

<p><strong>1.</strong> We are not told about which bag is which. This is an important point.</p>

<p><strong>2.</strong> For every pair there could be two possibilities. 1st color may come from 1994 bag or from 19996 bag. (x,y) or (y,x)</p>

<p><strong>3.</strong> We don't know which color came from where. We have to find the numerical value for the possibility.</p>

<h1>Evaluating Sample Space</h1>

<p>There are six colors in each set and thus we have 36 different possibilities. 
Points 2 and 3 in the question do not matter because the order does not matter. </p>

<p>B1 x B2 =  (b1 from B1 and b2 from B2)
All colors are common in both bags except two, viz Tan and Blue.</p>

<p>Since Tan is only present in B1, only (Tan, b2) is possible similarly only (b1,Blue) is possible and not the reverse.</p>

<h1>Apply Conditional probability (Bayes' Theorem)</h1>

<p>The friend gave me a yellow and green M&amp;M and there are two ways of getting them. (Y,G) and (G,Y). I don't know which is from where. Since I am only looking for yellow from 1994 bag, there are two different events as to how bags can be chosen.</p>

<p><strong>E1</strong> = B1, the first bag from which first M&amp;M was drawn was 1994 bag and the other one was from 1996.</p>

<p><strong>E2</strong> = B1, the first bag from which first M&amp;M was drawn was 1996 bag and the other one was from 1994.</p>

<p>In addition let <strong>I</strong> be the event of us getting a yellow M&amp;M and a green M&amp;M irrespective of choice of bags.</p>

<p>Now, we apply Bayes' Theorem as below.</p>

<p>P(E1 | I ) = P(I â£ E1).P(E1) / P(I)                       ......<strong>Eq (1)</strong></p>

<p>Evaluating P(I â£ E1) we get .04
Evaluating P(E1) we get .5</p>

<p>The denominator of Eq. (1) is a harder than above ones.</p>

<p>P(I) = P(I â© (E1 âª E2))= P[(I â© E1) âª (I â© E2)] </p>

<p>= P[(I â© E1) âª (I â© E2)] = </p>

<p>= P (E1). P(I â£ E1) + P (E2). P(I â£ E2) </p>

<p>Now filling in the values here  : </p>

<p>= (1/2). (.04) + (1/2).(0.014)</p>

<p>= 0.027</p>

<p>To compute the final answer it is just matter of substituting final values in Eq (1)</p>

<p>P(E1 | I ) = (0.04) (1/2) / (0.027)</p>

<p>= 0.741 (approx)</p>
"
"2390572","2390583","<p>You can't because it's generally not true.</p>

<p>However, there is a similar relation between the <em>differentials</em>: the trick here is ""logarithmic differentiation"": for positive $z$, you have</p>

<p>$$ \mathrm{d} \log(z) = \frac{\mathrm{d}z}{z} $$</p>

<p>and logarithms turn products into sums. Here, it compues</p>

<p>$$ \mathrm{d} \log(\rho) = \mathrm{d} \log(KP/T) = \mathrm{d}\log(K) + \mathrm{d}\log(P) - \mathrm{d}\log(T) $$</p>

<p>Since $\mathrm{d}K = 0$, this equation leads to</p>

<p>$$ \frac{\mathrm{d}\rho}{\rho} = \frac{\mathrm{d}P}{P} - \frac{\mathrm{d}T}{T} $$</p>

<p>If you're not integrating the differentials to obtain the difference, then the things you can say about the differences $\Delta \rho$ and such are mainly through <em>differential approximation</em>: given a small variation $v$, you have  $\Delta z \approx \nabla_v z$.</p>

<p>If you apply such a $v$ to the above equation and then substitute in the approximation, you get</p>

<p>$$ \frac{\Delta \rho}{\rho} \approx \frac{\Delta P}{P} - \frac{\Delta T}{T}  $$</p>

<p>Note the approximation sign here; unlike the equation between differentials, <em>this</em> equation is generally not exactly true.</p>
"
"2390580","2390616","<p>$4$ is enough: consider a $1 \times 3$ rectangle. Every point in the middle $1 \times 1$ square (and a couple more) satisfies your condition.</p>

<p>Indeed, there is no solution for a triangle $ABC$. Suppose otherwise; choose the endpoints of the longest side (WLOG, denote $AB$). Clearly, the last vertex $C$ has to be the nearest neighbor for both of these endpoints. But we need a point $P$ in the interior of the triangle that satisfies $AP + BP &gt; AC + BC$. You should be able to make some argument as to why this is impossible (I see a calc argument with Pythagoras, but may be a little messy; there's probably an easier way to do it).</p>
"
"2390584","2390588","<p>HINT: it is $$y_{1,2}=-\frac{x}{2}\pm\sqrt{\frac{1}{2}-\frac{3}{4}x^2}$$ and with that $y$ we can compue $z$</p>
"
"2390587","2390601","<p>Yes, this is correct.  Explicitly, since $h_k$ is a polynomial of degree $2k+1$, it has at most $2k+1$ zeroes, so $g_k(x)$ has at most $2k+1$ fixed points.  Note that this argument shows that $f$ and $g_k$ are not conjugate by any bijection at all, not merely that they are not topologically conjugate.</p>
"
"2390595","2390609","<p>To solve inequalities, you need to find a set of solutions as well as show that all solutions are in that set.</p>

<p>That means for an inequality $f(x)&lt;g(x)$ we need to find a set of solutions $S$ such that $f(x)&lt;g(x)\iff x\in S$. So the best way to work with inequalities is to use equivalences rather than implications between lines. Alternatively, you can split the working into $\implies$ and $\impliedby$.</p>

<p>The problem in your working occurs as the first line doesn't even imply the second line. When working with inequalities, you want to apply operations to both sides so that the inequality is preserved.</p>

<p>Taking the reciprocal of both sides does not preserve this (unlike for equalities). If you consider the graph of $f(x)=\frac{1}{x}$, you'll see that for $a,b\in\Bbb R^*$, $a&lt;b$ implies $f(a)&lt;f(b)$ if $a$ and $b$ have different signs, and $f(a)&gt;f(b)$ if $b$ and $a$ have same signs. Hence you need to split into different cases.</p>

<p>Alternatively, you can multiply both sides of the inequality by $2(|x|-3)$, but you still need to consider the different cases which arise from the sign of $|x|-3$.</p>

<hr>

<p>You should instead have something like (note $\land$ and $\lor$ are logical 'and' and 'or' respectively):</p>

<p>$$\frac{1}{|x|-3} \lt \frac{1}{2}$$</p>

<p>$\iff$ (note $|x|-3\neq0$ so this is excluded on the next line)</p>

<p>$$(|x|-3&gt;0\land|x|-3 \gt 2)\lor (|x|-3&lt;0\land|x|-3 \lt 2)$$</p>

<p>$\iff$</p>

<p>$$(|x|&gt;3\land|x| \gt 5)\lor (|x|&lt;3\land|x| \lt 5)$$</p>

<p>$\iff$</p>

<p>$$|x| \gt 5\lor |x|&lt;3$$</p>

<p>$\iff$</p>

<p>$$x\in(-\infty,-5)\cup(-3,3)\cup(5,\infty)$$</p>
"
"2390612","2390633","<p>Hint for simple answer: $\mathbb{Q}(2^{1/2},2^{1/3})=\mathbb{Q}(2^{1/6})$.</p>
"
"2390613","2390622","<p>Since the expressio0n on the left is symmetric in $a,b,c$ and we want to maximize $c$, we may assume that $a\le b\le c$. Then we can drop the absolute value form the second and third summand, i.e., we have
$$1396 = |a+b-c|+b+c-a+c+a-b=|a+b-c|+2c \ge 2c$$
with equality iff $a+b=c$.We conclude that $c\le 698$ and this bound is sharp as it is achieved with $a=b=349$, $c=698$.</p>
"
"2390618","2390690","<p>$$\int_0^{\infty } e^{-x} \sum _{k=0}^{\infty } \frac{(-1)^k (2015 x)^{2 k}}{((2 k)!)^2} \, dx=\sum _{k=0}^{\infty } \int_0^{\infty } \frac{(-1)^k e^{-x} ( x)^{2 k}}{((2 k)!)^2} \, dx$$
Bringing out of the integral sign all the stuff which doesn't depend on $x$
$$\sum _{k=0}^{\infty } \frac{(-1)^k \,2015^{2k}}{((2 k)!)^2} \int_0^{\infty } e^{-x}  x^{2 k} \, dx$$
Note that the integral is the Laplace transform of $x^{2 k}$ when $s=1$
$$\int_0^{\infty } e^{-sx} x^{2 k} \, dx=\mathcal{L}_x\left[x^{2 k}\right](s)=\frac{(2 k)!}{s^{2 k+1}}$$
so that 
$$\int_0^{\infty } e^{-x} x^{2 k} \, dx= (2 k)!$$
back to the series
$$\sum _{k=0}^{\infty } \frac{(-1)^k}{((2 k)!)^2} 2015^{2 k} (2 k)!=\sum _{k=0}^{\infty } \frac{(-1)^k 2015^{2 k}}{(2 k)!}=\color{red}{\cos 2015}\approx-0.32564$$</p>

<p>The series of functions converges absolutely in all $\mathbb{R}$ because of the ratio test
$$\lim_{n\to \infty } \, \frac{a_{n+1}}{a_n}=-\lim_{k\to \infty } \, \frac{4060225 x^2 ((2 k)!)^2}{((2 k+2)!)^2}=-\lim_{k\to \infty } \, \frac{4060225 x^2}{4 (k+1)^2 (2 k+1)^2}=0$$
For any $x\in\mathbb{R}$</p>

<p>Hope this helps</p>
"
"2390628","2391906","<p>Next you need to find the CDF and pdf of $T$ in order to get $\mathbb E[T]$. </p>

<p>Find CDF of $X_i+Y_i$:
$$
F_{X_i+Y_i}(t)=\mathbb P(X_i+Y_i \leq t) = \mathbb P(Y_i \leq t-X_i) 
$$
This is the probability that a point chosen at random in the triangular region with vertices at $(0,0)$, $(\theta, 0)$, and $(0,\theta)$ appears in the triangular region with vertices at $(0,0)$, $(t, 0)$, and $(0,t)$. For $0 \leq t\leq \theta$ this probability equals to ratio of squares of the triangles:
$$
F_{X_i+Y_i}(t)=\frac{t^2/2}{\theta^2/2}=\frac{t^2}{\theta^2}.
$$</p>

<p>So, the CDF of $T$ for $0 \leq t\leq \theta$ is
$$
F_T(t)=\mathbb P(\max_i (X_i+Y_i)\leq t)=\mathbb P(X_1+Y_1\leq T,\ldots,X_n+Y_n\leq T)=\left(\frac{t^2}{\theta^2}\right)^n=\frac{t^{2n}}{\theta^{2n}}
$$
Find pdf $f_T(t)$: 
$$
f_T(t)=2n\frac{t^{2n-1}}{\theta^{2n}}, \quad 0 \leq t\leq \theta.
$$
Find expectation of $T$ and get
$$
\mathbb E[T]=\frac{2n}{2n+1}\theta.
$$
Finally, $\hat\theta=\frac{2n+1}{2n}T$ is unbiased.</p>
"
"2390638","2390654","<p>The Lebesgue measure is regular, and in particular outer regular. Therefore there exists a sequence $(O_n)$ of open measurable sets containing $A$ such that $\lambda(A)=\inf \lambda(O_n)$.</p>

<p>Then $\displaystyle \int_A f(x)\, \mathrm{d}x=\int_{O_n} f(x)\, \mathrm{d}x - \int_{O_n - A} f(x)\, \mathrm{d}x$.</p>

<p>Since $O_n$ is an open set of $\mathbb{R}$, it is a countable union of open intervals and $\int_{O_n} f(x)\, \mathrm{d}x=0$. By dominated convergence you can show that $\int_{O_n - A} f(x)\, \mathrm{d}x \to 0$: use the sequence of functions $f1_{O_n-A}$ dominated by $|f|$.</p>
"
"2390639","2390703","<p>Simple induction on $m$ works. Indeed 
\begin{align}
\sum_{k=0}^{m} \binom{n+m}{k} &amp;= \sum_{k=0}^{m} \left(\binom{n+m-1}{k} + \binom{n+m-1}{k-1}\right) \\
&amp;= \sum_{k=0}^{m} \binom{n+m-1}{k} + \sum_{k=0}^{m} \binom{n+m-1}{k-1}\\
&amp;= \binom{n+m-1}{m} + 2 \sum_{k=0}^{m-1} \binom{n+m-1}{k}
\end{align}</p>

<p>and</p>

<p>\begin{align}
\sum^{m}_{k=0}\binom{n+k-1}{k}\left(2^{m+1-k-1}\right) &amp;= \sum^{m-1}_{k=0}\binom{n+k-1}{k}\left(2^{m-k}\right) + \binom{n+m-1}{m} \\
&amp;= \binom{n+m-1}{m} + 2 \sum_{k=0}^{m-1}\binom{n+k-1}{k}\left(2^{m-k-1}\right)
\end{align}</p>

<p>Since for $m = 1$ the two quantities are equal to $1$.  </p>
"
"2390641","2390656","<p>Everything is correct until you use the value of $x$ to calculate the area.
You should have $$A^2=(-\frac{3a}{2})^2b^2(1-\frac 14)$$ which will give you the correct answer $$A=\frac{3\sqrt{3}}{4}ab$$</p>
"
"2390648","2390795","<p>In general, you want to determine the definiteness of a $n\times n$ matrix $A$ given vectors $\vec x$, $\vec b$ such that $A\vec x=\vec b$. Just to clarify, I'll define definiteness as follows:</p>

<p>$A$ is <em>positive definite</em> $\implies \vec x^tA\vec x&gt;0\ \forall \vec x\neq \vec 0$</p>

<p>$A$ is <em>negative definite</em> $\implies \vec x^tA\vec x&lt;0\ \forall \vec x\neq \vec 0$</p>

<p>$A$ is <em>positive semidefinite</em> $\implies \vec x^tA\vec x\ge 0\ \forall \vec x\neq \vec 0$</p>

<p>$A$ is <em>negative semidefinite</em> $\implies \vec x^tA\vec x\le0\ \forall \vec x\neq \vec 0$</p>

<p>$A$ is <em>indefinite</em> if none of the above are true.</p>

<p>Since $\vec x^TA\vec x=\vec x\cdot \vec b$, we can evaluate the above inequality once for each value for $\vec x$ and $\vec b$, but only one set of values is insufficient to completely determine the definiteness of $A$ (except in the one dimensional case). However, if $\vec x\cdot\vec b$ is a counterexample to any of the above inequalities, we can of course conclude that $A$ does not have the corresponding definiteness.</p>

<p>If we want to check the inequalities above for all $\vec x$, it's sufficient (and, in fact, necessary) to have a set of vectors that form a basis for the entire space, since we can write any $\vec x^tA\vec x$ in terms of this basis. The simplest way to do this is to completely determine $A$.</p>

<p>Suppose we know a linearly independent set $\{\vec x_1,\vec x_2,...,\vec x_n\}\subset\mathbb R^n$, as well as another set $\{\vec b_1,\vec b_2,...,\vec b_n\}\subset\mathbb R^n$, such that $A\vec x_i=\vec b_i\ \forall\ i$. We can then let $X$ and $B$ be matrices whose columns are $\vec x_i$ and $\vec b_i$ respectively, giving us the following matrix relation.</p>

<p>$$AX=B$$</p>

<p>Since $X$ has linearly independent columns, we know its inverse must exist. Thus we can compute $A$.</p>

<p>$$A=BX^{-1}$$</p>

<p>Definiteness is typically only discussed for Symmetric matrices, so if $A$ is not symmetric, it may be preferred to consider its symmetric part $A_{sym}=\frac 12(A+A^T)$, which defines the same quadratic form.</p>

<p>Once we know $A$ we can determine its definiteness several ways. One is to find all of its eigenvalues. If $A$ is symmetric we can find the determinants of the principal minors and use <a href=""https://en.wikipedia.org/wiki/Sylvester%27s_criterion"" rel=""nofollow noreferrer"">Sylvester's Criterion</a>.</p>
"
"2390653","2390666","<p>Yes, this set is closed. To see this, note first that $\|x-y\|_1\ge2$ for all $x,y\in A$ with $x\neq y$. If $z\in\ell^1\setminus A$, either $B(z,1)\subset\ell^1\setminus A$ or there exists $x\in A$ such that $\varepsilon:=\|x-z\|_1\le1$. By our previous observation, this implies $B(z,\varepsilon/2)\subset\ell^1\setminus A$. This shows $\ell^1\setminus A$ is open.</p>

<p>This is a good example for what you are trying to prove since $\{f(x):x\in A\}=\{1+\frac1n:n\ge1\}$, which has no minimum.</p>
"
"2390667","2390677","<p>No, these notions of completeness mean different things. The first refers to <a href=""https://en.wikipedia.org/wiki/Complete_metric_space"" rel=""nofollow noreferrer"">completeness of metric spaces</a>. We need not have a vector space structure at all. The second refers to <a href=""https://en.wikipedia.org/wiki/Orthonormal_basis"" rel=""nofollow noreferrer"">orthonormal bases of Hilbert spaces</a>. Basically, an orthonormal subset $\Pi$ of a Hilbert space $H$ is called <em>complete</em> (or an <em>orthonormal basis</em>) if $\overline{\operatorname{span}(\Pi)}=H$.</p>
"
"2390670","2390684","<p>Your number is suspiciously close to $1-1/e$.  The fraction of values represented exactly $k$ times in your array should be close to $\exp(-1)/k!$, so it looks like your program counted the number of distinct values in the array, rather than the number of values represented only once.</p>
"
"2390694","2391768","<p>This is an active research area recently, e.g.,</p>

<blockquote>
  <p>Bobenko, Alexander I., ed. <em>Advances in Discrete Differential Geometry</em>. Springer, 2016. (<a href=""https://www.springer.com/de/book/9783662504468"" rel=""nofollow noreferrer"">Springer link</a>).</p>
</blockquote>

<p><hr />
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
<a href=""https://i.stack.imgur.com/5QFAq.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5QFAq.jpg"" alt=""cover""></a></p>

<hr />

<p>Here's a short essay (a <a href=""http://www.mdpi.com/journal/axioms/special_issues/discrete_geometry"" rel=""nofollow noreferrer"">special issue</a> introduction)
on the topic, with 36 references:</p>

<blockquote>
  <p>Gu, David, and Emil Saucan. ""Discrete GeometryâFrom Theory to Applications: A Case Study."" (2016): MPDI Axioms 27. (<a href=""http://www.mdpi.com/2075-1680/5/4/27/htm"" rel=""nofollow noreferrer"">MPDI link</a>.) </p>
</blockquote>

<p>They mention Ollivier's approach to Ricci curvature in a graph, as cited
also by Alessio Di Lorenzo in a comment, although ultimately they lobby
for ""Forman curvature.""</p>
"
"2390696","2390699","<p>$$\lim_{n\to \infty}(x)^{\dfrac{1}{2^n}}\to 1$$ only if $x\neq 0.$</p>

<p>In case of $x=0$, you have $$0=\lim_{n\to \infty}(0)^{\dfrac{1}{2^n}}\\=\lim_{n \to \infty}0\\=0$$ </p>

<p>Therefore, $0$ is also a solution.</p>
"
"2390697","2390702","<p>It is never periodic. Here is the listing for this sequence in the OEIS.</p>

<p>""This sequence is not ultimately periodic. This can be deduced from the fact that the sequence can be obtained as a fixed point of a morphism.""</p>

<p><a href=""https://oeis.org/A008904"" rel=""noreferrer"">https://oeis.org/A008904</a></p>
"
"2390715","2390760","<p>if $v \in I$ the result is trivial. Let $v \in I^{\perp}$ since the restriction of $Id - U$ on $I^{\perp}$ is bijective (Indeed it is injective : if $x \in I^{\perp}$, $(Id-U)(x) = 0 \Rightarrow x \in I \Rightarrow x = 0$ it is also surjective because if $z \in Im(Id-U)$ then there is $y \in H$ such that $z = (Id -U)(y)$. Since $(Id-U)(P(y)) = 0$ setting $x = (Id-P)(y)$ then $(Id-U)(x) = z$), Let $v_n = \frac{1}{N}\sum_{n=1}^{N}U^n(v) \in I^\perp$ and $w_n = (I-U)(v_n)$ then $w_n = \frac{1}{N}\sum_{n=1}^{N}\left(U^n(v) - U^{n+1}(v)\right) = \frac{1}{N}(U(v) - U^{N+1}(v)) \rightarrow 0$ because $||U|| \le 1$. Setting $V$ the inverse of the restriction of $Id - U$ on $I^\perp$, we have $v_n = V(w_n) \rightarrow 0$. The result is also verified for $v \in I^{\perp}$. Using $H = I + I ^{\perp}$ and the linearity of $P$ we have the result for all $v$. </p>
"
"2390716","2390729","<p>Since we are allowed to globally parallel transport in $\mathbb{R}^n$, we can assume that $u,z$ are based at the same point $p$. Even more, we can allow $p = \textbf{0}$ i.e in some plane you have this image,</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=""https://i.stack.imgur.com/kktJus.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kktJus.png"" alt=""enter image description here""></a></p>

<p>where $b =u $ and $z = a$. Define the projection of $z$ onto $u$ as,</p>

<p>$$\textbf{proj}_u z = \frac{\langle u, z \rangle }{\|u\|^2} \ u$$</p>

<p>Then it follows that $\{e_1=\textbf{proj}_u z, e_2=z- \textbf{proj}_uz\}$ forms an orthogonal basis at $\textbf{0}$ i.e any vector $v$ at $\textbf{0}$ is given by,</p>

<p>$$v = \langle v, e_1\rangle e_1 + \langle v, e_2 \rangle e_2$$</p>
"
"2390718","2390757","<p>If all you need is $[F:\Bbb{Q}]$, you have everything required to compute it.  You've determined that $f$ is irreducible, so let $\alpha$ be a root.  Then $[\Bbb{Q}[\alpha]:\Bbb{Q}]=3$ and since all roots are not real, $\Bbb{Q}[\alpha]$ cannot be the splitting field.  The minimum polynomial for the other two roots of $f$ is a quadratic with coefficients in $\Bbb{Q}[\alpha]$, in fact $f/(x-\alpha)$.  By multiplicativity of degrees the splitting field must have degree $3\cdot 2=6$ over $\Bbb{Q}$.</p>
"
"2390728","2390766","<p>Define the <a href=""https://en.wikipedia.org/wiki/Hamiltonian_mechanics#Basic_physical_interpretation"" rel=""nofollow noreferrer"">Hamiltonian function</a>:
$$
H(x, \dot{x}) = \frac{1}{2}\,m\, {|\dot{x}|}^2 \;+\; f(x)\, .
$$</p>

<p>Then:
\begin{align}
\frac{d}{dt}H(x, \dot{x})
&amp;= m\, \dot{x}\cdot\ddot{x} \;+\; \dot{x}\cdot\nabla f(x)\\[0.05in]
&amp;= \dot{x}\cdot\left[m\, \ddot{x} \;+\; \nabla f(x)\right]\\[0.05in]
&amp;=\dot{x}\cdot\left[-c\,\dot{x} \;-\; \nabla f(x)\;+\; \nabla f(x)\right]\\[0.05in]
&amp;= -c\, {|\dot{x}|}^2
\end{align}
Thus $x(t)$ is bounded in the same sense as in your original example, except with $H$ in place of $f$.</p>
"
"2390738","2391883","<p>I would handle this by observing that your finitary endofunctor category $[\mathrm{Set},\mathrm{Set}]_{\mathrm{fin}}$ is equivalent to the category of all functors $[\mathrm{Set}_{\mathrm{fin}},\mathrm{Set}]$ from the category of finite sets: sets are the free cocompletion under filtered colimits of finite sets. Now, the latter has a Cartesian closed structure defined by the same formula as yours, as does any presheaf category. Thus $[\mathrm{Set},\mathrm{Set}]_{\mathrm{fin}}$ must admit a Cartesian closed structure transferred from that on $[\mathrm{Set}_{\mathrm{fin}},\mathrm{Set}]$. But your formula necessarily computes the homs in any Cartesian closed structure, so this transferred structure is the one you wanted to verify exists.</p>
"
"2390746","2390780","<p>Fix a point $x_0$ and consider the countable family of balls centered in $x_0$ with radius $n\in \mathbb{N}$: every ball is limited and their union is $M$ (because $\forall y\in M, \ \exists n\in \mathbb{N} \ | \ d(y,x_0)&lt;n$)</p>
"
"2390762","2390771","<p>If $K=\mathbb Q\left[2^{1/2^m}\mid \forall m\in\mathbb Z^+\right]$ then $\mathcal O_{K}$ has ideals $I_j=\left\langle 2^{1/2^j}\right\rangle$ with $I_j\subsetneq I_{j+1}$.</p>
"
"2390763","2390989","<p>The problem comes from the discontinuities in particular at $\phi=0$. In fact, your equation has an infinite number of roots and, depending where you start iterating, you could cross many of these discontinuities. </p>

<p>Starting using $\phi_0=0.1$, you converge to the third positive root of the equation.</p>

<p>In order to overcome this problem, I should instead solve $$G(\phi)=F(\phi) \, \sin^2(\phi)=0$$ ignoring, for sure, the trivial solution $\phi=0$. This removes all the discontinuities.</p>

<p>Using your numbers, the plot is quite nice. However, in the range $-\frac \pi 2 \lt \phi \lt \frac \pi 2$  the function $G(\phi)$ goes through a minimum and you need to start on the left of it.</p>

<p>What is amazing is that, if you make a Taylor expansion of $G(\phi)$ up to second order around $\phi=-\frac \pi 4$, solving the quadratic, you find $\phi\approx -0.785398$ which is ... your solution !</p>

<p>Making a Taylor expansion of $G(\phi)$ up to second order around $\phi=0$, solving the quadratic, you find $\phi\approx -0.576201$ which is a very good starting point.</p>

<p><strong>Edit</strong></p>

<p>In any manner, when you know that the solution of $f(x)=0$ is unique and such that $a &lt; x &lt; b$, I strongly suggest the use of combination of bisection and Newton steps.</p>

<p>In the book ""Numerical Recipes"" (google for it), you could find subroutine <strong>rtsafe</strong> (Fortran source code <a href=""http://boson4.phys.tku.edu.tw/numerical_methods/N_Recipe/rtsafe.f"" rel=""nofollow noreferrer"">here</a>).</p>

<p>I have been using it millions of times (and even improved it implementing Halley and Householder methods).</p>
"
"2390768","2391224","<p>In the answer I'm going to use the following theorems:</p>

<ul>
<li>every Galois-field contains a primitive element</li>
<li>for all $x$, $x^q = x$</li>
<li>every field is closed in relation to it's multiplication</li>
</ul>

<p>Let $a$ be our primitive element, we define $b=a ^{p^h-1}$. Then 
$b^p = ( a^{p^h-1} ) ^p = a^q = a$.</p>

<p>Hence, since $a$ is our primitive element, for each $c$ in the field</p>

<p>$$c=a ^ l = (b^p) ^l = (b^l) ^ p$$</p>

<p>So every element has a $p$-root.</p>
"
"2390802","2391677","<p>The product of the first sixteen primes is $961380175077106319535$ (I looked it up <a href=""http://oeis.org/A070826"" rel=""nofollow noreferrer"">in the OEIS</a>, but I could just as easily have used a computer's calculator).</p>

<p>Obviously this number is not divisible by any even integer. However, it is divisible by these composite numbers: $$15, 21, 33, 39, 51, 57, 69, 87, 93, 111, 123, 129, 141, 159, 177$$</p>

<p>And that's just the composite numbers that are divisible by $3$ and one other prime. This product is also divisible by the following composite numbers:</p>

<p>$$35, 55, 65, 85, 95, 115, 145, 155, 185, 205, 215, 235, 265$$</p>

<p>This drives home the point that $961380175077106319535$ is divisible by composite numbers. There's no need to continue listing this number's composite divisors with two prime factors. Unless you really want to.</p>

<p>But let's move on to its divisors that are divisible by four primes, such as $1155$. Since $1155 \mid 961380175077106319535$ and $$1155 = 15 \times 77 = 21 \times 55 = 33 \times 35,$$ it follows that $961380175077106319535$ is divisible by composite numbers that are themselves products of composite numbers.</p>

<p>We can keep going with this until you're tired, the only limitation is that we just have sixteen primes to work with.</p>
"
"2390809","2390930","<p>Numerically, this does not seem to make any problem if we consider 
$$f_n=\int_{p_n}^{p_{n+1}} e^{-\pi (x)} \, dx\qquad \text{and}\qquad g_m=\sum_{n=1}^m f_n=\sum_{n=1}^{m} (p_{n+1}-p_n)\, e^{-n}$$ (see Antonio Vargas comment).</p>

<p>Below is a table of  generated values
$$\left(
\begin{array}{cc}
  m &amp; g_m \\
 5 &amp; 0.82486259393350324671035177334581504486982084749638 \\
 10 &amp; 0.83877447576693280717388127491652502994294656859973 \\
 15 &amp; 0.83890894500870098130370987262554128951261245468174 \\
 20 &amp; 0.83890982093187513586368658110075745562346018599660 \\
 25 &amp; 0.83890982757047394666269786169926490186502317042721 \\
 30 &amp; 0.83890982759192065348095727852873887161463474591048 \\
 35 &amp; 0.83890982759216205543854829896368181611163602891794 \\
 40 &amp; 0.83890982759216417947988624151469749806143841461256 \\
 45 &amp; 0.83890982759216418914669475325279067175180058855706 \\
 50 &amp; 0.83890982759216418932707086649050466597792079421674 \\
 55 &amp; 0.83890982759216418932767382058216343894327616862031 \\
 60 &amp; 0.83890982759216418932767754181410766698373920809439 \\
 65 &amp; 0.83890982759216418932767759291902842598347660179781 \\
 70 &amp; 0.83890982759216418932767759330393721260699403750457 \\
 75 &amp; 0.83890982759216418932767759330541827711774044359378 \\
 80 &amp; 0.83890982759216418932767759330542819614193086258447 \\
 85 &amp; 0.83890982759216418932767759330542823811431792371867 \\
 90 &amp; 0.83890982759216418932767759330542823854636033633091 \\
 95 &amp; 0.83890982759216418932767759330542823855117010762525 \\
 100 &amp; 0.83890982759216418932767759330542823855119385370828 \\
 105 &amp; 0.83890982759216418932767759330542823855119403474903 \\
 110 &amp; 0.83890982759216418932767759330542823855119403596887 \\
 115 &amp; 0.83890982759216418932767759330542823855119403597417 \\
 120 &amp; 0.83890982759216418932767759330542823855119403597418 
\end{array}
\right)$$</p>

<p>Inverse symbolic calculators do not find anything looking like this number.</p>

<p><strong>Edit</strong></p>

<p>What looks to be interesting is that, computed up to $m=5000$, a simple linear regression leads to  $$\log_{10}(g_{m+1}-g_m)=-0.434167\, m$$ showing that the $50$ significant figures are obtained for $m=116$ (as shown by the table) and that $100$ significant figures are obtained for $m=231$ (which has been checked).</p>
"
"2390812","2390835","<p>I'm not sure if this will help or not.</p>

<p>So, let us do case $k=2$.</p>

<p>Let $A$ be a solution and let $Q$ such that $QAQ^{-1}$ is $A$'s Jordan form. Multiplying the equation we get something of form</p>

<p>$$\begin{pmatrix}
c\lambda_1+1 &amp; 0\\
0 &amp; c\lambda_2+1
\end{pmatrix} = \lambda_1 \lambda_2 QSQ^{-1} = \lambda_1 \lambda_2 \begin{pmatrix}
s_1 &amp; 0\\
0 &amp; s_2
\end{pmatrix}$$</p>

<p>which can easily be solved for eigenvalues of $A$. Notice that this works since symmetric matrices are diagonalizable by orthogonal matrix. Also both $A$ and $S$ get into Jordan form under same base change.</p>

<p>Thus, to find solution, diagonalize $S$, calculate eigenvalues of $A$ and return it back: $$A = Q^{-1}\begin{pmatrix}
\lambda_1 &amp; 0\\
0 &amp; \lambda_2
\end{pmatrix}Q.$$</p>

<p><strong>Edit:</strong></p>

<p>To address the issue of finding eigenvalues in higher dimensions.</p>

<p>Since we have $c\lambda_i+1 = s_i\prod\lambda_j$, we can see that $\frac{c\lambda_i+1}{s_i}$ is constant, let us denote it by $d$. So, we have $c\lambda_i+1 =s_id\implies \lambda_i =\frac{ds_i-1}{c}$. Finally, to find $d$, you need to solve $\prod\frac{ds_i-1}{c} = d$, which can be done for $k\leq 4$ by Abel-Ruffini theorem, otherwise you would have to do it numerically.</p>
"
"2390818","2390889","<p>Shortest paths on an arbitrary surface, called (<em>pre</em>-)<em>geodesics</em>, are difficult to describe explicitly in general. On a sphere (a good approximation to the surface of the earth as far as geodesy is concerned), however, they're arcs of great circles.</p>

<p>Generally, if a geodesic triangle on a surface encloses a topological disk $T$, if $\Theta$ denotes the total interior angle of $T$, and if $K$ denotes the Gaussian curvature function, then
$$
\iint_{T} K\, dA = \pi + \Theta.
$$
Particularly, if $K &gt; 0$, a geodesic triangle has interior angle greater than $\pi$, and if $K &lt; 0$, a geodesic triangle has interior angle less than $\pi$.</p>

<p>On a sphere of radius $R$, we have $K = 1/R^{2}$, so a geodesic triangle of area $A$ has total interior angle $\pi + A/R^{2}$. For instance, a triangle with three right angles (one-eighth of a sphere) has area $4\pi R^{2}/8$ and total interior angle $\frac{3}{2}\pi$.</p>

<p>Curvature can be observed in practice: Longitude lines are geodesics, while latitude lines (except the equator) are not. If a surveyor wants to lay off one-mile (near-)square plots, the plots will fit well east-to-west (because two latitude lines are separated by a constant distance), but not well north-to-south (because two longitude lines get closer the farther from the equator one travels). Consequently, at moderate latitudes, every several miles the north-south boundaries of square plots must ""jog"" east or west in order for the plots to remain approximately square. The photographs below (own work) show this phenomenon in the nearly-planar plains of eastern Texas, taken from a plane.</p>

<p><a href=""https://i.stack.imgur.com/QOHFq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/QOHFq.png"" alt=""Curvature of the earth in geodesy""></a></p>
"
"2390819","2390824","<p><strong>Hint:</strong> For each 3-element subset of $\{1,2,\dots,n+2\}$ there is a highest element and two more elements which are strictly smaller than it.</p>
"
"2390825","2390836","<p>(note: in my numbers for tensor rank, I do not know the convention for ordering the numbers, so I may have them backwards)</p>

<p>A rank (0,2) tensor can be contracted with a vector &mdash; a rank (1,0) tensor &mdash; to produce a rank (0,1) tensor: that is, a 1-form.</p>

<p>A rank (1,1) tensor can be contracted with a vector to produce a vector.</p>

<p>In coordinates, matrices as you are thinking of them correspond specifically to (1,1) tensors &mdash; representing (0,2) tensors or (2,0) tensors as a matrix in the usual way is an abuse of notation. E.g. the matrix form of a (0,2) tensor <em>should</em> have two horizontal dimensions (i.e. a row of row vectors), not one horizontal and one vertical dimension. </p>

<p>If we write a 2-form $F$ as a bilinear functional, its contraction with a vector $v$, which I will write as $Fv$, is the 1-form given by the linear functional</p>

<p>$$ Fv(w) = F(v,w)$$</p>
"
"2390828","2390851","<p>$ \iint_{S} curl \ F \cdot \hat n \ dS \ =\int_{-8}^{8} \int_{-\sqrt{64-x^2}}^{\sqrt{64-x^2}}curl \ F \cdot \hat n \ dS $ should be the correct equation because the bounds of the integral correspond to the symmetry of the paraboloid; the equation allows for $-8$ and $8$ to be values for $x$ and $y$ in the equation for the boundary $x^2+y^2=64$. </p>
"
"2390831","2390850","<p>It depends whether you think of the vector bundles as smooth vector bundles, or as holomorphic vector bundles. Short exact sequences of smooth vector bundles always split (see <a href=""https://math.stackexchange.com/questions/691802/when-does-a-ses-of-vector-bundles-split"">here</a>). But if our vector bundles are holomorphic vector bundles on complex manifolds and if the morphisms in the SES respect the holomorphic structure of the bundles, then we might want to know whether it's possible to find a splitting where the inclusion/projection morphisms that define the splitting also respect the holomorphic structures. </p>

<p>In general, the answer is no. For example, the <a href=""https://en.wikipedia.org/wiki/Euler_sequence"" rel=""nofollow noreferrer"">Euler sequence</a> on $\mathbb{CP}^n$,
$$ 0 \to \mathcal O_{\mathbb P^n} \to \mathcal O_{\mathbb P^n}^{\oplus (n + 1)} \to \mathcal T_{\mathbb P^n} \to 0,$$
splits when viewed as a SES of smooth vector bundles, but it is not possible to find a splitting that respect the holomorphic structure. (See <a href=""https://math.stackexchange.com/questions/783633/tangent-bundle-of-pn-and-euler-exact-sequence"">here</a> - I particularly like Ben's answer.)</p>

<p>The second part of your question is about locally free sheaves. On a smooth complex projective variety, locally free sheaves and their morphisms are totally analogous to holomorphic vector bundles and their holomorphic-structure-preserving morphisms on the corresponding complex manifold. So the above Euler sequence serves as a counter-example to your second claim.</p>
"
"2390852","2390873","<p>Your answer is the <a href=""https://en.wikipedia.org/wiki/Euler_angles"" rel=""nofollow noreferrer"">Euler Angles</a>.</p>

<p>Indeed, you could find more useful the TaitâBryan angles, which express directly the angles $\psi$, $\theta$ and $\phi$ as the yaw, pitch and roll operation.</p>

<p>This standard sequence is denoted as $z-y'-x''$, meaning you first rotate in the $z$ axis, then in the $y$ axis, then in the $x$ axis, always with respect to your plane (intrinsic rotation). </p>

<p>From the reference article, the transformation matrix is:
$$
R=Z_1 Y_2 X_3 = \begin{bmatrix}
 c_1 c_2 &amp; c_1 s_2 s_3 - c_3 s_1 &amp; s_1 s_3 + c_1 c_3 s_2 \\
 c_2 s_1 &amp; c_1 c_3 + s_1 s_2 s_3 &amp; c_3 s_1 s_2 - c_1 s_3 \\
 - s_2 &amp; c_2 s_3 &amp; c_2 c_3 
\end{bmatrix}
$$
where $1,2,3$ are the $\psi$, $\theta$ and $\phi$ are the  yaw, pitch, roll angles; and  $c,s$ are the $\cos,\sin$ functions.</p>

<p>from there you can easily check that $[1;0;0]$ transforms into $R[1;0;0]=[c_2;c_2s_1;-s_2]$, which is the correct value after a yaw-pitch (roll do not affect this vector). </p>
"
"2390856","2390874","<p>The argument in (3) and (4) is incomplete. You go from ""the union of all these subgroups is all of $G$"" to concluding ""we must have found all the subgroups of order $p$,"" but don't explain why.</p>

<p>Indeed cardinality does not seem very relevant to me. Your idea of splitting subgroups of order $p$ into those of two types is a good one - it is an elementary version of the Schubert cell decomposition for projective spaces, $\mathbb{P}^n=\mathbb{A}^n\sqcup\mathbb{A}^{n-1}\sqcup\cdots\sqcup\mathbb{A}^1\sqcup\mathbb{A}^0$. (Don't worry about understanding that, I'm just saying your idea is one used in advanced math.)</p>

<p>To see that you've found every subgroup of order $p$, one can do so <em>directly</em>. Every such subgroup is cyclic, so generated by some $(a,b)$, and $\langle (a,b)\rangle$ is already in your list of found subgroups (with two cases depending on if $a=0$ or $a\ne0$).</p>

<hr>

<p>Here is another argument that works more generally but you may find too abstract.</p>

<p>The subgroups of $\mathbb{Z}_p^2$ of size $p$ are cyclic, and this is a special case of counting $1$-dim subspaces of a vector space $V$ over a finite field $k$. Let $\mathbb{P}(V)$ be the collection of all $1$-dim subspaces of $V$ and write $V^{\times}$ for the subset of nonzero vectors in $V$. Every nonzero vector $v$ generated a $1$-dim subspace $\langle v\rangle=kv$, so there is a function $V^{\ast}\to \mathbb{P}(V)$ given by $v\mapsto\langle v\rangle$. Any fiber of this map is the set of nonzero vectors of a $1$-dim subspaces, of which there are $|k^{\times}|$-many. Therefore, the number of $1$-dim subspaces is $|V^{\times}|/|k^{\times}|$.</p>

<p>Explicitly, if $k=\mathbb{F}_q$ and $V=\mathbb{F}_q^n$ then this is $(q^n-1)/(q-1)=q^{n-1}+\cdots+q+1$ (which matches the decomposition $\mathbb{P}^{n-1}=\mathbb{A}^{n-1}\sqcup\cdots\sqcup\mathbb{A}^1\sqcup\mathbb{A}^0$).</p>

<p>This should not be surprising if you are familiar with group actions, since the $1$-dim subspaces minus the origin are precisely the orbits of $k^{\times}$ acting on $V^{\times}$ freely.</p>
"
"2390862","2390868","<p>Suppose that $A = C$, then $A \cup C = A \cup A = A$. Now suppose that $B \cap D = \varnothing$. Then $| B \cup D | = |B| + |D| &gt; |A| = |A \cup C| \implies |B \cup D| &gt; |A \cup C|$ at least in some finite cases. A similar argument can show that no bijection $h : A \cap C \to B \cap D$ exists either as $A \cap C = A$ and $B \cap D = \varnothing$.</p>
"
"2390869","2390988","<p>Several facts you need to use:
$$ E[X_1] = \mu, Var[X_1] = \mu(1 - \mu), E[\bar{X}] = E[X_1], Var[\bar{X}] = \frac {1} {n} Var[X_1], E[S^2] = Var[X_1]$$</p>

<p>Then we have
$$ \begin{align}
E[\hat{D}] 
&amp;= E[\bar{X}] - E[\bar{X}^2]+\frac {1} {n} E[S^2] \\
&amp;= \mu - Var[\bar{X}]-E[\bar{X}]^2 + \frac {1} {n}\mu(1 - \mu) \\
&amp;= \mu - \frac {1} {n} \mu(1-\mu)-\mu^2+\frac {1} {n}\mu(1 - \mu) \\
&amp; = \mu - \mu^2 \\
&amp; = \mu(1 - \mu)
\end{align}$$</p>

<p>Therefore this is an unbiased estimator of $\mu(1 - \mu)$</p>
"
"2390875","2391286","<p>The notation $\mathcal{O}_X(1)$ in the statement of the theorem is just a name for an invertible sheaf which by hypothesis is very ample. The motivation of such a name is exactly the fact that a very ample invertible sheaf $L$, plus a choice of a basis of $\Gamma(X, L)$, where $X$ is a variety over some algebraically closed field, is the same as giving a closed immersion $i:X\hookrightarrow \mathbb{P}^n$ plus an isomorphism $\phi: L \simeq i^*\mathcal{O}_{\mathbb{P}^n}(1)$. As you were suggesting, $\mathcal{O}_X(l)$ is a shorthand for $L^l$.</p>

<p>The isomorphism $\phi$ above shows that every very ample line bundle is isomorphic to $i^*\mathcal{O}(1)$, or $\mathcal{O}_X(1)$ if you prefer this notation. Let me remark that there is not a canonically defined invertible sheaf $\mathcal{O}_X(1)$ on $X$ as in the case of $\mathbb{P}^n$. In other terms, there is not a canonically defined very ample invertible sheaf $L$ on $X$. This is the same as saying that you can embed $X$ in $\mathbb{P}^n$ in different ways, and it can be that the associated very ample line bundles $\mathcal{O}_X(1)$ are one different from each other. </p>

<p>For example, think of a non hyperelliptic curve of genus 3. You have different embeddings given by $K$ and $2K$, where $K$ is the canonical line bundle. By embedding the curve using these two line bundles, you have that $\mathcal{O}_X(1)$ will be isomorphic to $K$ in the first case and to $2K$ in the second one. You can also see this by looking at $X=\mathbb{P}^1$ : this is isomorphic both to $Proj(k[x,y])$ and $Proj(k[u,v,w]/(uw-v^2))$ , so that the $\mathcal{O}_X(1)$ in the first case is the sheafification of the $k[x,y]$-module $k[x,y](1)$ , in the other case is the sheafification of $k[x,y](2)$. Clearly, in the case of the projective line you do have a canonical $\mathcal{O}(1)$ but this is a special case, and that's because you have a canonical embedding, if you want, of the projective line into a projective space, namely the identity (you can also see this by looking at the functorial definition of the projective line). But in general, you do not have such a canonical embedding.</p>
"
"2390876","2390894","<blockquote>
  <p>Let's say I draw a random curve (assuming it can be infinitely thin), can I find a proper function for it? Let the curve be continuous and with its domain of all real numbers.</p>
</blockquote>

<p>There's two cases here. After you've drawn your curve (lets refer to your curve as $\Gamma_f$, for brevity's sake) in $\mathbb{R}^2$, attempt to find somewhere where can draw a vertical line that intersects your curve twice.</p>

<ol>
<li>If you found a vertical line that intersects $\Gamma_f$ twice, there is no such function.</li>
<li>If you are unable to find such a vertical line, then there exists a function.</li>
</ol>

<p>Let's elaborate now. First, note that a function is a ""mapping"" from one set to another (in this case, it is from $\mathbb R \to \mathbb R$). Functions that go from $A \to B$ have one crucial requirement:
$$(\forall a \in A)(\exists ! b \in B) \qquad f(a)=b$$
in English, this translates to ""for every element $a$ in $A$, there's only one $b$ in $B$ such that $f(a)=b$"". That is, you can't have the element $a$ be mapped to two elements simultaneously, saying ""$f(a)=b_1, b_2$"" in this case is nonsensical. This is why if #1 was the case, then the curve $\Gamma_f$ does not correspond to a function.</p>

<p>Now let's look at the second case. Here, your curve $\Gamma_f$ corresponds to a function <em>because it fits the requirements</em>. Doesn't even need to be continuous, if you threw random dots all over the place <em>then $\Gamma_f$ still corresponds to a function</em>.</p>

<p>Now of course this function cannot be described with a polynomial if it is just a collection of points, you would have to describe it explicitly. For instance,
the mapping $\mathbb R \to \mathbb R$ defined by
$$f= \{ (1,1), \ (2, 5), \ (8,9) \}$$
<em>is a function.</em> We'd have $f(1)=1, f(2)=5,$ and $f(8)=9$. As for $f(25)$? That's undefined. Regardless, <em>$f$ is still a function.</em></p>

<p>And that is why if #2 is your case, then your curve $\Gamma_f$ corresponds to a function.</p>
"
"2390880","2390898","<blockquote>
  <p>But, what I find weird is, it seems these axioms have already defined the function symbols and constants to be equivalent to the structure of $\mathfrak N$, ie. $\{+,\times,0,1\}$, instead of having undefined $f_1,f_2,c_1,c_2$, so as I understand it the mappings are already assigned.</p>
</blockquote>

<p>No, they haven't.  The symbols $+$, $\times$, $0$, and $1$ used in the axioms are <em>just symbols</em>.  There is no assumption that they refer to the usual meaning of addition, multiplication, zero, and one: $+$ could be interpreted as any binary operation, $\times$ as any binary operation, and $0$ and $1$ as any two constants.  If you like, you could rewrite the axioms using $f_1,f_2,c_1,c_2$ as you suggest and the axioms would have the <em>exact same meaning</em>: you're just using different symbols.</p>

<blockquote>
  <p>I read that there are non-standard models of PA for every cardinality, so what would a model with cardinality of one look like?</p>
</blockquote>

<p>This statement is not quite correct.  There are non-standard models of PA of every <em>infinite</em> cardinality, but there are no finite models of PA.  For instance, you can prove from PA that the elements $0$, $1$, $1+1$, $(1+1)+1$, $((1+1)+1)+1$, and so on are all distinct, so there are infinitely many different elements.</p>
"
"2390882","2390888","<p>Purely based on your quote - I haven't read the linked answer, the only claim is that:</p>

<ul>
<li>if none of $a, b, c, d$ is equal to $2$ and at least one of them is equal greater than $4$, then $0 &lt; f &lt; 1$.</li>
</ul>

<p>Now there are only finitely many cases where</p>

<ul>
<li>none of $a, b, c, d$ is equal to $2$ and all of them are smaller than or equal to $4$.</li>
</ul>

<p>These are not mentioned in the quote, but since there are only finitely many of them, they could all be checked.</p>

<p>But that still leaves the cases where </p>

<ul>
<li>at least one of $a, b, c, d$ is equal to $2$ (without any condition on being greater than or smaller than or equal to $4$).</li>
</ul>

<p>Are you maybe interpreting ""It turns out to be fairly easy to prove that if none of the variables are $2$, and any of them are greater than $4$, then the fraction lies between 0 and 1"" as ""if none of the variables are $2$, then $0 &lt; f &lt; 1$; and if any of them are greater than $4$, then $0 &lt; f &lt; 1$""?</p>
"
"2390885","2390920","<p>At least for an intuitive approach as to why the answer sheet's answer is right, note that the line $y = 3x - 1$ has slope 3. Your equation parametrizes a line that has slope 1/3, since if you let $t$ vary from 0 to 1, say, $x$ varies from 0 to 3, while $y$ varies only from -1 to 0; this is a line of slop 1/3, not of slope 3.</p>

<p>The answer key equation gives a line that, as $t$ varies from 0 to 1 (say), has $x$ vary from 0 to 1 and $y$ vary from -1 to 2, which is the correct slope of 3.</p>

<p>The directional vector $d$ should be the slope vector; if the slope, for instance is 3 (as it is in this case), any vector that has slope 3 will do. So just pick the vector 
$$
d = \begin{pmatrix}
1 \\ 
3
\end{pmatrix}.
$$</p>

<p>As for the -1, note that at $x = 0$, we have $y = 3\cdot 0 - 1 = -1$. This is captured in the vector form of the equation by the <em>initial position</em> vector 
$$
p = \begin{pmatrix}
0 \\ 
-1
\end{pmatrix}.
$$
Because look what happens when we put $t = 0$ in the vector equation; we get $(x, y) = (0, -1) + 0\cdot d = (0, -1)$, meaning that the point $(0, -1)$ is on our line, as it should be. Really, it just serves as a convenient starting point to draw the vector equation line from. </p>
"
"2390893","2394303","<p>The difference here is whether you want to study the geometry of the zero locus of a polynomial, or the polynomial itself. The first case corresponds to ""classical"" algebraic geometry, which is the study of varieties and algebraic curves and so on, while the second belongs more to the theory of schemes.</p>

<p>Beginning with a set of points $S \subseteq \mathbb{R}^2$, we can ask various geometric questions about smoothness and dimension and so on. From the algebraic perspective, we want to represent $S = \{(0, y) \mid y \in \mathbb{R}\}$ as the zero locus of some collection of polynomials in $\mathbb{R}[x, y]$. As you've noticed, there are many ways of doing this, for example the polynomials $x$, $x^2$, $x(x^2 + 1)(y^2+1)$ all have the correct zero sets.</p>

<p>Here $(x)$ is the correct ideal to study for various reasons, which in this case basically boil down the the fact that $x$ cannot be factored. In general, you can define the ideal
$$ I(S) = \{f \in \mathbb{R}[x, y] \mid f(s) = 0 \text{ for all } s \in S\}$$
and then find generators of the ideal. In our example, it can be shown that $I(S) = (x)$, and since the generator $x$ is irreducible, it will enjoy various nice properties, such as the fact that smoothness of $S$ will make sense in terms of the Jacobian of $f(x, y) = x$, and so on.</p>

<p>Classic algebraic geometry always puts sets of points through the $I$ function to get the ""right"" polynomials cutting out the set $S$, and then proves theorems about dimension and smoothness using the output of $I$. This stops silly equations like $x(x^2 + 1)(y^2 + 1)$ being used in place of the more simple $x$.</p>

<p>So that is one way of handling the ""problem"". The other is to use scheme theory, which regards the geometry cut out by $x = 0$ to be different to the geometry cut out by $x^2 = 0$. Schemes will notice that the difference between the intersection of two lines, and a line with a curve which is tangent at a point, since this is much like the difference between $x=0$ and $x^2 = 0$. However, the price you pay is that the theory is much more involved, and you need much more structure than a set of points sitting in a space.</p>
"
"2390915","2390926","<p>""How does one do research in any field?"" You are not expected to know how to answer that question at the start of a PhD program, and an advisor's primary job is to guide you through that process. The commenters are right that this is an extremely concerning situation that you should take seriously. Seek another advisor immediately.</p>
"
"2390917","2393796","<p>Actually this system have no solution, first $x_2,y_2,x_3,y_3,x_5,y_5 \in \mathbb{R} \geq 0$.</p>

<p>And $x_2+y_2=1 = \sum \limits_{k=1}^{\infty} \frac{1}{2^k}$</p>

<p>And $x_3+y_3 = \frac{1}{2} =  \sum \limits_{k=1}^{\infty} \frac{1}{3^k}$</p>

<p>And $x_5 +y_5 = \frac{1}{4} = \sum \limits_{k=1}^{\infty} \frac{1}{5^k}$.</p>

<p>So $\sum x_{i,4} = \frac{1}{2}\frac{2}{3}\frac{4}{5}* \frac{1}{2} * ((x_2+1)(x_3+1)(x_5+1)-(x_3+1)(x_5+1))$ because if $2\not|i$ then its not defined for $x_{i,4}$.</p>

<p>And $\sum x_{i,6} = \frac{1}{2}\frac{2}{3}\frac{4}{5}* \frac{1}{3} * ((y_2+1)(y_3+1)(y_5+1)-(y_5+1))$ because if $2\not|i$ and $3\not|i$ then its not defined for $x_{i,6}$.</p>

<p>Solving this system of equations using Wolfram|Alpha (also simplex method works) yield that there is <strong>no solution</strong> for your system.</p>
"
"2390923","2392791","<blockquote>
  <p>My reasoning is that statistics is a science and therefore its assertions must be based on scientific data.</p>
</blockquote>

<p>That is a false premise from which you will draw false conclusions.</p>

<p>There is nothing in the theory of probability or in mathematical statistics that states that a die will land on any specific side with $1/6$ probability, or that the die will not turn into a butterfly before hitting the ground. 
What probability and statistics <em>will</em> let you do is to draw inferences from the hypothesis that a particular die is fair, which is <em>defined</em> in terms of the probability of each side being selected by a process called ""rolling"" or ""throwing"" the die.</p>

<p>For example,
one of the things you can do with statistics
Is to try to detect that a die is ""weighted"" so that its faces do <em>not</em> each appear with equal probability. 
In order to make sense of that exercise, we must recognize that not all dice are fair. </p>

<p>If you would care to hypothesize that the die turns into a butterfly with a certain probability during the throw, you can draw statistical inferences from that hypothesis. </p>

<p>Statistics is a tool often used in scientific work, because it enables drawing inferences from scientific models and data. 
Therefore people who are experts in statistics sometimes end up doing science. 
In that context, however, 
statistics is a tool that people use in order to further develop science from scientific data. 
This is an <em>application</em> of statistics, not a set of assertions of statistics. 
The actual <em>assertions</em> of statistics (its theorems) have absolutely no scientific content. If they did, they would not be theorems. </p>
"
"2390937","2390943","<p>$\Bbb C[x,y]$ is a unique factorisation domain. Any ideal in a UFD generated by a square-free element is radical. The polynomial
$x^2+y^2-1$ is not just square-free, but also irreducible.</p>
"
"2390940","2391156","<p>Notice that if $\theta_i=0$ for $i=1,\dots,n-1$, then your metric $g_{ij}$ fails to be Riemannian (it's not an inner product for $\theta_i=0$, because it's degenerate).</p>

<p>Hence, if your metric is not well-defined in some subset, you definitely cannot expect the inverse to be well-defined (in this case, it does not even exist!)</p>

<p>What you can do, however, is to change coordinates so that you cover the part of the manifold which is not covered by your first set of coordinates. The coordinates that induce your metric are the following:
$$x_1={\cos{\theta_1}},$$</p>

<p>$$x_p={\cos{\theta_p}}{\prod_{m=1}^{p-1}}{\sin{\theta_{m}}},$$</p>

<p>$$x_n={\prod_{m=1}^{n-1}}{\sin{\theta_{m}}},$$</p>

<p>yet you can verify yourself that these are not one-to-one if one of the $\theta_i$ vanishes, so you can only have well-defined coordinates for the subset 
$$
\{(\theta_1,\dots,\theta_{n-1})\ |\ 0&lt;\theta_i&lt;\pi \textrm{ for }i\not=n-1,\ 0&lt;\theta_{n-1}&lt;2\pi\}.
$$
If you seek another coordinate patch, you may for instance ""flip"" the sphere and build the coordinates in the exact same way, and then notice the relation between the two sets of coordinates (which will be the translation given by the ""flip"").</p>
"
"2390949","2391339","<p>Let $a &gt; 0$. We define $X = \{x \ge 0, x^2 &lt; a\}$ and $Y = \{y \ge 0, a &lt; y^2\}$. Then</p>

<ul>
<li>$X \neq \emptyset$ because $0\in X$.</li>
<li>$Y \neq \emptyset$ because $(a+\frac{1}{2})^2 = a^2 + a + \frac{1}{4} &gt; a$, so that $a+ \frac{1}{2}\in Y$</li>
<li>For all $x\in X$ and $y\in Y$ one has $x^2&lt; a&lt; y^2$, hence $0 &lt; (y^2 - x^2) = (y - x)(y + x)$ and this implies $y-x&gt;0$ because $y+x\ge 0$. Hence $x &lt; y$</li>
</ul>

<p>From this, the completeness axiom says that there exists $c\in \mathbb{R}$ such that $\forall x\in X, \forall y \in Y, x \le c \le y$.</p>

<p>Let now $\epsilon \in (0, 1)$ be a real number. As $c+\epsilon &gt; c$, one has $c+\epsilon \notin X$, hence $(c + \epsilon)^2 \ge a$. This implies
$a \le c^2 + 2 \epsilon c + \epsilon^2 \le c^2 + \epsilon(2 c + 1)$.</p>

<p>Suppose that $c^2 &lt; a$, then $a-c^2&gt;0$ and we can choose $\epsilon = \min(1,\frac{a-c^2}{2 (2c+1)})$, so that $\epsilon (2c+1)\le \frac{a-c^2}{2}$ and finally $a \le c^2 + \epsilon(2c+1)\le \frac{a+c^2}{2}$. This implies $a \le c^2$, a contradiction.</p>

<p>It follows that necessarily, $a \le c^2$. Suppose now that $a &lt; c^2$. Then  one has $c &gt; 0$ and for all $\epsilon \in (0, c)$, $0 &lt; c-\epsilon &lt;c$. In particular, $c-\epsilon \notin Y$, which means that $(c - \epsilon)^2\le a$. It follows that $c^2 - 2 \epsilon c+ \epsilon^2\le a$, hence
$c^2 - a \le 2 \epsilon c-\epsilon^2\le 2\epsilon c$. Choosing $\epsilon = \min(c, \frac{c^2-a}{4 c})$, we get $c^2 - a\le 0$, a contradiction.</p>

<p>It follows from all this that $a = c^2$, QED.</p>
"
"2390956","2390960","<p>A decimal expansion is an infinite series. Your series is $$ 0+\frac{0}{10}+\frac{0}{100}+\frac{0}{1000}\ldots =0.$$</p>

<p>So yes, it's zero. To carefully prove it's zero we use the fact that the limit of a series is the limit of the sequence of partial sums and the partial sums are all zero. To show that the limit of the sequence of all zeros is zero you only need use the definition of a limit.</p>

<p>As to the rest of your question, no, it's not infinitessimal, it's zero. There are number systems that have infinitesimal elements, but decimals like you wrote are almost universally considered to refer to a real number that is the limit of the series the decimal represents.</p>

<p>And no, it's not countably infinite. You are confusing the fact that there are a countably infinite number of terms in the series with the idea that the number itself is countably infinite. Zero is not countably infinite. Nor is any infinitessimal. Infinitessimal things are small. Countably infinite things are big (at least compared to finite things.)</p>
"
"2390958","2390969","<p>It's just AM-GM:
$$\sum_{cyc}x^{y+1}z=xyz\sum_{cyc}\frac{1}{y}x^{y}\geq xyz\prod_{cyc}\left(x^y\right)^{\frac{1}{y}}=x^2y^2z^2.$$</p>
"
"2390965","2390986","<p>You showed that $mnp$ is a perfect square.</p>

<p>\begin{align}
   m + n + p - 2\sqrt{mnp} &amp;= 1 \\
   (m - 2\sqrt{mnp} + np) + n - np + p - 1 &amp;= 0 \\
   (\sqrt m - \sqrt{np})^2 - n(p-1) + (p-1) &amp;= 0 \\
   (\sqrt m - \sqrt{np})^2 &amp;= (n-1)(p-1) \\
   \sqrt m &amp;= \sqrt{np} \pm(n-1)(p-1) \\
   m &amp;= np +(n-1)^2(p-1)^2 \pm2\sqrt{np}(n-1)(p-1)
\end{align}</p>

<p>So $np$ is a perfect square.</p>

<p>So $m = \dfrac{mnp}{np}$ is a perfect square.</p>
"
"2390974","2390977","<p>We have the equation</p>

<p>$$\alpha_1+\beta_1\log(w)+\gamma_1k=\alpha_2+\beta_2\log(w)$$</p>

<p>Put terms with $\log(w)$ on a side and other constants on the other side and rearranging:</p>

<p>$$\beta_1 \log(w)-\beta_2\log(w)=\alpha_2-\alpha_1-\gamma_1k$$</p>

<p>$$(\beta_1 -\beta_2)\log(w)=\alpha_2-\alpha_1-\gamma_1k$$</p>

<p>Divide both sides by $\beta_1 -\beta_2$ :</p>

<p>$$\log(w)=\frac{\alpha_2-\alpha_1}{\beta_1 -\beta_2}-\frac{\gamma_1}{\beta_1 -\beta_2}k$$</p>
"
"2390975","2391895","<p>For any commutative ring $R$ with ideal $I$, the following are equivalent: a)  $I$ is radical; b) for all $x\in R$, $x^2\in I\implies x\in I$.</p>

<p>This is the closest thing I can think of to what you are talking about. In particular that shows that a nontrivial principle ideal $(x)$ in a UFD is radical iff $x$ is square free.</p>

<p>The proof isn't hard. Obviously radical ideals contain  square roots of elements. In the other direction, suppose $I$ contains square roots of all elements. Suppose $x^n\in I$ but $x\notin I$ for some $n&gt;2$, $n$ minimal with that property.  </p>

<p>Then there is a $k$, $1&lt;k&lt;n$ such that $x^{2k}\in I$. But by hypothesis $x^k\in I$ then, contradicting minimality of $n$. </p>
"
"2390978","2390985","<p>Two arbitrary orthogonal vectors in $\mathbb{R}^{3}$ need not determine a pre-specified plane. It is that if a point $(a,b,c)$ on a given plane $E \subset \mathbb{R}^{3}$ is given and if a normal vector $(l,p,q)$ to the plane is given, then $(x,y,z) \in E$ if and only if $(x-a,y-b,z-c)^{\top}(l,p,q) = 0$. The equation is called Cartesian equation of $E$. </p>

<p>Note that in Euclidean geometry it is concentional to view two parallel vectors of the same length as the same vector. Note that the dilation of a vector is not supposed to change its direction.</p>
"
"2390979","2390982","<p>You squared your equation. That gives you extra solutions. Specifically, your second approach picks up all solutions to $\sin x+\cos x=-1$ as well.</p>
"
"2390993","2391028","<p>Lee (in âIntroduction to Smooth Manifoldsâ) calls these maps  ârough sectionsâ to stress the difference from continuous or even smooth sections.</p>
"
"2391003","2391831","<p>We suppose you have a strategy.  I will be the person who can recognize the balls.  What I will do is I will tell you what my answers are (based on your strategy).  Then I will show that (a) there is an assignment of materials to the balls so that my answers are truthful and (b) you can't identify any iron ball after three questions.</p>

<p>If you ask me about two balls neither of which you have asked about before, I will say they are the same.  If you ask me about two balls at least one of which you have asked me about before, I will say they are different (except if you could already deduce what the correct answer was, in which case I give that answer, but it would be silly for you to ask such a question, because it's a wasted question).</p>

<p>Now, let's draw a graph whose vertices are the seven balls, and whose edges are the three pairs of balls you asked about.  For each of the connected components other than the isolated vertices, based on my answers, you know how the vertices are divided within that component into two materials.  These components have two, three, or four vertices (because there are only three edges).  Because of how I answered the questions, for each component, one material accounts for exactly two balls.    </p>

<p>We split into cases: either there is one component of size 4 split 2 and 2, together with 3 isolated vertices; or there is a component of size 3 split 2 and 1, a component of size 2 with both vertices the same, and 2 isolated vertices; or three components of size 2, each the same, and one isolated vertex.  In each of these cases, you don't have enough information to determine an iron ball, and clearly, my answers were correct for some assignment of materials to balls.  </p>
"
"2391017","2391027","<p>Remember that
$$
\frac{1}{(1+10\%)^{n}} = \left(\frac{1}{1+10\%}\right)^{n}
$$</p>

<p>Suppose we have 
$$a = \frac{1}{1+10\%}=\frac{1}{1.1}$$
and
$$S=a+a^2+a^3+a^4+a^5$$ 
Then multiply by $a$ 
$$a\times S=a^2+a^3+a^4+a^5+a^6$$
Subtract the second from the first to obtain 
$$
1\times S - a\times S
= (1-a)\times S
=a-a^6
=a \times 1 -a\times a^5
=a\times (1-a^5)$$ 
(the point is that most terms simply cancel) so that $$S=\frac a{1-a}(1-a^5)$$</p>

<p>Then you put 
$
a=\frac 1{1.1}
$</p>

<p>Finally
$$
S=\frac{1}{10\%}\times (1-(1+10\%)^{5^{-1}}))
\\
S=\frac{1-(1+10\%)^{-5}}{10\%}
$$</p>

<p>More generally, for an interest rate $r$, $a=\frac 1{1+r}$, whence $$S=\frac 1r\left(1-\frac 1{(1+r)^5}\right)$$</p>
"
"2391018","2391022","<p>For $\lambda = 1/4$, </p>

<p>$$\begin{split}
\varphi\left( \frac 14 x + \frac 34 y\right) &amp;= \varphi\left(\frac{1}{2} \left(\frac 12 (x+y)\right) + \frac 12 y\right) \\
&amp;\le \frac 12\left(\varphi \left( \frac 12 (x+y)\right)+ \varphi\left( y\right) \right) \\
&amp;\le \frac 12\left(\frac 12\left( \varphi (x) + \varphi(y)\right) + \varphi (y) \right) \\
&amp;= \frac 14 \varphi (x) + \frac 34 \varphi (y). 
\end{split}$$</p>

<p>(That is, I first think of $\frac 14 x + \frac 34 y$ as the midpoint of $\frac 12 (x+y)$ and $y$)</p>
"
"2391023","2391031","<p>Consider the function </p>

<p>$$f(z) = \left( z - \frac{1}{2} \right)^4 + 1.$$</p>

<p>The equation $f(z) = 0$ is equivalent to $\left( z - \frac{1}{2} \right)^4 = -1$, so the solutions are the four vertices of a tilted square centered at $\frac{1}{2}$, namely </p>

<p>$$\begin{align*}
&amp; \frac{1}{2}  + \frac{\phantom{-}1+i}{\sqrt{2}}, \qquad \frac{1}{2} + \frac{-1+i}{\sqrt{2}}, \\[1ex]
&amp; \frac{1}{2} + \frac{-1-i}{\sqrt{2}}, \qquad \frac{1}{2} + \frac{\phantom{-}1-i}{\sqrt{2}}.
\end{align*}$$</p>

<p>So $f$ has no real roots and $f(z) = f(1-z)$.</p>

<hr>

<p>Also note that each polynomial $f(z)$ with real coefficients such that $f(z) = f(1-z)$ will have roots both symmetric with respect to the line $\Im z = 0$ (because of the real coeffiecients $f(z) = 0 \iff f(\overline{z}) = 0$) and symmetric with respect to the point $z = \frac{1}{2}$, which makes for a symmetry with repsect to the line $\Re z = \frac{1}{2}$. </p>
"
"2391025","2391045","<p>Notice that: 
$$S_n=n^2+20n+12=(n+10)^2-88 \ .$$</p>

<p>Assume that $S_n$ to be a perfect square; i.e. $S_n=m^2$, 
then we have: $(n+10)^2-88=m^2$, </p>

<p>let's dfine $N:=(n+10)$. </p>

<p>This implies that $(N-m)(N+m)=88$; notice that $(N-m)$ and $(N+m)$ have the same pairity, both of them are odd or both of them are even. We have the following cases: </p>

<hr>

<hr>

<ol>
<li><p>$(N-m)= 1$ and $(N+m)=88$; which is imposible by the above notices.</p></li>
<li><p>$(N-m)= 2$ and $(N+m)=44$; which gives the solution $N=23$, $m=21$; $n=13$.</p></li>
<li><p>$(N-m)= 4$ and $(N+m)=22$;  which gives the solution $N=13$, $m=9$; $n=3$.</p></li>
<li><p>$(N-m)= 8$ and $(N+m)=11$; which is imposible by the above notices.</p></li>
<li><p>$(N-m)=11$ and $(N+m)= 8$; which is imposible by the above notices.</p></li>
<li><p>$(N-m)=22$ and $(N+m)= 4$;  which gives the solution $N=13$, $m=-9$; $n=3$.</p></li>
<li><p>$(N-m)=44$ and $(N+m)= 2$;  which gives the solution $N=23$, $m=-21$; $n=13$.</p></li>
<li><p>$(N-m)=88$ and $(N+m)= 1$; which is imposible by the above notices.</p></li>
</ol>

<hr>

<hr>

<p>At first you without loss of generality you can assume that $m$ is non-negative, 
and also note that $ N = n+10 &gt; 0+10=10.$</p>
"
"2391033","2391103","<p>Recall that a compact on the real numbers is close and bounded. If the distribution support of a r.v. is bounded, this implies that outside a bounded set the random variable takes values with probability zero. Equivalently, it is bounded with probability 1. However, the importance of the compact support depends of the particular theorem you are interested in, this is not strictly necessary and there are a myriad of commonly used and important r.v. without compact support. </p>

<p>I am not sure what you mean by ""compactness of the support of a measure"". But maybe checking some of the properties of compact spaces (<a href=""https://en.wikipedia.org/wiki/Compact_space"" rel=""nofollow noreferrer"">here</a>) would help you understanding why is it required for your specific case.</p>

<p>As this is your first question I would also recommend you to have a look at <a href=""https://stackoverflow.com/help/how-to-ask"">this</a> to improve your chances to get an answer on what you need.</p>

<p>Hope this helps.</p>
"
"2391034","2391046","<p>\begin{align*}
&amp;
\left(z+\frac{1}{z}\right)
\left(z+\frac{1}{z}+1\right)
=1\\[6pt]
\implies\;&amp;
\left(\frac{z^2+1}{z}\right)
\left(\frac{z^2 + 1 + z}{z}\right)
=1\\[4pt]
\implies\;&amp;(z^2+1)(z^2 + 1 + z)=z^2\\[4pt]
\implies\;&amp;z^4 + z^3 + 2z^2 + z +1= z^2\\[4pt]
\implies\;&amp;z^4 + z^3 + z^2 + z +1= 0\\[4pt]
\implies\;&amp;(z-1)(z^4 + z^3 + z^2 + z +1)= 0\\[4pt]
\implies\;&amp;z^5-1=0\\[4pt]
\implies\;&amp;z^5 = 1\\[4pt]
\implies\;&amp;z^{100}=1\\[4pt]
\end{align*}
The rest is easy.</p>
"
"2391038","2391062","<p>If the equation is $y'(x) = P(x, y(x))$ where $P$ is a polynomial and the approximations are defined by $y_0(x) = y(0)$ and</p>

<p>$$y_n(x) = y(0) + \int_0^x P(t, y_{n-1}(t)) dt$$</p>

<p>then, by induction $y_n(x)$ is a polynomial and its terms up to degree $n$ are equal to the Taylor coefficients of the solution at $0$ (but not necessarily the terms of degree $&gt; n$). Indeed the approximation formula above implies that the term of degree $k$ in $y_n$ depends only or the terms of degrees $1, \cdots, k-1$ in $y_{n-1}$. It means that the term of degree $k$ is stationary for all $n \ge k$. In particular, it is the term of the Taylor series because we know that a solution exists in an interval around $0$ and that this solution is $\mathcal{C}^\infty$.</p>

<p>In your case, the solution is
$$y(x) = \frac{1}{1-x} = 1 + x + x^2 + \cdots$$
The $y_2$ approximation has Taylor terms $1+x+x^2$ but the term in $x^3$ will only be equal to the Taylor coefficient in $y_3$.</p>

<p>Edit: <strong>A more detailed proof by induction</strong> (on John Ma's request)</p>

<p>If $q$ is any polynomial, let us denote $D_k(q)$ its coefficient of degree $k$. Let us consider the polynomial</p>

<p>$$r(x) = y(0)+ \int_0^x P(t, q(t)) dt$$
Then $D_k(r)$ depends only on the terms of degree $&lt; k$ in $q$. It means that there is a function $F_k$ such that
$$D_k(r) = F_k(D_0(q),\ldots,D_{k-1}(q))$$
The approximations $y_n$ above are polynomials that satisfy
$$D_k(y_{n+1}) = F_k(D_0(y_n),\ldots,D_{k-1}(y_n))$$
Our induction hypothesis $\cal{H}_n$ is that $\forall i\le n$, $D_i(y_n) = D_i(y_i)$.</p>

<p>Obviously, $\cal{H}_0$ is true because the term of degree $0$ in all the $y_n$'s is $y(0)$. Supposing that $\cal{H}_n$ is true, then for all $i\le n+1$, one has
$$D_i(y_{n+1}) = F_i(D_0(y_n),\ldots,D_{i-1}(y_n))
= F_i(D_0(y_0),\ldots,D_{i-1}(y_{i-1})) = D_i(y_i)$$
which proves that $\cal{H}_n \Rightarrow \cal{H}_{n+1}$.</p>

<p>Hence, $D_i(y_n)$ does not depend on $n$ when $n \ge i$. One last question remains: why is this term equal to the Taylor series term of degree $i$ in the solution? For this, let $T_k(x)$ be the Taylor polynomial up to order $k$ in the solution. One can write $y(x) = T_k(x) + x^k \epsilon(x)$ where $\epsilon(x)\to 0$ when $\epsilon \to 0$. By using
$$T_k(x) + x^k \epsilon(x) = y(0) + \int_0^x P(t, T_k(t) + t^k \epsilon(t)) dt$$ it is easy to see that the coefficients of $T_k$ must satisfy
$D_i(T_k) = F_i(D_0(T_k),\ldots,D_{i-1}(T_k))$
when $i\le k$. It follows easily that the coefficients of $T_k$ must be the same as the coefficients given by the approximation process.</p>
"
"2391039","2391041","<p>When $n\geq 3$
$$n!+5 \equiv 2  \pmod{3}$$ 
So it cannot be a perfect square when $n\geq 3$.</p>
"
"2391043","2391214","<p>$\color{Purple}{\text{ The answer is }}$
$\color{Green}{\text{ YES }}$.</p>

<hr>

<hr>

<p><strong>Lemma(Euler)</strong>: 
Let $n$ to be a positive ineger such that $\gcd(n,10)=1$; 
then we have: </p>

<p>$$ 10^{\varphi(n)} \overset{n}{\equiv} 1 \ ,$$ 
where $\varphi(n)$ euler's function; i.e. :</p>

<p>$$ \varphi(n) = \text{Card}\Big( 
\{ a \in \mathbb{N} \ | \ 1 \leq a \leq n \ \ \ \text{&amp;} \ \ \ \gcd(a,n)=1 \}
\Big) \ .$$ </p>

<hr>

<hr>

<p>At first assume that $\gcd(n,10)=1$;<br>
then let $k:=\dfrac{10^{\varphi(n)}-1}{n}$, then:</p>

<p>$$kn = \underbrace{9 9 ... 9 9}_{\varphi(n)-\text{ times}}.$$</p>

<hr>

<hr>

<p>Now let $N$ to be a positive arbitrary integer, 
then there exist integers $r \in \mathbb{N} \cup \{ 0 \} $ 
&amp; $s \in \mathbb{N} \cup \{ 0 \} $ and $n$; 
such that $\gcd(n,10)=1$ and $N=2^r 5^s n$.</p>

<p>Let $t:=\max \{ r,s \} $, and let:</p>

<p>$$k:=\dfrac{10^{\varphi(n)}-1}{n}.10^t \ ; $$</p>

<p>then we have: </p>

<p>$$kN = \underbrace{9 9 9 ... 9 9 9}_{\varphi(n)-\text{ times}} 
\underbrace{0 0 0 ... 0 0 0}_{t-\text{ times}} 
.$$</p>
"
"2391050","2391102","<p>Well, I'd use a second degree equation,</p>

<p>axÂ²+bx+c</p>

<p>And then profit P of increase in price would basically be the product between</p>

<p>(i) 300 + 20x,</p>

<p>which is how many products you're going to sell for a given decrease in price, and </p>

<p>(ii) 60 - x </p>

<p>which is the new price. (Though keep in mind that, to get profit you'ld have to subtract 40 of (ii) otherwise you'ld assume that the product is free to produce). </p>

<p>So: (i) * (ii)</p>

<p>Rest is up to you, but the easiest way is to use derivatives, whom you may or may not be familiar with...</p>
"
"2391051","2391057","<p>By definition, $f_y(x_0,y_0)$ is the derivative at $y_0$ of the function $f(x_0,\bullet)$, so the ""choice"" is sound (and not a choice at all). The solution is correct, because of course $f(0,y)$ is constantly $0$. The reason why wolframalpha does not get it is that it most likely first calculates $g=f_y$ in the region where it can do it symbolically - say, $\Bbb R\times (\Bbb R\setminus\{0\})$ or the likes of it - and then it tries to evaluate it at $x=0,\, y=0$. This results in the (true) remark that $\lim\limits_{(x,y)\to 0} g(x,y)$ does not exists.</p>
"
"2391054","2391090","<p>Certainly $2016$ does not divide $2^{2016}-1$.</p>

<p>If, more generally, you want to find the exact class of $2^{2016}$ in ${\Bbb Z}_{2016}$ since Euler's theorem does not apply you may proceed as follows.</p>

<p>First observe that
$$
2^{11}=2048\equiv2^5=32\bmod2016.
$$
Then use this congruence to lower the exponent of $2$. Since $2016=11^3+5\cdot11^2+7\cdot11+3$ you get first
$$
2^{2016}=2^{11^3}\cdot(2^{11^2})^5\cdot(2^{11})^7\cdot2^3\equiv
2^{5^3}\cdot2^{5^3}\cdot2^{10}\cdot2^3\bmod2016
$$
(this is justified by $2^{ab}=(2^a)^b$) and then you proceed again ""substituting"" every $2^{11}$ in the congruence with a $2^5$ until the final exponent becomes feasable.</p>
"
"2391056","2391080","<p>Notice that $\angle BFE + \angle BDF = \frac {\pi}{4}$, together with $\angle BFE=2\angle BDF$ makes a system of equations, easy to solve it.</p>
"
"2391060","2391070","<p>Being able to compute a Taylor series does not mean that the series converges. Convergence has to be tested additionally. Same case happens for $\log(1-x)$ or $\log(1+x)$. Furthermore, there is even Taylor series that converge and are not equal to the value of the function. These are examples of $\mathcal{C}^\infty$ not analytical functions like the traditional one:
$$\exp\left(-\frac{1}{x^2}\right)$$
for $x=0$. Maybe <a href=""https://en.wikipedia.org/wiki/Taylor_series"" rel=""nofollow noreferrer"">this</a> can help you clarify.</p>
"
"2391064","2391355","<p>Let $c$ denote composite numbers and $p$ denote prime numbers.</p>

<p>We need to prove that $\displaystyle{\sum \limits_{c \leq n} \frac{j}{1+\sum \limits_{p|j} p-1} \leq n}$ which is just $\displaystyle{\sum \limits_{c \leq n} \left(\frac{1}{j}+\frac{\sum \limits_{p|j} p-1}{j}\right)^{-1} \leq n}$ </p>

<p>And we want $\frac{\sum \limits_{p|j} p-1}{j}$ to be as big as possible.</p>

<p>$j=p_1^{e_1} p_2^{e_2} \cdots $ to be composite must have at least 2 distinct prime numbers or a prime raised to a power bigger than 1.</p>

<p>Now we will deal with $j= p_1 p_2 \cdots $ with out powers because $\frac{p_1+p_2+ \cdots}{p_1^{e_1} p_2^{e_2} \cdots} \leq \frac{p_1+p_2 +\cdots}{p_1 p_2 \cdots}$ which is easy to see.</p>

<p>Now for the first case $j=p_1 p_2$ so $\frac{\sum \limits_{p|j} p-1}{j} = \frac{p_1+p_2}{p_1 p_2}$ and its maximum value is $\frac{5}{6}$ (Why ?)</p>

<p>We have $\frac{p_1 +p_2}{p_1 p_2} \leq \frac{5}{6}$ so $6p_1+6p_2 \leq 5p_1 p_2$ divide by $p_1 p_2$ we get $\frac{6}{p_2}+\frac{6}{p_1} \leq 5$ since $p_1 \geq 2$ and $p_2 \geq 3$ so $\frac{6}{p_2}+\frac{6}{p_1} \leq \frac{6}{3}+\frac{6}{2} = 5 \leq 5$.</p>

<p>For the second case $j =p_1 p_2 p_3$ so $\frac{\sum \limits_{p|j} p-1}{j} = \frac{p_1+p_2+p_3}{p_1 p_2 p_3}$</p>

<p>We want to prove that $\frac{p_1+p_2 +p_3}{p_1 p_2 p_3} \leq \frac{p_1 +p_2}{p_1 p_2}$ multiply by $p_1 p_2 p_3$ we get that $p_1+p_2+p_3 \leq (p_1+p_2)p_3$ divide by $(p_1+p_2)p_3$ we get that $\frac{1}{p_3} +\frac{1}{p_1+p_2} \leq 1$ and since $p_1 \geq 2$ and $p_2 \geq 3 $ and $p_3 \geq 5$ so we get that  $\frac{1}{p_3} +\frac{1}{p_1+p_2}\leq \frac{1}{5}+\frac{1}{2+3} =\frac{2}{5} \leq 1$. </p>

<p>For the third case $j =p_1 p_2 p_3 p_4$ so $\frac{\sum \limits_{p|j} p-1}{j} = \frac{p_1+p_2+p_3+p_4}{p_1 p_2 p_3 p_4}$</p>

<p>We want to prove that $\frac{p_1+p_2 +p_3+p_4}{p_1 p_2 p_3 p_4} \leq \frac{p_1 +p_2}{p_1 p_2}$ multiply by $p_1 p_2 p_3 p_4$ we get that $p_1+p_2+p_3+p_4 \leq (p_1+p_2)p_3 p_4$ divide by $(p_1+p_2)p_3 p_4$ we get that $\frac{1}{p_3 p_4} +\frac{1}{(p_1+p_2) p_4} \frac{1}{(p_1+p_2)p_3} \leq 1$ and since $p_1 \geq 2$ and $p_2 \geq 3 $ and $p_3 \geq 5$ and $p_4 \geq 7$ so we get that  $\frac{1}{p_3 p_4} +\frac{1}{(p_1+p_2)p_3}+\frac{1}{(p_1+p_2)p_4}\leq \frac{1}{5*7}+\frac{1}{(2+3)*5} +\frac{1}{(2+3)*7} =\frac{17}{175}  \leq 1$. </p>

<p>For the general case $j = p_1 p_2 p_3 \cdots p_k$ so $\frac{\sum \limits_{p|j} p-1}{j} = \frac{p_1 +p_2 +p_3 + \cdots +p_k}{p_1 p_2 p_3 \cdots p_k}$</p>

<p>We want to prove that $\frac{p_1 +p_2 +p_3 + \cdots +p_k}{p_1 p_2 p_3 \cdots p_k} \leq \frac{p_1+p_2}{p_1 p_2}$ multiply by $p_1 p_2 p_3 \cdots p_k$ we get that $p_1 +p_2 +p_3 +\cdots +p_k \leq (p_1+p_2)p_3 p_4 \cdots p_{k-1} p_k$.</p>

<p>its easy to see that $p_1 +p_2 +p_3 +\cdots +p_k  \leq (k-1)p_k$, so $p_1 +p_2 +p_3 +\cdots +p_k \leq (k-1) p_k \leq (p_1+p_2)p_3 p_4 \cdots p_{k-1} p_k$. since $p_1 +p_2 \geq 5$ we get that $(k-1)p_k \leq 5 p_3 \cdots p_{k-1}p_k$.</p>

<p>its also easy to see that $p_3 \cdots p_{k-1} p_k \geq p_{k-1}p_k$ since all primes are bigger than $1$. so we get that $(k-1)p_k\leq 5 p_{k-1}p_k \leq 5 p_3 \cdots p_{k-1}p_k$.</p>

<p>divide by $5p_{k-1}p_k$ we get that $\frac{(k-1)p_k}{5p_{k-1}p_k} \leq 1$ from that we arrive at $\frac{k-1}{5p_{k-1}}$ we know from PNT that $p_{k-1} \geq (k-1) \ln{(k-1)}$ so we get that $\frac{k-1}{5 (k-1) \ln(k-1)} \leq \frac{k-1}{5(k-1)} = \frac{1}{5} \leq 1$, concluding that the <strong>maximum value for $\frac{\sum \limits_{p|j} p-1}{j}$ is $\frac{5}{6}$</strong>.</p>

<p>So $\displaystyle{\sum \limits_{c \leq n} \left(\frac{1}{j}+\frac{\sum \limits_{p|j} p-1}{j}\right)^{-1} \geq \sum \limits_{c \leq n} \left(\frac{1}{j}+\frac{5}{6}\right)^{-1} }$</p>

<p>And $\displaystyle{ \sum \limits_{c \leq n} \left(\frac{1}{j}+\frac{5}{6}\right)^{-1} } \geq \displaystyle{ \sum \limits_{c \leq n} \left(\frac{6}{7}\right)^{-1} }$ for all $j\geq 42$.</p>

<p>$ \displaystyle{ \sum \limits_{c \leq n} \left(\frac{6}{7}\right)^{-1}= \sum \limits_{c \leq n} \left(\frac{7}{6}\right) = \sum \limits_{k=1}^{n} \left(\frac{7}{6}\right)  -\sum \limits_{ p\leq n} \left(\frac{7}{6}\right)= \frac{7}{6}n-\frac{7}{6} \pi(n) \geq \frac{7}{6} n - \frac{7}{6} \frac{2n}{\ln n}  }  $ </p>

<p>We want to prove that $\frac{7}{6}n -\frac{7}{6} \frac{2n}{\ln n} &gt;n $ divide by $n$ we get that $\frac{7}{6}-\frac{7}{6}*\frac{2}{\ln n} &gt;1$ we arrive at $\frac{1}{6} &gt; \frac{14}{6 \ln n}$ so we arrive at $\ln n &gt;14$ and this is true for all $n \geq 1202605$.</p>

<p>Note : $\pi(n)$ is the prime counting function and by the PNT its asymptotic to $\frac{n}{\ln n}$ and proven to be smaller than $\frac{2n}{\ln n}$ for all $n\geq 2$.</p>

<p><strong>Conclusion : your inequality will be false for all $n\geq 1202605$ and may be by computer checking one can reduce this number.</strong></p>
"
"2391065","2391067","<p>Show that $r\mid s$ implies that the polynomial $X^r-1$ divides $X^s-1$.</p>
"
"2391066","2391119","<p>My initial idea would be to exploit a function like
$$ g(z) = \frac{f(z)}{\sin z}.$$
What is nice about $g(z)$ is that it is <em>holomorphic</em>, and it is holomorphic <em>everywhere</em>. It is true that $1 / \sin z$ has poles at $z = k\pi$ for $k \in \mathbb N$, but these poles are cancelled out by the zeroes of $f(z)$ in the numerator.</p>

<p>Our ultimate goal is to prove that $g(z) = 1$. Having established that $g(z)$ is holomorphic, we can try to show that $g(z) = 1$ using familiar theorems about holomorphic functions such as the maximum modulus principle and Liouville's theorem.</p>

<p>First, let us obtain a bound on $|g(x + iy)|$. Assuming I did my algebra correctly, we have an inequality:
$$|g(x + iy)| \leq \frac{e^{|y|}}{|\sin(x + iy)|}=\frac{\sqrt{2}e^{|y|}}{\sqrt{\cosh 2y - \cos 2x}}$$
Notice that this inequality is not uniformly useful! For example, at $x + iy = 0$, the expression on the right-hand side is infinite, so the inequality tells us nothing useful at $x + iy = 0$. However, there are some special contours within the complex plane where the expression on the RHS is not only useful for bounding $|g(x + iy)|$, but also easy to analyse:</p>

<ul>
<li>On vertical lines of the form $x = (N + \frac 1 2) \pi$ where $N \in \mathbb N$, the $\cos 2x$ term vanishes. It is easy to see that the expression on the RHS is bounded by $2$.</li>
<li>On horizontal lines of the form $y = M$ where $|M| \gg 0$, we can approximate $\cosh 2y$ by $e^{2|y|}$. The expression on the RHS is bounded by $2 / \sqrt{1 - 2e^{-2|M|}}$, which tends to $2$ as $M \to \infty$.</li>
</ul>

<p>The maximum modulus theorem allows us to use our knowledge about $|g(x + iy)|$ on these special contours to bound $|g(x + iy)|$ in other places. Let us define the rectangular domain
$$D_{N, M} =\{x + iy \in \mathbb C \ : -(N+\tfrac 1 2)\pi \leq x \leq (N + \tfrac 1 2)\pi, \ -M \leq y \leq M \},$$ for arbitrary $ N \in \mathbb Z,$ $M \gg 0.$ Using our above observations about the behaviour of $|g(x + iy)|$ on the boundary of this rectangular domain, the maximum modulus principle tells us that
$$ z \in D_{N,M} \implies |g(z)| \leq \frac{2}{\sqrt{1 - 2 e^{-2M}}} \ .$$
And since any $z \in \mathbb C$ is contained in $D_{N,M}$ for sufficiently large $N$ and $M$, we have
$$ z \in \mathbb C \implies |g(z)| \leq 2 \ .$$</p>

<p>Having shown that $|g(z)| \leq 2$, Liouville tells us that $g(z)$ is constant, i.e. that $f(z) = C \sin z$ for some $C \in \mathbb C$. We can fix the value of $C$ to be $1$ using the fact that $f'(0) = 1$, and this completes the proof.</p>
"
"2391089","2391091","<p>$$x^y = \exp(y \log(x))$$</p>

<p>so you want $$\exp(i \log(i)) = \exp\left(i \times \frac{i \pi}{2}\right) = \exp(-\pi/2)$$</p>

<p>The others are similar.</p>
"
"2391098","2391107","<p>It is equivalent to </p>

<p>$$x(y-2)=y^2-y+4\iff x=\frac{y^2-y+4}{y-2}=y+1+\frac6{y-2}$$</p>

<p>for $y\neq2$.</p>

<hr>

<p><strong>EDIT</strong></p>

<p>As @lab bhattacharjee suggested, we may also arrange the equation with $y$ as the subject. </p>

<p>$$y^2-(1+x)y+(2x+4)=0\tag1$$</p>

<p>For integer $y$, we must have some integer $z$ such that
$$\Delta=z^2=(1+x)^2-4(2x+4)=x^2-6x-15=(x-3)^2-24$$
$$\iff(x-3-z)(x-3+z)=24$$</p>

<p>So there exists some integer $a=x-3-z$ and $\frac{24}{a}=x-3+z$.</p>

<p>Thus we have $$a+\frac{24}a=2x-6\iff x=\frac a2+\frac{12}a+3$$</p>

<p>So $a=2,4,6,12\iff x=8,10$. Substitute both values of $x$ into $(1)$ and we get</p>

<p>$$(x,y)=(8,4),(8,5),(10,3),(10,8)$$</p>
"
"2391100","2391420","<p>For $x&gt;0$,</p>

<p>$$ \begin{align}\int_{0}^{\infty} \frac{1- \cos(tx)}{e^{t}-1} \, dt &amp;= \int_{0}^{\infty} \left(1- \cos (tx) \right)\frac{e^{-t}}{1-e^{-t}} \, dt \\ &amp;= \int_{0}^{\infty} \left(1- \cos (tx) \right) \sum_{n=1}^{\infty} e^{-tn} \, dt \\ &amp;= \sum_{n=1}^{\infty} \int_{0}^{\infty} \left(1- \cos (tx) \right) e^{-tn} \, dt \\ &amp;= \sum_{n=1}^{\infty} \left(\frac{1}{n} - \frac{n}{n^{2}+x^{2}} \right) \\ &amp;= \sum_{n=1}^{\infty} \frac{x^{2}}{n(n^{2}+x^{2})}. \end{align}$$</p>

<p>(Switching the order of summation and integration is justified by Tonelli's theorem.)</p>

<p>Using the <a href=""https://en.wikipedia.org/wiki/Digamma_function#Series_formula"" rel=""noreferrer"">series representation of the digamma function</a>, we see that</p>

<p>$$\begin{align} \psi(1+ix) + \psi(1-ix) &amp;= - 2 \gamma + \sum_{n=1}^{\infty} \left(\frac{ix}{n(n+ix)} + \frac{-ix}{n(n-ix)}\right) \\ &amp;= -2 \gamma +2 \sum_{n=1}^{\infty} \frac{x^{2}}{n(n^{2}+x^{2})}. \end{align}$$</p>

<p>Therefore, $$\begin{align}\int_{0}^{\infty} \frac{1-\cos(tx)}{e^{t}-1} \, dt &amp;= \frac{1}{2} \left(\psi(1+ix)+\psi(1-ix) \right)+ \gamma \\ &amp;= \Re \left(\psi(1+ix) \right) + \gamma \tag{1} \\ &amp;= \Re \left(\psi(ix) + \frac{1}{ix}\right) + \gamma \tag{2} \\ &amp;=  \Re \left(\psi(ix)\right) + \gamma.\end{align}$$</p>

<hr>

<p>$(1)$ <a href=""https://en.wikipedia.org/wiki/Schwarz_reflection_principle"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Schwarz_reflection_principle</a></p>

<p>$(2)$ <a href=""https://en.wikipedia.org/wiki/Digamma_function#Reflection_formula"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Digamma_function#Reflection_formula</a></p>
"
"2391106","2391117","<p>Row and columns are described algebraically by multiplying by invertible matrices.</p>

<p>You are looking for solutions to the equation $(A - I) v = 0$. The specific form of the matrix doesn't matter, so let's just consider the general problem of solving $Bv = 0$.</p>

<p>Row operations are nice for this, because they multiply on the <em>left</em>: if $E$ encodes the row operation, then applying it to the matrix $B$ gives the matrix $EB$.</p>

<p>We can make $EB$ appear in the equation simply by multiplying $Bv = 0$ on the left by $E$ to get $EBv = 0$.</p>

<p>So, that's why you are taught to use row operations to solve systems of linear equations!</p>

<p>Column operations multiply on the right; unfortunately it's harder to make $BE$ appear in the equation: the simplest way to do so is that $Bv = 0$ is equivalent to the equation $(BE) (E^{-1} v) = 0$</p>

<p>So, you <em>can</em> make using column operations work. The trick, through, is that once you've found the solutions to $(BE) x = 0$, the solutions you were <em>actually</em> looking for are those with $E^{-1} v = x$, or equivalently, $v = Ex$.</p>

<p>In other words, once you get the solution, you have to take all of the column operations you applied and correctly reinterpret them as column operations to be applied (in reverse order!) to the solution vectors.</p>

<p>In your example, this means after obtaining the solution you got to the column-modified matrix, you need to swap the first and third rows of the solution vectors to get the solution to the original system. (both operations are represented by the same elementary matrix, which is why this is the correct operation to do)</p>

<p>This is tricky, and (IMO) doesn't really offer any benefit; the usual method taught for solving systems of equations has no problem just doing row operations here, to compute</p>

<p>$$ \begin{pmatrix} 0 &amp;0&amp;1\\0 &amp;0&amp;1\\0 &amp;0&amp;1\\\end{pmatrix} 
\to\begin{pmatrix} 0 &amp;0&amp;1\\0 &amp;0&amp;0\\0 &amp;0&amp;0\\\end{pmatrix}  $$</p>

<p>and then to extract the solution space from this row reduced echelon form.</p>
"
"2391110","2391111","<p>You could solve all 3 equations symmetrically as
$$c=\frac{1}{2x}=-\frac{1}{2y}=\frac{1}{2z}. $$
This gives $$x=-y=z, $$
and the equation of the sphere gives
$$3x^2=R^2 \implies x= \pm R/\sqrt3 .$$
The two critical points are then $$ \pm R/\sqrt{3}(1,-1,1).$$
I'll leave it to you to find which one is the maximum. </p>
"
"2391112","2391337","<p>We know how to calculate the gradient with respect to $S$ 
$$G=\frac{\partial f}{\partial S}$$ 
We also know that
$$\eqalign{
 X &amp;= S^{-1} - A\cr
 dX &amp;= -S^{-1}\,dS\,S^{-1} &amp;\implies dS = -S\,dX\,S \cr
}$$
Let's use this to write the differential of the function, and then perform a change of variables to find a result in terms of $X$ 
$$\eqalign{
 df &amp;= G:dS \cr
  &amp;= -G:S\,dX\,S \cr
  &amp;= -S^TGS^T:dX \cr
  &amp;= -S^T\,\frac{\partial f}{\partial S}\,S^T:dX \cr
\cr
\frac{\partial f}{\partial X}  &amp;= -S^T\,\frac{\partial f}{\partial S}\,S^T \cr
}$$
where colon denotes the inner/Frobenius product, i.e.
$$A:B={\rm tr}(A^TB)$$
and the cyclic properties of the trace give rise to some rules for rearranging the product, i.e.
$$\eqalign{
 A:BC &amp;= AC^T:B \cr
 A:BC &amp;= B^TA:C \cr
 A:BC &amp;= BC:A \cr
}$$</p>

<p>As you've discovered, the chain rule can be difficult to apply to matrix problems when the intermediate quantities, i.e. matrix-by-matrix or vector-by-matrix derivatives, are higher-order tensors.  </p>

<p>The virtue of the differential approach is that the differential of a matrix behaves like an ordinary matrix.</p>
"
"2391135","2391175","<p>\begin{align*}
&amp;\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
\text{Suppose $a\setminus b$ exists.}\\[6pt]
&amp;\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
\text{Let $x = a\setminus b$, and let $S = \{y \mid a \le b \cup y\}$.}\\[6pt]
&amp;\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
\text{We need to show}\\[6pt]
&amp;(1)\;\;x \in S\\[4pt]
&amp;(2)\;\;y \in S \implies x \le y\\[6pt]
&amp;\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
\text{The proof of $(1)$ is immediate:}\\[5pt]
&amp;a \le a \cup b\\[4pt]
\implies\;&amp;a \le x \cup b&amp;&amp;\text{[since $x \cup b = a \cup b$]}\\[4pt]
\implies\;&amp;a \le b \cup x\\[4pt]
\implies\;&amp;x \in S&amp;&amp;\text{[by definition of $S$]}\\[6pt]
&amp;\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
\text{Next the proof of $(2)$:}\\[6pt]
&amp;y \in S\\[4pt]
\implies\;&amp;a \le b\cup y&amp;&amp;\text{[by definition of $S$]}\\[4pt]
\implies\;&amp;a \cup b \le b\cup y\\[4pt]
\implies\;&amp;x \cup b \le  b\cup y&amp;&amp;\text{[since $x \cup b = a \cup b$]}\\[4pt]
\implies\;&amp;x\cap (x \cup b) \le x  \cap (b\cup  y)\\[4pt]
\implies\;&amp;(x \cap x) \cup (x \cap b) \le  (x \cap b) \cup (x \cap y)
&amp;&amp;\text{[by the distributive law]}\\[4pt]
\implies\;&amp;x \cup \bot \le  \bot \cup (x \cap y)
&amp;&amp;\text{[since $x\cap b = \bot$]}\\[4pt]
\implies\;&amp;x \le  x \cap y\\[4pt]
\implies\;&amp;x \le y\\[6pt]
&amp;\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
\text{as required.}\\[6pt]
&amp;\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
\text{This completes the proof.}\\[6pt]
\end{align*}</p>
"
"2391141","2391147","<p>Let $G$ acts on the sets $A$ and $B$. We say that these actions are equivalent (isomorphic) if there is a bijection $\phi$ from $A$ to $B$ such that $$g.\phi(a)=\phi(g.a) \text{for all } g\in G \text{ and for all } a\in A.$$ </p>
"
"2391142","2391180","<p>Your argument is right (I don't check the calculations). I got two relevant details:</p>

<p><strong>(detail 1)</strong> One knows that $\dim\mathbb{R}_2 [x] = 3$ and $\{x^2-1,x+1\}$ is linearly independent, thus $\dim U = 2$ and $\dim U^{\perp} = 1$.</p>

<p>In that way, we can answer question 1: if your calculations are right, the set $\{-5x^2+2x+1\}$ is a basis for $U^{\perp}$.</p>

<p><strong>(detail 2)</strong> When you write $p(x) = q(x)+r(x)$ you are using implicitly that $\mathbb{R}_2[x] = U + U^{\perp}$, and this is right since $\mathbb{R}_2[x] = U\oplus U^{\perp}$.</p>
"
"2391143","2391144","<p>Adding all the equations, we get -</p>

<p>$$x^2+1+y^2+1+z^2+1=2x+2y+2z$$</p>

<p>$$x^2-2x+1+y^2-2y+1+z^2-2z+1=0$$</p>

<p>$$(x-1)^2+(y-1)^2+(z-1)^2=0$$</p>

<p>$$\implies x=y=z=1$$</p>
"
"2391152","2391288","<p><strong>Hint:</strong> You can treat $A$ and $B$ as (dependent) random variables. Then it  is asked to calculate </p>

<p>$$\sigma_{AM}=Cov\left(A,\frac12(A+B)\right)$$</p>

<p>where $Cov(A,A)=\sigma_A^2$ and $Cov(A,B)=\sigma_{AB}$</p>

<p>$\texttt{Additional hint}:$ You can derive the result straightforward if you use the property that the covariance is $\color{blue}{\texttt{bilinear}}$.</p>
"
"2391154","2391435","<p>Given the conic equation $a_{11} x^2 + a_{22} y^2 + 2a_{12}xy + 2a_{13} x + 2a_{23} y + a_{33} = 0 $</p>

<p>The conic is non-degenerate if the determinant of  the symmetric matrix $a_{ij}=a_{ji}$ is not zero</p>

<p>$$A=\left|
\begin{array}{ccc}
 a_{11} &amp; a_{12} &amp; a_{13} \\
 a_{21} &amp; a_{22} &amp; a_{23} \\
 a_{31} &amp; a_{32} &amp; a_{33} \\
\end{array}
\right|\ne 0$$</p>

<p>you can write the equation of the <em>polar line</em> from an external point $P(x_0,y_0)$</p>

<p>$$(a_{11} x_0 + a_{12} y_0 + a_{13} )\,x + (a_{21} x_0 + a_{22} y_0 + a_{23} )\,y + a_{31} x_0 + a_{32} y_0 + a_{33} = 0$$</p>

<p>This line intersects the conic in the points where the tangents do, so you can solve the system between the conic and the polar and find the distances from the intersection points to the point $P$</p>

<p>In the example of Jan Magnus we had</p>

<p>$x^2-xy+y^2-1=0$ so that $a_{11}=1;\;a_{12}=a_{21}=-\frac12;\;a_{22}=1;\;a_{33}=-1$</p>

<p>and the point is $P(-2,0)$</p>

<p>Thus the polar line has equation</p>

<p>$-2x + \left(-\frac12 (-2) \right)\,y -1= 0 $ that is $y=2x+1$</p>

<p>plug this equation in the equation of the conic, to solve the system</p>

<p>$x^2-x(2x+1)+(2x+1)^2-1=0$ that is $3 x^2+3 x=0$ which gives $x_1=0;\;y_1=1$ and $x_2=-1;\;y_2=1$ Intersection points are $A(0,1)$ and $B(-1,1)$</p>

<p>We find $PA=\sqrt 5$ and $PB=\sqrt 2$</p>

<p>Hope this helps</p>
"
"2391155","2391368","<p>Note that the composition $S^n\stackrel{\mathrm{pinch}}{\to} S^n\vee S^n\to\stackrel{r\vee \mathrm{id}}{\to} S^n$ is nullhomotopic (this is basically what you prove when you prove that inverses exist in $\pi_n$ of a space).  Looking at the induced map on cohomology, this means that $r^*+\mathrm{id}^*=0$, so $r^*$ is multiplication by $-1$.</p>
"
"2391157","2392920","<p>First of all in this risk model you are collecting a continuous of income, so your surplus is keep increasing when there is no claim. Before the first claim happen, the surplus at time $t$ is $u + ct$. </p>

<p>If you require the maximum surplus never exceed $b$, then you essentially require the first claim must be early enough such that the surplus will not exceed $b$, so at the time when the first claim happen, we have the surplus $u + cW_1$ and we require</p>

<p>$$ u + cW_1 &lt; b \iff W_1 &lt; \frac {b - u} {c} $$</p>

<p>So this explain the outer integral limits. </p>

<p>Now we consider the claim size $X$. If it exceed $u + cW_1$, then ruin occur immediately, therefore under this condition, the conditional probability is 1, and it form the second integral in the RHS (where $g = 1$).</p>

<p>Otherwise, the ruin is not happen yet and the immediate surplus after claim is $u + cW_1 - X_1$. By the markov property, it is just like we start over again with the new initial surplus $u + cW_1 - X_1$. And this is just the first integral in the RHS.</p>

<p>The most important thing is you know the law of total probability.</p>
"
"2391163","2391204","<p>As you present it you're right: You <em>can</em> just set
$$ \phi(s,p) = \begin{cases} 1 &amp; \text{if $s=p$ and $s$ has at least four prime factors} \\ 0 &amp; \text{otherwise} \end{cases} $$
and that will obviously be sound and complete.</p>

<p>If that is <em>not</em> a sufficient answer, it must be because there are additional conditions of $\phi$ that you have not reproduced in your question. What they might be is anyone's guess, though.</p>

<p>It is common to require that $\phi$ is <em>computable</em>, but that is obviously the case here -- we can easily check whether $s$ equals $p$, and we can also compute the prime factorization of $s$ and sum the exponents.</p>

<p>One also sometimes sees the stronger requirement that $\phi$ is <em>primitive recursive</em>, but that too is the case here (it takes a slight bit of ingenuity to show this, but not much).</p>

<p>Other than that we get into guesswork territory. One hypothesis could be that the textbook you get the exercise from might want to be able to verify proofs in <em>polynomial time</em>, in which case the above function won't do unless $s$ is given in unary notation. Then it would make some sense as an exercise, because $p$ would need to encode data about the factorization that cannt quickly be extracted from $s$ itself. (One option would be to let $p$ be an encoding of four factors $&gt;2$ that multiply together to yields $s$).</p>

<p>Alternatively there may be a hidden requirement that $\phi$ be specified in a particular formalism or notation.</p>
"
"2391171","2391223","<p>Draw a picture. Pick $p \in X -A$, $q \in Y-B$, and use that for $x \notin A$, the set $Y_x = \{x\} \times Y$ lies in $X\times Y - A \times B$ and is connected as a homeomorph of $Y$. The same holds for $X_y = X \times \{y\}$ for $y \notin B$, using $X$ is connected.</p>

<p>Suppose $(x,y), (x',y') \in X \times Y  -A \times B$. Connect them using unions of sets of the form $X_l (l \notin B)$ and $Y_m (m \notin A)$ that intersect each other one to the next (so sets like $S_1 \cup S_2 \cup S_3$ with $S_1 \cap S_2 \neq \emptyset$ and $S_2 \cap S_3 \neq \emptyset$. (There are some cases depending on e.g. whether $x \notin A$ or $y \notin B$) This implies these unions are connected and completely inside $X \times Y - A \times B$, and so the latter set is connected as well.</p>
"
"2391176","2395502","<p>Say we have two points $P_1 = (x_1, y_1)$ and $P_2 = (x_2, y_2)$, then the condition of one point seeing the other is equivalent to the condition that
$$\gcd (x_2-x_1, y_2-y_1) = \pm 1$$
(The $\pm$ comes from the fact that one of the differences might be negative, but this is not essential)
Lets call this the ""distance"" between points $P_1$ and $P_2$ and write it down as $|P_2 - P_1| = \gcd (x_2-x_1, y_2-y_1)$</p>

<p>Let us show a construction that produces a loop of size $n+2$ (for arbitrary $n$, for example, $n=98$).
First let us create $n+1$ points:
$$A_0 = (0, 0); A_1 = (a, b); A_2 = (2a, 2b) \dots A_n = (na, nb)$$
Where $a$ and $b$ are arbitrary numbers on which we impose simply that $\gcd (a, b) = \pm 1$. Notice that the points all lie on one straight line.
This implies that each of the points $A_1, A_2 \dots A_{n-1}$ sees exactly two others, while $A_0$ and $A_n$ see exactly one other point.</p>

<p>(The gcd condition makes it so that there are no points between $A_i$ and $A_{i+1}$, since then 
$$|P_{i+1} - P_i|= \gcd (a\cdot(i+1)-a \cdot i, b \cdot (i+1)-b \cdot i) = \gcd (a, b) = \pm 1$$
And it is clear that two points that have a point in between them cannot see each other from definition, thus any single point on this line can see no more than two others.)</p>

<p>Now all that is left is to find the coordinates of a point $B = (c, d)$ such that $B$ sees $A_0$ and $A_n$ and does not see any other point. So in essence this boils down to the following system:
$$|B-A_0| = \gcd(c-0, d-0) = \gcd(c, d) = \pm 1$$
$$|B-A_i| = \gcd(c-ia, d-ib) \neq \pm 1 $$
$$|B-A_n| = \gcd(c-na, d-nb) = \pm 1$$
where the middle equation has to be satisfied for all $0&lt;i&lt;n$. Now since we have freedom over what exactly is the $|B-A_i|$ for these $i$ then we will require that 
$$\gcd(c-ia, d-ib) = d-ib \neq \pm 1$$
This in essence means simply that 
$$c-ia \equiv 0 \mod d-ib$$
$$c \equiv ia \mod d-ib$$</p>

<p>This is starting to look like the chinese remainder theorem now, isnt it?
For $|B-A_0|$ we will simply ask that 
$$ c \equiv 1 \mod d$$
(it is not hard to verify that this means that $\gcd(c, d) = 1$)
And for $|B-A_n|$ let us require that
$$ c \equiv na+1 \mod d - nb$$</p>

<p>So we have translated our $\gcd$ requirements into a system of congruences:
$$ c \equiv 1 \mod d$$
$$ c \equiv ia \mod d - ib$$
$$ c \equiv na+1 \mod d - nb$$
The chinese remainder theorem guarantees the existence of such a $c$. However, we must first establish that all of the modulus are pairwise coprime.</p>

<p>Notice that $d, d-b, d-2b \dots d-nb$ form an arithmetic progression. Now by the well known <a href=""https://en.wikipedia.org/wiki/Green%E2%80%93Tao_theorem"" rel=""nofollow noreferrer"">Green-Tao theorem</a> there exists an arithmetic progression of $k$ terms consisting only of primes for any $k$. So we choose $k=n+1$ and take the corresponding $d$ and $-b$ to be the first member of the sequence and difference respectively. Thus all of $d, d-b, d-2b \dots d-nb$ are now primes and are obviously coprime to one another.</p>

<p>Obviously it is possible to choose $a$ such that $\gcd(a,b) = 1$. And the existence of $c$ is guaranteed by the chinese remainder theorem.</p>

<p>This completes the proof.</p>
"
"2391181","2391188","<p>Supposing that we are working in $\mathbb{Q}$ with the usual multiplication, than
$$
\left(\frac{1}{2}\right)^2=\frac{1}{2} \cdot \frac{1}{2}= \frac{1\cdot 1}{2\cdot 2}= \frac{1^2}{2^2}=\frac{1}{4} 
$$</p>
"
"2391187","2391324","<p>The map from a Noetherian local ring to its completion is faithfully flat, and a faithfully flat epimorphism is an isomorphism. It follows from this that the map from the localization of K[X] at (X) to K[[X]] is not epi. </p>
"
"2391191","2391218","<p>In this <em>particular</em> case you can probably get away with defining your set as
$$ X = \{ x\in\Sigma \mid \exists n\ge 0: c(f^n(x)) = T \} $$
But that works only because your recursive definition has a very particular form.</p>

<p>For <em>general</em> recursive definitions of this kind, I can say with some certainty that there isn't a generally understood way to write them that is as short and convenient as set builder notation. I did a PhD and 3 years of post-doc work in an area of computer science where such definitions are our bread-and-butter and I never saw one. What we would usually do was write an <em>inference system</em>:</p>

<blockquote>
  <p>$$ \frac{}{x\in X}\; c(X)=T \qquad \qquad \qquad \frac{f(x)\in X}{x\in X} $$</p>
</blockquote>

<p>and expect the reader to know how to interpret that rigorously. This can usually be assumed when you write for computer scientists, but not with a general mathematical audience.</p>

<p>You can say</p>

<blockquote>
  <p>$X$ is the smallest subset of $\Sigma$ that satisfies
  $$ X = \{ x\in \Sigma \mid c(x)=T \lor f(x)\in X \} $$</p>
</blockquote>

<p>where what you wrote is now presented as a <em>condition</em> rather than a <em>definition</em>. However, this depends on the reader being able to convince himself on his own that there <em>is</em> indeed a smallest subset of $\Sigma$ that satisfies the condition.</p>

<p>If that won't fly with your audience you'll need start by defining a helper function $\Phi:\mathcal P(\Sigma)\to\mathcal P(\Sigma)$:</p>

<blockquote>
  <p>$$ \Phi(A) = \{ x\in\Sigma \mid c(x)=T \lor f(x)\in A \} $$</p>
</blockquote>

<p>and then explicitly iterate it:</p>

<blockquote>
  <p>$$ X = \bigcup_{n=0}^{\infty} \Phi^n(\varnothing) $$</p>
</blockquote>
"
"2391196","2391313","<p>Instead of just ""mindlessly"" dividing into cases, write out the logical connectives ""and"" and ""or"", and everything should (hopefully) take care of itself:
$$
\begin{aligned}
\frac{|x+3|+x}{x+2}&gt;1
\quad
\iff
\quad
&amp; \Biggl( x+3 \ge 0 \quad\text{and}\quad \frac{(x+3)+x}{x+2}&gt;1 \Biggr)
\\
&amp;
\text{or} \quad
\Biggl( x+3 &lt; 0 \quad\text{and}\quad \frac{-(x+3)+x}{x+2}&gt;1 \Biggr)
\\[1em]
\iff \quad
&amp; \dots
\end{aligned}
$$</p>
"
"2391228","2391241","<p>You can arrive at this nice answer without much algebra if you know that the equation
$$
x + y + z = 1
$$
describes the plane containing the points $(1,0,0)$, $(0,1,0)$ and $(0,0,1)$ - the ends of the three unit coordinate vectors. Then you just scale the axes by $1/a$, $1/b$ and $1/c$ to get the equation you want.</p>

<p>Of course that works only of none of $a$, $b$ or $c$ is $0$, while your answer is always right.</p>

<p>Scaling axes is often a good approach to this kind of problem, so a technique worth remembering. It's a good way to turn circles into ellipses in the plane, with analogues is higher dimensions.</p>
"
"2391229","2391675","<p>I finally found a solution for $k = \sin(\pi/12)$. It is taken from an exercise in Borwein's <em>Pi and the AGM</em>.</p>

<p>Let's start with the integral $$I(a, b) = \int_{-a}^{a}\frac{dx}{\sqrt{(a^{2} - x^{2})(b^{2} + x^{2})}}\tag{1}$$ which is equal to $$\frac{2}{\sqrt{a^{2} + b^{2}}}K\left(\frac{a}{\sqrt{a^{2} + b^{2}}}\right)\tag{2}$$ Equivalence of $I(a, b)$ with $(2)$ can be seen by writing $K$ as
\begin{align}
K(k) &amp;= \int_{0}^{\pi/2}\frac{dx}{\sqrt{1 - k^{2}\cos^{2}x}}\notag\\
&amp;= \int_{0}^{\pi/2}\frac{dx}{\sqrt{1 - k^{2} + k^{2}\sin^{2}x}}\notag\\
&amp;=\int_{0}^{1}\frac{dt}{\sqrt{(1 - t^{2})(1 - k^{2} + k^{2}t^{2})}}\notag\\
\end{align}
Putting $k = a/\sqrt{a^{2} + b^{2}}$ we can see that $$K\left(\frac{a}{\sqrt{a^{2} + b^{2}}}\right) = \sqrt{a^{2} + b^{2}}\int_{0}^{1}\frac{dt}{\sqrt{(1 - t^{2})(b^{2} + a^{2}t^{2})}}$$ and using substitution $x = at$ we  get the link between $(1)$ and $(2)$.</p>

<p>Next we use the substitution $x = (1/t) - a$ in $(1)$ to get $$I(a, b) = \int_{1/2a}^{\infty}\frac{dt}{\sqrt{(2at - 1)((a^{2} + b^{2})t^{2} - 2at + 1)}}$$ The expression in square root can be expressed as $$2a(a^{2} + b^{2})t^{3} - (5a^{2} + b^{2})t^{2} + 4at - 1$$ and using the substitution $$t = x + \frac{5a^{2} + b^{2}}{6a(a^{2} + b^{2})}$$ the integral is transformed into $$I(a, b) = \int_{e}^{\infty}\frac{dx}{\sqrt{4x^{3} - g_{2}x - g_{3}}}\tag{3}$$ where $$g_{2} = \frac{(a^{2} - b^{2})^{2}}{12} - a^{2}b^{2},\, g_{3} = -\frac{(a^{2} - b^{2})\{(a^{2} - b^{2})^{2} + 36a^{2}b^{2}\}}{216}\tag{4}$$ and $e$ is the root of the cubic equation $4x^{3} - g_{2}x - g_{3} = 0$.</p>

<p>Next step is to put $g_{2} = 0$ so that $12a^{2}b^{2} = (a^{2} - b^{2})^{2}$ which gives $$a^{2} - b^{2} = -3(2g_{3})^{1/3}, a^{2} + b^{2} = 2\sqrt{3}(2|g_{3}|)^{1/3}\tag{5}$$ For our case we need to set $g_{3} = 1/2$ so that the root $e = 1/2$ and then we have $$a^{2} + b^{2} = 2\sqrt{3}, a^{2} - b^{2} = -3$$ so that $$\frac{a}{\sqrt{a^{2} + b^{2}}} = \frac{\sqrt{3} - 1}{2\sqrt{2}} = \sin(\pi/12)$$ From $(1), (2), (3)$ and above values of $a, b$ it follows that $$K(k) = \frac{3^{1/4}}{2}\int_{1}^{\infty}\frac{dx}{\sqrt{x^{3} - 1}}$$ And putting $x^{3} = 1/t$ we get $$K(k) = \frac{3^{1/4}}{6}\int_{0}^{1}t^{-5/6}(1 - t)^{-1/2}\,dt = \frac{3^{1/4}}{6}\frac{\Gamma(1/6)\Gamma(1/2)}{\Gamma(2/3)}$$ This can be expressed in terms of $\Gamma(1/3)$ only (via duplication and reflection formulas) to give the desired form.</p>

<hr>

<p>I must say that the above solution is totally non-obvious and without the hints in the exercise it does not appear possible to arrive at this solution. The value of $K(\sin(\pi/12))$ was first obtained by Legendre using a similar non-obvious approach.</p>

<hr>

<p>Borwein's book provides the evaluation of $K(\tan(\pi/8))$ as an exercise in a later chapter. But this is based on the representation of $2K(k)/\pi$ as the hypergeometric function ${}_{2}F_{1}(1/2,1/2;1;k^{2})$. To simplify typing we will use $F$ instead of ${}_{2}F_{1}$ in what follows.</p>

<p>The following two hypergeometric transformations are needed to create the magic
\begin{align}
&amp;F(a, b; a - b + 1; x)\notag\\
&amp;\,\,\,\,\,\,\,\,\,\,\,\,= (1 - x)^{1 - 2b}(1 + x)^{2b - a - 1}F\left(\frac{a - 2b + 1}{2}, \frac{a - 2b + 2}{2}; a - b + 1; \frac{4x}{(1 + x)^{2}}\right)\tag{6}
\end{align}
and $$F\left(a, b; a + b + \frac{1}{2}; 4x(1 - x)\right) = F\left(2a, 2b; a + b + \frac{1}{2}; x\right)\tag{7}$$ Equation $(7)$ is by Kummer and <a href=""http://paramanands.blogspot.com/2011/10/elementary-approach-to-modular-equations-hypergeometric-series-2.html"" rel=""nofollow noreferrer"">proved here</a>. Equation $(6)$ can be proved in a similar manner but with more difficulty. We start with the equation $$\frac{2K}{\pi} = F\left(\frac{1}{2},\frac{1}{2};1;k^{2}\right)$$ and put $a=b=1/2, x = k^{2}$ in $(6)$ to get $$\frac{2K}{\pi}=(1 + k^{2})^{-1/2}F\left(\frac{1}{4},\frac{3}{4};1;\frac{4k^{2}}{(1 + k^{2})^{2}}\right)$$ Using $a = 1/8, b = 3/8, x = 4k^{2}/(1 + k^{2})^{2}$ in $(7)$ we get $$\frac{2K}{\pi}=(1 + k^{2})^{-1/2}F\left(\frac{1}{8},\frac{3}{8};1; X\right)\tag{8}$$ where $$X = 4x(1 - x) = \frac{16k^{2}(1 - k^{2})^{2}}{(1 + k^{2})^{4}} = \left(\frac{4kk'^{2}}{(1 + k^{2})^{2}}\right)^{2}$$ It's time to put $k = \tan(\pi/8) = \sqrt{2} - 1$ to get $X = 1$ (check this!). And thus we finally get $$K(\tan(\pi/8)) = \frac{\pi}{2\sqrt{4 - 2\sqrt{2}}}F(1/8, 3/8;1;1)$$ And then we use the formula $$F(a,b;c;1) = \frac{\Gamma(c)\Gamma(c - a - b)}{\Gamma(c - a)\Gamma(c - b)}\tag{9}$$ to get $$K(\tan(\pi/8)) = \frac{\sqrt{\sqrt{2} + 1}\pi}{2^{7/4}}\cdot\frac{\Gamma(1/2)}{\Gamma(7/8)\Gamma(5/8)}$$ which simplifies to the given form using the reflection formula and $\Gamma(1/2) = \sqrt{\pi}$.</p>

<hr>

<p>The transformation $(6)$ is non-obvious and it is used by Borwein brothers to link Ramanujan's alternative theory of theta functions (those dealing with $F(1/4, 3/4;1; x)$) with the standard theory (dealing with $F(1/2, 1/2;1;x)$). This is a key step in Borwein's proof of Ramanujan's famous series for $1/\pi$ and this particular development is well covered in <a href=""http://paramanands.blogspot.com/2012/03/modular-equations-and-approximations-to-pi-part-3.html"" rel=""nofollow noreferrer"">my blog post</a>.</p>
"
"2391238","2391248","<p>In my opinion, the reference to principal value in this context is ridiculous. It would be better to say that $\sqrt{w^2-1}$ is <em>some</em> square root of $w^2-1$.</p>

<p>Now, concerning your question, what happens is that\begin{multline*}\left(w+PV\sqrt{w^2-1}\right)\left(w-PV\sqrt{w^2-1}\right)=1\Longrightarrow\\\Longrightarrow\operatorname{Log}\left(w+PV\sqrt{w^2-1}\right)+\operatorname{Log}\left(w-PV\sqrt{w^2-1}\right)=0\end{multline*}and so $\operatorname{Log}\left(w-PV\sqrt{w^2-1}\right)=-\operatorname{Log}\left(w+PV\sqrt{w^2-1}\right)$. Therefore$$\operatorname{Log}\left(w\pm PV\sqrt{w^2-1}\right)=\pm\operatorname{Log}\left(w+PV\sqrt{w^2-1}\right).$$</p>
"
"2391239","2391244","<p>There are only $10$ ways to have three consecutive seats.  Working clockwise, each block must have a starting seat and every seat can be the first seat in a consecutive block.</p>

<p>To your calculation:  you correctly compute the probability of getting three seats in a row where each successive seat (after the first) adjoins one of the previously selected seats.  However that is not what was asked.  You could also get three consecutive seats by, say, choosing $\#1$, then $\#3$, then $\#2$.</p>
"
"2391240","2391258","<p>Your idea to use Holder was very good!</p>

<p>By Holder $$\left(\sum_{cyc}\frac{a+b}{\sqrt{a+2c}}\right)^2\sum_{cyc}(a+b)(a+2c)\geq8(a+b+c)^3.$$
Thus, it's enough to prove that
$$8(a+b+c)^3\geq4(a+b+c)\sum_{cyc}(a+b)(a+2c)$$ or
$$2(a+b+c)^2\geq\sum_{cyc}(a+b)(a+2c),$$ which is
$$\sum_{cyc}(a-b)^2\geq0.$$
Done!</p>
"
"2391254","2391274","<p>The ""symmetry"" is just an application of the following  general fact: </p>

<blockquote>
  <p>two permutations in $S_n$ are conjugate if and only if they have the same cyclic structure. </p>
</blockquote>

<p>The proof of the normality of $V$ in $S_4$ can be made shorter by observing that the non-trivial elements of the  $V$ are precisely the permutations in $S_4$ whose cyclic structure is the product of two transpositions, so the claim immediately follows.</p>
"
"2391260","2391266","<p>$$\lim_{x\to 0}\frac{25-(x+25)}{x(5+\sqrt{x+25})}=\lim_{x\to 0}\frac{-x}{x(5+\sqrt{x+25})}=\lim_{x\to 0}\frac{-1}{5+\sqrt{x+25}}$$
Note that after the second step we cancel out the $x$ in the numerator and denominator, and are left with an expression with which we can evaluate at $x=0$.</p>
"
"2391277","2391367","<p>To find the number of three of a kind hand, choose one of the $13$ ranks, choose three of the four cards of that rank, choose two of the remaining $12$ ranks, and choose a suit for each of those ranks.<br>
$$\binom{13}{1}\binom{4}{3}\binom{12}{2}\binom{4}{1}^2$$
Hence, the probability of obtaining three of a kind when five cards are drawn is 
$$\frac{\dbinom{13}{1}\dbinom{4}{3}\dbinom{12}{2}\dbinom{4}{1}^2}{\dbinom{52}{5}}$$
In your calculation, you first chose one single card, then chose the other, so you made an ordered selection of the two single cards.  It does matter whether you choose $7\color{red}{\heartsuit}$ then $5\spadesuit$ or $5\spadesuit$ then $7\color{red}{\heartsuit}$.  Therefore, you must divide by the $2!$ orders in which you could have selected the single cards.  Notice that 
$$\frac{1}{2!}\binom{12}{1}\binom{4}{1}\binom{11}{1}\binom{4}{1} = \binom{12}{2}\binom{4}{1}^2$$</p>
"
"2391278","2391307","<p>It is true that the two metrics are not strongly equivalent, and this tells you that $r_2$ should depend on $x$ and $r$. We can argue as follows to show that such $r_2$ exists.</p>

<p>Since $$\left|\frac1x-\frac1y\right|&lt;r_2\iff \frac1x-r_2&lt;\frac1y&lt;\frac1x+r_2,$$ we choose $r_2&lt;\frac1x$, and the condition above is equivalent to
$$\frac{x}{1+xr_2}&lt;y&lt;\frac{x}{1-xr_2}.$$
Thus,
$$-\frac{x^2r_2}{1+xr_2}&lt;y-x&lt;\frac{x^2r_2}{1-xr_2}.$$</p>

<p>Note that the absolute values of both sides tend to $0$ as $r_2$ tends to $0$. We can thus find some $r_2$ such that the absolute values are less than the given $r$. </p>

<p>(For example, choose $r_2$ such that $r_2&lt;\frac{1}{2x}$ and $r_2&lt;\frac{r}{2x^2}$.)</p>
"
"2391281","2392557","<p>Let's follow <a href=""https://en.wikipedia.org/wiki/P-adic_exponential_function"" rel=""nofollow noreferrer"">Wikipedia</a> and define the 2-adic (Iwasawa) logarithm on $\Bbb{Q}_2^*$ as follows. Recall that the group $\Bbb{Q}_2^*$ is a direct product
$$
\Bbb{Q}_2^*=\langle 2\rangle\times(1+2\Bbb{Z}_2).
$$
The usual Taylor series of $\log(1+x)$ familiar from calculus
$$
x-\frac{x^2}2+\frac{x^3}3-\frac{x^4}4+\cdots=\sum_{n=1}^\infty(-1)^{n+1}\frac{x^n}n
$$
converges w.r.t. the 2-adic topology whenever $x\in2\Bbb{Z}_2$. This is because for all $n$ we have $\nu_2(x^n)=n\nu_2(x)\ge n$ but we easily see that $\nu_2(n)\le n/2$. Therefore the general term $x^n/n\to0$ which is sufficient for convergence in a complete non-archimedean case. We can thus use this series and define
$$
\log(1+x)=\sum_{n=1}^\infty(-1)^{n+1}\frac{x^n}n
$$
whenever $x\in 2\Bbb{Z}_2$. The familiar power series identities then show that the rule $$\log(xy)=\log x+\log y\qquad(*)$$ holds whenever $x,y\in1+2\Bbb{Z}_2$.
We can extend the definition of the logarithm to all of $\Bbb{Q}_2$ (actually even further, see WP) in such a way that $(*)$ holds by declaring that $\log(2)=0$, and, as a consequence of $(*)$ that
$$
\log(2^n(1+x))=\log(1+x)=x-\frac{x^2}2+\cdots
$$ 
for all integers $n$ and all $x\in 2\Bbb{Z}_2$.</p>

<p>The usual business of termwise differentiation of a power series carries over to the 2-adics. Therefore when $x,a\in1+2\Bbb{Z}_2$ we get
$$
\lim_{x\to a}\frac{\log x-\log a}{x-a}=1-(a-1)+(a-1)^2-(a-1)^3+\cdots=\frac1a
$$
because the geometric series converges as $a-1\in2\Bbb{Z}_2$.</p>

<p>As we took care to have $(*)$, the usual calculation of the real limit goes thru, and the result $\log'(x)=1/x$ holds everywhere.</p>
"
"2391293","2391298","<p>$$\underbrace{\frac12\cdot\frac23}_{&lt;\left(\frac12\right)^2}\cdot\underbrace{\frac34\cdot\frac45}_{&lt;\left(\frac34\right)^2}\cdots\underbrace{\frac{2n-1}{2n}\cdot\frac{2n}{2n+1}}_{&lt;\left(\frac{2n}{2n-1}\right)^2}=\frac1{2n+1}$$</p>
"
"2391295","2391311","<p>Toward an answer, too long for a comment.</p>

<p>I think you are right that there is no identity that simplifies
$$
\lfloor e_1 \rfloor \lfloor e_2 \rfloor
$$
if what you are looking for is an identity using algebraic operations on $\lfloor e_1 \rfloor$ and  $\lfloor e_2 \rfloor$. I suspect that the discontinuities in the floor function would prevent that.</p>
"
"2391308","2391320","<p>Hint. Note that
$$\int_{0}^{1/2}\frac{x^s-1}{-\log(x)}\,dx =\int_{0}^{1/2}\frac{1}{x^{-s}(-\log(x))}\,dx+\int_{0}^{1/2}\frac{1}{\log(x)}\,dx\\\geq 
\int_{0}^{1/2}\frac{1}{x(-\log(x))}\,dx-\frac{1}{2\log(2)}.$$
Moreover
$$\int \frac{1}{x\log(x)}\,dx=\log(|\log(x)|)+C.$$
Can you take it from here?</p>
"
"2391312","2391404","<p>Let $T$ be the identity operator $T:c_0\to c_0$. Its adjoint is the identity operator on $\ell_1$. The second adjoint is the identity operator on $\ell_\infty$. So the range of $T^{**}$ need not be in $J(Y)$ for general operators. </p>

<p>However, in  the context $T$ is a <strong>compact</strong> operator. Since $c_0$ has the <a href=""https://en.wikipedia.org/wiki/Approximation_property"" rel=""nofollow noreferrer"">approximation property</a>, $T$ is the norm-limit of finite rank operators, so it suffices to verify the statement for the case of finite rank. Each such operator is a sum of rank-one operators, so we can assume $\operatorname{rank} T=1$; the general case follows by linearity.</p>

<p>Every rank-one operator $T:X\to Y$ can be written as $T(x) = f(x)y$ for some $f\in X^*$ and $y\in Y$. Then the adjoint is given by $T^*(g) = g(y)x$ (here $g\in Y^*$) and the second adjoint by $T^{**}(\phi) = \phi(f)y$ (here $\phi\in X^{**}$ so it acts on the elements of $X^*$). Clearly, the range of $T$ is in $J(Y)$, and the relation $T^{**}(x)=J(T(x))$ holds for $x\in X$.</p>

<h3>Remarks</h3>

<ol>
<li><p>The relation $T^{**}\circ J_X = J_Y\circ T$ (where $J_X:X\to X^{**}$ and $J_Y:Y\to Y^{**}$ are canonical embeddings) holds for general bounded operators; this is just a matter of diagram chasing.</p></li>
<li><p>I don't know if $\operatorname{Im} T^{**} \subset J(Y)$ holds for compact operators in general (when $X$ does not have the approximation property).</p></li>
</ol>
"
"2391319","2391328","<p>Your proof is fine, except I don't see how you get the inequality
$$
\int_{X\setminus A}|f_n-f|\,d\mu \leq \epsilon\mu(A).
$$
Now you can obtain
$$\int_{X\setminus A}|f_n-f|\,d\mu \leq \|(f_n-f)|_{X\setminus A}\|_\infty\mu(X)$$
instead, which will give you the result by taking $\|(f_n-f)|_{X\setminus A}\|_\infty$ small enough. You use the finiteness of the measure in Egoroff and also by implicitly assuming that $f_n$ and $f$ are in $L^1$. Here's a slightly different argument in case you're interested.</p>

<p>Since $f_n\to f$ a.e. and $|f_n|\leq K$, we get that $|f_n-f|\to0$ a.e. and $|f_n-f|\leq(|f_n|+|f|) \leq 2K$. Since the space is finite, $2K\in L^1$. Then the dominated convergence theorem implies
$$
\int |f_n-f|\to 0.
$$</p>
"
"2391346","2391348","<p>I'll assume $R$ is commutative and unital. (One just needs a little more care in the non-commutative case.)</p>

<p>Let's consider a module homomorphism $\phi: R\to M$. Then
$$\phi(r)=\phi(r1)=r\phi(1)$$
for all $r\in R$, as $\phi$ is an $R$-module homomorphism.
Therefore $\phi$ is completely determined by the value $\phi(1)$,
which is an element of $M$.</p>

<p>Given an element $m$ of $M$, we define $\phi_m:R\to M$ by
$$\phi_m(r)=rm.$$
Then it is routine to prove that $\phi_m$ is an $R$-module homomorphism,
and that $\phi_m(1)=m$. This shows that for each $m\in M$ there is
exactly one $R$-module homomorphism from $R$ to $M$ taking $1$ to $m$.</p>
"
"2391349","2391352","<p>Since $y\in \overline{T_1(U)}$, $0\in y-\overline{T_1(U)}=\overline{y-T_1(U)}$.  Since $\overline{T_2(U)}$ is a neighborhood of $0$, this means it must intersect $y-T_1(U)$.</p>
"
"2391358","2391434","<p>To start I give you some hints: </p>

<p><strong>a)</strong> Firstly the inequlity $Y_A &gt; Y_B + 10$ can be transformed to $Y_A-Y_B&gt;10$. Then using the converse probability:</p>

<p>$P(Y_A-Y_B&gt;10)=1-P(Y_a-Y_B\leq 9)$</p>

<p>The expected values of the random variables are  $E(Y_A)=E(Y_B)=50\cdot 4.5$. </p>

<p>The variances of the random variables are  $Var(Y_A)=50\cdot 5.25, Var(Y_B)=50\cdot 12.25$</p>

<p>Let $Y_D=Y_A-Y_B$. Then $E(Y_D)=50\cdot 4.5-50\cdot 4.5=0$. </p>

<p>And $Var(Y_D)=Var(Y_A)+Var(Y_B)=50\cdot 17.5=875$</p>

<p>With the help of the central limit theorem we get</p>

<p>$$P(Y_D\geq 10)=1-P(Y_D\leq 9)\approx 1-\Phi\left(\frac{9+0.5-0}{\sqrt{875}}\right)$$</p>

<p>$+0.5$ is the continuity correction factor.</p>

<p><strong>b)</strong> $E(Y_A+Y_B)=E(Y_A)+E(Y_B), Var(Y_A+Y_B)=Var(Y_A)+Var(Y_B)$</p>

<p><strong>c)</strong> Here you have to evaluate at what combinations of coin-flips you get exactly 225 points. If IÂ´m right the only combination you get 225 points if you flip 25 times head and 25 times tail. Can you calculate the probability by using the binomial distribution ?</p>
"
"2391372","2391386","<p>If $p$ is a prime such that $p^2\mid a$ and $p^2\mid m$, the you must have $p\mid  x$ so $p^2\mid x^2$ so $p^2\mid x^2-a$ only if $p^2\mid a $. In this case, you reduce to the question:</p>

<p>$$y^2\equiv \frac{a}{p^2}\pmod{\frac{m}{p^2}}$$</p>

<p>This solves the cases in your question, since $2^2\mid 8$, but $2^2\not\mid 2$ and $2^2\not\mid 6$, so there cannot be solutions.</p>

<p>If $p\mid a$ and $p\mid m$ but $p^2\not\mid m$, then  $p$ is relatively prime to $\frac{m}{p}$, and by Chinese Remainder Theorem, this has a solution if and only if we can solve both:</p>

<p>$$\begin{align}x_1^2&amp;\equiv 0\pmod{p}\\
x_2^2&amp;\equiv a\pmod{\frac mp}
\end{align}$$
But the first has a solution, so $x^2\equiv a\pmod{m}$ has a solution in this case if and only if $x^2\equiv a\pmod{\frac mp}$ has a solution.</p>

<p>Repeated use of these two reductions lets you reduce the question to a question with $(a,m)=1$ or conclude the equation has no solution (if some step has a $p^2\mid m$ and $p^2\not\mid a$.)</p>
"
"2391389","2391616","<p>Numbering the four given clauses:</p>

<p>$$\lnot A \lor \lnot B \tag 1$$
$$C \lor \lnot D \tag 2$$
$$B \lor \lnot C \tag 3$$
$$C \lor D \tag 4$$</p>

<p>Resolution of $(1)$ and $(3)$ removes $B$ and results in</p>

<p>$$\lnot A \lor \lnot C \tag 5$$</p>

<p>Resolution of $(2)$ and $(4)$ cancels $D$</p>

<p>$$C \tag 6$$</p>

<p>$(5)$ combined with $(6)$ </p>

<p>$$\lnot A \tag 7$$</p>

<p>$(3)$ and $(6)$</p>

<p>$$B \tag 8$$</p>

<p>The three results $(6)$, $(7)$ and $(8)$ imply</p>

<p>$$\lnot A \land B \land C \tag 9$$</p>
"
"2391391","2391428","<p>The condition corresponds to an $n-1$ dimensional affine subspace. As $A$ is invertible, write $Q=A^{-1}$ and then e.g.
  $$ B = (I-u e^T Q) A$$
The rank of $B$ equals $n-1$ precisely when non-invertible, i.e. when
 $$ \det B = \det (I - u e^T Q) \det A = (1 - (e^T Q u)) \det A=0$$
(where I used that for a rank 1 operator $T$ we have: $\det(I-T) = 1 - {\rm tr\;} T $. This defines the wanted affine subspace: $$e^T Q u = 1.$$</p>

<p>Edit: Writing $u=A \lambda$ with $\lambda\in {\Bbb R}^n$ the condition reads: $e^T Q A \lambda = e^T \lambda = 1$ or simply $\lambda_1+\cdots + \lambda_n=1$ which seemingly provides a positive answer to your question (with the right interpretation).</p>
"
"2391402","2391410","<p>Note that</p>

<p>\begin{align}(2x-6)^2 + (2y-8)^2 &amp;= 104\iff (2(x-3))^2+(2(y-4))^2=104 \\&amp;\iff 4(x-3)^2+4(y-4)^2=104 \\&amp;\iff (x-3)^2+(y-4)^2=\frac{104}{4}=26.\end{align}</p>
"
"2391417","2391502","<p>I don't know much about ""differential triangles in polar coordinates in the plane"", but here's a pretty short proof, based on the <a href=""https://en.wikipedia.org/wiki/Frenet%E2%80%93Serret_formulas"" rel=""nofollow noreferrer"">Frenet-Serret</a> apparatus, which is more or less coordinate independent:</p>

<p>Let the curve be given by $\vec r(s)$, where $\vec r(s)$ is the vector from the origin $O$ to the point on the curve corresponding to the parameter value $s$, the arc-length from some fixed but arbitrary point (on the curve).  Then $\vec T(s)$, the unit tangent vector field along $\vec r(s)$ is given by</p>

<p>$\vec T(s) = \dot {\vec r}(s), \tag 1$ </p>

<p>where the ""$\dot {}$"" denotes $d/ds$, <em>viz</em>. </p>

<p>$\dot {\vec r} = \dfrac{d{\vec r}}{ds}.  \tag 2$</p>

<p>From $\vec T(s)$ we derive the <em>unit normal</em> vector $\vec N(s)$ and curvature $\kappa(s)$ <em>via</em></p>

<p>$\dot {\vec T}(s) = \kappa(s) \vec N(s); \tag 3$</p>

<p>we can then express $\vec r(s)$ in the $\vec T(s)$-$\vec N(s)$ frame:</p>

<p>$\vec r(s) = \langle \vec T(s), \vec r(s) \rangle \vec T(s) + \langle \vec N(s), \vec r(s) \rangle \vec N(s); \tag 4$</p>

<p>then we can take</p>

<p>$t(s) = \langle \vec T(s), \vec r(s) \rangle \tag 5$</p>

<p>and </p>

<p>$p(s) = \langle \vec N(s), \vec r(s) \rangle; \tag 6$</p>

<p>thus, from (6),</p>

<p>$\dfrac{dp(s)}{ds} = \dfrac{d}{ds}\langle \vec N(s), \vec r(s) \rangle = \langle \dot {\vec N}(s), \vec r(s) \rangle + \langle \vec N(s), \dot {\vec r}(s) \rangle;  \tag 7$</p>

<p>now by (1)</p>

<p>$ \langle \vec N(s), \dot {\vec r}(s) \rangle =  \langle \vec N(s), \vec T(s) \rangle = 0, \tag 8$</p>

<p>so</p>

<p>$\dfrac{dp(s)}{ds} = \langle \dot {\vec N}(s), \vec r(s) \rangle;  \tag 9$</p>

<p>from <a href=""https://en.wikipedia.org/wiki/Frenet%E2%80%93Serret_formulas"" rel=""nofollow noreferrer"">Frenet-Serret</a>, </p>

<p>$\dot {\vec N}(s) = -\kappa(s) \vec T(s), \tag{10}$</p>

<p>so (9) becomes</p>

<p>$\dfrac{dp(s)}{ds} = \langle -\kappa(s) \vec T(s), \vec r(s) \rangle = -\kappa(s) \langle \vec T(s), \vec r(s) \rangle = -\kappa(s) t(s).  \tag {12}$</p>

<p>We note there is, as Ted Shifrin indicates in his comment, ""a sign issue""; that is, our formula for $dp/ds$ gives a result with opposite sign of the corresponding equation in the text of the question, $dp/ds = \kappa t$.  Apparently this could be resolved by taking $p = - \langle \vec r, \vec N \rangle$, but present definition of $p(s)$, as the $\vec N(s)$ component of $\vec r(s)$ in the $T(s)$-$N(s)$ frame, is sufficiently natural and clean that I am inclined not to mess with it.  So there we have it.  Hope this helps.</p>

<p><strong><em>Note Added in Edit, Saturday 12 August 2017 10:40 AM PST:</em></strong>  It is perhaps worth adding that there is a corresponding equation for $dt/ds$, based upon (5), to wit:</p>

<p>$\dfrac{dt(s)}{ds} = \langle \dot {\vec T}(s), \vec r(s) \rangle + \langle \vec T(s), \dot {\vec r}(s) \rangle$
$ = \langle \kappa(s) \vec N(s), \vec r(s) \rangle + \langle \vec T(s), \vec T(s) \rangle = \kappa(s) p(s) + 1. \tag {13}$</p>

<p>Curious.  Engaging.  <strong><em>End of Note.</em></strong></p>
"
"2391421","2391441","<p>The short answer is that if $2\vec x$ is in our vector space, then we also want $\tfrac{1}{2}\vec x$ to be in the vector space. Vector spaces scale nicely under multiplication, and things behave weirdly if this scaling isn't true. Let me give one example of weird behavior.</p>

<p>One of the nice aspects of a vector space is that the dimension of a subspace is the same as the number of linearly independent vectors providing a basis for that subspace. Proving this result is usually done with a transferrence-type theorem that says something like the following:</p>

<blockquote>
  <p>Suppose $V$ is a vector space of dimension $n$, with $v_1, \ldots, v_n$ as a basis. Suppose $w_1, \ldots, w_m$ are $m$ linearly independent vectors in $V$. Then (perhaps after reordering), $w_1, \ldots, w_m, v_{m+1}, \dots, v_n$ is also a basis for $V$.</p>
</blockquote>

<p>This allows one to compare the sizes of two bases by progressively replacing one with the other.</p>

<p>The key element of the proof is evident even when there is only one $w_1$ vector of interest. Since $w_1 \in V$, there are coefficients $c_i$ such that
$$ w_1 = \sum_{i \leq n} c_i v_i.$$
As $w_1 \neq 0$, at least one of the $c_i$ are not zero. WLOG (otherwise, reorder), $c_1 \neq 0$. Then we can solve for $v_1$ and see that
$$ v_1 = c_1^{-1}\bigg(\sum_{2 \leq i \leq n} c_i v_i - w_1 \bigg).$$
Thus any vector written in terms of $v_1, \ldots, v_n$ can be written in terms of $w_1, v_2, \ldots, v_n$.</p>

<p>In this proof, note that we used $c_1^{-1}$. We used that generic inverses exist! This is essential.</p>

<hr>

<p>We can give a direct example as well. Consider $\mathbb{Z}$ as a $\mathbb{Z}$-module. (If you are not familiar with modules, then you can think of this as trying to define a vector space over a ring instead of a field).</p>

<p>Then $\mathbb{Z}$ has a simple ""basis"": $1$. Every integer in $\mathbb{Z}$ can be written as an integer multiple of $1$. In analogy to vector spaces, can we say that the ""dimension"" of $\mathbb{Z}$ is $1$?</p>

<p>Note that $\mathbb{Z}$ has a different ""basis"", $\mathbb{Z} = \langle 2, 3\rangle$ --- or rather, every integer can be written as an integer multiple of $2$ added to an integer multiple of $3$. But each individual ""basis vector"" ($2$ or $3$) are independent, in the sense that $2$ is not a multiple of $3$ (and vice versa). So is the ""dimension"" of $\mathbb{Z}$ actually $2$?</p>

<p>No --- the concept of dimension is just messier when there isn't a field lying underneath.</p>
"
"2391422","2391459","<p>You're right there. </p>

<p>Note, however, that you've defined $\pi_P$ for <em>planes</em> $P$, not <em>lines</em> $L$. So, having chosen $L$, you want to write $\pi_P$ where $P$ is the plane <em>orthogonal</em> to $L$.</p>

<p>What's the derivative of $\pi_P\circ g$? The chain rule says it's $(\pi_P\circ g)'(t)=d\pi_P(g'(t))= \pi_P(g'(t))$. Why is this everywhere nonzero? The kernel of $\pi_P$ is, as you said, the set of vectors orthogonal to $P$.</p>
"
"2391439","2391447","<p>In $\Bbb F_q$ the multiplicative group has order $q-1$. So each nonzero
element satisfies $x^{q-1}=1$, so that $x^q=x$ etc.</p>
"
"2391443","2391461","<p>(i) Consider the $n$ pairs $\{1,2\}, \{3,4\}, ... \{2n-1,2n\}$. Note that every pair of numbers is consecutive and are therefore coprime.</p>

<p>(ii) Consider the $n$ sets $\{1,2,4,8,...\}, \{3,6,12,...\}, \{5,10,20,...\}, ... \{2n-1\}$. Each set contains one odd number, and successive doublings of that number up to a maximum of $2n$. Note that for any pair of numbers taken from one of these sets, one number will divide the other.</p>

<p>In either case, the $n$ sets contain all the numbers $1...2n$ exactly once.</p>

<p>Now we can use the pigeonhole principle. Build a set $S$ containing more than $n$ numbers by selecting the numbers from the $n$ sets. We must then choose at least a pair of numbers that came from the same set. That pair of numbers then has the property we are looking for. This happens whichever way you choose the numbers. Therefore every set $S$ (with $S \subset \cal{N_{2n}}$, $|S| \gt n$) contains a pair of numbers with that property.</p>
"
"2391453","2391797","<p>I think the following argument works:</p>

<p>Let $(I\times I,\mathcal L, \lambda_2)$ be the completion of $(I\times I,\mathscr B([0,1])\times \mathscr B([0,1]), \lambda\times \lambda ),$ set  $f=1_A$ and choose a $\mathscr B([0,1])\times \mathscr B([0,1])$- measurable function $g$ such that $g=f\ \text{a.e.}\ \lambda_2.$   Note that the sections $g_x$ are $\mathscr B([0,1])$-measurable.</p>

<p>Then, $h=f-g=0\ $a.e.-$\lambda_2$  and therefore, there is a $B\in \mathscr B([0,1])\times \mathscr B([0,1])$ such that $\left \{ h\neq 0 \right \}\subseteq B$ and $\lambda_2(B)=\lambda \times \lambda (B)=0.$</p>

<p>We have now $0=\lambda \times \lambda (B)=\int_I \lambda (B_x)d\lambda$ so $\lambda(B_x)=0$ for almost all $x\in I.$ But then, $\lambda (\left \{ h\neq 0 \right \}_x)\le \lambda (B_x)=0\Rightarrow A_x=f_x=g_x$ for almost every $x\in I.$</p>
"
"2391457","2393528","<p>Note that $\chi_A(x)=\chi_B(x)=p(x)=x^3-40x^2+39x-1$ is irreducible and has $3$ distinct roots $(\lambda_i)$. Here $PA=BP$ with $\det(P)=1$; then $P$ sends any eigenvector of $B$ associated to $\lambda_i$ to an eigenvector of $A$ associated to the same $\lambda_i$; moreover, the eigenvectors are defined up to a multiplicative constant. The set $\{R;RA=BR\}$ is a vector-space of dimension $3$. Then an equality of the form $Ru=v$, where $u,v$ are given vectors, define in general a unique $R$.</p>

<p>An eigenvector of $A$ (resp. of $B$) associated to $\lambda_i$ is $U_i=[1+37\lambda_i,3\lambda_i,-2\lambda_i+\lambda_i^2]^T$ (resp. $W_i=[1+297\lambda_i,155\lambda_i,-12\lambda_i+\lambda_i^2]^T$) and $P(U_i)=a_iW_i$ where $a_i\in\mathbb{C}$. It is easy to see that $U=\sum_iU_i=[1483,120,1442]^T$ and $P(U)=\sum_ia_iW_i$ is an integer vector.</p>

<p>Thus (*) $\sum_ia_i,\sum_ia_i\lambda_i,\sum_ia_i\lambda_i^2$ are rational numbers and the $(a_i)$ are in $F$, the decomposition-field of $p$. On the other hand, $\dfrac{\det(U_1,U_2,U_3)}{\det(W_1,W_2,W_3)}=3/155$ implies that $a_1a_2a_3=3/155$. </p>

<p>Consequently, the $(a_i)$ are the roots of a polynomial $q(y)$ that is the image of $p$ by a transformation $y=u+vx+wx^2$ where $u,v,w$ are rational numbers. Note that $q$ must be in the form $q(y)=y^3+\cdots-3/155$. We seek solutions $(u,v,w)$ in the form $n/155$ where $n\in\mathbb{Z}$ and we obtain $3$ candidates: $(-13/155,28/155,-1/155),(-1/155,26/155,-12/155),(12/155,-2/155,-11/155)$ (maybe there are other solutions). It remains to see that the associated solutions $P$ are (or are not) integer-matrices.</p>

<p>Case 1. We obtain $P(U)=[-33012,-17227,-3004]^T$ and $P$ is the matrix obtained by Axel Kemper (with the little radius). $P$ is an integer matrix and is convenient.</p>

<p>Case 2. We obtain $P(U)=[-1288409,-672344,-117124]^T$, $P=\begin{pmatrix}-23&amp;2&amp;-870\\-12&amp;1&amp;-454\\-2&amp;-2&amp;-79\end{pmatrix}$ and $P$ is convenient.</p>

<p>Case 3. We obtain $P(U)=[-1255397,-655117,-11412]^T$; yet, $P$ is not an integer matrix and is not convenient</p>

<p>EDIT. Answer to @user7540.</p>

<p>Since $galois(p)=S_3$, there are no algebraic relations with coefficients in $\mathbb{Q}$ linking the $(a_i)$ and there is a cycle $\sigma\in Galois(p)$ s.t. $\sigma(\lambda_i)=\lambda_{i+1}$. According to (*) above, $a_i\in F$, the decomposition-field of $p$ and (reverse the Vandermonde matrix) $a_1=u+v\lambda_1+w\lambda_i^2,a_2=\sigma(a_1),a_3=\sigma(a_2)$, where $u,v,w$ are rational numbers. Finally, the $(a_i)$ are the roots of a polynomial that is the image of $p$ by the transform $x\rightarrow u+vx+wx^2$.</p>
"
"2391464","2391475","<p>the simple way is first to try hit and trial method and find one root after that
just try multiplication between one power less polynomial and $(x-root)$</p>

<p>so $-1$ was my root.now multiply $(x+1)$ with $x^{5-1}$<br>
now what you will you get: $x^5+x^4$</p>

<p>but you want  $-10x^4$ in your polynomial</p>

<p>so now add $-11x^4$ in your polynomial</p>

<p>after repeating this for several times you will get</p>

<p>$(x + 1) (x - 3)^2 (2 x^2 + 1)$</p>
"
"2391472","2391477","<p>You have $6$ possibilities to choose two cards out of $4$. In two cases, you get the same colour. So, the probability is $\frac{1}{3}$</p>

<p>An even easier way to get the result. Choose an arbitary card. The probability that the other card fits is $\frac{1}{3}$ because one has the same and two the other colour.</p>

<p>The catch in the $50-50$-argument is that after one card has been chosen, you do not have four cards anymore.</p>
"
"2391483","2391637","<p>Denote by $p_n$ the probability that after $n$ drawings you have not yet seen two consecutive blue balls. Then $p_0=p_1=1$ and
$$p_n={4\over5}p_{n-1}+{1\over5}\cdot{4\over5} p_{n-2}\qquad(n\geq2)\ .\tag{1}$$
In order to <em>prove</em> $(1)$ we argue as follows: Assume $n\geq2$. With probability ${4\over5}$ the first drawn ball is red, in which case we have to see another $n-1$ drawings without two consecutive blue balls. With probability ${1\over5}$ the first drawn ball is blue, in which case we need the second ball to be red, and no two consecutive blue balls in the remaining $n-2$ drawings.</p>

<p>Now $(1)$ can be easily solved with the ""Master Theorem"" for constant coefficient linear difference equations. One obtains
$$p_n= A u^n + B v^n\qquad(n\geq0)\ ,\tag{2}$$
whereby
$$u={2\over5}(1-\sqrt{2}),\quad v={2\over5}(1+\sqrt{2}),\quad A={4-3\sqrt{2}\over8},\quad B={4+3\sqrt{2}\over8}\ .$$
This leads to $p_{100}=0.0313721$. The $Au^n$ term in $(2)$ can be neglected when $n\gg1$.</p>
"
"2391485","2391551","<p>Assuming that the $f_n$ do not need to be continuous on $K$, we have a simple counterexample. Consider any $\{g_n\}_{n=1}^{\infty}$, $g_n : S_1\to \mathbb{R}$, such that $g_n\to g$ pointwise but not uniformly. Let $K\subset \mathbb{R}^2$ such that $\partial K$ is a Jordan curve (and therefore we have a homeomorphism $\varphi : S_1\to \partial K$). Then, let $f_n$ be defined such that $f_n$ converges uniformly in $\operatorname{Int}(K)$ and $f_n = g_n\circ \varphi^{-1}$ on $\partial K$. Then, $f_n$ converges pointwise but not uniformly to $g\circ \varphi^{-1}$ on $\partial K$.</p>

<p>As pointed out by @zhw, if the $f_n$ are continuous on $K = \overline{\operatorname{Int}(K)}$, then uniform convergence on all of $K$ is immediate by taking the limit of $\lvert f(x)-f_n(x)\rvert$ as $x$ approaches $\partial K$.</p>
"
"2391487","2391512","<p>You derived</p>

<p>$33a = a(a^2 - d^2) = a(a-d)(a+d)$</p>

<p>But before we divide both sides by $a$, we must consider that $a$ may equal $0$.</p>

<p>So Case 1:  If $a = 0$.</p>

<p>1) $a = 0; d\in \mathbb Z$. And $(-k, 0 , k)$ are solutions.  (Trivial solutions)</p>

<p>Those are infinite number of solutions. </p>

<p>Cases 2-???: $a \ne 0$.</p>

<p>Then we can divide both sides by $a$.</p>

<p>2) $a \ne 0$ and $(a-d)(a+d) = 33$</p>

<p>So $a-d$ and $a+d$ are two factors of $33$.  $33 = 3*11$ has as factors $\pm 1, \pm 3, \pm 11, \pm 33$.</p>

<p>So we have eight case all very similar:</p>

<p>a) $a + d = 1$ and $a-d = 33$.  So $a= 17$ and $d= -16$ and the numbers are $1, 17, 33$.</p>

<p>b) $a + d = 33$ and $a - d = 1$.  So $a = 17$ and $d = 16$ and the numbers are $1, 17, 33$.</p>

<p>and ...</p>

<p>this is too tedious to spell out.  We know that the <em>pair</em> $(a+d)$ and $(a-d)$ will be one of these pairs $(1, 33) , (-1, -33), (3,33)(-3,-33)$ and $a$ will be the number in the middle.  And we know that solving it for negative numbers will be the same as solving it for positive numbers.</p>

<p>So we have the numbers are:</p>

<p>$$1, 17, 33$$
$$-1, -17,-33$$
$$3,7, 11$$
$$-3, -7, -11$$.</p>

<p>=====</p>

<p>Note, if ""consecutive"" integers mean $k, k+1, k+2$ i.e. with period specifically $1$, which is <em>often</em> (but not always) implied.  Then $(-1, 0 , 1)$ is the only solution.</p>
"
"2391497","2391510","<p>3^9 is a multiple of 3^3 which can be written as 27 and what is 27=28-1 now if you divide 27 by 7 what is the remainder?. It is -1 so now you multiply it with 3 in every step </p>

<p>so= $-3 (7times )/7$</p>
"
"2391501","2391511","<p>Easy. We <em>don't</em> have$$\lim_{n\in\mathbb N}a_n=s=\lim_{n\in\mathbb N}b_b.\tag{1}$$Suppose that each $a_n$ is $0$ and that each $b_n$ is one. Then the intersection of all intervals is $[0,1]$. Take any $s\in[0,1]$. Then $\lim_{n\in\mathbb N}a_n=s$ if and only if $s=0$ and $\lim_{n\in\mathbb N}b_n=s$ if and only if $s=1$. In particular, we <em>never</em> have $(1)$ (I mean, in this example). The only case we have $(1)$ for every $s$ is when $\bigcap_{n\in\mathbb{N}}I_n=\{s\}$.</p>
"
"2391503","2391514","<p>Yes, $|p^{-n}|_p=p^n$ so the sequence is clearly unbounded in $\mathbb{Q}_p$ and has no limit.</p>

<p>Saying the limit is $0.000\cdots_p\,$ in $\mathbb{Q}_p$ is like saying $\lim\limits_{n\to\infty}10^n\,$ is $\cdots0000\,$ in $\mathbb{R}$.</p>
"
"2391505","2391834","<p>To be more clear, write $N(t)$ instead of $N$, write $\frac{dN(t)}{dt}$ instead of $\frac{dN}{dt}$. Also, it is more ideal to write $N'(t)$ than $\frac{dN(t)}{dt}$ in the answer, since we are talking the annoying Leibniz's mis-leding notation, right?</p>

<blockquote>
  <p>$\begin{alignedat}{3}\frac{dN(t)}{dt}=-\lambda N(t) &amp;\Longrightarrow \forall t,~N'(t)=-\lambda N(t)&amp;\text{rewrite to a rigorous notation}\\ &amp;\Longrightarrow \forall t,~\frac{1}{N(t)}N'(t)=-\lambda&amp;\text{move the term $N(t)$ to LHS}\end{alignedat}$</p>
</blockquote>

<p>I added a quantifier $\forall t$ in front. Since the true thing happens here is that the equation $N'(t)=-\lambda N(t)$ and $\dfrac{1}{N(t)}N'(t)=-\lambda$ not only hold for one particular, but rather <strong>all</strong> $t\in\mathbb{R}$. (we here ignore the little issue that when $t=0$, the equality might be problematic, since this is another(more little) question, off the topic.)</p>

<p>To repeat, the equation above, are something like: if we first defined $f:\Bbb R\to \Bbb R;x\mapsto x^2+1$, then we can say:</p>

<ul>
<li>$\forall x,~f(x)=x^2+1$</li>
<li>$\forall y,~f(y)=y^2+1$</li>
<li>$\forall t,~f(t)=t^2+1$ </li>
<li>$\forall t,~f(t)-1=t^2$</li>
<li>$\forall t,~(f(t))^2=t^4+2t^2+1$</li>
<li>$\forall t,~\dfrac{f(t)}{f(t)}=1$</li>
<li>$\forall t,~f'(t)=2t$, 
... etc.</li>
</ul>

<p>Here these all expression all have a quantifer in it, specifying the fact that not only the equation(say $\dfrac{f(t)}{f(t)}=1$) hold for one particular $t$(say $t=1.467$), but also <strong>all</strong> $t$ in the real numbers.</p>

<p>Why do I stress on this point? Because due to it, we can integrate both side. </p>

<blockquote>
  <p>$\begin{alignedat}{3} \left(\forall t,~\frac{1}{N(t)}N'(t)=-\lambda\right)\Longrightarrow \int\left(\frac{1}{N(t)}N'(t)\right) dt=\int (-\lambda)dt&amp;\quad\text{integrate both side w.r.t. $t$}\end{alignedat}$.</p>
</blockquote>

<p>$(\star)$ It should be notice that, if we are dealing with a equation in precalculus, like $2x^2+x+1=0$. To integrate both side with respect to the variable, getting that $\int (2x^2+x+1) dx = \int 0 dx$, is meaningless, and totally wrong. Here the reason that I can integrate the both side, with respect to $t$, is that we have known that the equation holds for all $t\in\Bbb R$ (or at least on some interval). And since the two expressions are <strong>identical</strong> (at least on some interval), there is nothing more or less to integrate $\dfrac{1}{N(t)}N'(t)$ than to integrate $-\lambda$. For example, the result of $\int (x^3+2x-5x+7)dx$ is the same of $\int (x^3+2x-5x+7)dx$, of course!</p>

<p>Now keep going.</p>

<blockquote>
  <p>$\begin{alignedat}{2}
&amp;\int\left(\frac{1}{N(t)}N'(t)\right) dt=\int (-\lambda)dt\\
&amp;\Longrightarrow \underbrace{\ln |N(t)|+c_1}_{\dagger}=-\lambda t+c_2\\
&amp;\Longrightarrow \ln |N(t)|=-\lambda t+c\\
&amp;\Longrightarrow e^{-\lambda t+c}=N(t)\\
&amp;\Longrightarrow N(t)=Ce^{-\lambda t}~~\text{(I forgot the reason why we can throw away abs-sign now :P}\\
\end{alignedat}$</p>
</blockquote>

<p>$(\dagger)$ <em>The integration by substitution used in the LHS is very classic, and rigorous; it doesn't require any annoying differential operations, such as canceling the $dt$'s or $dx$'s.</em></p>

<p>And get the answer. You may wonder why different constant $c_1$ and $c_2$ arises, this is because $\int d(\cdot)$ it not truly some kind of function(same input, same output), in fact, it produce a family of functions, each of these are distinct from a constant, as stressed in the calculus books.</p>
"
"2391508","2391530","<p>$O(n \log(\log n)) &lt; O(n\log n) &lt; O(n^2)$</p>

<p>The second chain of inequalities in your answer is incorrect, because $O(n) * O(\log(\log n)) &gt; O(n),$ since $O(\log(\log n)) &gt; O(1)$</p>
"
"2391509","2396710","<p>This question certainly is too broad to be answered completely. Before talking about infinite dimensional Lie groups you need a concept of infinite dimensional manifolds. As you note correctly, you first need model spaces in infinite dimensions, and the technical difficulties that have to be overcome strongly depend  how general spaces you want to allow. If you are fine with Banach spaces as model spaces, then the theory of smooth manifolds mostly works out closely parallel to the finite dimensional case. There are introductory books on differential geometry working in that setting, for example Serge Lang's classics. This allows dealing with diffeomorphism groups of finite differneitiability class or with Sobolev type completions. To deal with infinitely differentiable diffeomorphisms, you need manifolds modelled on Frechet-spaces, and there things become much more complicated. But infinite dimensional differential geometry works in far more general settings, see e.g. the book ""The Convenient Setting of Global Analysis"" by A. Kriegl and P. Michor. </p>

<p>Having infinite dimensional geometry at hand, you can look at infinite dimensional Lie groups, and indeed the Lie algebra of a diffeomorphism group is the Lie algebra of vector fields with the negative of the usual Lie bracket. Intuitively, you can understand that as follows: A curve in $Diff(M)$ can be viewed as a 1-parameter family $\phi_t$ of diffeomorphisms. To get a tangent vector at the point $\phi=\phi_0$ you should differentiate this curve with respect to $t$ at $t=0$. If you look what happens in a point $x\in M$, then $t\mapsto\phi_t(x)$ is a smooth curve starting at $\phi(x)$, so differentiating this at $t=0$ determines a tangent vector at the point $\phi(x)\in M$. Now you check that this can be done smoothly, so you associate to each $x\in M$ a tangent vector at $\phi(x)$ and this defines a section of the pullback bundle $\phi^*(TM)$. With a bit more work you can indeed construct charts with values in a neighborhood of the zero section in the space of smooth sections of $\phi^*(TM)$ and thus identify that vector space as the modelling vector space. From $\phi=id$, you just get smooth sections of $TM$, i.e. vector fields on $M$. (And the exponential map associates to each vector field its local flow.) You can find a lot of information in that direction in the book by Kriegl and Michor mentioned above.  </p>
"
"2391520","2391527","<p>We only have to add a quantity greater than $1$, to ensure that we have a strict inequality. The choice of $1$ is just a natural choice.</p>
"
"2391524","2393743","<p>Q.1 The author defined $C=A^{T}A$. Denote $C=(c_{ij})$ and $A^{T}=(a_{ij}^{'})$.
Note that $a'_{ij}=a_{ji}$. By the definition of matrix multiplication,
the $(i,j)$-entry of $C$ is given by 
$$
c_{ij}=\sum_{k=1}^{n}a'_{ik}a_{kj}=\sum_{k=1}^{n}a_{ki}a_{kj}.
$$
 Therefore, $k$ is just a dummy variable for writing summation.
There may be a typo in the book and you may need to double check it seriously. Reason: There is a $j$ on LHS but on RHS it becomes a dummy variable. On the other hand, there is no $k$ on LHS but $k$ appears on the RHS.</p>

<p>Q.2 The symbol $\sideset{^{j}}{}A$ denotes the $j-th$ column vector
of the matrix $A$ while the symbol $\sideset{^{i}}{}A^{t}$ denotes
the $i-th$ row vector of the matrix $A^{t}$. Therefore, $\sideset{^{i}}{}A^{t}.\sideset{^{j}}{}A$
denote the matrix multiplication of a $1\times n$ matrix to a $n\times1$
matrix and the outcome is a $1\times1$ matrix, which is identified
with a real number. In short, this symbol denotes the the multiplication
of the $i$-th row of $A^{t}$ by the $j$-th column of $A$. In my opinion, it is a poor choice of notation and I don't know why one denotes a row while the other one denotes a column. (You may need to ask the author.) </p>
"
"2391526","2393583","<p>Extending kimchi lover's answer,
you can use the
Taylor series expansion
as far as you want.</p>

<p>If
$f(x) = \log_k(x)$,
then
$f'(x) = \frac1{x\ln k}$,
$f''(x) = \frac{-1}{x^2\ln k}$,
$f'''(x) = \frac{2}{x^3\ln k}$,
and so on, with
$f^{(n)}(x) = \frac{(-1)^{n-1}(n-1)!}{x^n\ln k}$.</p>

<p>Therefore</p>

<p>$\begin{array}\\
f(x)
&amp;=f(a)+(x-a)f'(a)+\dfrac{(x-a)^2f''(a)}{2}+\dfrac{(x-a)^3f'''(a)}{6}
+...\\
&amp;=\log_k(a)+\dfrac{(x-a)}{a\ln k}-\dfrac{(x-a)^2}{2a^2\ln k}+\dfrac{(x-a)^3}{6a^3\ln k}
+...\\
&amp;=\log_k(a)+\dfrac{(x-a)}{a\ln k}\left(1-\dfrac{x-a}{2a}+\dfrac{(x-a)^2}{6a^2}
+...\right)\\
&amp;=\log_k(a)+\dfrac{(x-a)}{a\ln k} \left(\sum_{j=0}^n\dfrac{(-1)^j(x-a)^j}{(j+1)!a^{j}}+...\right)
\qquad\text{for any }n\\
\end{array}
$</p>

<p>Note that successive sums
are above and below the
actual value,
so you can get
upper and lower bounds.</p>
"
"2391531","2391565","<p>Although $\ln\left(\left|y-1\right|\right)=e^x+C$ implies $y-1=\pm e^{e^{x}+C}$, both solutions can be represented as $y=1+C'e^{e^{x}}$, where $C'$ can be positive or negative. So you are right that $y=1+C'e^{e^{x}}$ covers all the cases for absolute value.</p>
"
"2391533","2391539","<p>There is no universal standard notation for zero matrices or zero vectors. You are presumably asking within the context of a paper, book, or class --- you should figure out or ask about the convention used within this context.</p>
"
"2391538","2391548","<p>Eigenvalues are
$$\lambda_1=\frac{1}{2}(237+\sqrt{45469})$$
$$\lambda_1=\frac{1}{2}(237-\sqrt{45469})$$
Substituting into 
$$(A-I\lambda)x=0$$
yields a pair of eigenvectors
$$x_1=\left(\frac{1}{2}(-213+\sqrt{45469},1\right)$$
$$x_2=\left(\frac{1}{2}(-213-\sqrt{45469}),1\right)$$</p>
"
"2391546","2391646","<p>I'd recommend the appropriate chapters of Maclane or of Riehl's ""Category Theory in Context"" for Kan extensions, and the first chapter of Kelly's ""Enriched Category Theory"" for ends and coends.</p>

<p>If I remember, there's not a through consideration of extranatural transformations, which are what ends and coends represent, in Kelly's book (which also treats Kan extensions for enriched categories, which could be worth reading as in that context some concepts separate whose coincidence in ordinary categories is not really natural.) For this I'd send you back to the original source: Samuel Eilenberg and G. M. Kelly, A generalization of the functorial calculus, J. Algebra 3:3, 366â375 (1966)</p>
"
"2391552","2391579","<p>I agree that the proof is over complicated. You can give a constructive proof of the statement by constructing two sequences ($(r_i\ ;\ q_i\ i\geq 1)$ as follows. Starting with $r\mbox{ and } q_1\in (E\setminus\{p\})\cap N_r(p)$,  you suppose that $r_i,\ q_i$ are constructed up to $i=N$, the next step is then
$$
r_{N+1}=\frac{d(p,q_N)}{2}\ ;\ q_{N+1}\in (E\setminus\{p\})\cap N_1(p)
$$<br>
it is not difficult to see that $d(p,q_i)$ is strictly decreasing and then all $\{q_i\}_{i\geq 1}$ are different.  </p>
"
"2391556","2394166","<p>Let $g$ be the inverse function of $f$, then</p>

<p>$$f \left({\varphi}\right) = v \Leftrightarrow  \tan  \left(\frac{{\varphi}}{2}+\frac{{\pi}}{4}\right) = {e}^{v} \Leftrightarrow  \frac{{\varphi}}{2}+\frac{{\pi}}{4} = \arctan  \left({e}^{v}\right) \Leftrightarrow  {\varphi} = \boxed{g \left(v\right) = 2 \arctan  \left({e}^{v}\right)-\frac{{\pi}}{2}}$$</p>

<p>If ${\varphi}$ is the latitude, let $\boxed{v = f \left({\varphi}\right)}$, one has
${{\varphi}}_{S} = g \left(v-h\right)$ and ${{\varphi}}_{N} = g \left(v+h\right)$ for some $h$.</p>

<p>Let ${\Delta} = {{\varphi}}_{N}-{{\varphi}}_{S}$, one has</p>

<p>$${\Delta} = g \left(v+h\right)-g \left(v-h\right) = 2 \arctan  \left({e}^{v+h}\right)-2 \arctan  \left({e}^{v-h}\right)$$</p>

<p>Using the well known formula $\tan  \left(a-b\right) = \frac{\tan  \left(a\right)-\tan  \left(b\right)}{1+\tan  \left(a\right) \tan  \left(b\right)}$, it gives</p>

<p>$$\tan  \left(\frac{{\Delta}}{2}\right) = \frac{{e}^{v+h}-{e}^{v-h}}{1+{e}^{v+h} {e}^{v-h}} = \frac{{e}^{h}-{e}^{{-h}}}{{e}^{{-v}}+{e}^{v}} = \frac{\sinh  \left(h\right)}{\cosh  \left(v\right)}$$</p>

<p>Let now $\boxed{u = \cosh  \left(v\right) \tan  \left(\frac{{\Delta}}{2}\right)}$, one has</p>

<p>$$\sinh  \left(h\right) = u \Leftrightarrow  \boxed{h = \text{argsinh} \left(u\right) = \ln  \left(u+\sqrt{1+{u}^{2}}\right)}$$</p>

<p>One can now compute the south and north latitudes ${{\varphi}}_{S}$ and ${{\varphi}}_{N}$.  The use of exponential
and logarithms can be completely eliminated by setting</p>

<p>$$t = \tan  \left(\frac{{\varphi}}{2}+\frac{{\pi}}{4}\right)$$</p>

<p>then one has</p>

<p>$$u = \frac{1}{2} \left(t+\frac{1}{t}\right) \tan  \left(\frac{{\Delta}}{2}\right)$$</p>

<p>$${{\varphi}}_{S} = 2 \arctan  \left(\frac{t}{u+\sqrt{1+{u}^{2}}}\right)-\frac{{\pi}}{2}$$</p>

<p>$${{\varphi}}_{N} = 2 \arctan  \left(t \left(u+\sqrt{1+{u}^{2}}\right)\right)-\frac{{\pi}}{2}$$</p>
"
"2391557","2391642","<blockquote>
  <p>Is there any metric on $C(\mathbb{R})$ that allows for a continuous surjection from $\mathbb{R}$ to $C(\mathbb{R})$.</p>
</blockquote>

<p>Of course there is, for a boring reason. Let $F\colon C(\mathbb{R})\to \mathbb R$ be any bijection (the sets have the same cardinality). Define a metric on $C(\mathbb{R})$ by $d(f,g) = |F(f) - F(g)|$. Then $F^{-1}:\mathbb{R}\to C(\mathbb{R})$ is not only a continuous surjection, but an <em>isometric bijection</em>.</p>

<p>Totally useless, of course. </p>

<p>Here is something of more interest. Suppose $(V, d)$ is an infinite-dimensional vector space with a metric $d$ that makes it a complete metric space. Then there is no continuous surjection from $\mathbb R$ onto $V$. </p>

<p>Indeed, if $f:\mathbb{R}\to V$ is such a surjection, then $V$ is $\sigma$-compact:
$$
V = \bigcup_{n=1}^\infty K_n,\quad \text{where } K_n=f([-n,n])
$$
But a compact subset of $V$ has empty interior, since <a href=""https://terrytao.wordpress.com/2011/05/24/locally-compact-topological-vector-spaces/"" rel=""noreferrer"">every locally compact Hausdorff topological vector space is finite-dimensional</a>. So we have written $V$ as the union of countably many closed subsets with empty interior, contradicting the <a href=""https://en.wikipedia.org/wiki/Baire_category_theorem"" rel=""noreferrer"">Baire category theorem</a>.  </p>

<p>In particular, the above applies to $C([0,1])$ with the uniform metric, or to $C(\mathbb R)$ with some natural metrics like 
$$
d(f,g)=\sum_{k=1}^\infty 2^{-n}\frac{\|f-g\|_n}{1+\|f-g\|_n} \quad\text{where } 
\|f-g\|_n = \sup_{[-n,n]}|f-g|$$</p>
"
"2391561","2391580","<p>We know that $|\sin(x)|&lt;|x|$, which I assume is taken to be known, since you differentiated $\sin(x)$. Hence, it follows from this and Pythagorean identities that</p>

<p>$$\cos^2(\cos(x))=1-\sin^2(\cos(x))&gt;1-\cos^2(x)=\sin^2(x)$$</p>

<p>$$\sin^2(\sin(x))&lt;\sin^2(x)$$</p>

<p>Thus, it follows that</p>

<p>$$\cos^2(\cos(x))&gt;\sin^2(x)&gt;\sin^2(\sin(x))$$</p>

<p>And since $\cos(\cos(x))\in[\cos(1),1]\implies\cos(\cos(x))&gt;0$ and $a^2&gt;b^2\implies|a|&gt;|b|\ge b$, it follows that</p>

<p>$$\cos^2(\cos(x))&gt;\sin^2(\sin(x))\\\implies|\cos(\cos(x))|&gt;|\sin(\sin(x))|\ge\sin(\sin(x))$$</p>
"
"2391564","2391575","<p>\begin{align}
A &amp;= - \sqrt{\left(\frac{\sqrt{6}+\sqrt{2}}{16}\right)^2}\\
&amp;=- \sqrt{\left(\frac{\sqrt{3}+1}{8\sqrt{2}}\right)^2}\\
&amp;=- \frac18\sqrt{\left(\frac{3+1+2\sqrt3}{2}\right)}\\
&amp;=- \frac18\sqrt{2+\sqrt{3}}\\
\end{align}</p>
"
"2391569","2393633","<p>Such problems can be solved explicitly (geometrically).  Suppose that a check has been done that the line does not intersect the ellipse (this amounts to solving a quadratic for points of intersection, and will fail to find real roots here).</p>

<p>Use implicit differentiation to find the slope of tangents to the ellipse:</p>

<p>$$ \frac{x^2}{4} + y^2 = 1 $$</p>

<p>$$ \frac{x}{2} + 2y\frac{dy}{dx} = 0 $$</p>

<p>$$ \frac{dy}{dx} = \frac{-x}{4y} $$</p>

<p>The point of the ellipse closest to line $x + y = 4$ has a tangent parallel to the line (because the line segment connecting that point to the nearest point on the line is perpendicular to both the tangent and the line).  But the slope of the line is easily $m=-1$.</p>

<p>Therefore we need to identify the two points on the ellipse where:</p>

<p>$$ \frac{-x}{4y} = -1 $$</p>

<p>$$ x = 4y $$</p>

<p>$$ \frac{(4y)^2}{4} + y^2 = 1 $$</p>

<p>$$ 5y^2 = 1,\; y = \frac{\pm\sqrt{5}}{5} $$</p>

<p>$$ (x,y) = \pm \left( \frac{4\sqrt{5}}{5}, \frac{\sqrt{5}}{5}\right) $$</p>

<p>and check which of these is the closest to the line (there is another artifact point on the ""far side"" of the ellipse, having the same slope tangent).  The point of closest approach is clearly the one in the first quadrant (as one sees from the positive $x$- and $y$-intercepts of the line).</p>

<p>The actual distance from this point to the line is gotten by plugging it into the ""normal form"" of the line:</p>

<p>$$ d(x,y) = \left| \frac{x}{\sqrt{2}} + \frac{y}{\sqrt{2}} - \frac{4}{\sqrt{2}} \right| $$</p>

<p>$$ d\left( \frac{4\sqrt{5}}{5}, \frac{\sqrt{5}}{5}\right) = 
 \left| \frac{\sqrt{5}}{\sqrt{2}} - \frac{4}{\sqrt{2}} \right| = \frac{4-\sqrt{5}}{\sqrt{2}} $$</p>
"
"2391594","2395003","<h2>1. Preliminary</h2>

<p>We introduce some facts and observations to be used throughout the computation. This may as well help those who are already familiar with such computations and want to skip the details.</p>

<p><strong>1.1. Dirichlet kernel.</strong> By a simple application of the geometric sum formula, we have</p>

<p>$$ \sum_{k=-n}^{n} e^{\mathrm{i}kx} = \frac{\sin\left(\frac{2n+1}{2}x\right)}{\sin\left(\frac{1}{2}x\right)}. \tag{1.1} $$</p>

<p><strong>1.2. Beta function identity.</strong> For $\operatorname{Re}(z) &gt; 0$ and $\operatorname{Re}(w) &gt; 0$ we have</p>

<p>$$ \int_{0}^{1} x^{z-1}(1-x)^{w-1} \, dx = \frac{\Gamma(z)\Gamma(w)}{\Gamma(z+w)}, \tag{1.2} $$</p>

<p>where $\Gamma$ is the <a href=""https://en.wikipedia.org/wiki/Gamma_function"" rel=""nofollow noreferrer""><em>gamma function</em></a>.</p>

<p><strong>1.3. An integral identity.</strong> For $|\Re(s)| &lt; \frac{1}{2}$, we have</p>

<p>$$ \sec(\pi s)
= \frac{2}{\pi} \int_{0}^{\infty} \frac{u^{2s}}{1+u^2} \, du
= \frac{2}{\pi} \int_{0}^{1} \frac{u^{2s} + u^{-2s}}{1+u^2} \, du. \tag{1.3}$$</p>

<p>This can be obtained directly by adopting some contour integration technique. Alternatively, this can be derived by plugging the substitution $x = \frac{1}{1+u^2}$ to $\text{(1.2)}$ with $z = s = 1-w$ and applying the <a href=""https://en.wikipedia.org/wiki/Gamma_function#General"" rel=""nofollow noreferrer""><em>Euler's reflection formula</em></a>. </p>

<hr>

<h2>2. Solution</h2>

<p><strong>2.1. Derivation of the key formula.</strong> Notice that we can write $g(n) = S_n^2$, where $S_n$ is given by</p>

<p>$$S_n = 4^n\left( 1 + 2\sum_{j=1}^{n}(-1)^j \cos^{2n}\left(\frac{j\pi}{2n+1}\right)\right).$$</p>

<p>We perform some algebraic manipulation on $S_n$ by utilizing the identity $2\cos x = e^{\mathrm{i}x} + e^{-\mathrm{i}x}$ and the binomial theorem.</p>

<p>\begin{align*}
S_n
&amp;= 4^n \sum_{j=-n}^{n} (-1)^j \cos^{2n}\left(\frac{j\pi}{2n+1}\right) \\
&amp;= \sum_{j=-n}^{n} (-1)^j \left( 2\cos\left(\frac{j\pi}{2n+1}\right) \right)^{2n}\\
&amp;= \sum_{j=-n}^{n} (-1)^j \sum_{l=0}^{2n}\binom{2n}{l} \exp\left(\mathrm{i}\frac{(2l-2n)j\pi}{2n+1} \right).
\end{align*}</p>

<p>Interchanging the summations and applying $\text{(1.1)}$ and $\text{(1.3)}$ successively, this simplifies to</p>

<p>\begin{align*}
S_n
&amp;\stackrel{\text{(1.1)}}{=} \sum_{l=0}^{2n}\binom{2n}{l} (-1)^{l} \sec\left(\frac{(l-n)\pi}{2n+1}\right) \\
&amp;\stackrel{\text{(1.3)}}{=} \frac{2}{\pi} \sum_{l=0}^{2n}\binom{2n}{l} (-1)^{l} \int_{0}^{1} \frac{u^{2(l-n)/(2n+1)} + u^{-2(l-n)/(2n+1)}}{1+u^2} \, du.
\end{align*}</p>

<p>Applying the binomial theorem and plugging the substitution $u = v^{2n+1}$, we obtain the following integral representation which is a key step to our goal.</p>

<p>\begin{align*}
S_n
= \frac{4}{\pi} \int_{0}^{1} \frac{\left(1-u^{2/(2n+1)} \right)^{2n}}{u^{2n/(2n+1)} (1+u^2)} \, du
= \bbox[border:2px dashed green,6px]{ \frac{4(2n+1)}{\pi} \int_{0}^{1} \frac{\left(1-v^2 \right)^{2n}}{1+v^{4n+2}} \, dv }. \tag{2.1}
\end{align*}</p>

<p><strong>2.2. Evaluation of the limit.</strong> We estimate our integral representation $\text{(2.1)}$ as follows:</p>

<p>\begin{align*}
\int_{0}^{1} \frac{\left(1-v^2 \right)^{2n}}{1+v^{4n+2}}\, dv
&amp;= \int_{0}^{1} \left(1-v^2 \right)^{2n}\, dv - \int_{0}^{1} \frac{\left(1-v^2 \right)^{2n}v^{4n+2}}{1+v^{4n+2}}\, dv \\
&amp;= \frac{\sqrt{\pi}}{2}\frac{\Gamma(2n+1)}{\Gamma(2n+\frac{3}{2})} +\mathcal{O}(16^{-n}).
\end{align*}</p>

<p>Here, the first integral is computed using the beta function identity $\text{(1.2)}$ and the second integral is estimated using the fact that $0 \leq v^2(1-v^2) \leq \frac{1}{4}$ for all $v \in [0,1]$. So it follows that</p>

<p>$$ S_n = \frac{2}{\sqrt{\pi}}\frac{\Gamma(2n+2)}{\Gamma(2n+\frac{3}{2})} + \mathcal{O}\left(\frac{n}{16^n}\right). $$</p>

<p>By the Stirling's approximation, we easily check that $\Gamma(2n+2)/\Gamma(2n+\frac{3}{2}) \sim \sqrt{2n}$ as $n\to\infty$. Here, the relation $\sim$ means that the ratio of two quantities goes to $1$. From this, we obtain</p>

<p>$$ g(n) = \bbox[border:2px dashed green,6px]{ \frac{4}{\pi} \left( \frac{\Gamma(2n+2)}{\Gamma(2n+\frac{3}{2})} \right)^2 + \mathcal{O}\left(\frac{n^{3/2}}{16^n}\right) }. \tag{2.2}$$</p>

<p>This already tells that $g(n) \sim \frac{8}{\pi}n$ as $n\to\infty$, so we can expect that $g(n+1) - g(n) = \frac{8}{\pi}$ if it converges. This is indeed true, as we have</p>

<p>$$ g(n+1) - g(n)
= \frac{4}{\pi}
\underbrace{ \left( \left( \frac{4n+6}{4n+5}\cdot\frac{4n+4}{4n+3} \right)^2 - 1 \right) }_{\sim \frac{1}{n}}
\cdot 
\underbrace{ \left( \frac{\Gamma(2n+2)}{\Gamma(2n+\frac{3}{2})} \right)^2 }_{\sim 2n} + o(1) $$</p>

<p>The upshot of this calculation is what we have expected:</p>

<blockquote>
  <p>$$ \lim_{n\to\infty} \left( g(n+1)-g(n) \right) = \frac{8}{\pi}.$$</p>
</blockquote>
"
"2391595","2391601","<p>$x=8, y=4, z=8, w=9$  is the solution.</p>

<p>edit:
your solution is also correct. I don't know why are you multiplying $6â10â6â5$?</p>

<p>$xâ1=y+3=zâ1=wâ2=7$  means 7*7*7*7=2401    so please upvote this answer for our efforts.</p>
"
"2391599","2391756","<p>I will do it for $p_1 = 1/5$ and $p_{mb} = p_{ab} = 1/2.$ [For example, each person tosses a fair coin to decide whether to take the Bus (Heads) or the train (Tails).]</p>

<p>(a) $P(\text{Both take bus}) = (1/2)^2.$ [HH.]</p>

<p>(b) $P(\text{Different modes of travel}) = 2(1/2)^2 = 1/2.$ [HT or TH.]</p>

<p>(c) $P(\text{Different modes} \cap {Rain}) = P(\text{Different modes})P(\text{Rain}) = (1/2)(1/5) = 1/10.$ [The symbol $\cap$ means 'and'.]</p>

<p>All three answers use Independence (to multiply). Equation (b)
uses the Addition Rule for Disjoint Events (to add $1/4 + 1/4 = 1/2).$</p>

<p>If you don't know about independence and adding probabilities of disjoint events, it is time to read your text
carefully or to seek out a basic probability text. Now, I hope you can do
the more general case, in which $p_{mb} \ne p_{ab},$ so that (b) is just a little
more difficult--as in the excellent clue from @lulu. </p>
"
"2391603","2391620","<p>You're asking if you can verify that $S = \sum_{k=0}^n f(n,k)$, only knowing $S$. The answer is no. Think if I asked you to show that $x=5$, but I didn't tell you anything about $x$. You couldn't unless I told you the value of $x$.</p>

<p>It's the same thing with your question. Without summing the terms or evaluating a closed form, I know nothing about the value of $\sum_{k=0}^n f(n,k)$.</p>

<p>Now there are some observations you can make, but they don't work $100\%$ of the time. </p>

<ul>
<li><p>Look at the signs. If $S&gt;0$, but $f(n,k)&lt;0,\ \forall n,k$, then there is certainly no equality. More generally, if $\text{sgn}(S)\ne\text{sgn}(f(n,k)),\ \forall n,k \implies S \ne \sum_{k=0}^n f(n,k)$.</p></li>
<li><p>Look at the behavior. If $S&lt;f(n,0)$ and $f'(n,k)&gt;0,\ \forall k&gt;0$, then there is no equality.</p></li>
</ul>

<p><strong>In response to your comment on the question:</strong>
without knowing $f(n,k)$, there's no general solution to solve this without calculating the sum. You can find properties of $f(n,k)$ that you can leverage to answer your problem. To show this I'll address the example you've given</p>

<blockquote>
  <p>I have a sum involving floor, sqrt, ceil</p>
</blockquote>

<p>The best you're going to get here is approximations. Take, $$ \sum_{k=a}^n \lfloor c\cdot k\rfloor $$ There's no closed-form for this, so the best we can do is bound it very tight. See my <a href=""https://math.stackexchange.com/questions/2308258/bounds-on-floor-summation-sumn-k-lfloor-c-cdot-k-rfloor"">my question here</a>, where I show $$ \sum_{k=a}^n \lfloor c\cdot k\rfloor \approx \frac{(a-n-1)(ac+cn-1)}{-2} $$ is a very accurate approximation. Even for large $n$.</p>

<p>This is just an example of approximations you can make and unfortunately, without calculating the series or a closed form, probably the best you can do is exploit properties of the sum.</p>
"
"2391604","2391633","<p>A point being extreme (or not) has nothing to do with the norm or topology of the space; it's just a concept of linear algebra.  Topology enters the picture only when we talk about $K$ being compact. But then again, on a finite-dimensional space all norms induce the same topology, so they choice of a norm does not matter. </p>

<p>Upshot: if $V$ is a finite-dimensional normed vector space and $K$ is its compact subset, consider the Euclidean norm on $V$ instead and take the point of $K$ with maximal norm.  That's an extreme point, by the parallelogram law.</p>
"
"2391608","2391611","<p>You are not required to find whether the convergence is uniform so
it is unnecessary to find maxima/minima.</p>

<p>There are three cases:</p>

<p>(i) $|x|&lt;1$. Then $nx^n\to 0$ and so $nx^n(1-x^2)\to0$.</p>

<p>(ii) $|x|=1$. Then $f_n(x)=0\to0$.</p>

<p>(iii) $|x|&gt;1$. Then $1-x^2\ne0$. Also $(nx^n)$ diverges, so $(nx^n(1-x^2))$
also diverges.</p>
"
"2391614","2391628","<p>The right way to look at your situation is to consider the minimal polynomial for $a$, say $g(X)\in K[X]$, and to ask how many <em>other</em> roots of $g$ lie in $K(a)$. Depending on $a$ (equivalently, on $g$) there may be no other roots lying in $K(a)$, or all roots of $g$ may be there. This is the core of the concept of normality of an extension.</p>
"
"2391617","2391689","<p>Yes there is.</p>

<p>\begin{align*}
\operatorname{Gr}_{n,k}(\mathbb{R}) &amp;= O(n)/(O(k)\times O(n-k))\\
\operatorname{Gr}_{n,k}(\mathbb{C}) &amp;= U(n)/(U(k)\times U(n-k))\\
\operatorname{Gr}_{n,k}(\mathbb{H}) &amp;= Sp(n)/(Sp(k)\times Sp(n-k)).
\end{align*}</p>

<p>The total space of the tautological bundle is the corresponding Stiefel manifold $V_{n,k}(\mathbb{F})$ which are homogeneous spaces:</p>

<p>\begin{align*}
V_{n,k}(\mathbb{R}) &amp;= O(n)/O(n-k)\\
V_{n,k}(\mathbb{C}) &amp;= U(n)/U(n-k)\\
V_{n,k}(\mathbb{H}) &amp;= Sp(n)/Sp(n-k).
\end{align*}</p>

<p>Then, for example, we have the bundle $O(k) \to V_{n,k}(\mathbb{R}) \to \operatorname{Gr}_{n,k}(\mathbb{R})$.</p>
"
"2391618","2391625","<p>Note that $\hat{i}$, $\hat{j}$ and $\hat{k}$ are orthonormal. Consequently, $\hat{i}\cdot\hat{j}=\hat{i}\cdot\hat{k}=\hat{j}\cdot\hat{k}=0$, and $\hat{i}\cdot\hat{i}=\hat{j}\cdot\hat{j}=\hat{k}\cdot\hat{k}=1$. Therefore, if $\mathbf{v} = a\hat{i}+b\hat{j}+c\hat{k}$ then $\hat{i}\cdot\mathbf{v} = a \hat{i}\cdot\hat{i}+ b \hat{i}\cdot\hat{j} + c \hat{i}\cdot\hat{k}=a$. Similarly, $\hat{j}\cdot\mathbf{v}=b$, and $\hat{k}\cdot\mathbf{v}=c$.</p>
"
"2391619","2391644","<p>At first notice that if $\gcd(a,p) &gt; 1$, then clearly there is no such natural number $n$. <strong>So we can assume that $\gcd(a,p) = 1$.</strong> </p>

<hr>

<hr>

<p>Let $a$ be a natural number such that $\gcd(a,M)=1$. 
By the $\text{ord}_M(a)$ we mean the minimum power $r$ 
for which $a^r-1$ is divisible by $M$.  </p>

<p><strong>Lemma(II)</strong>: Let $a$ be a natural number such that $\gcd(a,M)=1$. 
Let $R$ to be an arbitrary natural number. 
Then we have:</p>

<p>$$ M \mid a^R-1  \ \ \ \Longleftrightarrow \ \ \ \text{ord}_M(a) \mid R \ \ \ . $$</p>

<hr>

<hr>

<p>By the $v_p(t)$, we mean the highest power of $p$ which divides $t$. 
Now see the <strong>Lemma1</strong> from here: </p>

<p><a href=""https://math.stackexchange.com/questions/2378558/p-adic-valuation-of-xn1or-how-many-times-does-a-prime-number-divides-xn/2378606#2378606"">$p$-adic valuation of $x^n+1$[or how many times does a prime number divides $x^n+1$]</a></p>

<p>From the mentioned <strong>lemma1</strong> and <strong>Lemma(II)</strong> we have the following lemma: </p>

<p><strong>Lemma(III)</strong>: Let $p$ to be an odd prime and let $a$ be a natural number such that $\gcd(a,p)=1$. 
Let $r:=\text{ord}_p(a)$; i.e. 
the minimum power $r$ for which $a^r-1$ is divisible by $p$. </p>

<p>Also let $t:=v_p(a^r-1)$; i.e. the highest power of $p$ which divides $a^r-1$. 
If $k \leq t$ let $s:=0$, else let $s:=k-t$. 
Then we have: </p>

<p>$$\text{ord}_{p^k}(a)=rp^s.$$ </p>

<hr>

<p><strong>Lemma(IV)</strong>: Let $p=2$ 
and assume that $a\overset{4}{\equiv} 1$.
Then we have $1=\text{ord}_4(a)$. </p>

<p>Also let $t:=v_2(a-1)$; i.e. the highest power of $2$ which divides $a-1$. 
If $k \leq t$ let $s:=0$, else let $s:=k-t$. 
Then we have: </p>

<p>$$\text{ord}_{2^k}(a)=2^s.$$ </p>

<hr>

<p><strong>Lemma(V)</strong>: Let $p=2$ 
and assume that $a\overset{4}{\equiv} 3$.
Then we have $2=\text{ord}_4(a)$. </p>

<p>Also let $t:=v_2(a^2-1)$; i.e. the highest power of $2$ which divides $a^2-1$. 
If $k=1$ let $s:=-1$, if $1 &lt; k \leq t$ let $s:=0$, else let $s:=k-t$. 
Then we have: </p>

<p>$$\text{ord}_{2^k}(a)=2.2^s \ .$$ </p>

<hr>

<hr>

<hr>

<hr>

<hr>

<hr>

<ul>
<li><strong>First case</strong>: Assume that $p$ be an odd prime.
then let $r:=\text{ord}_p(a)$; i.e. 
the minimum power $r$ for which $a^r-1$ is divisible by $p$. 
Also let $t:=v_p(a^r-1)$; i.e. the highest power of $p$ which divides $a^r-1$. 
If $k \leq t$ let $s:=0$, else let $s:=k-t$. 
Then <strong>Lemma(III)</strong> implies the following proposition : </li>
</ul>

<p><strong>Proposition3</strong>: $n$ satisfies the <strong>above divisibility diophantine equation</strong> 
if and only if it is divisible by $rp^s$.</p>

<hr>

<hr>

<ul>
<li><strong>Second case</strong>: Assume that $p=2$ 
and assume that $a\overset{4}{\equiv} 1$,
then we have $1=\text{ord}_4(a)$. 
Also let $t:=v_2(a-1)=v_2(a^1-1)$; 
i.e. the highest power of $2$ which divides $a-1$. 
If $k \leq t$ let $s:=0$, else let $s:=k-t$. 
Then <strong>Lemma(IV)</strong> implies the following proposition : </li>
</ul>

<p><strong>Proposition4</strong>: $n$ satisfies the <strong>above divisibility diophantine equation</strong> 
if and only if it is divisible by $2^s$.</p>
"
"2391621","2391656","<p>Let $U, V, W$ be an open sets in $\mathbb{R}^n, \mathbb{R}^m, \mathbb{R}^k$ and $f: U \rightarrow V, g: V \rightarrow W$ differentiable functions.  </p>

<p>If we write $f(x) = (f_1(x), ... , f_m(x))$ and $g(y) = (g_1(y), ... , g_k(y))$, then we have $g \circ f(x) = (g_1 \circ f(x), ... , g_k \circ f(x))$.</p>

<p>The derivative of $f$ at a given point $p \in U$ is by definition a certain linear transformation $Df(p): \mathbb{R}^n \rightarrow \mathbb{R}^m$. The standard bases of $\mathbb{R}^n$ and $\mathbb{R}^m$ allow us to identify $Df(p)$ with the $m$ by $n$ matrix</p>

<p>$$A = \begin{pmatrix} \frac{\partial f_1}{\partial x_1}(p) &amp; \cdots &amp; \frac{\partial f_1}{\partial x_n}(p) \\ \vdots &amp; &amp; \vdots \\ \frac{\partial f_m}{\partial x_1}(p) &amp; \cdots &amp; \frac{\partial f_m}{\partial x_n}(p) \end{pmatrix}$$</p>

<p>In turn, the derivative of $Dg(f(p))$ of $g$ at $f(p)$ is a linear transformation $\mathbb{R}^m \rightarrow \mathbb{R}^k$ can be identified with the $k$ by $m$ matrix</p>

<p>$$B = \begin{pmatrix} \frac{\partial g_1}{\partial y_1}(f(p)) &amp; \cdots &amp; \frac{\partial g_1}{\partial y_m}(f(p)) \\ \vdots &amp; &amp; \vdots \\ \frac{\partial g_k}{\partial y_1}(f(p)) &amp; \cdots &amp; \frac{\partial g_k}{\partial y_m}(f(p)) \end{pmatrix}$$</p>

<p>Finally, the derivative of $g \circ f$ at $p$ can be identified with the $k$ by $n$ matrix whose $ij$th entry is $\frac{\partial(g_i \circ f)}{\partial x_j}(p)$.</p>

<p>The linear transformation $\mathbb{R}^n \rightarrow \mathbb{R}^k$ corresponding to the product $BA$ is the composition $Dg(f(p)) \circ Df(p)$.  From multivariable calculus, the partial derivative of $g_i \circ f: \mathbb{R}^n \rightarrow \mathbb{R}$ with respect to $x_j$ at $p$ is </p>

<p>$$\sum\limits_{l=1}^n \frac{\partial(g_i)}{\partial y_l}(f(p)) \frac{\partial f_l}{\partial x_j}(p)$$</p>

<p>This shows that the derivative of a composition is the composition of the derivatives, which combined with the fact that the derivative of the identity map is the identity map, gives you your result.</p>
"
"2391624","2391853","<p>If $x&gt;y$, then $x*x &gt; xy$. But $xy = \frac12$ so $x^2 &gt; \frac12$.</p>

<p>Similar for $\frac12 &gt; y^2$.</p>
"
"2391647","2391653","<p>Remember that $n$ is <strong>fixed</strong>. So you've shown a lower bound that looks like</p>

<p>$$f'(x) &gt; \frac{e^{n + 1}}{n^{n + 1}} = c &gt; 0$$</p>

<p>once $x$ is large enough. An easy application of the mean value theorem shows that this will force $f(x) \to \infty$ as $x \to \infty$; geometrically, $f$ must sit above a line with slope $c$. (Note that you don't actually even need to know $f(x) &gt; e^n/n^n$ here...).</p>
"
"2391649","2391663","<p>For $x$ dogs, pick how many two-dog cages are needed.  This will range from $0$ to $\left\lfloor\frac{x}{2}\right\rfloor$.  Let us call the number of two-dog cages used $k$.</p>

<p>Given that we are using $k$ two-dog cages, pick which $2k$ dogs are being placed into two-dog cages.  Then, sort these dogs according to an arbitrary order (<em>e.g. by number</em>)</p>

<p>Next, among all dogs designated to be put in two-dog cages pick who is put in the cage with the smallest dog.  Next, pick who is put in the cage with the now smallest remaining dog.  Repeat this process until all dogs designated to be put into two-dog cages have their partners selected.</p>

<p>Ranging over all values of $k$ we have as a result:</p>

<p>$$1+\sum\limits_{k=1}^{\left\lfloor\frac{x}{2}\right\rfloor}\binom{x}{2k}(2k-1)!!$$</p>

<p>(<em>double factorial notation $n!!=n(n-2)(n-4)(n-6)\cdots(2~\text{or}~1~\text{depending on if $n$ is even or odd})$.  For example $6!!=6\cdot 4\cdot 2$ and $9!!=9\cdot 7\cdot 5\cdot 3\cdot 1$</em>)</p>

<p>(<em>edit: changed index to start from $k=1$ instead to avoid having to deal with attempting to define $(-1)!!$ as equalling $1$ or using more complicated methods to write out the product</em>)</p>

<hr>

<p>As for an alternate solution, from the comments above we can follow the links and eventually arrive to the page for <a href=""https://en.wikipedia.org/wiki/Telephone_number_(mathematics)"" rel=""nofollow noreferrer"">telephone numbers(mathematics)</a> which are exactly those values you are wishing to calculate which yields some other rather nice ways to calculate the number.</p>

<p>One such method that they exhibit is that of recurrence relations.</p>

<p>$T(0)=1,T(1)=1$ and $T(n)=T(n-1)+(n-1)T(n-2)$ for all $n\geq 2$</p>

<p>The idea being that assuming we know the value $T(k)$ for all $k&lt;n$, to find $T(n)$ our $n$'th dog can either be in a cage by himself which can be accomplished in $T(n-1)$ ways, or our $n$'th dog can be in a cage with another dog.  Pick which dog that is, and then the remaining $n-2$ dogs can be arranged in $T(n-2)$ ways.</p>
"
"2391668","2397222","<p>Yes, it is a standard notation for products (not only of measures).</p>
"
"2391671","2391674","<p>Hint: $\hat{i}$ points in the direction of south.</p>
"
"2391673","2391679","<p>The proof for this statement is simply about showing the existence of such a $y$ for which the statement that $\exists y \forall x (x+y = y)$.  </p>

<p>In other words, is asking if there exists an additive identity $y\in \mathbb R$ such that for all $x\in \mathbb R(x+y=x).$</p>

<p>Well, we do know that $$\forall x \in \mathbb R\,(x+ 0 = x).$$</p>

<p>So there does in fact exist a $y \in \mathbb R$ such that for all $x\in \mathbb R$, it is true that $x+y=x$, because we can use $0$ as a witness for $y\in \mathbb R$.</p>

<p>That leaves us with only $$\forall x \in \mathbb R( x+0 = x)$$</p>
"
"2391681","2391738","<p>In various places (e.g., Avner Friedman's book on PDE) one can find the theorem that a distribution supported on a nicely-imbedded submanifold is (at least locally) the composition of application of normal derivatives with a distribution on the submanifold. After a simplifying coordinate change (e.g., to make the submanifold the span of a subset of the canonical basis vectors) Fourier transform shows that for $u\in H^s(\mathrm{sub})$ the composition with restriction of codimension $m$ is in $H^{s-{m\over 2}-\epsilon}$ for every $\epsilon&gt;0$. This is a nice extension of the fact that Dirac delta is in $H^{-{n\over 2}-\epsilon}(\mathbb R^n)$.</p>
"
"2391702","2391824","<p>The following proposes a family of solutions, assuming that $\,f\,$ is continuously differentiable.</p>

<p>Let $\,f(x)=2^{-x+2/3}\,g(x)\,$, then:</p>

<p>$$\require{cancel}
\bcancel{2^{-2x+2/3}} g(2x) = \bcancel{2^{2x}}\,\bcancel{2^{-2x+4/3}} g(x)\,\bcancel{2^{-2(x+1)+4/3}}g(x+1) \iff g(2x)=g^2(x)g^2(x+1)
$$</p>

<p>Let $\,g(x)=e^{h(x)}\,$, then:</p>

<p>$$
e^{h(2x)} = e^{2h(x)}e^{2h(x+1)} \iff h(2x) = 2 \big(\,h(x)+h(x+1) \,\big)
$$</p>

<p>Assuming $\,f\,$ is differentiable then so are $\,g\,$ and  $\,h\,$, and:</p>

<p>$$
2 h'(2x) = 2 \big(\,h'(x)+h'(x+1) \,\big) \iff h'(2x) = h'(x)+h'(x+1)
$$</p>

<p>The latter functional equation has the obvious linear solution $\,h'(x)=a(x-1)\,$ with $\,a \in \mathbb{R}\,$.</p>

<p>Tracing the steps back, in the end:</p>

<p>$$
f(x) = 2^{-x+2/3} \cdot c^{x^2 - 2x+2/3} \quad\quad \text{with} \;c \in \mathbb{R}^+
$$</p>
"
"2391727","2393924","<p>The leading error terms  for $f'\left(x+\frac{h}{2}\right)$ and $f'\left(x-\frac{h}{2}\right)$ are $-\frac{h^2}{24}f'''\left(x+\frac{h}{2}\right)$ and $\frac{h^2}{24}f'''\left(x-\frac{h}{2}\right),$ respectively. You can see this, for example, for $f'\left(x+\frac{h}{2}\right)$ by subtracting the Taylor series expansions of $f(x+h)$ and $f(x)$ about $x+(h/2)$ and then dividing the result by $h.$ So, we get</p>

<p>$$\dfrac{f'\left(x+\frac{h}{2}\right)-f'\left(x-\frac{h}{2}\right)}{h} = \dfrac{f(x+h)-2f(x)+f(x-h)}{h^2}+\dfrac{h}{24}\left[f'''\left(x-\frac{h}{2}\right)-f'''\left(x+\frac{h}{2}\right)\right]+\cdots.$$</p>

<p>As you pointed out, it appears like the leading error term is $O(h),$ but if you Taylor expand $f'''\left(x-\frac{h}{2}\right)$ and $f'''\left(x+\frac{h}{2}\right)$ about $x,$ you will see that the term inside the square bracket above simplifies to $-hf''''(x)+\cdots,$ and therefore the leading error term is indeed $O(h^2)$ as expected. </p>
"
"2391729","2391789","<p>Rob and lulu basically answered it in the comments.</p>

<p>1) Find the prime factorization of $N = p_1^{n_1}*p_2^{n_2}.....*p_m^{n_m}$</p>

<p>2) If any of the $n_i$ are equal to one it can not be done.  This will happen about $60\%$ of the time!</p>

<p>3) If you only have two prime you are done: $N = p_1^{n_1}*p_2^{n_2}$.</p>

<p>4) If you have only one prime less than or equal to $3$ i.e. $N= p^3$ or $p^2$ it can not be done but other wise you are done: $N = p^n = p^a*p^b$ for any $a+b=n$.</p>

<p>5) If all then $n_i$ are even then you are done $N=(p_1)^{n_1}*(p_2^{n_2/2}*....p_m^{n_m/2})^2$.</p>

<p>6) Separate the even $n_i$ from the odd from the odd so that </p>

<p>$N = p_l^{2k_l}...p_j^{2k_j}*p_h^{2k_h + 3}...p_i^{2k_n + 3}$</p>

<p>7)  Break this into $N = (p_1^{k_1}..... p_n^{k_n})^2*(p_h....p_i)^3$.</p>

<p>8) The only case not covered  above  is if $N= p_1^3.....p_n^3$.  In this case $N = (p_1)^3*(p_2.....p_3)^3$.</p>

<p>So for example, if I take  $N=16810159716000$</p>

<p>Prime factorization is $N=2^5 * 3^6 * 5^3 * 7^8$</p>

<p>$= (2^2*3^6*7^8)*(2^3*5^3) = (2*3^3*7^4)^2*(2*5)^3 = 129654^2*10^3$</p>

<p>Which is  not the <em>only</em> way to do it. Ex. $(2*7)^2(2*3^2*5*7^2)^3$ will also work.  It's just a matter of splitting the prime factors so that all the exponents are all multiples of one of two numbers.  Which sounds hard but is easy to do in practice.  That is if it can be done at all,  Which it usually won't, but will be easy to see when it can't.</p>
"
"2391730","2391732","<p>$2017^2=(2018-1)^2=2018^2-2\cdot2018+1$,</p>

<p>$2019^2=(2018+1)^2=2018^2+2\cdot2018+1$,</p>

<p>You deduce that $2017^2-2018^2+2019^2=2018^2-2\cdot2018+1-2018^2+2018^2+2\cdot2018+1=2018^2+2$</p>
"
"2391731","2391741","<p>$$\left(1-\frac{j}{r-k}\right)\left(1-\frac{k}{r}\right)=\left(\frac{r-k-j}{r-k}\right)\left(\frac{r-k}{r}\right)=\frac{r-k-j}{r}=1-\frac{j+k}{r}.$$</p>
"
"2391749","2391781","<p>Let $S$ be any simple left $A$ module. Obviously it is nonzero and finitely generated. </p>

<p>$IS$ is a submodule of $S$, so it is either $S$ or $\{0\}$. But $IS\neq S$ since $S\neq \{0\}$. Therefore $I$ annihilates $S$. This is true for every simple left $A$ module, so $I\subseteq J(A)$.</p>

<p>This has a very direct translation to maximal left ideals, if you'd prefer that. Ask questions if you get stuck.</p>
"
"2391752","2391763","<p>Guide:</p>

<p>(a)</p>

<p>$$\frac{P(B)}{P(A \cup B)}=\frac23$$</p>

<p>$$P(A)=\frac12$$</p>

<p>Hence $$P(A \cup B)=P(A)+P(B)-P(A)P(B)=\frac12+P(B)-\frac12P(B)=\frac12+\frac12P(B)$$</p>

<p>Substitute that into the first equation and you can solve for $P(B)$.</p>

<p>(b)</p>

<p>Use</p>

<p>$$P(A \cup B) = P(A^C \cap B)+P(A)$$</p>

<p>Now, for your mistake.</p>

<p>in part (a),</p>

<p>$$P(B \cap (A \cup B))=P(B),$$</p>

<p>and since $P(A \cup B) &lt; 1$, hence $B$ and $A \cup B$ are not independent.</p>

<p>in part (b),</p>

<p>$$P(B|A)=1-P(B|A^c)$$ is not true in general.</p>

<p>We do have $$P(B|A)=1-P(B^c|A)$$ though.</p>
"
"2391762","2391780","<p>Yes: Since every ordinal is a surreal number, the sets you're looking for can be taken to be the initial ordinals that represent $\aleph_n$, $\beth_n$, and so forth.</p>
"
"2391769","2392545","<p><a href=""https://i.stack.imgur.com/52D4H.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/52D4H.png"" alt=""runs_ones_1""></a></p>

<p>Consider a binary string  and let's put an additional (dummy) fixed $0$ at the start of the it.
We individuate as a <em>run</em> a $0$ followed by contiguous  $1$'s.</p>

<p>So, given a string of length $N$, total of ones $C$, and number of runs $R$,
we will have $N-C$ zeros, of which $N-C-R+1$ are ""free"", that is not tight to mark the runs.</p>

<p>The number of ways to constitute the runs is the number of (standard) compositions of $C$ into $R$ parts, that is
$$ {{C-1} \choose {R-1}}$$.<br>
The number of ways to place the ""free"" zeros will be equal to the number of <em>weak</em> compositions of their number into $R+1$ parts (in front of the first and then after each run), i.e.:
$$ {{N-C-R+1+R+1-1} \choose {R}}={{N-C+1} \choose {R}}$$.   </p>

<p>Which confirms <em>N.Shales</em>'s answer, so that the merit should go to him.</p>

<p><em>Addendum</em>   </p>

<p>Concerning formula (24), 
for what I can see, it looks like that in the 3rd binomial ${{y+1+g_k} \choose {j}}$ there is a sign typo,
and that it should be $\cdots -g_k$.<br>
Then putting for instance $k=1,\; s_k=n-1 \; g_k=2$ it will correctly give $n-2$,<br>
and with $k=1,\; s_k=C \; g_k=R$ it will  give the formula above.<br>
But not having the full text, I cannot check that further.</p>
"
"2391770","2391777","<blockquote>
  <p>Is this counter example correct?</p>
</blockquote>

<p>Yes, but it's more complicated than necessary. Don't bother with formulas. Can you build an example when $A$ and $C$ have one element each and $B$ has many? That's the essence of your argument.</p>
"
"2391771","2391793","<p>You don't want to start pulling $\mathbf{p}$ apart: it's just a constant vector. Instead, find
$$ -\nabla \left( \frac{\mathbf{r} \cdot \mathbf{p}}{r^3} \right) = -\left(\nabla \frac{1}{r^3} \right) \mathbf{r} \cdot \mathbf{p} - \frac{1}{r^3}\nabla (\mathbf{r} \cdot \mathbf{p}) = +3\frac{\mathbf{r}}{r^5} (\mathbf{r} \cdot \mathbf{p}) - \frac{\mathbf{p}}{r^3} = \frac{1}{r^3}(3 (\hat{\mathbf{r}} \cdot \mathbf{p})\hat{\mathbf{r}}-\mathbf{p}) $$
using $\nabla f(r) = f'(r)\mathbf{r}/r$ and $\partial_i r_j p_j = \delta_{ij} p_j = p_i $.</p>
"
"2391773","2391809","<p>Let $a=(x\wedge y)\wedge z$ and $b = x \wedge (y \wedge z)$. Then $a \le x\wedge y$ and $a \le z$, so we must also have $a \le x$ and $a \le y$. Similarly for $b$, $b \le x$, $b \le y $, and  $b \le z$.</p>

<p>The above relations, and the definition of $\wedge$ tell us that $a \le b$ and $b \le a$, so we must have that $a=b$.</p>

<p>To see the use of the definition of $\wedge$, notice that since $b \le x$ and $b \le y$, it must be that $b \le x\wedge y$, so that $b \le a$
.</p>
"
"2391775","2391779","<p><a href=""https://en.wikipedia.org/wiki/Hash_table"" rel=""nofollow noreferrer"">Hash table</a> search and insert are usually both considered $O(1)$ in the average. So if you start with an empty hash table, go through all the elements search and insert at the same time, you find the element in the average at index $k=(n+1)/2$ assuming there is only one duplicate, so you need $O(n)$ hash computations and equality comparisons (inside the hash table) or less.</p>

<p>If you dont use a hash table, but an ordinary list, checking for the collision is $O(m)$ where m is the length of the ordinary list, assuming we dont detect a collision. Taking again the average under the same distribution as before we get $1/6\,n\,(n + 2)$. So you would then need $O(n^2)$ equality comparisons (inside the ordinary list) or less. </p>

<p>The different program codes look very similar (in Java):</p>

<p>Hash table:</p>

<pre><code>HashSet res = new HashSet();
for (int i=0; i&lt;A.size(); i++) {
    if (res.contains(A.get(i))     /* O(1) */
      return true;
    res.add(A.get(i));             /* O(1) */
}
return false;
</code></pre>

<p>Ordinary list:</p>

<pre><code>ArrayList res = new ArrayList();
for (int i=0; i&lt;A.size(); i++) {
    if (res.contains(A.get(i))     /* O(m) */
      return true;
    res.add(A.get(i));             /* O(1) */
}
return false;
</code></pre>

<p>We can now make the probability model I was using more explicit. I am using a random variable $X$, for the number of iterations of the outer for loop. The variable is in the range $\{0, .., n\}$, $0$ means a duplicate was found in the first iteration and true was return. $n$ means no duplicate was found and false was return.</p>

<p>I did work with $P[X=i] = 1/(n+1)$ an uniform distribution. The effort of the programm can be viewed as the expected value $E[Y]$ of another random variable $Y = f(X)$, that computes the effort for performing the loop X times. For the hash table we have $Y = X$, and for the ordinary list we have $Y = 1/2\,X\,(X+1)$, neglecting the add() and only considering the contains().</p>

<p>Hash table:
$$E[Y]=E[X]=\sum_{i=0}^n P[X=i] i=\frac{1}{n+1} \frac{1}{2} n (n+1)=\frac{1}{2} n$$
Ordinary list:
$$E[Y]=E[\frac{1}{2} X (X+1)]=\sum_{i=0}^n P[X=i] \frac{1}{2} i (i+1)=\frac{1}{n+1} \frac{1}{6} n (n + 1) (n + 2) = \frac{1}{6} n (n+2)$$</p>

<p>The distribution that was chosen can be viewed as an upper bound, distributions that are more realistic or that also consider that there are multiple duplicates, would give a higher weight to lower values of $X$, which also means a higher weight to lower values of $Y = f(X)$, and thus in the end a lower value of $E[Y]$.</p>
"
"2391784","2391821","<p>Consider the phrase at the end of the first paragraph: </p>

<blockquote>
  <p>$\xi^k_n\to \eta^k$ holds weakly in $L^2$ and then also in $L^1$. </p>
</blockquote>

<p>I think you are meant to read that conclusion as ""also <em>weakly</em> in $L^1$.""</p>
"
"2391787","2391854","<p>Assume first that $E$ is bounded (so $m^*(E)&lt;\infty$) and that $m^*(E)&gt;0$. Let $\varepsilon&gt;0$. Then there exists $I_n$ such that  $E\subset\bigcup_{n=1}^\infty I_n$ and
$$\sum_{n=1}^\infty l(I_n)\le m^*(E)+\varepsilon\le \sum_{n=1}^\infty m^*(E\cap I_n)+\varepsilon\le \rho \sum_{n=1}^\infty l(I_n)+\varepsilon,$$which implies that
$$(1-\rho)\sum_{n=1}^\infty l(I_n)\le \varepsilon.$$
Note that we used the fact that the series is convergent. It follows that
$$(1-\rho)m^*(E)\le \varepsilon$$ and so letting $\varepsilon\to 0$ you get $m^*(E)=0$.</p>

<p>If $E$ is not bounded, you apply the previous argument to $E\cap (k-1,k]$ for every $k$.</p>
"
"2391790","2391816","<p>First, note that this is not specific to probability or the Stieljes integral. You could have asked the question about a plain vanilla integral $$\int_{-\infty}^\theta f(x)dx = \int_{-\infty}^\infty H(\theta-x)f(x)dx$$ where $H$ is the step function. Actually, we can simplify even further and consider derivative with respect to $\theta$ of $$\theta= \int_0^\theta dx = \int_0^\infty H(\theta-x)dx$$ where $\theta &gt; 0.$ Everything about your question still applies here.</p>

<p>We know the derivative of the LHS is one. So what's wrong with the logic of bring the derivative inside the integral and saying it's zero. First, it's not always okay to bring a derivative under the integral sign. Second, I understand the logic that it's zero almost everywhere so thus the integral must be zero, but that's applying rules about functions to something that's fundamentally a distribution (or at least you must treat it as a distribution if you bring the derivative inside). By the same logic the Dirac delta function is zero almost everywhere so it must integrate to zero... in fact we shall see that this analogy is exact.</p>

<p>Let's go back to the definition of the derivative. We want $$\lim_{h\to0}\frac{f(\theta+h)-f(\theta)}{h} = \lim_{h\to 0} \frac{1}{h}\left(\int_0^\infty H(\theta+h-x)dx - \int_0^\infty H(\theta-x)dx\right).$$ Of course these integrals are just silly ways of writing $\theta+h$ and $\theta$ so they converge and there's no question of us combining them under one and we can write our expression $$ \lim_{h\to 0} \int_0^\infty \frac{H(\theta+h-x)-H(\theta-x)}{h}dx.$$</p>

<p>Now we face the all-important question of whether we can bring the limit inside the integral. If we do, we know that we get zero at almost every $x.$ But at the all-important point $x=\theta$ the derivative is undefined. This is where all the action is. </p>

<p>So instead of trying to bring the limit inside, let's look at what the inside function looks like as a function of $x$ for fixed $h.$ It's a rectangle of width $h$ and height $\frac{1}{h}$ with support between $\theta$ and $\theta+h$. Hmm. This is a rectangle of area one that gets arbitrarily tall and thin as $h\to 0.$ So it is an approximation to a Dirac delta. Of course for any $h\ne 0$ the integral is one so the limit is also one, as we knew it had to be. </p>

<p>So, we see the ""derivative of the step function"" behaves exactly as a delta function. This gives us the distribution identity $$ \frac{d}{d\theta} H(\theta-x) = \delta(\theta-x)$$</p>
"
"2391792","2392077","<p>We have four binary operations, not all of them associative, let alone in combinations. Therefore we have to bother about parentheses. There are five ways to put parentheses among four  operands, namely
$$(ab)(cd),\quad\bigl((ab)c\bigr)d,\quad\bigl(a(bc)\bigr)d,\quad a\bigl((bc)d\bigr)\quad a\bigl(b(cd)\bigr)\ .\tag{1}$$
Each time we have three binary operations performed, which we may choose freely from the four basic operations. It follows that there are $5\cdot 4^3=320$ formally different expressions.</p>

<p>If all four variables are put equal to some ""generic"" value $t$ then some of these $320$ expressions will be undefined since there is a division by $0$ involved, and a lot of them will have equal value on account of the rules of algebra. It is an interesting programming exercise to list and count the ""semantically different"" expressions in $t$ resulting in this way. With Mathematica this can be done as follows:</p>

<p>Define a function $f$ of three variables implementing the basic operations  as follows:
$$f(1,x,y):=x+y,\quad f(2,x,y):=x-y,\quad  f(3,x,y):=x*y,\quad f(4,x,y):=x/y\ .$$
Then define a function $g$ of four variables implementing the five terms $(1)$ as follows:
$$g(1,i,j,k):={\tt Simplify}[f(i,f(j,t,t),f(k,t,t))],$$
and so on until $g(5,i,j,k)$. Produce the list of $320$ resulting expressions,  eliminate doubles by using ${\tt Union}$, and remove entries ${\tt Indeterminate}$ etc. by hand. The following picture shows the resulting  final list; it has 64 entries. When, e.g.,  $t=7$ one in fact obtains $64$ different values.</p>

<p><a href=""https://i.stack.imgur.com/1tHSs.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1tHSs.jpg"" alt=""enter image description here""></a></p>
"
"2391796","2391798","<p>Hint:</p>

<p>Your conjecture is true.</p>

<p>Let $X_i$ be the indicator variable that the $i$-th card is at the right position.</p>

<p>You are interested in computing $\mathbb{E}\left[\sum_{i=1}^NX_i\right]$.</p>
"
"2391808","2391811","<p>For the first one use a $\textbf{u}$-substitution with $\textbf{u} = \cos(t)$. For the second use the parametrization $\gamma(t) = (t,e^t)$ for the curve $C$ and solve like the way you did on problem $1$. Also for problem $1$, the limits should be from $0$ to $\pi$ since you want to use the parametrization of the ellipse; </p>

<p>$$\alpha(t) = \left(a \cos t, b \sin t\right)$$</p>
"
"2391812","2392559","<p>Let $n$ be the largest integer that divides $p^4-1$ for all prime $p\geq 7.$ </p>

<p>We have $11^4-1=14640$ and $7^4-1=2400.$ The $gcd$ of $14640$ and $2400$ is $240.$ So  $$n\leq 240.$$ If $p$ is odd then modulo $16$ we have $p^4\in \{(\pm 1)^4, (\pm 3)^4,(\pm 5)^4,(\pm 7)^4\}=\{1^2,9^2, 25^2, 49^2\}=\{1^2,9^2,9^2,1^2\}=$
 $=\{1,81,81,1\}=\{1\}.$  </p>

<p>If $p$ is not divisible by $3$ then modulo $3$ we have $p^4\in \{(\pm 1)^4\}=\{1\}.$</p>

<p>If $p$ is not divisible by $5$ then modulo $5$ we have $p^4\in \{(\pm 1)^4,(\pm 2)^4\}=\{1,16\}=\{1\}.$</p>

<p>So for any integer $p$ that is not divisible by $2,3,$ or $5$  we have $p^4\equiv 1 \pmod {16}$ and $p^4  \equiv 1 \pmod 3$ and $p^4 \equiv 1 \pmod 5;$ and  since $16,3,$ and 5 are pair-wise co-prime, therefore  $p^4\equiv 1 \pmod {16\cdot 3\cdot 5}=240,$ so $$n\geq 240.$$</p>
"
"2391828","2391838","<p>You went one step further than you want. $$\sup_{m,n\geq N}(\lvert a_n-a_m\rvert)\geq \sup_{m,n\geq N}(a_n-a_m) = \sup_{m,n\geq N}(a_n)+\sup_{m,n\geq N}(-a_m) = \sup_{n\geq N}(a_n)-\inf_{m\geq N}(a_m)$$ The second-to-last step is due to the independence and boundedness of $a_m$ and $a_n$, and the last step is due to <a href=""https://proofwiki.org/wiki/Negative_of_Infimum_is_Supremum_of_Negatives"" rel=""nofollow noreferrer"">this</a>.</p>
"
"2391836","2392713","<p>This is a direct application of <a href=""https://en.wikipedia.org/wiki/Lebesgue_differentiation_theorem"" rel=""nofollow noreferrer"">Lebesgue's Differentiation Theorem</a>: 
$$
\frac{m(E\cap[x,x+h])}{h}=\frac1h\,\int_{[x,x+h]}\,1_E\to 1_E\,\ \ \ \ \text{ a.e.}
$$</p>
"
"2391837","2394410","<p>OK. Here's a solution, as far as one is possible. It makes two assumptions:</p>

<ol>
<li><p>For each edge $ (a, b) $, there's exactly one other edge whose start or end point is $a$; that other edge's other endpoint is <em>not</em> $b$. So if you have edge $(2, 5)$, then there's an edge like $(2, 4)$ or $(17, 2)$ as well. The same goes for the vertex $b$. </p></li>
<li><p>The number of edges is finite. </p></li>
</ol>

<p>OK, here goes. </p>

<p>STEP 1. </p>

<p>TL; DR: under the equivalence relation defined by ""neighbor"", find all equivalence classes, and organize each into a chain using ""other"". </p>

<p>We're going to form a list of chains (a ""chain"" being a sequence of (edge, vertex) pairs, where the vertex is one of the two vertices of the edge). I'll use the function $other$ which consumes an edge and a vertex of that edge, and returns the other vertex of the edge, so that $other( (2, 5), 2) = 5$ and $other((2, 5), 5) = 2$. I'll also use $neighbor( (a, b), b)$, which returns the unique edge $(b, c)$ or $(c, b)$ that shares vertex $b$ but for which $c \ne a$. Both ""other"" and ""neighbor"" can be implemented by a hashdictionary, for instance, using a single pass through the data. </p>

<p>Init: make the chain list empty; set $i$ to $0$. </p>

<p>While edge list is not empty:
   Create a new empty chain $C$. Remove from the edge list the first edge,  call it $(a, b)$. Set done = false. Place $( (a, b), b)$ into the chain $C$.</p>

<p>while not done:</p>

<ul>
<li><p>Let $(e, v)$ be the last item in $C$, and let $e' = neighbor(e, v)$. Let $d = other(e', v)$.</p></li>
<li><p>If d = a, then set done = true, and insert $C$ into the chain list. </p></li>
</ul>

<hr>

<p>When you're done, you'll have a list $L$ of chains. For each chain, you can ignore the edges and just read off the vertices, and these will be the ""faces"" that you're looking for. </p>

<p>STEP 2:</p>

<p>Now you have to decide which are ""faces"" and which are ""holes"". </p>

<p>Unfortunately, from the data given, this cannot be determined. For instance, in the following diagram , there are three chains, which form concentric squares. On the left, the innermost chain bounds an orange square and the outer two together bound a blue ring-shaped face. But on the right, the outer one bounds a blue square, while the inner two bound a ring-shaped orange face. (The blue outer square shows through the hole in the inner ring). 
<a href=""https://i.stack.imgur.com/oJhmS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oJhmS.png"" alt=""enter image description here""></a></p>

<p>Perhaps you meant to include some constraint on whether faces can overlap (as they do in the right-hand example). But without that information (which is not part of the input or output you described), it's hopeless, as this example shows. </p>
"
"2391840","2398765","<p><a href=""https://i.stack.imgur.com/NvU9l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NvU9l.png"" alt=""enter image description here""></a></p>

<p>I exchanged the postions of $\angle 3, 4$ in the figure above, which should not affect the result. The proof below will use this new figure.</p>

<p>Construct $EG{/\!\!/}BD$ and $DG{/\!\!/BE}$, then $BDGE$ is a parallelogram.</p>

<p>Connect points $G$ and $C$, label angles $\measuredangle 1=\angle EGD=\angle 1$, $\angle 5 =\angle CGD$ and $\angle 6=\angle DGC$.</p>

<p>Then $CE=BD=EG\Rightarrow \measuredangle 1+\angle 6=\angle 3+\angle 5$</p>

<p>Next use the proof by contradition technique:</p>

<p>If $\angle 3=\angle 1$, the result will be easy to prove, so we suppose $\angle 3\ne\angle 1$. Specifically, without the loss of generality, let us suppose:</p>

<p>$\angle 3&gt;\angle 1\Rightarrow \angle 5&lt;\angle 6\tag{1}$</p>

<p>and $\because\angle 1+\angle 2=\angle 3+\angle 4$</p>

<p>On the other hand, the current assumption leads toï¼</p>

<p>$\angle 3&gt;\angle 1\Rightarrow \angle 4&lt;\angle 2\Rightarrow CD&lt;BE\Rightarrow CD&lt;DG\Rightarrow \angle 6&lt;\angle 5\tag{2}$</p>

<p>(1) conflicts with (2); similarly, the assumption $\angle 3&lt;\angle 1$ will also lead to such contradictionï¼ therefore, we conclude that:</p>

<p>$\angle 1=\angle 3$ ï¼ </p>

<p>therefore $\angle 2=\angle 4$ ï¼and  $\angle 5=\angle 6$;</p>

<p>and then $\angle 1+\angle 4=\angle 3+\angle 2$ and $AB=AC$</p>
"
"2391849","2391855","<p>Your error relates to your claim
$$g(z)=\frac{-a_0}{a_n z^{n-1}+a_{n-1}z^{n-2}+\cdots +a_1}\neq z.$$</p>

<blockquote>
""If $g$ is <b>not</b> continuous, then the polynomial $a_n z^{n-1}+a_{n-1}z^{n-2}+\cdots +a_1$ must vanish somewhere in $\mathbb{C}$, and we are done since $n \in \mathbb{N}$ is arbitrary.""
</blockquote>

<p>More specifically, your error is the fragment</p>

<blockquote>
"" . . . and we are done since $n \in \mathbb{N}$ is arbitrary.""
</blockquote>

<p>What you actually proved is that at least one of the polynomials
$$a_n z^{n-1}+a_{n-1}z^{n-2}+\cdots +a_1$$
$$a_n z^n+a_{n-1}z^{n-1}+\cdots +a_1 z+a_0$$
has a root in $\mathbb{C}$.</p>
"
"2391863","2392949","<p>Let's take a look at the first integral. First of all, you can use per partes method to get rid of $x$ term in the integral. Let me denote
$$
  A = \int_0^1 x F^k(x)(1-F(x))^{n-k}f(x)\,\mathrm{d}x.
$$
After per partes, we get
$$
  A
  =
  \left[
    x
    \int_0^x F^k(t)(1-F(t))^{n-k}f(t)\,\mathrm{d}t
  \right]_{x=0}^1
  -
  \int_0^1
  \int_0^x
  F^k(t)(1-F(t))^{n-k}f(t)\,\mathrm{d}t\,\mathrm{d}x.
$$
So we have
$$
  A = \int_0^1 F^k(t)(1-F(t))^{n-k}f(t)\,\mathrm{d}t
  -
  \int_0^1 \int_0^x F^k(t)(1-F(t))^{n-k}f(t)\,\mathrm{d}t\,\mathrm{d}x.
$$
Now we can use the substitution $y = F(t)$, $\mathrm{d}y = f(t)\,\mathrm{d}t$ and we get:
$$
  A = \underbrace{\int_0^1 y^k(1-y)^{n-k}\,\mathrm{d}y}_{B(k+1,n-k+1)}
  - \int_0^1 \underbrace{\int_0^{F(x)} y^k(1-y)^{n-k}\,\mathrm{d}t}_{B(F(x);k+1,n-k+1)}\,\mathrm{d}x
$$
where $B(x; a,b)$ is the incomplete Beta function [1]. I don't see how to continue now for general $F(x)$, but for $F(x) = x$ it is easy.</p>

<hr>

<p><strong>Update:</strong> I made an error. The following holds only if $F(x) = x$.</p>

<p>The integral of incomplete beta function can also be found in [1] while getting:
$$
  A
  =
  \frac{k!(n-k)!}{(n+1)!}
  -
  \left[
    x B(x;k+1,n-k+1) - B(x; k+2,n-k+1)
  \right]_{x=0}^1.
$$
It is easy to see that $B(0; a, b) = 0$ and we are left with
$$
  A
  =
  \frac{k!(n-k)!}{(n+1)!} - B(1;k+1,n-k+1) + B(1;k+2,n-k+1)
  \\
  = B(1;k+2,n-k+1) = B(k+2,n-k+1)
  \\
  = \frac{(k+1)!(n-k)!}{(n+2)!}.
$$</p>

<hr>

<p>[1] <a href=""http://mathworld.wolfram.com/IncompleteBetaFunction.html"" rel=""nofollow noreferrer"">http://mathworld.wolfram.com/IncompleteBetaFunction.html</a></p>
"
"2391873","2392298","<p>Each $X_k$ can be thought of as the sum of $k$ iid copies of a Poisson(1) random variable. Let $\{W_i\}_{i=1}^\infty$ be an iid sequence of Poisson(1) random variables. Then $\sum_{k=1}^n X_k$ is the sum of $1+2+\cdots+n=n(n+1)/2$ iid Poisson(1) random variables. Let $m=n(n+1)/2$. </p>

<p>By the Law of Large Numbers,</p>

<p>$$\frac 1m \sum_{j=1}^m W_j \xrightarrow{d}E[W_1]=1. $$</p>

<p>Now note that because $\lim_{n\to\infty} \frac m{n^2}=\lim_{n\to\infty}\frac{n(n+1)/2}{n^2}=\frac12$</p>

<p>$$Y_n=\frac1{n^2}\sum_{k=1}^n X_n = \frac{m}{n^2}\cdot \frac1m \sum_{j=1}^m W_j\xrightarrow{d}\frac12 \cdot 1=\frac12.$$</p>
"
"2391887","2391890","<p>The more debt Emily has, the less progress she is able to make paying it off per month, since she owes more interest each month.  The fixed point is exactly the point at which the 200 she pays back cancels out the interest she owes, so her balance remains constant.  If she has less debt than the fixed point, then the interest each month is less than what she pays back, so her debt decreases each month and she can eventually pay it off.  But if she has more debt than the fixed point, then she owes more in interest than she pays back each month.  That means each month she ends up owing more than she did the previous month, so her balance only gets worse and worse and she can never pay off the debt.</p>
"
"2391897","2391898","<p>No: suppose that $F$ is a finite field of order $q=p^d$ with $p$ prime. Since $F$ has characteristic $p$, it follows that $px=0$ for all $x\in F$, so every element of the additive group has order at most $p$. In particular the additive group can only be cyclic for $d=1$.</p>
"
"2391900","2392035","<p>Instead of saying that $\left&lt;E_k, E_l \right&gt;$ is equal to the identity matrix, I would write $\left&lt;E_k, E_l \right&gt; = \delta_{kl}$. Since this is a constant function (independent of $(u,v)$), the first three terms in your formula always vanish. Thus, we are left with</p>

<p>$$ \Gamma_{ij}^k = \frac{1}{2} \left( -\left&lt; E_j, [E_i, E_k] \right&gt; -
 \left&lt; E_k, [E_j, E_i] \right&gt; + \left&lt; E_i, [E_k, E_j] \right&gt;\right).$$</p>

<p>As you calculated, we have</p>

<p>$$ [E_1, E_2] = -E_1, [E_2, E_1] = E_1, [E_1, E_1] = [E_2, E_2] = 0. $$</p>

<p>Thus, we get for example</p>

<p>$$ \Gamma_{11}^2 = \frac{1}{2} \left(  -\left&lt; E_1, [E_1, E_2] \right&gt; -
 \left&lt; E_2, [E_1, E_1] \right&gt; + \left&lt; E_1, [E_2, E_1] \right&gt; \right) = \frac{1}{2} \left( - \left&lt; E_1, -E_1 \right&gt; + \left&lt;E_1, E_1 \right&gt; \right) = \left&lt; E_1, E_1 \right&gt; = 1. $$</p>

<p>Thus, your calculation is correct although you missed a sign in the last formula.</p>
"
"2391903","2391920","<p>It seems you just need the following lemma. If $x_n$ weakly converges to $x$ and $||x_n|| \le C$ for all $n$, then $||x|| \le C$.
The proof is that for any continuous linear functional $l \in X^*$ of norm 1, $|l(x)| = \lim_n |l(x_n)| \le \lim_n ||x_n||\cdot ||l|| \le C$, so that $||x|| = ||\hat{x}|| = \sup_{||l|| = 1} |l(x)| \le C$.</p>
"
"2391908","2391980","<p>Hmm, so how can I <em>teach</em> one to see this?</p>

<p>Whenever you are asked to figure out if $A$ is greater than $B $ there are two things you should consider if you don't know what else to do:</p>

<p>1) look at $\frac AB $ and see if it is greater or less than $1$ (but be VERY careful about observing if $B $ is positive or negative or zero.</p>

<p>2) Look at $A-B $ and see if it is greater or less than $0$.</p>

<p>Doing 1) we get $\frac {a^2+b^2}{2ab} $.   (But only if $a $ and both are both non-zero!  And we don't know if $ab $ is positive or negative.) Maybe or maybe not the geometric vs arithmetic mean inequality will jump out at you. ($\frac {x+y}2\ge \sqrt{xy} $ for positive $x,y $)  .... I'll forgive you if it doesn't.  It didn't jump out at me either.  And maybe you haven't learned it yet.</p>

<p>But I'd be remiss if I didn't point out that $\frac {a^2+b^2}2\ge \sqrt {|a^2b^2|}=|ab|\ge ab $ so $\frac {a^2+b^2}{2|ab|} \ge 1$ and $a^2+b^2\ge 2|ab|\ge 2ab $.  I'd be especially remiss as <em>I'm</em> the one who told you to look in the first place.</p>

<p>But that was the hard way and I wasn't actually trying to teach you to see that.  And again, if you haven't learned the AM-GM inequality yet, <em>none</em> of that will make sense.</p>

<p>But what I was <em>really</em> getting at was doing 2).  </p>

<p>We get $a^2+b^2-2ab $.  Now that should <em>really</em> look like something <em>very</em> familiar.  It should <em>scream</em>  $a^2+b^2-2ab=a^2-2ab+b^2=(a-b)^2$.</p>

<p>Now what were we doing?  Oh, yes.  We were trying to see if $a^2+b^2-2ab=(a-b)^2$ was greater or less than  $0$.</p>

<p>Well.... it's a square.  Squares are always greater or equal to $0$.  So $a^2+b^2\ge 2ab $.  And $(a-b)^2=0$ only if $a=b $.  So $a^2+b^2 &gt;2ab $ unless $a=b $ (in which case they are equal...obviously).</p>
"
"2391911","2391925","<p>The likelihood function is $(7\theta)^{-n}$ for $-2\theta &lt;L$ and $5\theta&gt;M,$ otherwise zero. So the maximum likelihood value of theta is the smallest value of $\theta$ satisfying $-\theta &lt;L/2$ and $\theta&gt;M/5.$ Thus it is  $\max\{M/5,-L/2\}.$</p>
"
"2391917","2391923","<p>For both example, you have to know what is the solution that $x_n$ represents.</p>

<p>For the first example, $x_{\color{blue}n} = 2\color{blue}{n}+\color{red}{1}$</p>

<p>Hence $$x_{\color{blue}{n+1}}=2(\color{blue}{n+1})+\color{red}{1}$$</p>

<p>For the second example. $x_{\color{blue}n} = \frac14 \left( \frac15\right)^{\color{blue}n}+\frac14$</p>

<p>Hence $$x_{\color{blue}{n+1}} = \frac14 \left( \frac15\right)^{\color{blue}{n+1}}+\frac14$$</p>

<p>Edit:</p>

<p>\begin{align}x_{\color{blue}{n+1}} &amp;= \frac14 \left( \frac15\right)^{\color{blue}{n+1}}+\frac14 \\
&amp;= \frac14 \frac15\left( \frac15\right)^{n}+\frac14 \\
&amp;=\frac1{20}\left( \frac15\right)^{n}+\frac14\end{align}</p>
"
"2391928","2391937","<p>Te left inequality.</p>

<p>We need to prove that
$$\frac{\sin\alpha_1}{\cos\alpha_1}&lt;\frac{\sin\alpha_1+\cdots+\sin{\alpha_n}}{\cos\alpha_1+\cdots+\cos\alpha_n}$$ or
$$\sin(\alpha_1-\alpha_2)+\cdots+\sin(\alpha_1-\alpha_n)&lt;0,$$
which is obvious.</p>

<p>We can prove the right inequality by the same way.</p>

<p>Finally we'll get there
$$\sin(\alpha_n-\alpha_1)+\cdots+\sin(\alpha_n-\alpha_{n-1})&gt;0.$$
Done!</p>
"
"2391931","2392005","<p>You cannot say $r=\alpha b$ if you know $r$ depends on another variable as well (in this case, $n$). For example, consider that the volume of a cylinder is directly proportional to both the height and the square of the radius. Then it does not follow that $V=\alpha h$ for some constant $\alpha$. Rather, $\alpha$ must itself depend on $r^2$.</p>

<p>For this reason, <em>your</em> $\alpha$ must depend on $n^{-2}$. If we hold $b$ constant, we know $r$ must be directly proportional to $n^{-2}$, so we conclude $\alpha=k\cdot n^{-2}$.</p>
"
"2391941","2391951","<p>Green's theorem says for sufficiently nice functions $M(x,y)$ and $N(x,y)$ and a sufficiently nice region $D$ that $$ \iint_D \left(\frac{\partial M}{\partial x} - \frac{\partial N}{\partial y}\right) \, dA = \oint_{\partial D} N\,dx + M\,dy.$$ So, we can apply this to $M=x$ and $N=0$ to get $$ \iint_D dA = \oint_{\partial D} x\,dy.$$</p>

<p>The other formulae can be gotten by applying it to $M=0$ and $N=-y$ and $M=x/2$ and $N=-y/2.$</p>
"
"2391947","2392848","<p>An algebraic approach: use the same Descartesian coordiante system as yours, then the coordinates: </p>

<p>$$A:(0,\dfrac{49}3),B:(-\dfrac{49}4,0),C:(\dfrac{49}4,0)$$</p>

<p>Let $E: (x,y)$, since $\cos\angle AED=-\dfrac{1}2$ and $\cos\angle BEC=-\dfrac{3}5$, </p>

<p>apply Cosine theorem to $\Delta BEC$ and $\Delta AED$ respectively so that you can obtain the following two equations:</p>

<p>$$\left\{
\begin{array}{rl}
 6 x^2+6 y^2-98 y+\sqrt{\left(9 x^2+(49-3 y)^2\right) \left(x^2+y^2\right)}&amp;=0 \\
80 x^2+80 y^2+3 \sqrt{256 x^4+512 y^2 x^2-76832 x^2+256 y^4+76832 y^2+5764801}&amp;= 12005 \\
\end{array}
\right.$$</p>

<p>Visualize them and solve it (you can use any symbolic software, Maxima, Axiom,  Mupad in matlab, maple, mathematica  and so on):</p>

<p><a href=""https://i.stack.imgur.com/hya7U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hya7U.png"" alt=""enter image description here""></a></p>

<p>there are two solutions to the equations:</p>

<p>$$x=\pm\dfrac{5 \sqrt{3}}{2},y=\frac{11}{2}$$</p>

<p>Which means $E$ can locate at either of the two positions inside $\Delta ABC$.</p>

<p>Then $DE=\sqrt{x^2+y^2}=7$</p>
"
"2391953","2391966","<p>Q0: no, not correct.  The image of $f$ can be written
$$f(\mathbb{R}) = \bigcup_n f([-n,n])$$
which is a countable union of compact (hence closed) sets.  So it is Borel (even $F_\sigma$) and Lebesgue measurable.</p>

<p>This still happens if you replace $\mathbb{R}$ by another Polish space; the continuous image of a Polish space into another Polish space is (by definition) analytic, and all analytic sets in a Polish space are universally measurable (though not necessarily Borel).  The same happens if you only require $f$ to be Borel measurable.</p>

<p>But it is true that the image of $f$ need not equal the support of the pushforward, since the support must be closed and the image need not be.  Example: $f(x) = \arctan x$.</p>

<p>Q1: let $\mu$ be the pushforward of $m$ under $f$ and let $E$ be the support of $\mu$.  The inclusion $E \subset \overline{f(\mathbb{R})}$ is an immediate consequence of definitions.  Conversely, let $U = E^c$.  Then $f^{-1}(U)$ is an open set of Lebesgue measure zero, so it must be empty.  That means $f(\mathbb{R}) \subset E$.  Since $E$ is closed we also have $\overline{f(\mathbb{R})}\subset E$.</p>
"
"2391959","2391965","<p>I trust that you know what it means for a set of vectors to be linearly independent, and what it means for a set of vectors to be linearly dependent. (If not, then <em>that's</em> the question you should be asking)</p>

<p>Now, if $A$ is a matrix, and you have a few solutions of the equation $Ax=0$, then you can ask whether those solutions â those vectors that you have found â are linearly dependent or linearly independent. </p>
"
"2391973","2391976","<p>Your proof is correct.</p>

<p>Also,</p>

<p>$$A \cup B = A \implies B \subseteq A$$</p>

<p>$$A \cap B = A \implies A \subseteq B$$</p>

<p>Hence $A=B$.</p>
"
"2391982","2392000","<p>Let R be a commutative ring, and suppose $I = (a)$ is an idempotent ideal, i.e., $I^2=I$.
<p>
The goal is to show $I$ can be generated by an idempotent element.
<p>
Is that the correct problem?
<p>
If so, the proof is easy . . .
\begin{align*}
&amp;I^2 =I\\[4pt]
\implies\;&amp;(a)^2 =(a)\\[4pt]
\implies\;&amp;(a^2) = (a)\\[4pt]
\implies\;&amp;a = ra^2,\;\text{for some $r \in R$}\\[8pt]
&amp;\!\!\!\!\!\!\!\!\!\!\!\!\!
\text{Claims:}\\[4pt]
&amp;(1)\;\;(a) = (ra)\text{.}\\[4pt]
&amp;(2)\;\;ra\;\text{is idempotent.}\\[8pt]
&amp;\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
\text{Proof of $(1)$:}\\[8pt]
&amp;ra \in (a)\\[4pt]
\implies\;&amp;(ra) \subseteq (a)\\[8pt]
&amp;a = ra^2\\[4pt]
\implies\;&amp;a = a(ra)\\[4pt]
\implies\;&amp;a \in (ra)\\[4pt]
\implies\;&amp;(a) \subseteq (ra)\\[8pt]
&amp;\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
\text{Proof of $(2)$:}\\[8pt]
&amp;(ra)^2 =r(ra^2) = r(a) = ra\\[4pt]
\end{align*}
Therefore the ideal $(a)$ has an idempotent generator, as was to be shown.
<p>
Note: I never used the part of the hypothesis relating to the Jacobson radical.</p>
"
"2391983","2391996","<p>By restriction, $H$ is a representation of the finite group $K$.
The group $K$ has a finite number of isomorphism classes of
irreducible representations $\rho_1,\ldots,\rho_k$. For each $\rho_j$
consider $H_j$, the sum of all $K$-submodules of $H$ isomorphic to
$\rho_i$. The $H_i$ are the $K$-isotypic components of $H$: $H$
is the direct sum of the $H_i$.</p>
"
"2391991","2394928","<p>Write the implicit equation into parametric form:
$$\begin{cases}
x(t)=t\\
y(t)=\pm\arccos(1-\cos t)
\end{cases}\quad t\in [-\dfrac{\pi}2,\dfrac{\pi}2]\tag{1}$$</p>

<p>The curve looks like:</p>

<p><a href=""https://i.stack.imgur.com/7G6jZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7G6jZ.png"" alt=""enter image description here""></a></p>

<p>Use Green's theorem,</p>

<p>$${\rm Area}=4\int_0^{\tfrac{\pi}2}x(t){\rm d}y(t)=4\int_0^{\tfrac{\pi}2}\dfrac{-t \;\sin t\; {\rm d}t}{\sqrt{(2-\cos t) \cos t}}\tag{2}$$</p>

<p>Substitute $u=\cos t$,  $t=\arccos u$:</p>

<p>$${\rm Area}=4\int_0^1 \dfrac{\arccos  u}{\sqrt{2 u-u^2}}{\rm d}u \tag{3}$$</p>

<p>Then calculate (3) in Mathematica you can obtain the desired output:</p>

<blockquote>
  <p>(1/(9 Sqrt[[Pi]]))8 (9 Gamma[3/4]^2 HypergeometricPFQ[{1/4, 1/4, 3/4, 3/4}, {1/2, 5/4, 5/4}, 1/4] + Gamma[5/4]^2 HypergeometricPFQ[{3/4, 3/4, 5/4, 5/4}, {3/2, 7/4, 7/4}, 1/
       4])</p>
</blockquote>
"
"2391994","2392444","<p>Let $(X,\mathcal{U})$ be a uniform space, defined in terms of
entourages, following the axioms as described in <a href=""https://https://en.wikipedia.org/wiki/Uniform_space"" rel=""nofollow noreferrer"">wikipedia</a></p>

<p>First check that $\mathcal{B}_x:= \{V[x]: V \in \mathcal{U}\}$ (which is
non-empty, as $\mathcal{U}$ is non-empty) is a filter base (closed under
finite intersections even):</p>

<ul>
<li>$x \in V[x]$ for every $V \in \mathcal{U}$, as $(x,x) \in \Delta
\subseteq V$. So all $V[x]$ are non-empty. </li>
<li>If $V[x], W[x]$ are in
$\mathcal{B}_x$, so $V,W \in \mathcal{U}$, then  $U = V \cap W\in
\mathcal{U}$. Then $U[x] = V[x] \cap W[x]$, which follows from the
definitions: $y \in U[x]$ iff $(x,y) \in U$ iff $(x,y) \in V$ and $(x,y)
\in W$ iff $y \in V[x]$ and $y \in W[x]$ iff $y \in V[x] \cap W[x]$.</li>
</ul>

<p>The claim on the page should  be that the generated filter</p>

<p>$$\mathcal{U}_x = \{O \subseteq X \mid \exists U \in \mathcal{U}: U[x]
\subseteq O\}$$</p>

<p>is the neighbourhood filter of the topology</p>

<p>$$\mathcal{T} = \{O \subseteq X \mid \forall x \in O: O \in
\mathcal{U}_x\}$$</p>

<p>This $\mathcal{T}$ is the topology induced by the uniformity.<br>
Let's check first that this actually defines a topology:</p>

<ul>
<li>$\emptyset \in \mathcal{T}$ because a $\forall x \in O$ formula always
holds for $O = \emptyset$, it's vacuously true. $X \in \mathcal{T}$
because for every $x \in X$, $X \in \mathcal{U_x}$ because $X$ is a
superset of any member of $\mathcal{B}_x$ (which as said is non-empty).</li>
<li>If $O_1, O_2 \in \mathcal{T}$, it suffices to show that $O_1 \cap O_2
\in \mathcal{T}$: let $x \in O_1 \cap O_2$. As $O_1$ is open, $O_1 \in
\mathcal{U_x}$. Also, because $O_2$ is open, $O_2 \in \mathcal{U}_x$.
This in turn means we have $U \in \mathcal{U}$ with $U[x] \subseteq O_1$
and $V \in \mathcal{U}$ such that $V[x] \subseteq O_2$. We saw at the
top that $(U \cap V)[x] = U[x] \cap V[x] \subseteq O_1 \cap O_2$, and as
$U \cap V \in \mathcal{U}$ we see that $O_1 \cap O_2 \in \mathcal{U}_x$.
As this holds for all $x \in O_1 \cap O_2$, $O_1 \cap O_2 \in
\mathcal{T}$. </li>
<li>If $O_i \in \mathcal{T}$, for $i \in I$, then $O =
\bigcup_i O_i \in \mathcal{T}$: let $x \in O$. Then for some $i_0 \in
I$, $x \in O_{i_0}$, and as the latter set is open, we know that $O_{i_0}
\in \mathcal{U}_x$, and so, as $O_{i_0} \subseteq O$,
so also $O \in \mathcal{U}_x$, and as $x \in O$ was arbitrary, $O \in
\mathcal{T}$ as required.</li>
</ul>

<p>So far, we've used very little of the axioms for a uniformity. To see
that $\mathcal{B}_x$ is actually a local base at $x$ for $\mathcal{T}$, we need a small 
lemma, that will use the missing axioms later on:</p>

<blockquote>
  <p>If $U \in \mathcal{U}$ be symmetrical and suppose $V$ is an entourage such that 
  $V \circ V \subseteq U$, we have that 
  $$\forall y \in V[x] : V[y] \subseteq U[x]$$</p>
</blockquote>

<p>Proof: take $y \in V[x]$ and to see the inclusion pick $z \in V[y]$. We have $(x,y) \in V$
and $(y,z) \in V$ so $(x,z) \in V \circ V \subseteq U$, so $(x,z) \in U$ by symmetry,
and hence $z \in U[x]$; this shows $V[y] \subseteq U[x]$.</p>

<p>This lemma provides a link between neighbourhood systems of different points, while
so far everything has been very ""pointwise"".</p>

<p>Then let $\mathcal{N}_x$ be the neighbourhood filter of $x$ as determined by the topology $\mathcal{T}$: i.e.</p>

<p>$$\mathcal{N}_x = \{N: \exists O \in \mathcal{T}: x \in O \subseteq N\}$$</p>

<p>We want to show that $\mathcal{N}_x = \mathcal{U}_x$ for all $x$.
This would show that the $\mathcal{B}_x = \{U[x]: U \in \mathcal{U}\}$ form a base of neighbourhoods for $\mathcal{T}$ at $x$,
which is what we wanted to show.</p>

<p>Left to right inclusion:
Suppose $N \in \mathcal{N}_x$ so there is $x \in O \subseteq N$. As $O$ is open and $x \in O$ we
have that $O \in \mathcal{U}_x$, and so $N \in \mathcal{U}_x$ as this is a filter. Pretty much by construction.</p>

<p>Right to left:
Let $N \in \mathcal{U}_x$.<br>
Define:
$$O = \{p \in X: N \in \mathcal{U}_p\}$$</p>

<p>Then by definition $x \in O$. Also, $O \subseteq N$ is clear ($y \in O \implies N \in \mathcal{B}_y$ 
and as all members of $\mathcal{B}_y$ contain $y$, $y \in N$ as well).</p>

<p>Let $y \in O$, so that $O \in \mathcal{U}_y$. This says that there is some $U \in \mathcal{U}$
such that $U[y] \subseteq O$. WLOG we can choose $U$ to be symmetrical, otherwise we replace it by 
$U \cap U^{-1} \in \mathcal{U}$, which is symmetrical. (by axioms 3,5). By axiom 4 we then find 
$V \in \mathcal{U}$ such that $V \circ V \subseteq U$.
We then have for all $z \in V[y]$ that $V[z] \subseteq U[y] \subseteq O$, so $O \in \mathcal{U}_z$ 
for all $z \in V[y]$. Hence $V[y] \subseteq O$ by definition.
So in summary </p>

<p>$$\forall y \in O: \exists V \in \mathcal{U}: V[y] \subseteq O$$ </p>

<p>This implies that (by the definition of $\mathcal{U}_x$):</p>

<p>$$\forall y \in O:  O \in \mathcal{U}_x$$</p>

<p>So $O \in \mathcal{T}$ and hence is open. So we have found $O$ open such that $x \in O \subseteq N$, so $N \in \mathcal{N}_x$.
This concludes the proof of the inclusion. </p>

<hr>

<p>We only need one invocation of axiom 4 (the ""halving axiom"") so I don't quite understand the remark about recursive use of this axiom that you mentioned.</p>
"
"2391995","2392361","<p>$p$ is the initial price per dozen</p>

<p>If candies costed $x$ cent less per dozen, the total cost is $\dfrac{x+3}{12}\left(p-x\right)$</p>

<p>If candies costed $x$ cent more per dozen it is $\dfrac{x+3}{12}\left(p+x\right)$</p>

<p>The problem says that in the first case the cost is $3$ cents less so we have the equation</p>

<p>$$\frac{x+3}{12}(p-x)=\frac{x+3}{12}(p+x)-3$$
Least common denominator
$$(x+3) (p-x)=(x+3) (p+x)-36$$
Expand
$$p x+3 p-x^2-3 x=p x+3 p+x^2+3 x-36$$
move everything in the RHS</p>

<p>$2 x^2+6 x-36=0$ simplify dividing all by $2$</p>

<p>$x^2+3x-18=0$ which gives $x_1=-6;\;x_2=3$</p>

<p>The actual solution is $x=3$</p>
"
"2391997","2392008","<p>Hint:
you can use the <a href=""https://en.wikipedia.org/wiki/Chebyshev%27s_sum_inequality"" rel=""nofollow noreferrer"">Chebyshev sum inequality</a> to prove that
$$
\frac{1}{n}\sum_{i=1}^n x_i \cdot \frac{i}{n+1}
\geq \left(\frac{1}{n}\sum_{i=1}^n x_i\right) \cdot \left(\frac{1}{n}\sum_{i=1}^n 
\frac{i}{n+1}\right) = \frac{1}{2n} \sum_{i=1}^n x_i.
$$</p>
"
"2392007","2392022","<p>It's easier to transform the question to a question about matrices. Once you choose a basis for your vector space $V$, a linear map $T$ is represented uniquely by an $n \times n$ matrix $A$ whose entries are elements of $\mathbb{F}_2$ ($0$ or $1$). The linear map will be surjective iff it is injective iff it has full rank. In terms of matrices, this means that the matrices that represent surjective matrices are matrices $A$ for which $\operatorname{rank}(A) = n$. </p>

<p>Let's start with some basic observations. The number of $n \times 1$ column vectors with entries in $\mathbb{F}_2$ is $2^n$ (each entry can be either $0$ or $1$). The number of elements in a vector space of dimension $k$ over $\mathbb{F}_2$ is $2^k$ because once you choose a basis $e_1,\dots,e_k$ for $V$, each element is a unique linear combination $a_1 e_1 + \dots + a_k e_k$ where $a_i \in \mathbb{F}_2$. </p>

<p>A matrix $A \in M_n(\mathbb{F}_2)$ has full rank if and only if its columns are linearly independent. This means that a matrix $A$ has full rank if and only if the first column is non-zero and the $k$-th column of the matrix doesn't belong to the span of the $1,\dots,k-1$ columns for $2 \leq k \leq n$. Having this in mind, we have $2^n - 1$ options for the first column (any non-zero vector in $\mathbb{F}_2^n$), $2^n - 2$ options for the second column (any vector that doesn't belong to the span of the first column), $2^n - 2^2$ options for the third column and so on. Hence, the number of full rank $n \times n$ matrices over $\mathbb{F}_2$ is</p>

<p>$$ (2^n - 1)(2^n - 2) \cdots (2^n - 2^{n-1}).$$</p>
"
"2392017","2392028","<p>I assume you understood how the relation $4xQ(x^2)=xQ(2x)^2+16Q(2x)$ was obtained. Now you just have to substitute $Q(x)=x^nR(x)$.</p>

<p>In particular, $4xQ(x^2)=4x^{2n+1}R(x^2)$, $xQ(2x)^2=2^{2n}x^{2n+1}R(2x)^2$ and $16Q(2x)=2^{n+4}x^nR(2x)$.</p>

<p>After simplifying $x^n$ on both sides, you find $4x^{n+1}R(x^2)=2^{2n}x^{n+1}R(2x)^2+2^{n+4}R(2x)$, which is (the correct form of) (*).</p>
"
"2392023","2392032","<p>Please, use capitals when dealing with sets. It's an unwritten convention.</p>

<p><strong>Claim</strong>: $X^C \cap Y = \emptyset = Y^c \cap X \implies X = Y$</p>

<p><strong>Proof:</strong> We prove the contrapositive: $X \neq Y \implies X^C \cap Y \neq \emptyset \neq Y^c \cap X$</p>

<p>Suppose $X \neq Y$. Then, assume that  $X \not\subset Y$ . Then, there exists $x \in X$ such that $x \notin Y$. So $x \in X$ and $x \in Y^c$, meaning that $x \in Y^c \cap X$. Hence $Y^c \cap X \neq \emptyset$. In the same way, $Y \not\subset X$ leads to $X^c \cap Y \neq \emptyset$. </p>
"
"2392027","2392039","<p>What you are doing is to calculate $\mathbb{E}(X^{\color{red}{2}})$. ItÃ´'s isometry allows you to compute $\mathbb{E}(X^{\color{red}{2}})$; you are interested in $\mathbb{E}(X)$! </p>

<p>It is well-known the stochastic integral $$\int_0^t H_s \, dW_s$$ has expectation zero for <strong>any</strong> (nicely measurable) function $H$ such that $$\mathbb{E} \left( \int_0^t H_s^2 \, ds \right)&lt;\infty$$ for any $t \geq 0$. To prove this, recall that</p>

<p>$$M_t := \int_0^t H_s \, dW_s$$</p>

<p>is a martingale which implies that $$\mathbb{E}(M_t)= \mathbb{E}(M_0)=0, \qquad t \geq 0$$ since martingales have constant expectation.</p>
"
"2392029","2392047","<p>The author means that $\mathbb Q$ as an ordered field is incomplete, i.e. not every Cauchy sequence in $\mathbb Q$ converges in $\mathbb Q$, or equivalently not every nonempty subset of $\mathbb Q$ that is bounded above has a least upper bound in $\mathbb Q$. This makes $\mathbb Q$ ""inadequate"" for many purposes in analysis, where the least-upper-bound property is required. For example, when $a$ is a positive rational number and $n$ a positive integer, you want the equation $x^n=a$ to have a solution; this does not always happen in $\mathbb Q$.</p>

<p>$\mathbb Q$ can be made complete by enlarging it to the set $\mathbb R$ of real numbers. There are two main ways by which this can be achieved: either by considering equivalence classes of Cauchy sequences in $\mathbb Q$, or by means of Dedekind cuts. It can be shown that $\mathbb R$ as a complete ordered field is unique: any two complete ordered fields are isomorphic.</p>
"
"2392044","2392147","<p>For $C^1$ maps $f: \mathbb R^n \to \mathbb R$, it is not the case that for every $x \in \Sigma$, $\epsilon \geq 0$, there is an open ball $B_x$ containing $x$ and an interval $J_x$ such that $f(B_x) \subset J_x$ and $m(J_x) \leq \epsilon m(B_x)$. All we can guarantee is $m(J_x) \leq \epsilon m(B_x)^{ 1 / n}$.</p>
"
"2392061","2392065","<p>They're still triangular numbers. Shift by $1$ like so: $n\mapsto n-1$.</p>
"
"2392062","2392178","<p>Note that $3x^2+2y^2+1=28$ is <strong>not</strong> equivalent to $x^2+y^2=9$.</p>

<p>I suggest to compute the surface integral. To this end let $a$, $b$, $c$ be the semiaxes of the ellipsoid. Then you have a parametric representation of the form
$${\bf r}(\phi,\theta)=\bigl(a\cos\theta\cos\phi,b\cos\theta\sin\phi,c\sin\theta\bigr)\ ,$$
whereby $\phi$ and $\theta$ are GPS coordinates on $S^2$. Now it so happens that $F_1$ is odd in $y$, and $F_2$ is odd in $x$, whereas the normal vector ${\bf r}_\phi\times{\bf r}_\theta$ is even in both $x$ and $y$, by symmetry of the ellipsoid. This implies that from $F\cdot({\bf r}_\phi\times{\bf r}_\theta)$ we only have to compute $$F_3\&gt;({\bf r}_\phi\times{\bf r}_\theta)_3=a^2c\cos^2\theta\sin\theta\cos^2\phi\cdot ab\cos\theta\sin\theta\ .$$
Now you have to integrate this over $0\leq\phi\leq2\pi$ and over $\theta_1\leq\theta\leq{\pi\over2}$, whereby $\theta_1$ is defined by the condition $c\sin\theta_1=1$. Since $\int_0^{2\pi}\cos^2\phi\&gt;d\phi=\pi$ we obtain in this way
$$\Phi=\pi a^3bc \int_{\theta_1}^{\pi/2}\cos^3\theta\sin^2\theta\&gt;d\theta\ .$$
Here the last integral can be written as
$$\int_{\theta_1}^{\pi/2}(\sin^2\theta-\sin^4\theta)\&gt;\cos\theta\&gt;d\theta=\int_{\sin \theta_1}^1(u^2-u^4)\&gt;du\ ,$$
which simplifies matters considerably.</p>
"
"2392084","2392102","<p>You may be locking up because you think it is a ""type problem"" where you need to follow some specific procedure that you don't know what is. But actually it is one where it's a pure numerical coincidence that it can even be solved with the given data.</p>

<p>There were $90+80+70+60=300$ correct answers in total. Since nobody had 4 rights, this means that <em>everybody had exactly 3 rights</em>. (If this hadn't been the case we wouldn't have enough information!)</p>

<p>This means that everybody got exactly one question <em>wrong</em>.</p>

<p>So the ones who won a prize must be those whose wrong answer was either question 1 or question 2. There are $(100-90)+(100-80)$ of those.</p>
"
"2392085","2392131","<p>Yes. Simply separate each sequence into two sequences. If $s$ is a sequence, then we can decompose it to $u$ and $t$ such that $u(n)=s(2n)$ and $t(n)=s(2n+1)$. Namely, taking the even coordinates as the sequence $u$ and the odd coordinates as the sequence $t$.</p>

<p>It is not hard to check that the map $s\mapsto(u,t)$ as defined above is indeed a bijection between $S$ and $S\times S$.</p>
"
"2392086","2392120","<p>Suppose
$$
\sum_{i=1}^n \lambda_if_i=0\tag{*}
$$
with $f_1,\dots,f_n$ pairwise distinct and $n&gt;1$ (the case $n=1$ is already done). Choose $y\in A$ such that $f_1(y)\ne f_n(y)$. Evaluate (*) at $x$ and $yx$:
\begin{align}
&amp;\sum_{i=1}^n\lambda_if_i(x)=0\\
&amp;\sum_{i=1}^n\lambda_if_i(y)f_i(x)=0
\end{align}
Multiply by $f_n(y)$ the first equality and subtract the second one:
$$
\sum_{i=1}^{n-1}\lambda_i(f_n(y)-f_i(y))f_i(x)=0
$$
for all $x$. Can you finish?</p>
"
"2392088","2392101","<p>take $x=e^{-y}$</p>

<p>$$\sum_{n=1}^{\infty}x^{\ln n}=\sum_{n=1}^{\infty}e^{-y\ln n}=\sum_{n=1}^{\infty}\frac{1}{n^y}$$</p>

<p>The last series converges iff $y&gt;1$.</p>

<p>Hence $0&lt;x&lt;\frac{1}{e}$, for which the series converges.</p>
"
"2392090","2392095","<p>Expanding $\det B$ on first column we have that </p>

<p>$$\det B=1\det\begin{pmatrix}1 &amp;c&amp;0\\0&amp;1&amp;c\\0&amp;0&amp;1\end{pmatrix}-c\begin{pmatrix}c &amp;0&amp;0\\1&amp;c&amp;0\\0&amp;1&amp;c\end{pmatrix}=1-c^4$$</p>

<p>The determinant of triangular matrix is the product of the diagonal entries</p>
"
"2392092","2392098","<p>Lemma: $f(k)&gt;k$.  </p>

<p>Pf: it is clear that $f(1)â¥1$ and that $f(1)\neq 1$ so the claim is true for $k=1$.  Inductively suppose it is true up to $k-1$.  Then $f(k-1)&gt;k-1$ so $f(k-1)â¥k$.  Since $f(k)&gt;f(k-1)$ we are done.</p>

<p>Claim:  $f(n)=n+1$</p>

<p>Pf:  Indeed the Lemma shows that $f(n)â¥n+1$.  But $f(f(n))=n+2\implies f(n)â¤n+1$ and we are done.</p>
"
"2392104","2392143","<p>As @Gribouillis mentionned, it is generally a good reflex to try to fit the recurrence equation in the product (or the sum).
Here you have 
$$\prod_{k=1}^n (1+\frac{1}{a_k}) = \prod_{k=1}^n \frac{1+a_k}{a_k} = \prod_{k=1}^n \frac{a_{k+1}}{(k+1)a_k} = \frac{a_{n+1}}{(n+1)!a_1}$$
Let's see if we can find a limit for this. Note $b_n=\frac{a_n}{n!}$. With the recurrence relation, you get, by dividing by $n!$ :
$$b_n=\frac{a_n}{n!} = \frac{1}{(n-1)!}(1+a_{n-1}) = \frac{1}{(n-1)!} + \frac{a_{n-1}}{(n-1)!} = \frac{1}{(n-1)!} + b_{n-1}$$
So
$$b_n=b_1+\sum_{k=2}^n\frac{1}{(k-1)!} = \sum_{k=0}^{n-1}\frac{1}{(k-1)!}$$
and it is well known that $(b_n)$ has limit $e$.</p>
"
"2392114","2392122","<p>You put $$x^3+ax^2+bx+c=\left(y-\frac a3\right)^3+a\left(y-\frac a3\right)^2+b\left(y-\frac a3\right)+c=$$$$=y^3-ay^2+\frac {a^2}3y-\frac {a^3}{27}+ay^2-\frac {2a^2}3y+\frac {a^3}9+by-\frac {ab}3+c=$$and collect terms$$=y^3+3\left(\frac {3b-a^2}9\right)y+\frac{-2a^3-9ab+27c}{27}$$</p>
"
"2392115","2392124","<p>No.</p>

<p>Suppose there were.  Note that $f(0,y)=b(y)$. This means that $b(y)$ must be injective. Let $y_1\neq y_2$.  this implies that $b(y_1)\neq b(y_2)$.   Suppose $ \exists y_0$ such that $a(y_0)\neq 0$. Then we can find $x_i$ such that $f(x_i,y_0)=b(y_i)$ namely $$x_i=\frac {b(y_i)-b(y_0)}{a(y_0)}$$  As $x_1\neq x_2$ they can't both be $0$ so we are done in this case.  Of course, if $a(y)$ is identically $0$ then the problem is trivial so we are done.</p>
"
"2392116","2392121","<p>The probability of rolling no $3$s in $n$ rolls is
$$
\left(\frac56\right)^n
$$
so the probability of rolling at least one $3$ in $n$ rolls is
$$
1-\left(\frac56\right)^n
$$</p>
"
"2392137","2392146","<p>Let's try and guess the answer directly. Note that $u$ is a polynomial in $x,y$ of degree four so $f$ should be a polynomial in $z$ of degree four. Guessing $f = z^4$, we get that</p>

<p>$$ \operatorname{Re}(f) = \operatorname{Re}((x + iy)^4) = x^4 + y^4 - 6x^2y^2 $$</p>

<p>so we got already three terms but missed $4xy$. How can we get $xy$? This is a second degree polynomial so we can try $z^2$. Since</p>

<p>$$ z^2 = (x + iy)^2 = x^2 - y^2 + 2ixy $$</p>

<p>we see that $\operatorname{Re}(-2i z^2) = 4xy$. Hence, if we take $f(z) := z^4 - 2i z^2$, we get 
$$\operatorname{Re}(f) = x^4 + y^4 - 6x^2y^2 + 4xy. $$</p>

<p>Obviously, we can add to $f$ any constant $iC$ where $C \in \mathbb{R}$ without changing the real part or the analyticity of $f$. Now check if this coincides with your result.</p>
"
"2392144","2392166","<p>Your solution is good, here is another approach:
$$\left(\arctan(xy)+\frac{xy-2xy^{2}}{1+x^{2}y^{2}}\right)dx+\left(\frac{x^{2}-2x^{2}y}{1+x^{2}y^{2}}\right)dy=0$$
$$\arctan(xy)dx +\frac{xy}{1+x^{2}y^{2}}dx +\frac{x^{2}}{1+x^{2}y^{2}}dy -\frac{2xy^{2}}{1+x^{2}y^{2}}dx  -\frac{2x^{2}y}{1+x^{2}y^{2}}dy=0$$
$$\arctan(xy)dx +\frac{xy\,dx+x^2\,dy}{1+x^{2}y^{2}}  -\frac{2xy^{2}\,dx+2x^{2}y\,dy}{1+x^{2}y^{2}}=0$$
$$\arctan(xy)dx +x\frac{d(xy)}{1+x^{2}y^{2}}  -\frac{2xy\,d(xy)}{1+(xy)^{2}}=0$$
$$d(x.\arctan(xy))-\frac{2xy\,d(xy)}{1+(xy)^{2}}=0$$
$$x\arctan(xy)-\ln(1+(xy)^{2})+C=0$$</p>
"
"2392145","2392897","<p>As per my comments, having more stocks than observed returns per stocks results in a singular covariance matrix if this matrix is estimated as follows: </p>

<p>Let $r_t$ be a vector of $N$ returns at time $t$. If we estimate $\mathbf{E}[r_tr_t']$ by
$S:=\frac{1}{T}\sum_{t=1}^T r_tr_t'.$
Then any column $i$ of $S$ is
$\frac{1}{T}\sum_{t=1}^T r_tr_{i,t}$
with $r_{i,t}$ being the $i$th element of $r_t$. This means that column $i$ is a linear combination of $T$ vectors. Moreover, all columns are linear combinations of the same $T$ vectors.
Hence, the rank of $S$ cannot exceed $min(T,N)$. And so, if $T&lt;N$ this results in a singular covariance matrix. </p>

<p>This potentially transforms a ""minimization of a quadratic form""-problem into a (partly) linear optimization problem, probably resulting in a (piecewise) linear Markovitz frontier instead of the usual curved once and which may have multiple solutions.</p>

<p>Sidenote, $S$ is the covariance matrix for demeaned return vectors. However, the rank argument holds true for non demeaned returns.</p>
"
"2392154","2392163","<p>If you already have found the formula for how the polynomial should look like it is easy. Note that in your formula $a$ and $b$ have to satisfy no constraints: so take $f(x)$ corresponding to $a=1,b=0$ and $g(x)$ corresponding $a=0,b=1$, then $\{f(x). g(x)\}$ should be a basis i.e. $\{x^2- 4/3, x\}$ for $X$.</p>
"
"2392160","2393619","<p>I found the following paper which defines what I am looking for as a purely non-free group action:</p>

<p><a href=""https://link.springer.com/article/10.1007/s00013-017-1068-6"" rel=""nofollow noreferrer"">https://link.springer.com/article/10.1007/s00013-017-1068-6</a></p>

<p>It contains a construction and well as a lower bound for the genus.</p>

<p>As any group of order $p^2$ is abelian, Theorem 3.4 of the above paper yields the minimal genus of a such a non-cyclic group to be $p(p^2-2p-1)/2+1$.</p>
"
"2392170","2392854","<p>Please refer the book: D. Applebaum, <em>LÃ©vy Processes and Stochastic Calculus</em>, 2nd Ed, Cambridge University Press, 2005. Th.4.4.7, p.251.</p>
"
"2392172","2392306","<p>Let us call a subset $BâA$ <em>valid</em> if two elements of $B$ never give an element of $A$ when added.</p>

<p>A greedy algorithm is to place in $B$ at each step the greatest element $xâA\setminus B$ such that $Bâª\{x\}$ is still valid. Let us prove that this algorithm will always select at least $k+1$ elements.</p>

<p>When we place a new element $x$ in $B$, we mark it and we also mark the elements $y$ of $A$ such that $x+yâA$. Remark that we cannot mark more elements than the number of elements of $A$ greater than $x$.</p>

<p>Let $u_m$ be the maximum number of marked elements before step $m$ (starting at $0$). Then $u_0 = 0$: we start with no element marked. Before step $m$, we have marked at most $u_m$ elements in $A$. So, if $u_m&lt;|A|$, there is a greatest element which is not marked and we can select it. That element have at most $u_m$ elements greater than it, and so we will mark at most $u_m+1$ new elements at this step. We then have $u_{m+1} = u_m + u_m + 1$. This gives $u_m = 2^m-1$.</p>

<p>Since $u_k=2^k-1&lt;2^k$, the algorithm will do at least $k+1$ steps (we start at the step number $0$).</p>
"
"2392179","2392180","<p>Maybe because $x-\sqrt{x^2+1}&lt;0$</p>
"
"2392187","2392219","<p>Let $Y_k$ be the count of daughters for mother $k$ so $Y=\sum_{k=1}^X Y_k$ and $(Y_k)\overset{\text{iid}}\sim\mathcal{Po}(\mu)$</p>

<p>The generating function of a sum of random variables is equal to the product of their generating functions <em>only</em> when the random variables are independent. </p>

<p>$Y=\sum_{k=1}^X Y_k$ means that $Y$ is rather dependent on $X$, so the step $\mathsf G_{X+Y}(t)=\mathsf G_X(t)\,\mathsf G_Y(t)$ is invalid.</p>

<p>Instead, return to the definition of probability generation, and use the identical distribution of all $Y_k$, and the total independence of all $Y_k$ and $X$.</p>

<p>$\begin{align}\mathsf G_Z(t) &amp;= \mathsf E(t^Z) \\ &amp;=\mathsf E(\mathsf E(t^{X+\sum_{k=1}^X Y_k}\mid X)) \\ &amp; =\mathsf E(\phantom{t^{X(1+Y_1)}}) \\ &amp; =\mathsf G_X(\phantom{\mathsf G_{1+Y_1}(t)}) \\ &amp; =\mathsf G_X(t\,\mathsf G_{Y_1}(t)) \\ &amp; = \exp\Big(\lambda \big(t\exp(\mu (t-1))-1\big)\Big)\end{align}$</p>
"
"2392198","2392208","<p>No. Let's take for example $A = (1,\infty)$ and $B = (-1,\infty)$. Consider the function $f(x) = e^{-\frac{1}{x^2}}$ on $A$. Two different smooth extensions to $B$ are given by
$$ g_1(x) = \begin{cases} e^{-\frac{1}{x^2}} &amp; x &gt; 0, \\ 0 &amp; -1 &lt; x \leq 0. \end{cases} $$
and
$$ g_2(x) = \begin{cases} e^{-\frac{1}{x^2}} &amp; x \in (-1,\infty) \setminus \{ 0 \}, \\
0 &amp; x = 0. \end{cases} $$</p>

<p>In fact, you can construct infinitely many other extensions of $f$. This example comes from the theory of <a href=""https://en.wikipedia.org/wiki/Bump_function"" rel=""nofollow noreferrer"">bump functions</a>.</p>
"
"2392203","2394472","<blockquote>
  <p>Explore the relationship between $\partial_P S$ and $\partial_{X_i} \pi_i(S)$ for $i \in \mathbb  N_n.$</p>
</blockquote>

<p>As I have understood, you question is only about a particular problem. </p>

<p>Also,  as I have understood given a point $t\in P$ by $t_i$ you denote $\pi_i(t)$.</p>

<p>The error is here:</p>

<blockquote>
  <p>$~\tau_i(x_i, ~\pi_i~(S)~)=0=~\tau_i(x_i, ~\pi_i^c~(S)~)$</p>
</blockquote>

<p>Whereas the first equality follows from the fact that $\tau_i(x_i, s_{ri})&lt;r$, for the second the situation is different, because if $s_r^c$ be a point in $S^c$ such that $d(x,s^c_r)&lt;r$ then $s_{ri}$ is contained in $\pi_i(S^c)$, but not necessarily in $\pi^c_i(S)$ (which I understood as $\pi_i(S)^c$). The later set may be empty.
As I understood, this possibility is illustrated by the second part of Q 3.11.</p>

<p>The other way around is even worse. Let $n=2$ and $x_i\in\partial_PS$ for each $i$. 
Then for each positive $r$ there exist points $(s_{r1}, s_{r2})$ and $(sâ_{r1}, sâ_{r2})\in P$ such that $\tau_1(x_1, s_{r1})&lt;r$ and $\tau_2(x_2, sâ_{r2})&lt;r$. But this does not imply that there exists a point $(s^*_{r1},s^*_{r2})\in P$ such that $\tau_i(x_i, s^*_{ri})&lt;r$ for each $i$. This possibility is illustrated by the first part of Q 3.11.</p>
"
"2392206","2392269","<p>I will prove the statement by <a href=""https://en.wikipedia.org/wiki/Strong_induction"" rel=""nofollow noreferrer"">strong induction</a>. Of course, the statement is trivial if the degree of the polynomial is $1$ or $2$. Suppose that you have a polynomial $P(x)\in\mathbb{R}[x]$ of degree $n&gt;2$ and assume that every non-constant polynomial in $\mathbb{R}[x]$ whose degree is smaller than $n$ can be written as a product of polynomials with degree $1$ or $2$.</p>

<p>Let $x_0$ be a root of $P(x)$; the Fundamental theorem of Algebra assures that such a root existes. Let us consider two possibilites: $x_0\in\mathbb R$ and $x_0\notin\mathbb R$.</p>

<ul>
<li>Since $x_0\in\mathbb R$ and $x_0$ is a root of $P(x)$, $P(x)$ can be written as $Q(x)(x-x_0)$, with $Q(x)\in\mathbb{R}[x]$. Since the degree of $Q(x)$ is $n-1$, $Q(x)$ can be written as product of polynomials whose degree is $1$ or $2$ and therefore $P(x)$ has the same property.</li>
<li>It happens that $\overline{x_0}$ is a root of $P(x)$ too, because, since the coefficients of $P(x)$ are real, $\overline{P\bigl(\overline{x_0}\bigr)}=P(x_0)=0$ and therefore $P\bigl(\overline{x_0}\bigr)=0$. Since $x_0$ and $\overline{x_0}$ are distinct roots of $P(x)$, $P(x)$ can be written as $Q(x)(x-x_0)\bigl(x-\overline{x_0}\bigr)$. Furthermore,$$(x-x_0)\bigl(x-\overline{x_0}\bigr)=x^2-2\operatorname{Re}(x_0)x+|x_0|^2\in\mathbb{R}[x]$$and this implies that $Q(x)\in\mathbb{R}[x]$. Since the degree of $Q(x)$ is $n-2$, $Q(x)$ can be written as product of polynomials whose degree is $1$ or $2$ and therefore $P(x)$ has the same property.</li>
</ul>
"
"2392210","2393587","<p>I was able to corroborate your result by two independent methods, using Matlab. In the first instance, I developed my own Monte Caro simulation. Using a million points, the fraction $\pi/4$ land in the circular cylinder, I find that $V\approx 0.25513$. (The calculation takes less than 1 second.)</p>

<p>The second method was to create matrix of $(x,y)$ points on the plane and calculate the height of the surface $z=|\ln(x+y)|$. Then I sum the incremental volume elements, $dV=z~dx~dy$. In this way I found the volume to be $V\approx 0.25501$ using an $(x,y)$ array of $2000\times2000.$</p>
"
"2392212","2392231","<p>A set is called ""countable"" if there exists a 1-1 map to naturals.</p>

<p>By taking $$f(n) = n-k$$ as bijection it's obvious that $$\{k,k+1,k+2,\ldots\}$$ is countable.</p>
"
"2392220","2392273","<p>You may notice that
$$ \frac{a_{n+1}}{a_n} \geq \left(1-\frac{1}{n}\right)\left(1+\frac{1}{n^2-n-1}\right)^{-1} \tag{1}$$
hence:
$$ \frac{a_{N+1}}{a_2}\geq \frac{1}{N}\prod_{n=2}^{N}\left(1+\frac{1}{n^2-n-1}\right)^{-1} \tag{2}$$
but the infinite product $\prod_{n\geq 2}\left(1+\frac{1}{n^2-n-1}\right)^{-1}$ is convergent to a positive number ($-\frac{1}{\pi}\cos\frac{\sqrt{5}}{2}\approx 0.296675134743591$). It follows that $a_{N+1}\geq \frac{C}{N}$ so $\sum_{n\geq 2}a_n$ is divergent.</p>
"
"2392222","2392234","<p>Your proof is wrong, but it can be easily fixed. In step one, argue that there is a subsequence $(a_{n_k})_k$ of $(a_n)_n$ for which the conclusion in step $1$ holds. Then this subsequence will be bounded, so we can find a subsequence of this subsequence and make the same conclusions you did.</p>

<p>The reason this sort of argument requires a further subsequence is that just knowing that $(a_n)_n$ doesn't converge to $a$ means only that infinitely many terms of $a_n$ are away from $a$. However, in order to proceed with the proof, we need to have all terms away from $a$. Thus we need to take a subsequence first before proceeding with the proof - which will require a further subsequence. </p>

<p>In fact, something important regarding sequential convergence on topological spaces (or in particular, the real line) is at work here.</p>

<blockquote>
  <p>A sequence converges to $x$ iff every subsequence has a further subsequence that converges to $x$.</p>
</blockquote>

<p><em>Proof:</em>
The forward direction is immediate. For the converse, suppose $x_n\not\to x$. Then there is a subsequence $(x_{n_k})_k$ of $(x_n)_n$ that stays $\varepsilon_0$ away from $x$. Hence no subsequence of $(x_{n_k})_k$ converges to $x$.</p>
"
"2392232","2392260","<p>Probably the easiest way to obtain the result is by looking at $f^2$. The assumption says $f^2$ has constant imaginary part, and that implies $f^2$ is constant (if that's not yet officially known, it's straightforward to deduce it from the Cauchy-Riemann equations). Say $f^2(z) \equiv c$. If $a^2 = c$, then $f(z) \in \{a, -a\}$ for all $z$, and by continuity both, $f^{-1}(a)$ and $f^{-1}(-a)$ are open. Since the domain of $f$ is connected, one of the two preimages must be empty, i.e. $f$ must be constant.</p>

<hr>

<p>We can write the relations you found as</p>

<p>$$\begin{pmatrix} u_x &amp; v_x \\ u_y &amp; v_y \end{pmatrix} \begin{pmatrix} v \\ u\end{pmatrix} = 0.$$</p>

<p>Since $(u,v) \neq 0$, it follows that</p>

<p>$$0 = \det \begin{pmatrix} u_x &amp; v_x \\ u_y &amp; v_y \end{pmatrix} = u_x v_y - u_y v_x,$$</p>

<p>and by the Cauchy-Riemann equations, that determinant is $u_x^2 + u_y^2 = v_x^2 + v_y^2$. A sum of squares of real numbers is only zero if all the numbers are zero, so that implies $u_x = u_y = v_x = v_y = 0$, hence $f$ is constant.</p>
"
"2392233","2392267","<p>In $|z+1|\le |z|+1$, equality only holds when $z$ is <em>real and positive</em> (i.e. collinear with $1$ and on the same side), which never occurs ! Hence the bound isn't tight.</p>
"
"2392236","2392252","<p>Consider the sequence of continuous functions $(f_n)_{n\geq 1}$, where
$$f_n(x):=\begin{cases}
1/n &amp;\mbox{if $|x|\leq n$}\\
\frac{(n+1)-|x|}{n} &amp;\mbox{if $n\leq |x|\leq n+1$}\\
0   &amp;\mbox{if $|x|\geq n+1$}.\\
\end{cases}$$
which converges to $0$ uniformly in $\mathbb{R}$. Then
$$\int_{-\infty}^{\infty}f_n(x)\,dx\ge 2&gt;0=\int_{-\infty}^{\infty}f(x)\, dx.$$</p>
"
"2392237","2392270","<p>The exponential distribution is irrelevant. The independence assumption, which is relevant, is missing. </p>

<p>The following fact follows from Fubini.</p>

<blockquote>
  <p><strong>Fact</strong> For independent $X_1,X_2$ and measurable (say, bounded) $f$,
  $$
\mathrm{E} [f(X_1,X_2)] = \mathrm{E}[\mathrm{E}[f(x,X_2)]|_{x=X_1}].
$$</p>
</blockquote>

<p>In our case, for any measurable $A$,
$$
\mathrm{E} [\mathbf{1}_{X_1&lt;X_2} \mathbf{1}_{X_1\in A}] = \mathrm{E} [\mathrm{E} [\mathbf{1}_{x&lt;X_2}]|_{x=X_1} \mathbf{1}_{X_1\in A}] = \mathrm{E} [\mathrm{P}(x&lt;X_2)|_{x=X_1} \mathbf{1}_{X_1\in A}],
$$
whence <em>by definition</em>
$$
\mathrm{P} [{X_1&lt;X_2}\mid X_1 = x] = \mathrm{E} [\mathbf{1}_{X_1&lt;X_2}\mid X_1 = x] = \mathrm{P}(x&lt;X_2).
$$</p>
"
"2392253","2392312","<ul>
<li><p>Under condition $F_nC_i$ we are dealing with the $i$-th coin. Also there have been $n$ consecutive flips allready resulting in $n$ heads but that is not relevant when it comes to the $n+1$-th flip (a coin has no memory). The only thing that counts is that you flip a coin that gives you a probability of $\frac{i}{k}$ on heads. That explains $P(H\mid F_nC_i)=P(H\mid C_i)=\frac{i}{k}$.</p></li>
<li><p>Note that $P(H\mid F_n)$ is <em>not</em> depending on index $i$ while your suggested $\frac{i}{k}$ <em>does</em> depend on $i$. That explains why assumption $P(H\mid F_n)=\frac{i}{k}$ is absurd.</p></li>
</ul>
"
"2392256","2392516","<p>Let's take a quick tour on coding theory. </p>

<p>A <a href=""https://en.wikipedia.org/wiki/Generator_matrix"" rel=""nofollow noreferrer"">generator matrix</a> is a matrix whose rows form a basis for a linear code.</p>

<p>Let's inspect $2$nd, $4$th, and $6$th row of the generator matrix, they are $\begin{bmatrix}  1&amp; 0 &amp; 0\end{bmatrix},\begin{bmatrix}  0&amp; 1 &amp; 0\end{bmatrix}$, and $\begin{bmatrix}  0&amp; 0 &amp; 1\end{bmatrix}.$</p>

<p>Hence the codewords are every element of $F_2^3$.</p>

<p>A <a href=""https://en.wikipedia.org/wiki/Parity-check_matrix"" rel=""nofollow noreferrer"">check matrix</a> maps all codewords and only the codewords to the zero vector. To map every element of $F_2^3$ to the zero vector,  the corresponding parity check matrix has to be the zero matrix.</p>

<p>Edit:</p>

<p>As OP uses $6$-bit codeword, the convention is different. Hence one strategy would be take your matrix $G$ and take its transpose, $G^T$. Perform row operations to reduce it to its RREF form, that is in the form of $(I|P)$. After which, the parity check matrix is of the form of $(P^T|I)$</p>
"
"2392261","2392276","<p>If $f$ is convex, then
$$f((1-t)a+tb)\le (1-t)f(a)+tf(b)$$
for $0\le t\le 1$. Therefore
$$\int_0^1 f((1-t)a+tb)\, dt\le \int_0^1\left((1-t)f(a)+tf(b)\right)\,dt.$$</p>
"
"2392264","2392268","<p><strong>Hint</strong>: Let's call the matrix</p>

<p>$$ A = \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}. $$</p>

<p>Then we must have</p>

<p>$$ \begin{pmatrix} F_{k + 2} \\ F_{k + 1} \end{pmatrix} = \begin{pmatrix} a F_{k+1} + b F_{k} \\ c F_{k+1} + d F_{k} \end{pmatrix}. $$</p>

<p>Can you choose $a,b,c,d$ so that the equality above will hold?</p>
"
"2392266","2392284","<p>If $M = I$ then $B_M$ is just the closed unit ball. When $E$ is infinite dimensional, it is well known that the closed unit ball is not compact. When $E$ is finite dimensional, the compact sets of $E$ are precisely the closed and bounded sets (with respect to the distance induced by $\| \cdot \|$). For arbitrary $M$, your set is closed but not necessarily bounded so it won't generally be compact. For the finite dimensional case, let's characterize completely when $B_M$ is compact.</p>

<p><strong>Claim</strong>: The set $B_M$ is compact if and only if $\left&lt; Mv, v \right&gt; = 0$ implies that $v = 0$.</p>

<p><strong>Proof</strong>: Assume that $B_M$ is compact and let $v \in V$ such that $\left&lt; Mv, v \right&gt; = 0$. In particular, $v \in B_M$. But then $nv$ also belongs to $B_M$ because
$$\left&lt; M(nv), nv \right&gt; = n^2 \left&lt; Mv, v \right&gt; = 0. $$
Since $B_M$ is compact, it is bounded so there exists some $C &gt; 0$ such that $\| nv \| = n \| v \| \leq C$ for all $n \in \mathbb{N}$. This implies that $\| v \| \leq \frac{C}{n}$ for all $n \in \mathbb{N}$ so $v = 0$. On the other hand, assume that $B_M$ is not compact. Since it is always closed, we must have an unbounded sequence $v_n$ of vectors in $B_M$. Set $w_n = \frac{v_n}{\| v_n \|}$. Since the $\| \cdot \|$ unit sphere is compact and $\| w_n \| = 1$, it must have a convergent subsequence which we will still denote by $w_n$ which converges to $w$. Since $\| w_n \| = 1$ we also have $\| w \| = 1$ and in particular $w \neq 0$. Since $v_n \in B_M$ we have
$$ 0 \leq \left&lt; Mw_n, w_n \right&gt; = \frac{\left&lt; Mv_n, v_n \right&gt;}{\| v_n \|^2} \leq \frac{1}{\| v_n \|^2} \to 0 $$
so by continuity of the inner product, we have $\left&lt; Mw_n, w_n \right&gt; \to \left&lt; Mw, w \right&gt; = 0$.</p>

<p>When $\mathbb{K} = \mathbb{C}$, we have that $B_M$ is compact if and only if $M$ is invertible. The reason is that the condition $\left&lt; Mv, v \right&gt; \geq 0$ for all $v \in V$ implies that $M$ is Hermitian and a Hermitian operator which satisfies $\left&lt; Mv, v \right&gt; &gt; 0$ for all $v \neq 0$ must be invertible. When $\mathbb{K} = \mathbb{R}$, $M$ can be invertible with $B_M$ non-compact. For example, if $V = \mathbb{R}^2$ and $M$ is the rotation by $\frac{\pi}{2}$ degrees then $\left&lt; Mv, v \right&gt; = 0$ for all $v \in V$ so $B_M = \mathbb{R}^2$ and $M$ is invertible. In fact, one can show that $B_M$ is compact if and only if the symmetrization $M + M^T$ of $M$ is invertible.</p>
"
"2392289","2392294","<p>Your idea is great. First, you shouldn't call the two matrices that lie in the subspace $A$ and $B$ because the name $B$ is already taken. If you take two matrices $A_1,A_2$ in this set so that $A_1B = BA_1$ and $A_2B = BA_2$ and $a,b \in \mathbb{R}$ then</p>

<p>$$ (aA_1 + bA_2)B = aA_1B + bA_2B = aBA_1 + bBA_2 = B(aA_1 + bA_2) $$</p>

<p>which shows that this set is closed under addition and scalar multiplication (and it is clearly non-empty) so this is a vector subspace.</p>
"
"2392304","2392325","<p>Let $A=K[[t]]$, which satisfies your condition, since it has only one maximal ideal generated by $t$.  Then in $A[x]$, $xt-1$ generates a maximal ideal and the quotient is $K((t))$.</p>
"
"2392308","2392342","<p>$s$ is the side of the small square and $L$ is the side of the large square.  You want $L=s(\cos \theta + \sin \theta)$</p>
"
"2392309","2392315","<p>Yes, $(C,d)$ is a metric space. And, yes, for points in $C$, the open balls are intersection of usual open balls (in $\mathbb{R}^2$) with $C$.</p>
"
"2392313","2392653","<p>Indeed, the literal negative answer to the first question includes the standard counter-example as given by @JoseCarlosSantos.
Still, modifying the question/example somewhat in various ways gives a more positive answer. First, if we insist on ""algebraic"" representations (in some sense...) this could be arranged to exclude the counter-example.</p>

<p>Or, for example, instead of $G=GL_2$, trying $G=SL_2$ avoids the counter-example.</p>

<p>In either style of avoiding that (and related) counter-examples, H. Weyl's ""unitarian trick"" tries to arrange that a finite-dimensional repn (under some hypotheses...) of a reductive (linear...) real Lie group can be <em>complexified</em>, and then restricted to a repn of the compact real form, thus reducing many questions to the repn theory of compact Lie groups.</p>

<p>Thus, under mild (but non-trivial) hypotheses such repns are completely reducible, and so on. But you are right, that they don't have invariant inner products for the (original) non-compact group, although they do have such for the compact real form, which enables Weyl's proofs (under various suitable hypotheses to make the argument possible).</p>

<p>And, yes, then a classification of (""algebraic""?) irreducibles by highest weights succeeds.</p>
"
"2392318","2392371","<p>Differentiate $x=2uv$ and $y=u^2 - v^2$ each with respect to $x$ and with respect to $y$, to get four equations:</p>

<p>$$\begin{cases} u_x v + uv_x = 0.5 \ \\ uu_y - vv_y = 0.5  \\ u_yv + uv_y = 0 \\ uu_x - vv_x = 0 \end{cases}$$</p>

<p>Multiply the last equation by $v$ to get: $(vu_x)u = v^2 v_x$, and substitute $vu_x = 0.5 - uv_x$ to get:</p>

<p>$$v_x = \frac{u}{2(u^2 + v^2)}$$</p>

<p>which gives (using the last equation once more):</p>

<p>$$u_x = \frac{v}{2(u^2+v^2)}$$</p>

<p>Doing the same thing to the other equations we get:</p>

<p>$$v_y = -\frac{v}{2(u^2 + v^2)}$$</p>

<p>and:</p>

<p>$$u_y = \frac{u}{2(u^2+v^2)}$$</p>

<p>It's then clear that:</p>

<p>$$\|\nabla u\|^2 + \|\nabla v\|^2 = \frac{0.5}{(u^2 + v^2)}$$</p>

<p>notice that:</p>

<p>$$x^2 + y^2 = u^4 + v^4 + 2u^2v^2 = (u^2 + v^2)^2, \therefore \sqrt{x^2 + y^2} = u^2 +v^2$$</p>

<p>so we get the desired equation.</p>
"
"2392321","2392335","<p>""for all subsets $A$ of $S$.."" </p>

<p>That actually reveals that $\mathbb P$ is <strong>not</strong> a function $S\to[0,1]$ (as you stated) but a function $\wp(S)\to[0,1]$, and this collection $\wp(S)$ is in this context a $\sigma$-field. </p>

<p>Formally we are dealing with <a href=""https://en.wikipedia.org/wiki/Probability_space#Definition"" rel=""nofollow noreferrer"">probability space</a> $\langle S,\wp(S),\mathbb P\rangle$.</p>

<p>Function $\mathbb P$ is not unique in general. </p>

<p>In non-trivial case there is a distinct function $\mathbb Q:\wp(S)\to[0,1]$ having the same properties (or fulfilling the same axioms of probability in your terminology). </p>

<p>To convince yourself try to find two distinct probabilities on  <a href=""http://mathworld.wolfram.com/MeasurableSpace.html"" rel=""nofollow noreferrer"">measurable space</a> $\langle\{0,1\},\wp(\{0,1\})\rangle$.</p>
"
"2392341","2392369","<p>Note that
$$|\alpha'(t)|^2=\left|\left(1,\frac{d}{dt}\sqrt{4t-2t^2},-1\right)\right|^2=1+\frac{2(1-t)^2}{t(2-t)}+1=\frac{2}{{t(2-t)}}.$$
Hence the total mass of the wire can be evaluate by the following line integral
\begin{align*}
\int_{t=0}^2x(t)y(t)|\alpha'(t)|\,dt=\sqrt{2}\int_0^2t\sqrt{t(2-t)}\,\frac{\sqrt{2}}{\sqrt{t(2-t)}}\,dt=2\int_0^2 tdt=4.
\end{align*}</p>
"
"2392344","2392362","<p>If the curve was contained on a plane $p$, let $(a,b,c)$ be a non-zero vector orthogonal to $p$. Then all vectors $c'(t)$ would be orthogonal to $(a,b,c)$ too. But the method that you described shows that there is not vector $(a,b,c)$ such that$$(\forall t\in\mathbb{R}):c'(t).(a,b,c)=0.\tag{1}$$Therefore, the curve is not contained in a plane.</p>

<p>There is nothing wrong with choosing random points to prove that $(1)$ is not true. The method that you described consists in supposing that such a vector exits and by, by carefully choosing certain values of $t$, to reach a contradiction. It's as if I asked: is the graph of $x\mapsto x^2$ a straight line? No, because it contains $(0,0)$, $(1,1)$ and $(2,4)$ and no straight line contains these points.</p>
"
"2392347","2392365","<p>I often find it helpful to rewrite equations like $$y^2=x^2$$ as $$y^2-x^2=(y+x)(y-x)=0$$</p>

<p>Then one of the two factors must be zero i.e. $y+x=0$ or $y-x=0$ that is $y=-x$ or $y=x$, which we express as $y=\pm x$</p>

<p>If instead we take the square root directly with the convention that the principal value of the square root is positive we get $y=|x|$. Of course we know that there is also a negative solution $y=-|x|$, and if we want both we put $y=\pm |x|$.</p>

<p>These two ways of addressing the problem give two different expressions for the answer, but the answers are two different ways of expressing the same thing. If $x\gt 0$ we have $x=|x|, -x=-|x|$ and if $x\lt 0$ it is $x=-|x|, -x=|x|$. </p>

<p>If $y=x=0$ we have just one value. But that special case clearly doesn't cause a problem.</p>
"
"2392348","2392436","<p>If $\vec b = \vec 0$, then $A\vec a = B\vec b$ is solveable if and only if $A\vec a = 0$, and in this case any matrix $B$ works.</p>

<p>On the other hand if $\vec b\neq \vec 0$ then a solution is always given by</p>

<p>$$ B = A\frac{\;\;\vec a \cdot \vec {b}^T}{\vec b^T \cdot \vec b} $$</p>
"
"2392350","2392364","<p>You're pretty much there and applied the right idea.</p>

<p>The GM telescopes. It's ${\left(\frac{2}{1}.\frac{3}{2}.\frac{4}{3}...\frac{n+1}{n}\right)}^{\frac{1}{n}}=(n+1)^{\frac{1}{n}}$.</p>

<p>Presumably $n&gt;1$.</p>

<p>So therefore (noting equality cannot hold as the terms are not equal)</p>

<p>$$\frac{n+s}{n}\gt (n+1)^{\frac{1}{n}}$$
$\implies$
$$n+s\gt n(n+1)^{\frac{1}{n}}$$
The LHS and RHS are positive, so we can exponentiate both sides by $n$.
$$(n+s)^n\gt n^n(n+1)$$</p>
"
"2392356","2392423","<p>You want to compare:
$$\log_7 47 \ \ or \ \ \log_{14} 94.$$
Express both sides as power of $14$:
$$(2\cdot 7)^{\log_7 47} \ \ or \ \ 14^{\log_{14} 94} \iff $$
$$2^{\log_7 47}\cdot 47 \ \ or \ \  94 \iff$$
$$2^{\log_7 47} \ \ or \ \ 2^1 \iff$$
$$\log_7 47 \ \ or \ \ 1 \ \iff$$
$$47&gt;7^1.$$</p>
"
"2392374","2392384","<p>Let $T &gt; 0$ be a period of $f$, and let $C := \int_0^T f(x)\, dx$. By the assumptions on $f$ we have that $C &gt; 0$.</p>

<p>Let $n\in\mathbb{N}$ and let us compute
$$
\int_1^{1+nT} \frac{f(x)}{x}\, dx =
\sum_{k=1}^n \int_{1+(k-1)T}^{1+kT} \frac{f(x)}{x}\, dx
\geq \sum_{k=1}^n \int_{1+(k-1)T}^{1+kT} \frac{f(x)}{1+(k-1)T}\, dx
= \sum_{k=1}^n \frac{C}{1+(k-1)T}.
$$
As $n\to +\infty$, the last term goes to $+\infty$ (since the corresponding series is divergent).</p>
"
"2392379","2392393","<p>We have $$p^2(1+q)^2-4\left(1-q+\frac{p^2}{2}\right)\left(q^2-q+\frac{p^2}{2}\right)=0$$ or
$$(p^2-4q)(p^2+q^2-2q+1)=0,$$
which gives $p^2=4q$ because  $p^2+(q-1)^2=0$ gives $p=0$ and $q=1$</p>

<p>and from  here $1-q+\frac{p^2}{2}=0$,  which is impossible.</p>

<p>Done!</p>
"
"2392386","2392396","<p>Given $\epsilon&gt;0$, we look for $\delta $ such that</p>

<p>$$0 &lt;|x-1|&lt;\delta\implies |1-\frac {2}{x}+1|&lt;\epsilon $$</p>

<p>or
$$0 &lt;|x-1|&lt;\delta\implies 2|\frac {x-1}{x}|&lt;\epsilon $$</p>

<p>As $x $ goes to $1$, we can suppose that $x $ is not far from $1$, for example we can assume that $$|x-1|&lt;\color {red}{\frac {1}{2}} $$ or
$$\frac {1}{2}&lt;x &lt;\frac {3}{2}$$</p>

<p>and $$\frac {2}{3}&lt;\frac {1}{x}&lt;2$$</p>

<p>With this additional condition, we will look for $\delta&gt;0$ such that
$$|x-1|&lt;\delta\implies 2|\frac{x-1}{x}|&lt;4|x-1|&lt;\epsilon $$</p>

<p>So you take
$$\delta=\min (\color {red}{\frac {1}{2}},\frac {\epsilon}{4}) $$</p>
"
"2392390","2392578","<p>Let $E$ be the set of discontinuities of $f$. Since $M\setminus E$ is measurable, <a href=""https://math.stackexchange.com/q/1011310"">there exists a sequence of compact sets</a> $K_j$ such that 
$$
M\setminus E = N\cup \bigcup_{j=1}^\infty K_j
$$
where $N$ has measure zero. </p>

<p>Fix $j$. Cover $K_j$ by open sets $U_{k}$ such that $\operatorname{diam} f(U_{k})&lt;1/j$ for every $k$; this is possible since $f$ is continuous on $K_j$. By compactness, we only need finitely many such sets. Pick $y_{k}\in f(U_k)$ and define 
$$
f_j = y_1\chi_{U_1} +y_2\chi_{U_2\setminus U_1} + y_3\chi_{U_3\setminus U_2\setminus U_1} + \cdots
$$
Also let $f_j=0$ on $M\setminus K_j$. Observe that $f_j$ is a step function and that $|f-f_j|&lt;1/j$ on $K_j$. </p>

<p>For every point $x\in \bigcup_{j=1}^\infty K_j$ we have $f_j(x)\to f(x)$ by construction. The rest of $M$ has measure zero. $\quad\Box$</p>

<p>Didn't need the separability of $Y$.</p>
"
"2392394","2396495","<p>There has to be a typo somewhere, either in the paper, or in your code.</p>

<p>I tried to reconstruct the Lyapunov function (using YALMIP) and it is not possible to get that Lyapunov function.</p>

<p>First try where I try to find coefficients in the Lyapunov function as close as possible to the claimed data. The closest solution is far from the claimed</p>

<pre><code>sdpvar x1 x2 u1 u2;

f = [x2; u1*u2 - 10*u1; x2*u2; -x2*u1];

c1 = sdpvar(5,1);
c2 = sdpvar(4,1);    
V = c1'*[1; u2; u2^2; u1^2; x2^2];
s1 = c2'*[x1^2; x2^2; u1^2; u2^2]
g1 = u1^2 + u2^2 - 1;

Model = [sos(V-0.1*(1-u2)-0.1*x2^2) + sos(-jacobian(V,[x1;x2;u1;u2])*f + s1*g1)];    
Model = [Model, sum(c1(1:3)) == 0]; 
optimize(Model,norm(c1 - [4.8931;-6.689;1.7959;1.4615;0.33445]),[],[c1;c2])
value(c1)
</code></pre>

<p>The closest I can get is quickly found by solving a mixed-integer program (YALMIP has a built-in framework for MISDP) where I try to maximize the number of matching coefficients</p>

<pre><code>fits = binvar(5,1);
Model = [sos(V-0.1*(1-u2)-0.1*x2^2) + sos(-jacobian(V,[x1;x2;u1;u2])*f + s1*g1)];
Model = [Model, sum(c1(1:3)) == 0]; 
Model = [Model, implies(fits, c1 == [4.8931;-6.689;1.7959;1.4615;0.33445]),-100 &lt;= c1 &lt;= 100];
optimize(Model,-sum(fits),[],[c1;c2])
value(c1)
</code></pre>

<p>2 of the 5 coefficients match</p>

<p>Hence, my summary would be that the reported coefficients are wrong, or reported with to few significant digits.</p>

<p>Indeed, the following is infeasible</p>

<pre><code>Vclaimed = 0.33445*x2^2 + 1.4615*u1^2 + 1.7959*u2^2 - 6.689*u2 + 4.8931;
Model = [sos(Vclaimed -0.1*(1-u2)-0.1*x2^2) + sos(-jacobian(Vclaimed ,[x1;x2;u1;u2])*f + s1*g1)];
optimize(Model,[],[],c2)
</code></pre>

<p>The fact that you get a completely different solution sounds natural as you're not minimizing any objective, and the solution to the problem is not unique.</p>

<p>To compute a verifiable solution, we can try to find an integer or rational solution. An integer solution is not feasible, but we can search for a rational solution.</p>

<pre><code>Model = [sos(V-0.1*(1-u2)-0.1*x2^2) + sos(-jacobian(V,[x1;x2;u1;u2])*f+s1*g1)];    
aux = sdpvar(5,1);
Model = [Model, sum(c1(1:3)) == 0,integer(aux), c1 == aux/20]
optimize(Model,[],[],[c1;c2])
format long 
value(c1)
</code></pre>

<p>The solution we are interested in is the rational solution which is given by aux/20 (c1 will have errors from solver tolerances in the equality). We can compute the value of $\dot{V}$ in some random points and see that it is 0 everywhere</p>

<pre><code>Vdot = replace(-jacobian(V,[x1;x2;u1;u2])*f,c1,value(aux)/20)
for i = 1:100
    x = randn(2,1);
    assign([x1 x2 u1 u2], [x(1) x(2) sin(x(1)) cos(x(1))]);
    value(Vdot)
end
</code></pre>
"
"2392405","2392407","<p>You answer is not true, but the start is right.</p>

<p>$$\frac{x}{1-x}&gt;0$$ or
$$0&lt;x&lt;1$$
by the intervals method.</p>
"
"2392411","2392415","<p>First, exponents do not distribute over addition. To start with the simplest example, $(a+b)^2=(a+b)(a+b)$. Applying the distributive rule, we see that this is the same as $a(a+b)+b(a+b) = a^2 + ab + ab + b^2$, which is different from $a^2+b^2$. This can be seen geometrically, too: A square built on a side of length $a+b$ has greater area than the square with side length $a$, combined with the square with side length $b$.</p>

<p>You can also see your result if you consider order of operations, and substitute an actual number for $x$. Let's consider $x=3$. Then we have:</p>

<p>$$(x+5)^0=(3+5)^0=8^0=1,$$</p>

<p>because Parentheses come before Exponents in PEMDAS.</p>
"
"2392416","2392472","<p>Hint: If $\mathbf{v}$ is a unit-norm eigen vector of a matrix $\mathbf{A}$ corresponding to the eigen value of $\lambda$ then $-\mathbf{v}$ is also a unit-norm eigenvector of $\mathbf{A}$. In particular, $$\mathbf{A}\mathbf{v}=\lambda \mathbf{v} \implies \mathbf{A}(-\mathbf{v})=\lambda(-\mathbf{v}).$$</p>
"
"2392427","2392431","<p>Hint. Note that for $N&gt;1$,
$$0&lt;\int_{N}^{e^N} xe^{-x^{2017}}dx&lt; \int_{N}^{+\infty} xe^{-x^{2017}}dx\leq \int_{N}^{+\infty} xe^{-x}dx=(N+1)e^{-N}$$
then use the Squeeze Theorem.</p>
"
"2392430","2392442","<p>For $t&gt;0$:
$$\frac{t-1}t\leq\ln t\leq t-1$$
so
$$\int_x^{x^2}\frac{1}{t-1}dt\leq\int_x^{x^2} \frac{1}{\ln t}\,dt\leq \int_x^{x^2}\dfrac{t}{t-1}dt$$
with limit $x\to1$ you have answer $\ln2$.</p>
"
"2392439","2392446","<p>Sufficiently close simply means there is some $\delta &gt; 0$ such that the condition holds for $x \in (c - \delta, c + \delta) \setminus \{ c \}$.</p>

<p>To get a feel for what this means visually, look at the graphs of $x$ and $x^2$ around the origin. Clearly $x^2 &lt; x$ sufficiently close to $0$, and so we say $x^2 = O(x)$. However, notice that $\frac{1}{2}x$ is also 'lower than' $x$, so it too is $O(x)$. The function $2x$, however, fits the definition too, and yet it is <em>larger</em> than $x$ near $0$. What's going on? The idea is that $2x$ isn't approaching $0$ faster in any 'fundamental way' than $x$ is; up to constants, they approach $0$ equally fast. The same is not true for $x^2$, however: no matter how large you scale up $x^2$, making it $C x^2$ for some really large $C$, you will always be able to zoom in enough to find that $Cx^2 &lt; x$ around $0$. Hence it doesn't just approach $0$ 'not slower' than $x$, but <em>faster</em> even. This is noted by a lowercase $o$: $x^2 = o(x)$. Rigorously, the small $o$ simply requires that the limit in the defintion of $O$ be zero, not just finite.</p>
"
"2392440","2392465","<p>If you're in Paris, then you're in Europe. Even more particularly, you're in France. However, ""if you're not in Europe, then you're not in Paris,"" is still a valid application of the contrapositive, even though we've ""forgotten"" the information about being in France.</p>

<p>""You're in Paris."" $\longleftrightarrow  x$ is even.</p>

<p>""You're in Europe."" $\longleftrightarrow  x^2$ is even.</p>

<p>""You're in France."" $\longleftrightarrow  x^2$ is a multiple of $4$.</p>
"
"2392451","2392804","<p>To answer your first question: It appears you have already recognized that
$$\psi:=\phi_x\quad\&amp;\quad\rho:=\phi_t\\\implies\psi_x=\phi_{xx}\quad\&amp;\quad\rho_t=\phi_{tt}\\\implies\psi_x=\rho_t\qquad(*)$$
To obtain the second equation, we need to factor the operator from the original PDE.
$$\begin{align}\phi_{tt}-\phi_{xx}&amp;=\left(\partial_t^2-\partial_x^2\right)\phi=\left(\partial_t-\partial_x\right)\left(\partial_t+\partial_x\right)\phi\\&amp;=\left(\partial_t-\partial_x\right)\left(\psi+\rho\right)=\psi_t+\rho_t-\psi_x-\rho_x=0\end{align}$$
We know from $(*)$ that $\psi_x-\rho_t=0$, thus
$$\psi_t-\rho_x=\psi_x-\rho_t=0\implies\psi_t=\rho_x$$</p>

<p>To answer your second question: Because we modified the PDE of a single function to be a system with 2 functions, we need twice the number of conditions to make sure we properly satisfy the original PDE. In order to obtain these boundary conditions, we need to assume smoothness in $\phi$, meaning
$$\phi_{xt}=\phi_{tx}$$
on the closure. Now take derivatives of the radiation conditions:
$$\partial_t\left(\phi_t-\phi_x\right)=\phi_{tt}-\phi_{tx}=\rho_t-\rho_x=0\\\implies\rho_t(0,t)=\rho_x(0,t)$$
$$\partial_x\left(\phi_t-\phi_x\right)=\phi_{tx}-\phi_{xx}=\psi_t-\psi_x=0\\\implies\psi_t(0,t)=\psi_x(0,t)$$
$$\partial_t\left(\phi_t+\phi_x\right)=\phi_{tt}+\phi_{tx}=\rho_t+\rho_x=0\\\implies\rho_t(1,t)=-\rho_x(1,t)$$
$$\partial_x\left(\phi_t+\phi_x\right)=\phi_{tx}+\phi_{xx}=\psi_t+\psi_x=0\\\implies\psi_t(1,t)=-\psi_x(1,t)$$
This is not my favorite, but one of many approaches to solve the wave equation.</p>
"
"2392452","2392456","<p>From  Robin we have an unconditional result (1984) that says that for $n \geq 13,$ we have the bound
$$ \frac{\sigma(n)}{n} &lt; \; e^\gamma \log \log n + \frac{0.64821364942...}{\log \log n},$$ with the constant in the numerator giving equality for $n=12.$ Note
$$  e^\gamma  = 1.7810724179901979852365\ldots  $$ </p>

<p>This is stated in <a href=""http://projecteuclid.org/download/pdf_1/euclid.em/1175789744"" rel=""nofollow noreferrer"">a paper in English by Briggs</a> as Theorem 1.1.</p>

<p>If you would like to experiment, the easiest sequence of numbers that still gives very large values (almost $1$) of $\frac{\sigma(n)}{n \; e^\gamma \; \log \log n}$ is 
$$ A_n = \operatorname{lcm} \{ 1,2,3,4,5, ..., n-1,n \},  $$
where $A_n \neq A_{n-1}$ only when $n$ is a prime or prime power. The very best values come from the sequence of Colossally Abundant numbers, but that is a more difficult computer program. </p>

<p>Let's see, the Riemann Hypothesis is equivalent to the statement that, for $n \geq 5041 = 1 + 7!,$ the number $0.64821364942...$ can be replaced by $0.$</p>
"
"2392463","2392476","<p>Consider the dual operator of P.</p>
"
"2392473","2392678","<p><strong>Remark</strong>: Every integer has only finitly many prime divisors. </p>

<hr>

<hr>

<p><strong>Soppose on contrary</strong> that there exists such an integer $n$, </p>

<p>now notice that, for infinitely many primes $p_i$: </p>

<p>$$p_i \mid (a-x_i) 
\ \ \ \ \ \ \ \ 
\text{and}
\ \ \ \ \ \ \ \ 
p_i \mid (b-y_i) 
\ \ \ \ \ \ \ \ 
\text{and}
\ \ \ \ \ \ \ \ 
x_i-y_i=n. 
$$ </p>

<p>So we must have: </p>

<p>$$p_i \mid \Big( (a-x_i)-(b-y_i) \Big) 
\ \ \ \ 
\Longrightarrow 
\ \ \ \ 
p_i \mid \Big( (a-b)-(x_i-y_i) \Big) 
\ \ \ \ 
\Longrightarrow 
\ \ \ \ 
p_i \mid \Big( (a-b)-n \Big) . 
$$ </p>

<hr>

<p>This means that the integer $a-b-n$ infinitely many prime divisors $p_i$, which is an obvious <strong>contradiction</strong>.</p>
"
"2392474","2392515","<p>I think you were a bit too fast. After multiplying both sides of the equality $A\,x = c\,x$ by $A^{-1}$, what you get is $x = c\,A^{-1}x$. Now you must explain that $c\neq0$ (of course, that's because if $c=0$, then $x\in\ker A$ and therefore $A$ would not be invertible). And now, yes, from the equality $x = c\,A^{-1}x$ you can deduce that $A^{-1}x=\frac1cx$ and that, therefore, $x$ is an eigenvector of $A^{-1}$.</p>
"
"2392479","2392564","<p>I think it is helpful to think about these concepts on an arbitrary manifold. In that setting, <em>there is no such thing as a position vector</em>, because the manifold may not be a vector space; positions are specified instead by local coordinates.</p>

<p>The only reason we can identify positions with vectors in Euclidean space and move them around is that (1) Euclidean space is a vector space, and (2) the space carries an affine connection, which supplies a notion of parallel transport. You can't even ""move around"" <em>tangent</em> vectors on an arbitrary <em>manifold</em> unless you have a connection (though every <em>Riemannian</em> manifold comes with a natural one, the Levi-Civita connection); and in addition, as Ted says in his answer, you can't imagine those tangent vector glued ""wherever you like"" unless the tangent bundle is trivializable. The tangent bundle on a <em>vector space</em> is always trivializable.</p>

<hr>

<p>A very useful example to keep in mind is polar coordinates in the Euclidean plane. Here a position is specified by $(r,\theta)$ or, especially in physics and engineering, a ""position vector"" $r\mathbf{\hat{r}}$. The former is the manifold point of view; the latter exploits the fact that we are laying curvilinear coordinates on what, at bottom, is a vector space. But you quickly learn this thing called a position vector is not really a position vector, because the vector $\mathbf{\hat{r}}$ changes from point to point. It doesn't live in the underlying space; rather, it lives in the tangent bundle, and the tangent spaces change from point to point. We are only able to think of it as a bona fide position vector because we imagine uprooting it from its tangent space and gluing it instead to the tangent space at the origin. But, to repeat, in a general manifold, the underlying space isn't necessarily a vector space and doesn't necessarily have a zero vector. </p>

<p>In general, to specify a position we just gives its local coordinates, and that is that: we don't need a vector to specify a position. You might wonder, then, where velocity vectors come from, because we're so used to getting velocity vectors by differentiating position vectors. But <strong>velocity vectors in arbitrary manifolds don't come from differentiating position vectors</strong>. They come from partial differentiation in directions specified by the local coordinates. More precisely, given local coordinates $(x_1, x_2)$, we represent a velocity vector as the operator  $a\frac{\partial}{\partial x_1}+b\frac{\partial}{\partial x_2}$. (This is part of the reason, I think, many people struggle with the abstract definition of a tangent vector. For years they have been allowed to think of a velocity vector as the derivative of a position vector.)</p>

<p>You can see how the two points of view diverge when you ask a physicist or engineer to tell undergraduates how to represent velocity or acceleration vectors in planar polar coordinates. Most likely, they will draw a nice picture showing why the (time) derivative (along some curve) of $\mathbf{\hat{r}}$ is $\dot{\theta}\mathbf{\hat{\theta}}$. But the picture only works if you can parallel transport the tangent basis vectors $\mathbf{\hat{r}}$ and $\mathbf{\hat{\theta}}$ at a later point back along the curve to the tangent space at the initial point -- and it is exactly this seemingly innocent feature of the picture that requires an affine connection in general. What the calculation is really doing is exploiting the standard Euclidean connection to specify the covariant derivative / Christoffel symbols of planar polar coordinates.</p>
"
"2392480","2392484","<p>Hints: $ab=q$, $a+b=p$, $$a^3b^2+a^2b^3=(ab)^2(a+b),$$ $$(a^2-b^2)(a^3-b^3)=(a+b)(a-b)^2((a+b)^2-ab),$$ and $$(a-b)^2 = (a+b)^2-4ab.$$</p>
"
"2392481","2392490","<p>Note that the ""real cost"" in this case is less than the cost of the panels, since you will be paying back the loan with inflated dollars. The actual value depends on the frequency of payment. If it's all at once at the end of $y$ years you'll have to pay $P$ dollars then, but that will be just $P/(1+r)^y$ 2017 dollars.</p>

<p>For yearly installments the calculation is straightforward but a little tedious. The bill in year $k$ will be
$$
\frac{P/y}{(1+r)^k} 
$$
so that the total cost is given by
$$
P\left[\frac{1 - (1+r)^{-y}}{ry} \right]
$$
Assuming 2% inflation and a 10 year repayment, this means your total cost is ${\sim}0.89P$.</p>

<p>For $n$ payments when $n &gt; y$ you have to pay attention to the partial inflation during each year. I'm sure you can't estimate $r$ well enough over $y$ years to make that computation precise enough to be worth the effort.</p>
"
"2392483","2392485","<p>This is fine, assuming that $\sum\sum f \ne 0$.  Since the sums are finite, you don't have to worry about convergence, and so the usual rules of associativity, commutativity, and distributivity apply without difficulty.</p>

<p>To elaborate:
\begin{equation}
\sum \sum (f - h) = \sum \sum f - \sum \sum h
\end{equation}
by the commutative and associative properties of addition.  Then
\begin{equation}
\frac{\sum\sum (f-h)}{\sum\sum f}
= \frac{\sum\sum f - \sum \sum h}{\sum \sum f}
= \frac{\sum\sum f}{\sum \sum f} - \frac{\sum\sum h}{\sum \sum f}
\end{equation}
by the distributive property of multiplication over addition (where we are multiplying both sums in the numerator by $\left(\sum\sum f\right)^{-1}$, which is just a nonzero real number.  Simplifying gives your result.</p>
"
"2392486","2392527","<p>For linearly ordered topological spaces $X$ (LOTS) in the order topology, the following are equivalent:</p>

<ul>
<li><p>$X$ is compact.</p></li>
<li><p>$\forall A \subseteq X$: $\sup A \in X$ exists. (where $\sup \emptyset = \min(X)$, by common convention)</p></li>
</ul>

<p>Now suppose that $X$ is a LOTS such that every interval $[a,b],a &lt; b \in X$ is compact.
Then $X$ has the lub-property: Let $A \subset X$ be a non-empty set that has an upperbound $B$. We want to see that $\sup A$ exists in $X$.
Pick any $a_0 \in A$ and by assumption $[a_0,B]$ is compact. Then $s=\sup(A \cap [a,B])$ exists in $[a,B]$ (note that as an order convex closed subset of a LOTS, the subspace topology and the topology from the restricted order topology coincide, so that $[a,B]$ is a compact LOTS and so has all sups). One easily checks that $s = \sup(A)$ as well. </p>

<p>So if we translate ""closed and bounded"" as a closed interval, yes, we do have a sort of converse..</p>
"
"2392487","2392553","<p>Let $A=\{e_1,e_2...e_n\}$ be a basis of $V$ and $B=\{v_1,v_2...v_n\}$ a basis of $W$ </p>

<p>Let $x \in V$, then $x=a_1e_1+...+a_ne_n$</p>

<p>Define $T: V \rightarrow W$ such that :</p>

<p>$T(x)=a_1v_1+...+a_nv_n$</p>

<p>We have the forall $i \in \{1,2...n\}$: </p>

<p>$e_i=0e_1+0e_2+...+0e_i+...+0e_n$, thus $T(e_i)=vi$</p>

<p>Also $T$ is linear:</p>

<p>If $x,y \in V$ then $$x=x_1e_1+...+x_ne_n$$ $$y=y_1e_1+...+y_ne_n$$</p>

<p>Thus $$T(x+y)=T((x_1+y_1)e_1+...+(x_n+y_n)e_n)=(x_1+y_1)v_1+...+(x_n+y_n)v_n=(x_1v_1+...+x_nv_n)+(y_1v_1+...+y_nv_n)=T(x)+T(y)$$</p>

<p>I leave to you  the proof of the claim: $$T(ax)=aT(x), \forall a \in \mathbb{F}$$ where $\mathbb{F}$ is the field.</p>
"
"2392489","2392495","<p>Yes, if you know the $d_n$ are positive and increasing that is enough to conclude the sequence diverges.  Explicitly, since $d_n\geq d_1$ for all $n$, you have $a_n=a_0+d_1+d_2+\dots+d_n\geq a_0+nd_1$, which grows without bound since $d_1$ is just some positive constant.  So more specifically, you can conclude that $a_n$ diverges to $\infty$.</p>

<p>(Note, however, that in this case it's not quite true that $d_n$ is increasing.  For instance, if $a_0=-2.1$ then $a_1=2.41$ and $a_2=3.8081$ so $d_1=4.51$ and $d_2=1.3981$.  What is true is that the sequence $(d_n)$ is <em>eventually</em> increasing, and a similar argument still shows then that $a_n$ must diverge to $\infty$.)</p>
"
"2392494","2392499","<p>Hints: If $d+a \neq 0$ then $b=0$, $c=0$, $a^2=5$, and $d^2=9$. Compare the first and last equation. Can $d=-a$?</p>
"
"2392498","2392502","<p>Any family closed under countable unions is clearly closed under countable <em>disjoint</em> unions. However, the converse is not true. For example, consider the set $$\mathcal{U}=\{\{1, 2\},\{1, 3\}\}.$$ This set is definitely <em>not</em> closed under countable, or even finite!, unions, but it is <em>trivially</em> closed under arbitrary disjoint unions since its two elements are <em>not disjoint</em>.</p>
"
"2392504","2392525","<p>Recall that if $X$ is normally distributed with mean zero and variance $\sigma^2$, then the moment generating function of $X$ is
$$ \mathbb{E}[e^{uX}]=e^{\frac{u^2\sigma^2}{2}}$$</p>

<p>Now apply this to $X=W_t-W_s$ and set $u=-\alpha$.</p>
"
"2392511","2392518","<p>Just multiply point by point. You will see that $f(x)=0$.</p>
"
"2392517","2392548","<p>The space $\mathbb{N}^{\omega_1}$ is Baire (as any product of Cech-complete spaces is, Engelking 3.9.J(e), due to Oxtoby or Bourbaki), but it is not a $k$-space (see Engelking 3.3.E(a)) so not Cech-complete.</p>

<p>A metrisable example: $X = \mathbb{Q}\times \{0\} \cup \mathbb{R} \times (0,\infty) \subseteq \mathbb{R}^2$ (subspace topology) is Baire as $\mathbb{R} \times (0,\infty)$ is open and dense and Baire (as a locally compact space, e.g.).
But $X$ cannot be Cech-complete as the closed subset $\mathbb{Q} \times \{0\} \simeq \mathbb{Q}$ would be Cech-complete (the class is closed under closed subsets and $G_\delta$ subsets) which it is not.</p>
"
"2392534","2392543","<p>Gauss lemma can be stated in form that product of primitive polynomials is primitive polynomial. It is not loss of generality to assume that $P$ is primitive. Now, we know that $S(x)$ has rational coefficients, so there exists primitive polynomial with integer coefficients $S_0(x)$ such that $S_0(x) = qS(x)$. Thus, $qP(x) = (3x-2)S_0(x)$, so $qP(x)$ must be primitive which implies that $q = \pm 1$, i.e. $S$ has integer coefficients.</p>

<hr>

<p>Let me do the general statement. Let $p(x)$ be primitive polynomial with integer coefficients and let $p(x) = f(x)g(x)$ be its factorization over $\mathbb Q$. Then, there exist primitive polynomials with integer coefficients $f_0$ and $g_0$ and rational numbers $q_1$ and $q_2$ such that $q_1f(x) = f_0(x)$, $q_2g(x) = g_0(x)$. Let $q = q_1q_2$. Then we have $qp(x) = f_0(x)g_0(x)$ and by $f_0$ and $g_0$ being primitive, $qp$ is primitive as well. This implies $q=\pm 1$. </p>

<p>Indeed, let $q = a/b$ with $a$ and $b$ relatively prime and let $\alpha_i$ be coefficients of $p$. First of all, we have that $a\alpha_i/b$ are all integers, so $b$ divides all $\alpha_i$. By assumption of $p$ being primitive, $b = 1$. Finally, we have that $ap$ is primitive, and since $p$ has integer coefficients, $a = \pm 1$. Finally, this gives us factorization $p(x) = \pm f_0(x)g_0(x)$ over $\mathbb Z$.</p>
"
"2392549","2392586","<p>We will show that $\lvert f(0)\rvert\geq \lvert f(z)\rvert$ for all $z$ in the open unit disk $\mathbb{D}$. Consider $z\in \mathbb{D}$, and note that $z^{2^k}\to 0$ as $k\to \infty$, as $\lvert z^{2^k}\rvert = \lvert z\rvert^{2^k}\to 0$ because $\lvert z\rvert &lt; 1$. Then, as $f$ is analytic, $$\lim_{k\to \infty} f(z^{2^k}) = f(0)$$ As $\lvert f(z^{2^k})\rvert\geq \lvert f(z^{2^{k-1}})\rvert$ for all $k$, we have that $\lvert f(z^{2^k})\rvert$ increases monotonically (not necessarily strictly) to $\lvert f(0)\rvert$, so $\lvert f(0)\rvert\geq \lvert f(z)\rvert$. As this is true for all $z\in \mathbb{D}$, $f$ must be constant by the maximum modulus principle.</p>
"
"2392550","2392615","<p>I always prefer BÃ©zout's Identity proofs.</p>

<p><strong>Lemma 1:</strong> If $\gcd(c,d)=1$ then $\gcd(c,ck+d)=1$ for any $k$.</p>

<p><strong>Proof:</strong> If $cx+dy=1$, then $c(x-ky)+(ck+d)y=1$.</p>

<p><strong>Lemma 2:</strong> If $\gcd(c_1,d)=1$ and $\gcd(c_2,d)=1$ then $\gcd(c_1c_2,d)=1$.</p>

<p><strong>Proof:</strong> Solve $c_1x_1+dy_1=1$ and $c_2x_2+dy_2=1$. Multiplying, and you get:</p>

<p>$$c_1c_2(x_1y_1)+d(y_2c_1x_1+y_1c_2x_2+dy_1y_2)=1$$</p>

<p><strong>Theorem:</strong> If $\gcd(a,b)=1$ then $\gcd(ab,a^2+b^2)=1$.</p>

<p><strong>Proof:</strong> Lemma 2 implies that $\gcd(a,b^2)=1$. Lemma 1 implies that $\gcd(a,a^2+b^2)=1$. </p>

<p>Likewise, we get that $\gcd(b,a^2+b^2)=1$.</p>

<p>Then, by Lemma 2, we have that $\gcd(ab,a^2+b^2)=1$.</p>
"
"2392551","2392624","<p>If the system is consistent, there is a solution $x$ such that $Ax = b$. The components of $x$ give a linear combination of $A$'s columns to eliminate $b$ in the matrix $(A|b)$, hence $\text{rank}(A|b) = \text{rank}(A|0) = \text{rank}(A)$.</p>

<p>Conversely, if $\text{rank}(A|b) = \text{rank}(A)$, it means that the columns of $A$ generate the same subspace as the columns of $(A|b)$. As $b$ belongs to this subspace, there is a linear combination of $A$'s columns that is equal to $b$, but this means exactly that the system $A x=b$ has a solution.</p>
"
"2392552","2392582","<p>I would, instead, use <a href=""https://en.wikipedia.org/wiki/Greatest_common_divisor#The_gcd_in_commutative_rings"" rel=""nofollow noreferrer"">the definition of gcd</a>, and observe that </p>

<ul>
<li>The factors of $x^5$ are of the form $d_1=ax^n$ with $a\in\Bbb{Q}^*$
and $n\in\{0,2,3,5\}$.</li>
<li>Similarly the factors of $x^6$ are the monomial of the form $d_2=bx^m$ with $b\in\Bbb{Q}^*$, $m\in\{0,2,3,4,6\}$.</li>
<li>The common factors are thus non-zero monomials of degree either $0,2$ or $3$.</li>
<li>But among those there are none that all the common factors would be divisors of: the lower degree monomials are not divisible by $x^3$, and a cubic monomial is not divisible by $x^2$. Thus no greatest common divisor exists.</li>
</ul>
"
"2392555","2392659","<p>Yes, they are different inner products. In fact, because there are two different conventions for how the wedge product relates to the tensor product, there are actually three different possible inner products on differential forms.</p>

<p><strong>1. The Pointwise Hodge Inner Product:</strong> This is the inner product on differential forms that satisfies 
$$
\langle\varepsilon^{i_1}\wedge\dots\wedge\varepsilon^{i_k},\varepsilon^{i_1}\wedge\dots\wedge\varepsilon^{i_k}\rangle = 1
$$
whenever $i_1&lt;\dots&lt;i_k$ and $(\varepsilon^1,\dots,\varepsilon^n)$ is a local orthonormal frame for $T^*M$. It's also characterized by
$$
\omega \wedge *\eta = \langle\omega,\eta\rangle dV_g,
$$
where $*$ is the Hodge star operator and $dV_g$ is the Riemannian volume form. In particular, on $\mathbb R^2$ with the Euclidean metric, it satisfies 
$$
\langle dx^1\wedge dx^2,dx^1\wedge dx^2\rangle = 1. 
$$
This inner product is universally used in Hodge theory.</p>

<p>(The terminology ""pointwise Hodge inner product"" is not standard, but it's the most accurate name I can think of for this inner product, given that the term <strong><em>Hodge inner product</em></strong> usually refers to the corresponding global inner product on differential forms obtained by integrating this pointwise inner product.)</p>

<p><strong>2. The Tensor Inner Product:</strong> This is obtained from the usual inner product on tensors, by viewing the bundle of differential $k$-forms as a subbundle of the bundle of covariant $k$-tensors. The relationship between this and the pointwise Hodge inner product depends on which convention is used for wedge products. In my books, I refer to these conventions as the <strong><em>determinant convention</em></strong> and the <strong><em>Alt convention</em></strong>.</p>

<p><strong>2a. Using the determinant convention:</strong>
This is the wedge product convention that satisfies 
$$
\omega^1\wedge\dots \wedge \omega^k(v_1,\dots,v_k) = \det (\omega^j(v_k))
$$
when $\omega^1,\dots,\omega^k$ are covectors and $v_1,\dots,v_k$ are vectors.
In particular, with this convention, we have $dx^1\wedge dx^2 = dx^1\otimes dx^2 - dx^2 \otimes dx^1$, so
$$
\langle dx^1\wedge dx^2,dx^1\wedge dx^2\rangle = 2. 
$$</p>

<p><strong>2b. Using the Alt convention:</strong>
With this convention, the wedge product is determined by 
$$
\omega\wedge\eta = \operatorname{Alt} (\omega\otimes \eta)
$$
when $\omega$ and $\eta$ are any differential forms whatever.
In particular, with this convention, we have $dx^1\wedge dx^2 = \tfrac 1 2 (dx^1\otimes dx^2 - dx^2 \otimes dx^1)$, so
$$
\langle dx^1\wedge dx^2,dx^1\wedge dx^2\rangle = \tfrac 1 2. 
$$</p>

<p>More generally, the tensor inner product is $k!$ times the pointwise Hodge inner product on $k$-forms if you use the determinant convention, and $(1/k!)$ times if you use the Alt convension.</p>
"
"2392560","2392570","<p>Yes it looks good! You can make it a little shorter by not considering the case $n=2$, since you only need $n=1$ as your base case.</p>

<p>You could also improve your wording and notation a bit. For instance, when you say ""Therefore"", it would be better to say something like ""Therefore, by induction"". Also, instead of writing ""...from Axiom II $a_1+a_2&gt;0$"", perhaps include some words to separate II from the inequality, such as
""...from Axiom II, we obtain $a_1+a_2&gt;0$."" There are mainly stylistic remarks.</p>

<p>However, in regards to the notation, AndrÃ©s E. Caicedo already pointed out in the comments that you should have
$$ a_1+\cdots+a_{n-1}&gt;0$$
or
$$\sum_{i=1}^{n-1}a_i&gt;0,$$
and not
$$\sum_{i=1}^{n-1}a_1+\cdots+a_{n-1}.$$
Indeed, the latter is equal to $(n-1)(a_1+\cdots+a_{n-1})$, which isn't what you want.</p>
"
"2392562","2392787","<p>Perhaps I can be of the most help by encouraging you to use notation
that makes sense.  Your write:</p>

<p>$$P(x &gt; 1200) = 1-P\left(\frac{1200-1100}{60}\right) = 1-P(1.67) = 1-0.9525 = 0.0475,$$</p>

<p>in which the second and third terms contain no events and so make no sense.</p>

<p>I suggest something like this: You have 
$X \sim \mathsf{Norm}(\mu = 1100,\, \sigma=60)$ and seek
$P(X &gt; 1200).$ Then
$$P(X &gt; 1200) = 1 - P(X \le 1200) = 
1-P\left(\frac{X-\mu}{\sigma} \le \frac{1200 - 1100}{60}\right)\\
= 1 - P(Z \le 1.67) = 1 -  0.9525 = 0.0475,$$
where the approximate numerical answer can be obtained from printed tables of the standard
normal CDF.</p>

<p>You can get a slightly more accurate answer using software without having to standardize. For
example in R statistical software. The improved accuracy is because rounding
is avoided. (In R, <code>pnorm</code> is the CDF of the normal distribution with mean
and SD given in the second and third arguments.)</p>

<pre><code>1 - pnorm(1200, 1100, 60)
## 0.04779035
</code></pre>

<hr>

<p>Then in the last part, you seek $w$ such that $P(X \le w) = 0.10.$ So write
$$P(X \le w) = P\left(\frac{X - \mu}{\sigma} \le \frac{w-1100}{60}\right)
= P(Z \le (w-1100)/60) = 0.10,$$
Then by normal tables $(w-1100)/60 \approx -1.28,$ and you can solve for $w.$</p>

<p>A slightly more accurate answer can be obtained by software:</p>

<pre><code> qnorm(.1, 1100, 60)   # 'qnorm' is the inverse CDF or 'quantile' function
 ## 1023.107
 pnorm(1023, 1100, 60) # as a check
 ## 0.09968766
</code></pre>

<p>Notice (as some Commenters did not) that the company offering the warranty is stingy, not
wanting to pay out for more than 10% of purchases.</p>

<hr>

<p>Also, it usually helps to make sketches. Here is a graph of the density
function of $\mathsf{Norm}(1100, 60),$ with vertical red lines marking
locations of interest in the parts above.</p>

<p><a href=""https://i.stack.imgur.com/9yzMz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9yzMz.png"" alt=""enter image description here""></a></p>

<p>Of course, you can't sketch with such good accuracy just by hand, but with
a little effort you can learn to make a facsimile of a normal
density curve that is a lot better than no sketch at all.</p>
"
"2392568","2392748","<p>Yes, it's true. Indeed, we can locally write the BCH formula as
$$\log(e^xe^y)=x+y+\frac12[x,y]+Q(x,y,[x,y]).$$
Here $Q$ is defined as follows: the remainder (sum of terms of order $\ge 3$ in BCH) is written as $\sum_i \lambda_iP_i(x,y)([x,y])$, where $P_i$ is some nonempty finite product of the operators $\mathrm{ad}(x)$ and $\mathrm{ad}(y)$, and $Q(x,y,z)=\sum \lambda_iP_i(x,y)(z)$. In particular we can bound around zero $\|Q(x,y,z)\|\le C\max(\|x\|,\|y\|)\|z\|$ for a suitable constant (and choice of norms). </p>

<p>If $e^xe^y=e^ye^x$ for small $x,y$, this yields<br>
$$x+y+\frac12[x,y]+Q(x,y,[x,y])=y+x+\frac12[y,x]+Q(y,x,[y,x]),$$
that is,
$$[x,y]=Q(y,x,-[x,y])-Q(x,y,[x,y]).$$</p>

<p>This yields $\|[x,y]\|\le 2C\max(\|x\|,\|y\|)\|[x,y]\|$. If $[x,y]\neq 0$, this yields $1\le 2C\max(\|x\|,\|y\|)$, which is a contradiction if $(x,y)$ is too close to $(0,0)$.</p>
"
"2392572","2392636","<p>$g$ takes all complex values.  In a neighbourhood of $g(z)$ where $g'(z) \ne 0$, $g$ is invertible and $f = h \circ g^{-1}$.  Any singularities are removable.</p>
"
"2392577","2392585","<p>$1_n$ has eigenvalues $n$ with multiplicity $1$ and $0$  with multiplicity $n-1$, so $1_n - I_n$ has eigenvalues $n-1$ with multiplicity $1$ and $-1$ with multiplicity $n-1$.  The determinant is the product of the eigenvalues, thus $(-1)^{n-1} (n-1)$.</p>
"
"2392587","2392610","<p>Let $U = \prod_{\alpha} U_{\alpha}$ a neighbourhood of $0$. For each $\alpha$, choose a symmetric neighbourhood $V_{\alpha}$ of $0$ in $E_{\alpha}$ such that $V_{\alpha} + V_{\alpha} \subset U_{\alpha}$. Let $V = \prod_{\alpha} V_{\alpha}$. Since $x^{\lambda}$ is a Cauchy net, there is a $\lambda_0$ such that for all $\alpha$ we have $x^{\lambda}_{\alpha} - x^{\mu}_{\alpha} \in V_{\alpha}$ for all $\lambda, \mu \geqslant \lambda_0$. Since $x^{\lambda}_{\alpha} \to x_{\alpha}$ it follows that $x_{\alpha} - x^{\mu}_{\alpha} \in \overline{V_{\alpha}} \subset U_{\alpha}$ for all $\alpha$ and all $\mu \geqslant \lambda_0$. But that means $x - x^{\mu} \in U$ for all $\mu \geqslant \lambda_0$.</p>
"
"2392597","2392611","<p>Note that from the line $$a(y+b)=a(y+c)$$ that makes you conclude that $y+b=y+c$.</p>

<p>If you could have perform that trick from the beginning, you would have solved the problem directly.</p>

<p>Proposal:</p>

<p>From $ab=ac$, why not perform $y(ab)=y(ac)$.</p>
"
"2392599","2392606","<p>Because if you multiply an even number and a multiple of $5$ you get a multiple of $10$.</p>

<p>Namely, the number of trailing zeros of $n!$ is
$$\sum_{k=1}^\infty\left\lfloor\frac n{5^k}\right\rfloor$$</p>

<p>Note that this sum has only finitely many non-zero terms.</p>

<p>For $n=999$ is
$$199+39+7+1=246$$</p>
"
"2392616","2392620","<p>Hint:</p>

<p>Not exactly the same question but this can help you find your error.</p>

<p>$$\frac{\sum_{i}(a_i-b_i)^2}{\sum_ia_i^2}=\frac{\sum_i(a_i^2-2a_ib_i+b_i^2)}{\sum_ia_i^2}=1-\frac{2\sum_ia_ib_i-\sum_ib_i^2}{\sum_ia_i^2}$$</p>
"
"2392623","2392704","<p>$\color {blue}{B \subset A  \implies (A-B) \cup B = A}$</p>

<p>$\boxed{(A-B) \cup B\subset A}$</p>

<p>$x\in (A-B) \cup B \implies \left\lbrace \begin{array}l x\in A- B\subset A\\ \text{or}\\x\in B\subset A\end{array}\right.\implies(A-B) \cup B\subset A$</p>

<p>$\boxed{A \subset(A-B) \cup B}$</p>

<p>$x\in A \implies\left \lbrace \begin{array}lx\in A-B\\\text{or}\\x\in A\cap B\implies x\in B\end{array}\right.\implies x\in (A- B)\cup B\implies A \subset(A-B) \cup B$</p>

<hr>

<p>$\color{blue}{(A-B) \cup B = A  \implies B \subset A}$</p>

<p>$x\in B\implies x\in (A-B) \cup B \implies x\in A\implies B \subset A$</p>
"
"2392625","2392642","<p>In polar coordinates you have that<br>
 - the numerator is $5 \cdot j20=j100=100 \, e^{j \pi /2}$.<br>
 - the denominator is $5 + j20=\sqrt{5^2+20^2}\, e^{j \arctan{(20/4)}}$.</p>

<p>But to rationalize the formula you cite, you do not need to go through polar. Just multiply over and below the fraction for $5-j20$
$$
\eqalign{
  &amp; {{5 \cdot j20} \over {5 + j20}} = {{\left( {5 - j20} \right)j100} \over {\left( {5 - j20} \right)\left( {5 + j20} \right)}} =   \cr 
  &amp;  = {{\left( {5 - j20} \right)j100} \over {\left( {25 + 400} \right)}} = {{2000 + j500} \over {425}} =   \cr 
  &amp;  = {{80} \over {17}} + j{{20} \over {17}} \cr} 
$$</p>
"
"2392634","2392677","<blockquote>
  <ol>
  <li>The last sentence in the question start with ""That is,"". How the last two sentences of the question equivalent? That is, how putting $E=E_2$ and $P(H_j)=P(H_j|E_1)$ means $P(H_j|E_1)$ is the prior probability and $E_2$ is the posterior probabilities?</li>
  <li>Does the last two sentences in the question starting with ""One might wonder,..."" mean to say $P(H_i|E_1E_2)=P(E_2|H_iE_1)$</li>
  </ol>
</blockquote>

<p>They're just wondering if there is an analogous form, and what is required. &nbsp; Turns out the answers are ""yes"" and ""when the events are conditionally independent when given a hypothesis"".</p>

<p>$$\def\P{\operatorname{\sf P}} \begin{align}
\P(H_i\mid E) &amp; = \dfrac{\P(E\mid H_i)\P(H_i)}{\sum_j \P(E\mid H_j)\P(H_j)}
\\[1ex] \P(H_i\mid E_2,E_1) &amp; = \dfrac{\P(E_2\mid H_i, E_1)\P(H_i\mid E_1)}{\sum_j \P(E_2\mid H_j,E_1)\P(H_j\mid E_1)}
\\[1ex] &amp; = \dfrac{\P(E_2\mid H_i)\P(H_i\mid E_1)}{\sum_j \P(E_2\mid H_j)\P(H_j\mid E_1)} &amp;E_1\perp E_2\mid H_i
\end{align}$$</p>

<blockquote>
  <ol start=""3"">
  <li>How does the title ""Updating information sequentially"" makes sense?</li>
  </ol>
</blockquote>

<p>If you have calculated all the conditional probabilities for hypothesis given events, $\P(H_i\mid E_1)$ you can now <em>update</em> when given additional information by the next event in the <em>sequence</em>, $\P(H_i\mid E_1, E_2)$.</p>

<blockquote>
  <ol start=""4"">
  <li>How does the last/third point in the solution makes sense?</li>
  </ol>
</blockquote>

<p>Suppose I have two coins with biased probability for obtaining heads, $p_1, p_2$, and I randomly select a coin and toss it, obtaining a heads.  $E_1$ being the event of doing so.</p>

<p>$$\P(H_i)=\tfrac 12, \P(E_1\mid H_i)=p_i$$</p>

<p>I can calculate $\P(H_1\mid E_1)$ and $\P(H_2\mid E_1)$</p>

<p>$$\begin{align}\P(H_i\mid E_1) &amp; = \dfrac{p_i\cdotp\tfrac12}{p_1\cdotp\tfrac12+p_2\cdotp\tfrac12} \\ &amp; = \dfrac{p_i}{p_1+p_2}\end{align}$$</p>

<p>I toss the same coin once more, obtaining another head, and $E_2$ is the event of doing so. $$\text{again }\P(E_2\mid H_i)=p_i$$ Now, I <em>could</em> calculate $\P(H_1\mid E_1,E_2)$ from scratch, but since the events are conditionally independent given a particular coin, I may <em>update</em> the previous calculation.</p>

<p>$$\begin{align}\P(H_i\mid E_1,E_2) &amp; = \dfrac{p_i \cdot \dfrac{p_i}{p_1+p_2}}{p_1 \cdot \dfrac{p_1}{p_1+p_2}+p_2 \cdot \dfrac{p_2}{p_1+p_2}} \\ &amp; = \dfrac{p_i^2}{p_1^2+p_2^2}\end{align}$$</p>

<p>(In <em>this</em> scenario it would have been easier to find from scratch, but that is not always going to be the case.)</p>
"
"2392648","2392684","<p>So your question is why do we use the formula</p>

<p>$$ 4p(y-k)=(x-h)^2 $$</p>

<p>rather than the formula</p>

<p>$$ 2p(y-k)=(x-h)^2 $$</p>

<p>Certainly, this could be done, but it would replace each $p$ in the following parabolic relationship diagram with the fraction $\dfrac{p}{2}$. This is the only reason I can think of.</p>

<p>Note also that $4p$ also happens to be the length of the <strong>focal chord</strong> (Latin: <em>latus rectum</em>).</p>

<p><a href=""https://i.stack.imgur.com/kfHrn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kfHrn.png"" alt=""parabolic relationships""></a></p>
"
"2392658","2392724","<p>If you are allowed to use that a countable union of countable sets is countable then things are not so difficult:</p>

<p>Put $A(i)=\{\frac{i}{j}| j \in \mathbb{Z}$ \ $\{0\}\}$ which is a countable set because $\mathbb{Z}$ \ $\{0\}$ is countable.</p>

<p>Thus we have $\mathbb{Q}=\bigcup_{i \in \mathbb{Z}}A(i)$ which is a countable union of countable sets.</p>

<p>This is not exactly the way your book proposes but its very similar.</p>

<p>I believe you get the idea.</p>
"
"2392662","2392714","<p>Well, there isn't a single answer, it depends. For example <a href=""https://en.wikipedia.org/wiki/Modern_portfolio_theory"" rel=""nofollow noreferrer"">Modern portfolio theory</a> is a way to mathematically show that spreading the investment is safer (that work won a Nobel Prize). This is what most of the funds are doing.</p>

<p>Another way is to study correlations between different assets, events and assets etc. For instance if the price of petrol goes up, then the price of transporting the commodities will also go up, which will affect the prices of the transported commodities too. <a href=""https://www.quora.com/Can-Twitter-sentiment-analysis-guide-stock-market-investment"" rel=""nofollow noreferrer"">Sentiment analysis and correlation with stock markets</a> was once a subject of serious studies, probably still is.</p>

<p><a href=""https://en.wikipedia.org/wiki/Hedge_(finance)"" rel=""nofollow noreferrer"">Hedging</a> is another good strategy. For example a $-1$ or close to $-1$ correlation of the share prices of two rival companies (Apple and Microsoft was used once as an example). If one goes up, the other one goes down and vice-versa. Investing in both will reduce the risk of huge losses compared to investing only in one.</p>

<p>Looking for <a href=""https://en.wikipedia.org/wiki/Arbitrage"" rel=""nofollow noreferrer"">arbitrage possibilities</a>, from differences in prices of the same assets on different markets (and markets these days tend to be well synchronised, but due to different software, networking and hardware related issues there are gaps sometimes, which are still successfully exploited even at the microseconds level) to simple <a href=""https://www.kreslik.com/forums/viewtopic.php?t=307"" rel=""nofollow noreferrer"">rounding issues</a>.</p>

<p>And I only listed 4. One important thing I should mention, finding specific examples of anomalies (rather than classes of anomalies) which work is quite hard. Those in books are almost surely outdated (this isn't suggesting not to read books, books are still good sources of inspiration). The new ones are being exploited and once made public - they aren't successful anymore. It's like a law of conservation of wealth - if more wealth appears somewhere, that means it disappeared from somewhere else. According to this rule, it's impossible for everyone to exploit successfully the same anomaly.</p>
"
"2392685","2392688","<p>It's a standard terminology. The maximum of $n$ random variables, $X_1, X_2, \cdots, X_n$ is denoted by $X_{(n)}$. Likewise, the minimum is denoted by $X_{(1)}$. In general, the $k$th maximum random variable is denoted by $X_{(n-k+1)}$.</p>
"
"2392690","2392692","<p>$\mathbb{R}^n$ is not a $\mathbb{C}$-vector space in itself. $i(1,0,0)\notin\mathbb{R}^3$, so it fails to be closed. But, if you have an even-dimensional vector space $\mathbb{R}^{2n}$, you can manufacture an isomorphism (of real vector spaces) to $\mathbb{C}^n$. Can you construct it?</p>
"
"2392691","2392706","<p>For the first series with $x$ taking small positve integer values ... $x=0,1,2$
\begin{eqnarray*}
\sum_{n=1}^{\infty} \frac{1}{y^n} = \frac{\frac{1}{y}}{1-\frac{1}{y}}=\frac{1}{y-1} \\
\sum_{n=1}^{\infty} \frac{n}{y^n} = \frac{\frac{1}{y}}{(1-\frac{1}{y})^2}=\frac{y}{(y-1)^2} \\
\sum_{n=1}^{\infty} \frac{n^2}{y^n} = \frac{\frac{1}{y}(1+\frac{1}{y})}{(1-\frac{1}{y})^3}=\frac{y(1+y)}{(y-1)^3} \\
\end{eqnarray*}
by repeatedly appling the operator $ -y \frac{d}{dy} $ to these formulea one could calculate for any $x \in \mathbb{N} $.</p>

<p>The second series is related to the derivative of the Riemann zeta function. I will give more detail, if needs be.</p>

<p>Edit: The next three formulea in the sequence are 
\begin{eqnarray*}
\frac{y(1+4y+y^2)}{(y-1)^4} \\
\frac{y(1+11y+11y^2+y^3)}{(y-1)^5} \\
\frac{y(1+26y+66y^2+26y^3+y^4)}{(y-1)^6} \\
\end{eqnarray*}
The coefficients of the polynomials in the numerators are related to the Eulerian numbers. <a href=""https://en.wikipedia.org/wiki/Eulerian_number"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Eulerian_number</a></p>
"
"2392702","2392802","<p>I also found recursive solution at <a href=""https://en.wikipedia.org/wiki/Pell&#39;s_equation#Additional_solutions_from_the_fundamental_solution"" rel=""nofollow noreferrer"">Additional solutions from the fundamental solution</a>
$\displaystyle x_{k}+y_{k}{\sqrt {D}}=(x_{1}+y_{1}{\sqrt {D}})^{k}(1)$
that led them to $ \displaystyle x_{k+1}=x_{1}x_{k}+Dy_{1}y_{k} (2)$
and $\displaystyle y_{k+1}=x_{1}y_{k}+y_{1}x_{k} (3)$  </p>

<p><strong>Here's the proof</strong>:  </p>

<p>From (1) for $k+1$ solutions </p>

<p>$\displaystyle x_{k+1}+y_{k+1}{\sqrt {D}}=(x_{1}+y_{1}{\sqrt {D}})^{k+1}=(x_{1}+y_{1}{\sqrt {D}})^{k}*(x_{1}+y_{1}{\sqrt {D}}) (4)$ </p>

<p>First parentheses in (4) represents $k$-th solution , so (4) can be rewritten as </p>

<p>$\displaystyle x_{k+1}+y_{k+1}{\sqrt {D}}=(x_{k}+y_{k}{\sqrt {D}})*(x_{1}+y_{1}{\sqrt {D}}) (5)$. </p>

<p>Then open parentheses </p>

<p>$x_{k+1}+y_{k+1}{\sqrt {D}} = x_kx_1+ x_ky_1\sqrt {D}+y_kx_1\sqrt {D}+y_ky_1D(6)$ </p>

<p>Combine similar terms</p>

<p>$x_{k+1}+y_{k+1}{\sqrt {D}} = (x_kx_1+y_ky_1D) + \sqrt {D}(x_ky_1+y_kx_1) (7)$</p>

<p>Now it is easy to see that</p>

<p>$ \displaystyle x_{k+1}=x_{1}x_{k}+Dy_{1}y_{k} (2)$</p>

<p>and </p>

<p>$\displaystyle y_{k+1}=x_{1}y_{k}+y_{1}x_{k} (3)$</p>

<p>are true.</p>

<p><strong>Let's find formula expressing $x_{k+2}$ by using equations (2) and (3)</strong></p>

<p>from (2) $x_{k+2}=x_{1}x_{k+1}+Dy_{1}y_{k+1} (8)$</p>

<p>substitute $y_{k+1}$ from (3) in (8)</p>

<p>$x_{k+2}=x_{1}x_{k+1}+Dy_{1}(x_{1}y_{k}+y_{1}x_{k}) (9)$ </p>

<p>simplify (9)  </p>

<p>$x_{k+2}=x_{1}x_{k+1}+Dy_{1}x_{1}y_{k}+Dy_{1}^2x_{k} (10)$ </p>

<p>$x_{k+2}=x_{1}(x_{k+1}+Dy_{1}y_{k})+Dy_{1}^2x_{k} (11)$ </p>

<p>use (2) to simplify</p>

<p>$x_{k+2}=x_{1}(x_{k+1}+x_{k+1}-x_{1}x_{k})+Dy_{1}^2x_{k} (12)$ </p>

<p>$x_{k+2}=x_{1}(2x_{k+1}-x_{1}x_{k})+Dy_{1}^2x_{k} (13)$ </p>

<p>$x_{k+2}=2x_{1}x_{k+1}-x_{1}^2x_{k}+Dy_{1}^2x_{k} (14)$ </p>

<p>$x_{k+2}=2x_{1}x_{k+1}-x_{k}(x_{1}^2-Dy_{1}^2) (15)$ </p>

<p>You can see that in parentheses $x_{1}^2-Dy_{1}^2$ is original Pell's equation , and it equals to 1, so</p>

<p>$x_{k+2}=2x_{1}x_{k+1}-x_{k} (16)$ </p>

<p>Formula (16) for $k^2-5z^2=1$ with the first solution $\{k,z\}=\{9,4\}=\{x_1,y_1\}$ gives us a recursive family of solutions $x_{k+2}=2*9*x_{k+1}-x_{k}$
or $x_{k+2}=18x_{k+1}-x_{k}$</p>

<p><strong>Let's find formula expressing $y_{k+2}$ by using equations (2) and (3)</strong></p>

<p>$\text{from (3)  } y_{k+2}=x_{1}y_{k+1}+y_{1}x_{k+1} (17)$  </p>

<p>substitute $x_{k+1}$ from (2)  </p>

<p>$y_{k+2}=x_{1}y_{k+1}+y_{1}(x_{1}x_{k}+Dy_{1}y_{k}) (17)$</p>

<p>simplify</p>

<p>$y_{k+2}=x_{1}y_{k+1}+y_{1}x_{1}x_{k}+Dy_{1}^2y_{k} (18)$</p>

<p>get $x_k$ from (3) and apply it to (18)</p>

<p>$x_k=\frac{y_{k+1} - x_1y_k}{y_1} (19)$</p>

<p>$y_{k+2}=x_{1}y_{k+1}+y_{1}x_{1}(\frac{y_{k+1} - x_1y_k}{y_1})+Dy_{1}^2y_{k} (20)$</p>

<p>simplify (20)</p>

<p>$y_{k+2}=x_{1}y_{k+1}+x_{1}y_{k+1} - x_1^2y_k+Dy_{1}^2y_{k} (21)$</p>

<p>or</p>

<p>$y_{k+2}=2x_{1}y_{k+1} - y_k(x_1^2-Dy_{1}^2) (22)$</p>

<p>Again, you can see that in parentheses $x_{1}^2-Dy_{1}^2$ is original Pell's equation , and it equals to 1, so</p>

<p>$y_{k+2}=2x_{1}y_{k+1} - y_k (23)$</p>

<p><em>Questions?</em></p>
"
"2392708","2392759","<p>I do not think that there is any special symbol for this relationship between two functions of real numbers, so perhaps it is best to pick some little-used symbol and define it to mean that in your context.</p>

<p>For example:</p>

<p>If $F,G: \mathbb{R}\to\mathbb{R}$ then $F\bowtie G$ means that if $a,b\in\mathbb{R}$ and $a\le b$ then $F(a)\le F(b)\iff G(a)\le G(b)$ and $F(a)\ge F(b)\iff G(a)\ge G(b)$.</p>
"
"2392712","2393119","<p>$\def\Q#1{\bar Q_{#1}}\def\M#1{\max_{\Q{#1}}w}$Compactness and continuity makes this fairly easy. We know that the maximum over $\Q T$ is attained at some point $(x_0,t_0)$. If $t_0 &lt; T$ then $\M {T - \epsilon} = \M{T}$ for all $\epsilon \in [0,T-t_0]$, so we are done; so we just need to consider the case $t_0 = T$. </p>

<p>Given an arbitrary $\epsilon &gt; 0$, the continuity of $w$ provides a $\delta &gt; 0$ such that $|w(x,t) - \M T| &lt; \epsilon$ whenever $|(x,t) - (x_0,T)| &lt; \delta$. Thus if we take a time $t &gt; T - \delta$ we can conclude $$\M t \ge w(x_0,t) &gt;\M T - \epsilon.$$ On the other hand we have $\M t \le \M T$ since $\Q t \subset \Q T$; so we have shown that $\M t \to \M T$ as $t \nearrow T$.</p>
"
"2392720","2393517","<p>Yes, such a tournament can be made. It is called a <a href=""http://www.devenezia.com/downloads/round-robin/rounds.php?it=12&amp;v=w1"" rel=""nofollow noreferrer"">Whist Tournament Schedule</a>. </p>
"
"2392723","2392904","<p>Yes, the asymptotes tell you the global behavior of the hyperbola, and in general it is extremely useful to be able to explain the global behavior of something complicated using simpler things that you already understand well (such as linear functions).</p>

<p>Specifically, they tell you that a hyperbola becomes linear far away from its center.</p>
"
"2392725","2392737","<p>Perhaps you misunderstand what it means to say that Fourier transform is one-to-one. It means that if $f$ and $g$ are two essentially different functions (i.e. the measure of the set of values of $x$ for which $f(x)\ne g(x)$ is more than $0$) then the Fourier transforms of $f$ and $g$ are different. Note that it says different <b>functions,</b> not different values of $x.$</p>
"
"2392728","2392741","<p>I use the following fact below, so I thought it would be useful to prove it first.</p>

<blockquote>
  <p>If $f:A\to\mathbb{R}$ is such that $f(a)\geq 0$ for every $a\in A$ and $\sum_{a\in A}f(a)&lt;\infty$, then $[f&gt;0]$ (notation for $\{a\in A \mid f(a)&gt;0\}$) is countable.</p>
</blockquote>

<p><em>Proof:</em>
For each $x&gt;0$, consider the set $[f&gt;x]$. Note that
$$
\operatorname{card}([f&gt;x])x \leq \sum_{a\in[f&gt;x]}f(a) \leq \sum_{a\in A}f(a) &lt; \infty.
$$
Thus $[f&gt;x]$ is finite. Therefore $[f&gt;0] = \cup_{n\in\mathbb{N}}[f&gt;1/n]$ is countable, as it is a countable union of finite sets. $\square$</p>

<p>Now we consider the problem. Since $f(a)\geq0$ for all $a\in A$ and $\sum_{a\in A}f(a)$ converges, it must converge absolutely. Let $A':=\{a\in A \mid f(a)&gt;0\}$. Then $A'$ must be countable, for otherwise $\sum_{a\in A}f(a)$ would diverge. Let's enumerate $A'=\{a_1,a_2,\ldots\}$ [thanks to Adayah for noticing mistake here]. Now we have
$$
\sum_{n=1}^\infty f(a_n) = \sum_{a\in A'}f(a)=\sum_{a\in A}f(a)= 1.
$$
Let $\varepsilon&gt;0$. Since the above sum converges, there exists $N\in\mathbb{N}$ such that $\sum_{n=N}^\infty f(a_n) &lt; \varepsilon$.
Then whenever $x&lt;\min_{n&lt; N}a_n$, we have
$$
F(x) = \sum_{a\leq x}f(a) \leq \sum_{n=N}^\infty f(a_n) &lt; \varepsilon.
$$
This proves that $\lim_{x\to-\infty}F(x)=0$. $\square$</p>

<p>I want to remark that we didn't actually need $\sum_{a\in A}f(a)=1$. We just needed the sum to converge.</p>

<hr>

<p>Response to old version of question:</p>

<p>Even with the edits, there still seems to be something wrong. Consider $A=\{0\}$ and $f:A\to\mathbb{R}$ given by $f(0) = 1$. Then indeed $f(a)\geq 0$ for any $a\in A$ and $\sum_{a\in A}f(a)=1$. However, for all $x\geq 0$, we have $F(x)=1$. Therefore $\lim_{x\to\infty}F(x)=1$.</p>
"
"2392742","2392744","<p>This one has a really cute trick, namely that the integrand is equal to</p>

<p>$$\int_a^b e^{-xt} \, dt$$</p>

<p>as you can verify by a direct computation. This means we can write a double integral and change the order of integration, yielding</p>

<p>\begin{align*}
\int_a^b \int_0^{\infty} e^{-xt} \, dx \, dt &amp;= \int_a^b -\frac{e^{-xt}}{t}\big|_{x = 0}^{x = \infty} \, dt \\
&amp;= \int_a^b \frac 1 t \, dt \\
&amp;= \ln \frac b a
\end{align*}</p>

<hr>

<p>By the way, the integral you mention does converge. It's bounded by $e^{-ax}$ as $x \to \infty$, and bounded at the origin because $e^{-ax} - 1 = -ax + O(x^2)$. </p>
"
"2392749","2392768","<p>Let us see... as correctly stated by the OP, $\text{Surj}(n,m)=m!{n\brace m}=\sum_{j=0}^{m}(-1)^{m-j}\binom{m}{j}j^n$, hence:</p>

<p>$$ a_n = \frac{2^n}{n!}\sum_{m=1}^{n}\Gamma(m){n\brace m}=\frac{2^n}{n!}\int_{0}^{+\infty}\sum_{m=1}^{n}\lambda^m{n \brace m}\frac{d\lambda}{\lambda e^{\lambda}}$$
equals:
$$ a_n=\frac{2^n}{n!}\int_{0}^{+\infty}\mathbb{E}[X_{\mu=\lambda}^n]\frac{d\lambda}{\lambda e^{\lambda}}=\frac{2^n}{n!}\int_{0}^{+\infty}\sum_{k\geq 0}\frac{\lambda^k e^{-\lambda}k^n}{k!}\cdot\frac{d\lambda}{\lambda e^{\lambda}}\,d\lambda $$
where $X_{\mu=\lambda}$ <a href=""https://en.wikipedia.org/wiki/Stirling_numbers_of_the_second_kind#Moments_of_the_Poisson_distribution"" rel=""nofollow noreferrer"">is a Poisson random variable with expected value $\lambda$</a>. By switching $\int$ and $\sum$ we get:</p>

<p>$$ a_n = \frac{2^n}{n!}\sum_{k\geq 0}\frac{k^{n-1}}{2^k}\approx\frac{2^n}{n!}\int_{0}^{+\infty}k^{n-1}2^{-k}\,dk=\frac{1}{n}\left(\frac{2}{\log 2}\right)^n $$
hence the radius of convergence of $\sum_{n\geq 0}a_n x^n$ is positive but finite, around $\frac{\log 2}{2}\approx \frac{1}{3}$.</p>
"
"2392752","2392758","<p>Given a finite group $G$ with subgroup $H$, the notation $(G:H)$ is basically $|G|/|H|$.  So, when $H$ is the trivial group $\{1\}$ (often shortened just to $1$), $(G:H) = |G|/|H| = |G|/1=|G|$.</p>

<p>Seems silly for the book to use the heavy-handed $(G:1)$ for something as simple as $|G|$, but here we are. </p>

<p>$(G:H)$ is defined in complete generality as a possibly-infinite cardinal, so that it may be used with infinite groups.  But in the finite case it's as simple as what I wrote above. </p>
"
"2392754","2392762","<p>Please check the condition. You are given $\bar x = 600$,and $\mu$ is not given . It seems there is a mistake somewhere in the wording of the problem. Any way, to proceed to the $99$ % confidence interval ( correspond to $\alpha = 0.01$ is: CI of $\mu$ is: $ = \left( \bar x - z_{\alpha/2}\cdot \frac{\sigma}{\sqrt{n}}, \bar x + z_{\alpha/2}\cdot \frac{\sigma}{\sqrt{n}}\right)= \left(600 - 2.575\cdot \frac{12}{\sqrt{9}}, 600 + 2.575\cdot \frac{12}{\sqrt{9}}\right)= ...$</p>
"
"2392763","2395300","<p>Yes, this is computable now that you have edited the problem by putting a minus sign in front of the second integral term (rather than a plus sign).</p>

<p>You are on the right track by using Girsanov's theorem. I will use the same change-of-measure and the same notation that you have used in the question.</p>

<p>Since $\tilde B_t = B_t - \int_0^t B_s ds$, it follows from the product rule that that $$\tilde B_t e^{-t} = \frac{d}{dt} \bigg[ e^{-t} \int_0^t B_sds \bigg] \;\;\;\; \Longrightarrow\;\;\;\; \int_0^t \tilde B_se^{-s}ds = e^{-t} \int_0^t B_sds $$ where I integrated both sides to get the implication. Now multiplying both sides by $e^t$ and then differentiating yields \begin{align*} B_t &amp;= \frac{d}{dt} \bigg[ e^t \int_0^t \tilde B_s e^{-s}ds \bigg] \\ &amp;= \tilde B_t +\int_0^t \tilde B_s e^{t-s}ds \\ &amp;= \int_0^t e^{t-s}d \tilde B_s\end{align*}</p>

<p>where I used a stochastic integration-by-parts in the last equality. </p>

<p>Thus we have written $B_t$ in terms of the process $(\tilde B_s)$. But since $(\tilde B_s)$ is a Brownian motion under $\tilde P$, the above computation reveals that $B_t$ is distributed as a mean-zero Gaussian under $\tilde P$. Moreover its variance can be easily computed by using the ItÃ´ isometry: $$\sigma^2=\Bbb E^{\tilde P} [ B_t^2 ]  = \Bbb E^{\tilde P} \bigg[ \bigg( \int_0^t e^{t-s}d\tilde B_s \bigg)^2 \bigg] = \int_0^t e^{2(t-s)}ds = \frac{1}{2}(e^{2t}-1)$$</p>

<p>Now, if $Z$ is a mean-zero Gaussian, then it is well-known that $E[e^Z] = e^{\frac{1}{2}\sigma^2}$, therefore we find that $$\Bbb E^P\left[ e ^ { B_t + \int_{0}^{t}B_s\mathrm dB_s - \frac{1}{2}\int_{0}^{t}B^2_s\mathrm ds  } \right]=\Bbb E^{\tilde P}[e^{B_t}] = e^{\frac{1}{4}(e^{2t}-1)}$$ I may have made an error in some of the computations, maybe someone can post a different method to verify.</p>
"
"2392771","2392778","<p>Define $h: A \times B \rightarrow B \times A$ by $h(x,y) = (2x-2y, 5x-3y)$.  </p>

<p><strong>Claim:</strong> $h$ is not surjective.</p>

<p>Proof:  The point $(0,1)$ is not in the image of $h$.  If it were, there would be a point $(x,y) \in A \times B$ with $h(x,y) = (2x-2y, 5x-3y) = (0,1)$.  This implies that $2x-2y=0$, so that $x=y$.  But $x$ is odd and $y$ is even, so this is a contradiction.</p>

<p><strong>Claim:</strong> $h$ is injective.</p>

<p>Proof:  Suppose that $h(x,y)=h(a,b)$.  This says that $2x-2y=2a-2b$, so we get $x-y=a-b$, or $x=y+a-b$.   The second coordinates give us $5x-3y=5a-3b$.  Substitute in $x=y+a-b$ and do the algebra, and you'll get $y=b$.  Plug this back in to $x=y+a-b$ to see that $x=a$.  We now have $(x,y)=(a,b)$, so $h$ is injective.</p>
"
"2392773","2392781","<p>The Hilbert-Schmidt inner product is defined by
$$
\langle A,B \rangle = \operatorname{Tr}(AB^*)
$$
where $B^*$ denotes the adjoint (conjugate-transpose) of $B$.  Notably, if $B$ is Hermitian, then $B = B^*$.</p>

<p>The Pauli matrices indeed form an orthonormal basis with respect to the Hilbert-Schmidt inner product, which is the usual sesquilinear inner product over the complex vector space $M_n(\Bbb C)$.</p>
"
"2392774","2392825","<p>Here's a counterexample.  Let $X$ be a countably infinite set and choose a partition $$X=\bigcup_{n\in\mathbb{N}} A_n\cup \bigcup_{n\in\mathbb{N}} B_n\cup\bigcup_{n\in\mathbb{N}} C_n$$ where each $A_n,B_n,$ and $C_n$ is infinite.  Consider the topology on $X$ generated by the sets $A_n$, $A_n\cup B_n$, and $C_n$ for all $n$.  Note that $X$ is not homogeneous: for instance, if $x\in X$, then there exists a point $y\in X$ such that $x\in\overline{\{y\}}$ but $y\not\in\overline{\{x\}}$ if and only if $x\in B_n$ for some $n$ (in which case you can take $y$ to be any point of $A_n$).  However, I claim all the neighborhood spaces of $X$ are homeomorphic.</p>

<p>Fix points $a\in A_0$, $b\in B_0$, and $c\in C_0$.  Note that there are homeomorphisms of $X$ sending $a$ to any point in any $A_n$, or sending $b$ to any point in any $B_n$, or sending $c$ to any point in any $C_n$.  So it suffices to show that $N(a)$, $N(b)$, and $N(c)$ are homeomorphic.</p>

<p>First, we construct a homeomorphism $f:N(a)\to N(c)$.  We take $f$ to be any bijection $X\to X$ such that $f(A_0)=C_0$, $f(B_0)=C_1$, $f(A_n)=A_{n-1}$ for all $n&gt;0$, $f(B_n)=B_{n-1}$ for all $n&gt;0$, and $f(C_n)=C_{n+2}$ for all $n$.  Note that $f$ fails to be a homeomorphism $X\to X$ only because $B_0$ is not open but $f(B_0)=C_1$ is open.  However, $f$ is a homeomorphism $N(a)\to N(c)$, since to get an open set in $N(a)$ you must add in $A_0$ and $A_0\cup B_0$ is open.</p>

<p>Now we construct a homeomorphism $N(b)\to N(c)$.  We take $f$ to be any bijection $X\to X$ such that $f(A_0\cup B_0)=C_0$, $f(A_n)=A_{n-1}$ for all $n&gt;0$, $f(B_n)=B_{n-1}$ for all $n&gt;0$, and $f(C_n)=C_{n+1}$ for all $n$.  Such an $f$ fails to be a homeomorphism $X\to X$ only because $A_0$ is open but $f(A_0)$ (which is half of $C_0$) is not open.  This problem goes away when you consider $f$ as a map $N(b)\to N(c)$, since you must add $B_0$ to $A_0$ to get an open set in $N(b)$ and $f(A_0\cup B_0)$ is open.</p>

<hr>

<p>However, there are no Hausdorff counterexamples.  In fact, something stronger is true:</p>

<blockquote>
  <p>Let $X$ be a Hausdorff space, $a,b\in X$, and $f:N(a)\to N(b)$ be a homeomorphism.  Then $f(a)=b$ and $f$ is also a homeomorphism as a map $X\to X$.</p>
</blockquote>

<p>First, $a$ is the unique point of $N(a)$ that is contained in all nonempty open sets and similarly for $b$ in $N(b)$, so $f(a)=b$.  To prove $f:X\to X$ is a homeomorphism, it suffices to show $f$ is continuous, since the same reasoning will show $f^{-1}$ is continuous as well.</p>

<p>Suppose $U\subseteq X$ is open.  If $b\in U$, then $U$ is open in $N(b)$, so $f^{-1}(U)$ is open in $N(a)$ and hence in $X$.  So we may assume $b\not\in U$.  Let $x\in f^{-1}(U)$.  Since $b\not\in U$, $x\neq a$.</p>

<p>Since $X$ is Hausdorff, there exists an open neighborhood $V$ of $a$ in $X$ whose closure does not contain $x$.  Since $V$ is open in $N(a)$, $f(V)$ is open in $N(b)$ and hence in $X$. Now observe that $U\cup f(V)$ is open in $N(b)$, and therefore $f^{-1}(U\cup f(V))=f^{-1}(U)\cup V$ is open in $N(a)$ and hence in $X$.  Therefore $(f^{-1}(U)\cup V)\setminus \overline{V}=f^{-1}(U)\setminus\overline{V}$ is also open in $X$ (here the closure is computed in $X$).  By our choice of $V$, $x\in f^{-1}(U)\setminus\overline{V}$.  Thus $f^{-1}(U)\setminus\overline{V}$ is an open neighborhood of $x$ in $X$ which is contained in $f^{-1}(U)$.</p>

<p>Since $x\in f^{-1}(U)$ was arbitrary, this proves that $f^{-1}(U)$ is open in $X$.  Thus $f$ is continuous from $X$ to $X$, as desired.</p>
"
"2392775","2392785","<p>Yes, as you say understanding the image of just the boundary line is easy if you take it as a given that $1/z$ maps lines not through the origin to circles that pass through the origin. Then, since a circle is determined by three points, you know that the image of the boundary is the circle containing the origin, $z = 1/3$, and $z = -2i/3$ (the latter two points come from looking at where $1/z$ sends the intercepts). </p>

<p>Your description of the image of the entire region as adding up the images of a bunch of lines is essentially the correct way to think about it. If you want to do it algebraically though then notice that if we write 
$$1/z = u + iv$$
with $z = x+iy$ then 
$$u = \frac{x}{x^2 + y^2},\quad v = \frac{-y}{x^2+y^2}.$$ 
Since $1/z$ is its own inverse we then get the relations
$$x = \frac{u}{u^2 + v^2},\quad y = \frac{-v}{u^2+v^2}.$$ 
So, to understand the image of our region
$$x + 2y &gt; 3$$
in the $u,v$ plane we can just plug in to obtain
$$\frac{u}{u^2+v^2} -2\frac{v}{u^2 + v^2} &gt; 3.$$
If you manipulate this inequality and plug through the algebra (complete the square...) then you should find that the image is the interior of a circle. In particular, it will be the interior of the circle that is the image of the boundary.</p>
"
"2392782","2392812","<p>When $L$ is large enough ($L = 4$ or $\ge 6$), there will be infinitely many pairs of  $L$-tuple of tetrahedron numbers which sum to same number. </p>

<p>In particular, this happens at $L = 8$. $T_n$ satisfies a recurrence relation of the form:</p>

<p>$$\begin{align} &amp;\; T_{n+15}+T_{n+12}+T_{n+10}+T_{n+9}+T_{n+6}+T_{n+5}+T_{n+3}+T_n
\\
= &amp;\;T_{n+14}+T_{n+13}+T_{n+11}+T_{n+8}+T_{n+7}+T_{n+4}+T_{n+2}+T_{n+1}\end{align}
\tag{*1}
$$
The key is $\displaystyle\;T_n = \frac{n(n+1)(n+2)}{6}$ is a cubic polynomial in $n$.  </p>

<p>Let $\ell = \mathbb{R}^{\mathbb{N}}$ be the space of real sequences indexed by natural numbers. Let $R : \ell \to \ell$ be the operator on $\ell$ which shift  a sequence to the right. More precisely, for any real sequence $a = (a_n)$, $R(a)$ is the sequence with entries $R(a)_n = a_{n+1}$. One property of $R$ is if $a_n$ is a polynomial in $n$ of degree $m$, then $(R-1)(a) = R(a) - a$ is a polynomial in $n$ of degree $m-1$. </p>

<p>Since $T_n$ is a polynomial in $n$ of degree $3$, we have</p>

<p>$$(R-1)^4(T) = R^4(T) - 4R^3(T) + 6R^2(T) - 4R(T) + T = 0$$</p>

<p>Equivalently, this means $T_n$ satisfies a recurrence relation of the from</p>

<p>$$T_{n+4} - 4T_{n+3} + 6T_{n+2} - 4T_{n+1} + T_n = 0$$</p>

<p>For any $L &gt; 4$, if one can find a polynomial $q(x)$ with $2L$ non-zero terms, $L$ of which has coefficient $1$ and other $L$ has coefficient $-1$ and $(x-1)^4 | q(x)$, then one can turn it into recurrence relation like $(*1)$ for that particular $L$. </p>

<p>As an example, the recurrence relation in $(*1)$ is constructed using the polynomial
$$\begin{align}
q(x) = &amp; (x^8-1)(x^4-1)(x^2-1)(x-1)\\
= &amp;\;\;(x^{15}+x^{12}+x^{10}+x^9+x^6+x^5+x^3+1)\\
  &amp;-(x^{14}+x^{13}+x^{11}+x^8+x^7+x^4+x^2+x)
\end{align}$$
I hope the pattern is obvious.</p>

<p><strong>Update</strong></p>

<p>It turns out for even $L$, $L = 4$ is large enough.  </p>

<p>By trial and error, I obtain following recurrence relation:</p>

<p>$$T_{9k+8}+T_{8k+5}+T_{5k+3}+T_{4k+4}
= T_{9k+6}+T_{8k+7}+T_{5k+5}+T_{4k+3}$$</p>

<p>This implies there are infinite many pairs of $4$-tuples which sum to same number.</p>

<p>For $L = 6$, we can use $q(x) = (x^6-1)(x^4-1)(x^2-1)(x-1)$ to obtain another
recurrence relation:</p>

<p>$$\begin{align} &amp;T_{n+13}+T_{n+10}+T_{n+8}+T_{n+5}+T_{n+3}+T_{n}\\
=\;\;&amp;T_{n+12}+T_{n+11}+T_{n+9}+T_{n+4}+T_{n+2}+T_{n+1}\end{align}$$</p>

<p>This means same thing happens to $L = 6$.  Since every even integer $L \ge 4$ can be rewritten as a non-negative combination of $4$ and $6$. We can combine these two sets of recurrence relations to conclude whenever $L \ge 4$ is even, there are infinite many pairs of $L$-tuples which sum to same number.</p>

<p><strong>Update2</strong></p>

<p>By brute force, I have located a pair of $3$-tuple of tetrahedron numbers
which sums to same number.</p>

<p>$$T_{24}+T_{18}+T_{3} = 3750 = T_{19} + T_{23} + T_{8}$$</p>

<p>Since every odd $L \ge 7$ can be written as $3$ plus some non-trivial non-negative combinations of $4$ and $6$. It is easy to see for any odd $L \ge 7$, there are also infinitely many pairs of $L$-tuple which sum to same number.</p>

<p>The only thing we haven't settle is whether there are infinitely many pairs
of $L$-tuples for $L = 3$ and $5$.</p>

<p><strong>Update3</strong></p>

<p>About the corrected question whether there are triples, quadruples or even more
$L$-tuples that sum to same number. The answer is yes.</p>

<p>Consider the case $L = 3$, triples, quadruple and even quintuple are possible.
In fact, there are tons of them. By brute force, the smallest examples I can find are:</p>

<ul>
<li><em>triple</em>
$$n = 305 
    = T_{10}+T_7+T_1
    = T_9 + T_8 + T_4
    = T_9 + T_7 + T_6
$$</li>
<li><em>quadruple</em>
$$\begin{align}
n = 1795 
  &amp;= T_{21} + T_4 + T_2
  = T_{20}+T_{10}+T_5\\
  &amp;= T_{19} + T_{13}+T_3
  = T_{17}+T_{16}+T_3
\end{align}
$$</li>
<li><em>quintuple</em>
$$\begin{align}
n = 2366 &amp;= T_{23} + T_6 + T_3
          = T_{22} + T_{11} + T_6
           = T_{21} + T_{14} + T_5\\
         &amp;= T_{20} + T_{16} + T_3
         = T_{19} + T_{16} + T_{10}
\end{align}
$$</li>
</ul>

<p>I have no idea whether there are hextuples of $3$-tuples that sum to same number.<br>
My CAS simply crash when I try to search for one by brute force.</p>
"
"2392790","2392815","<p>Depending on what you are looking for, I would suggest different resources. I will assume that you have a basic knowledge of functions, sets and relations, and thus do not need a basic reference on those.</p>

<p>A good way to start is by reading some classic books: ""Algebra"" by Grove, ""Topology"" by Munkres and ""Calculus"" by Spivak provide the very basics that you should know, and more. Probably all of them are available at your local library. Even if they are only three, they will take you some time to read, even more to master, and do not even cover what a freshman should know in his first year. If you feel ambitious, consider: ""Linear Algebra done right"" by Axler, ""Algebra I, II, III"" by Cohn (I believe the modern editions have different names, but the content from the old editions is priceless), ""Calculus I, II"" by Apostol, ""Complex Analysis"" by Ahlfors, ""A fisrst course in Algebraic Topology"" by Kosniowski and ""Ordinary Differential Equations"" by Tenenbaum and Pollard. This is by no means a comprehensive list, but it will get you started: most of the above listed are books that you can use as an initiate and carry through most of your learning.</p>

<p>For further reading, with some time of getting familiar with books in the area, you will recognize most of the publishers that provide great advanced mathematical content, some of them being <a href=""http://www.springer.com"" rel=""nofollow noreferrer"">Springer-Verlag</a> (with their characteristic yellow and white books), <a href=""http://www.cambridge.org"" rel=""nofollow noreferrer"">Cambridge University Press</a>, <a href=""http://www.wiley.com"" rel=""nofollow noreferrer"">Wiley</a> and <a href=""http://press.princeton.edu"" rel=""nofollow noreferrer"">Princeton University Press</a>: just look at the references that the books above give.</p>

<p>Nowadays there are multiple sources of reliable Mathematics online (including Stack Exchange), and a lot of Professors post their notes free of charge on their websites: Terence Tao has <a href=""https://terrytao.files.wordpress.com/2016/12/linear-algebra-notes.pdf"" rel=""nofollow noreferrer"">Linear Algebra notes</a>, Sergei Treil posted his <a href=""https://www.math.brown.edu/~treil/papers/LADW/book.pdf"" rel=""nofollow noreferrer"">exceptional parody</a> of one of the books above mentioned and Keith Conrad has <a href=""http://www.math.uconn.edu/~kconrad/blurbs/"" rel=""nofollow noreferrer"">expository papers</a> that cover a wide array of subjects, not to mention the multiple Universities that provide free content through their websites, like the <a href=""https://ocw.mit.edu/courses/mathematics/"" rel=""nofollow noreferrer"">MIT</a>, I also suggest you skim through them.</p>

<p>However, in Mathematics the most important thing is to do your own work, to get involved and to try and solve problems. You may find that you understand a concept or even that you feel that you know everything about a particular one. Do not be fooled, if you have not done any exercises or problems (you don't have to look for them, almost every single Mathematics book will leave proofs to the reader and have exercises at the end of the chapter), your knowledge is not cemented and the base you are building will be weak and crumble when more complex concepts appear. Make sure that for every hour that you spend reading, you put another hour into problem solving.</p>

<p><strong>Disclaimer:</strong> The above lectures are unfortunately biased towards a more algebraic approach of Mathematics. If that is not what you desire, I can do my best to provide further literature complementing the above.</p>
"
"2392795","2392798","<p>$$f_{n+1}=f_n+2g_n$$ and
$$f_n=f_{n-1}+2g_{n-1}.$$
Thus, $$f_{n+1}-f_n=f_n-f_{n-1}+2f_n$$ or
$$f_n=4f_{n-1}-f_{n-2}$$
and we are done!</p>
"
"2392809","2392843","<p>The statement is trivally false if $|U|=|V|=0$, so assume otherwise.</p>

<p>Suppose by way of contradiction that each vertex in $V$ has degree $&gt;1$. The complete matching of $G$ gives a bijection $f:V\to U$. For each $v\in V$ we can pick an edge $(v,g(v))$ with $g(v)\neq f(v)$ (note that $g$ need not be a bijection). Since $V$ is finite, the usual pigeonhole argument implies $(f^{-1}g)^n(v_0)=v_0$ for some $n&gt;0$ and $v_0\in V$. Now define a new map $h:V\to U$ given by
$$
  h(v)=\begin{cases}
    g(v)&amp;\text{if }v=(f^{-1}g)^i(v_0)\text{ for some }i,\\
    f(v)&amp;\text{otherwise}.
  \end{cases}
$$
Note that $(v,h(v))$ is an edge for all $v\in V$. Also $h(v_0)=g(v_0)\neq f(v_0)$. Finally consider any $u\in U$. If $f^{-1}(u)=(f^{-1}g)^i(v_0)$ for some $i$ then
$$
  u=f\left((f^{-1}g)^{n+i}(v_0)\right)
    =g\left((f^{-1}g)^{n+i-1}(v_0)\right)
    =h\left((f^{-1}g)^{n+i-1}(v_0)\right).
$$
Otherwise $u=f(f^{-1}(u))=h(f^{-1}(u))$. Thus $h$ is surjective, so it is a complete matching of $G$ distinct from $f$, contradicting the hypothesis.</p>
"
"2392811","2392840","<p>Your claim that the probability is $\frac 8{6!}$ is correct, but $6! \neq 36$.  $\frac 8{6!}=\frac 8{720}=\frac 1{90}$  If you line up the women with the red shirts, then the white shirts, then the blue, the chance that the men's shirts match is $\frac 13 \cdot \frac 15 \cdot \frac 12 \cdot \frac 13\cdot 1 \cdot 1=\frac 1{90}$</p>
"
"2392813","2392837","<p>First of all, $w_1$ and $w_2$ are simply the first two components of the vector $\mathbf{w}$ which you are integrating on. What the authors refer to as the orthogonal coordinates are the last $m-2$ components of $\mathbf{w}$.</p>

<p>Now, since the input for the function is only two vectors $\mathbf{x}$ and $\mathbf{y}$, we can always, without loss of generality, rotate our coordinates such that $\mathbf{x}$ lies along the first coordinate, that is,</p>

<p>$\mathbf{x} = (\Vert\mathbf{x}\Vert,0,\ldots,0).$</p>

<p>Furthermore, we can now perform another rotation, as long as we leave the first coordinate unchanged, such that $\mathbf{y}$ is on the plane determined by the first and second coordinates, that is,</p>

<p>$\mathbf{y} = (\Vert \mathbf{y} \Vert \cos\theta ,\Vert \mathbf{y} \Vert \sin\theta,0,\ldots,0),$</p>

<p>where $\theta$ is the angle between $\mathbf{x}$ and $\mathbf{y}$.</p>

<p>Now, in our integral we integrate over an $m$-dimensional vector $\mathbf{w}$. However, since the integrand only depends on $\mathbf{x}$ and $\mathbf{y}$ through the scalar products $\mathbf{w\cdot x}$ and $\mathbf{w\cdot y}$, the last $m-2$ components of $\mathbf{w}$ are irrelevant; the scalar products do not care about the last $m-2$ components, since they are all zero for $\mathbf{x}$ and $\mathbf{y}$.</p>

<p>In other words, the reduction in dimensions follows from the fact that the integrals over each of the last $m-2$ components vanish and thus may be disregarded.</p>

<p>We have reduced the integral to an integral over only the first two components of $\mathbf{w}$, and thus</p>

<p>$\mathbf{w} = (w_1,w_2,0,\ldots,0).$</p>

<p>We can now easily calculate the scalar products and find</p>

<p>$\mathbf{w\cdot x} = \Vert \mathbf{x} \Vert w_1,$</p>

<p>$\mathbf{w\cdot y} = \Vert \mathbf{y} \Vert (w_1 \cos\theta + w_2 \sin\theta),$</p>

<p>which lead to the solution presented in the paper.</p>
"
"2392819","2392828","<p>Using the Power Rule for complex exponents 
$$
\begin{align}
(e^i)^i&amp;= e^{(i+2n\pi i)i}\text{ for }n\text{ in }\Bbb{Z} \\
&amp;= e^{-1-2n\pi}
\end{align}$$</p>

<p>Working from your chain of equalities $$
(e^i)^i=e^{i\cdot \log {(e^i)}}=e^{i\cdot(i \arg(e^i)+2k\pi i)}=e^{i\cdot(i+2k\pi i)}=e^{-1-2k\pi}
$$</p>
"
"2392822","2392938","<p>We have</p>

<p>$$
l(r) = 2r \angle ADC.
$$</p>

<p>So we just need to compute $\angle ADC$. Let $Q$ be the center of the circle on the left. Note that $AD=r$ and $DQ=AQ=R$. It follows that $\angle ADQ=\angle ADC=\arccos(r/2R)$. To see why, note that $\triangle ADQ$ is isosceles and drop a perpendicular from $Q$ to the midpoint of $AD$.</p>

<p>Is that clear?</p>
"
"2392823","2392859","<p>One way, but perhaps not the most enlightening way, is just to calculate $\phi(g)$ for every $g \in C_8 = \langle x\rangle$.</p>

<p>So:</p>

<p>$\phi(e) = \phi(x^0) = (y^3)^0 = y^0 = e'\\
\phi(x) = y^3\\
\phi(x^2) = [\phi(x)]^2 = (y^3)^2 = y^6 = e'\\
\phi(x^3) = (y^3)^3 = y^9 = y^6y^3 = y^3\\
\phi(x^4) = (y^3)^4 = y^{12} = (y^6)^2 = (e')^2 = e'\\
\phi(x^5) = (y^3)^5 = y^{15} = y^{12}y^3 = y^3\\
\phi(x^6) = (y^3)^6 = y^{18} = (y^6)^3 = (e')^3 = e'\\
\phi(x^7) = (y^3)^7 = y^{21} = y^{18}y^3 = y^3.$</p>

<p>This makes it clear that $\text{ker }\phi = \{e,x^2,x^4,x^6\}$, and $\text{im }\phi = \{e',y^3\}$. So that gives us the answer, but it doesn't give any clues as to how we might find the answer with different cyclic groups.</p>

<p>If we attempt to use your approach to find the kernel, we have:</p>

<p>$\phi(x^k) = e' \implies y^{3k} = e' \implies 6|3k$.</p>

<p>So this gives us $2|k$, that is, $k$ must be even. But that is only <em>half</em> the story: although we now know with confidence:</p>

<p>$\text{ker }\phi \subseteq \{e,x^2,x^4,x^6\}$, we haven't established that the kernel isn't actually <em>smaller</em> than this set.</p>

<p>However, if we know that a kernel is a subgroup (which you should know), and that any subgroup of a cyclic group is <em>also</em> cyclic, we have three choices for the kernel:</p>

<p>$\{e\} (= \langle e\rangle)\\
\{e,x^4\} (= \langle x^4\rangle)\\
\{e,x^2,x^4,x^6\} = (\langle x^2\rangle).$</p>

<p>You have wisely eliminated the first possibility, by noting $\phi$ is not injective (since $8 &gt; 6$). The answer above shows you $x^2 \in \text{ker }\phi$, which then settles the matter.</p>

<p>From our determination of the kernel, we know exactly half the elements of $C_8$ are mapped to the identity of $C_6$. So the image (which is a subgroup of $C_6$) has at least two elements, and therefore is either:</p>

<p>$\{e',y^3\} = \langle y^3\rangle$, $\{e',y^2,y^4\} = \langle y^2\rangle$, or $C_6$.</p>

<p>Since $y^3 = \phi(x) \in \text{im }\phi$, the second option is out. Since we can write , for any <em>odd</em>  $k$, $k = 2t+1$,</p>

<p>it is clear that for such an odd $k$:</p>

<p>$\phi(x^k) = \phi(x^{2t+1}) = \phi(x^{2t})\phi(x) = \phi((x^2)^t)\phi(x) = [\phi(x^2)]^t\phi(x) = (e')^t\phi(x) = \phi(x) = y^3$.</p>

<p>Note that:</p>

<p>$|\text{im }\phi| = 2 = \dfrac{8}{4} = \dfrac{|C_8|}{|\text{ker }\phi|}$. This is no accident. It is also no accident that $C_8$ splits cleanly into those elements that map to $e'$, and those that map to $y^3$ (and that each such set has exactly the same size-because these are <em>cosets</em>).</p>
"
"2392827","2392863","<p>Same basic way you show $\Phi(X)$ is $U(0,1)$ or more generally calculate the CDF of a monotonic function of a random variable whose CDF you know. </p>

<p>Let $Y = \Phi(a+bX)$ and calculate $$ P(Y\le y) \\= P(\Phi(a+ bX) \le y) \\= P(a+bX\le \Phi^{-1}(y)  )\\=P\left(X\le \frac{\Phi^{-1}(y) -a}{b}\right) \\=\Phi\left(\frac{\Phi^{-1}(y) -a}{b}\right)$$ (assuming $b&gt;0$). If $a=0$ and $b=1$, this gives $y$ which is the CDF of the uniform distribution.</p>
"
"2392841","2393557","<p>With regard to the follow-up question in the comment: it is <em>possible</em> to make a countable projective family of Hilbert spaces $H^k$ whose projective limit is the Schwartz space (further, with Hilbert-Schmidt transition maps), e.g., by completing Schwartz functions (or test functions) with respect to norms $|f|^2_k=\langle (-\Delta+x^2)^k f,f\rangle$. The compactness of the resolvent gives an orthonormal basis of eigenfunctions, which are provably in the Schwartz space, and which are <em>orthogonal</em> in every $H^k$ (these are Hermite polynomials times a suitable Gaussian). </p>

<p>Then it is true that $f\in H^k$ (with $k\ge 0$) has an expansion in $H^k$, and an expansion in $L^2$. Of course, the expansion in $H^k$ converges in $L^2$, and by uniqueness the coefficients must be the same. (But beware that the norms of the eigenfunctions vary depending on $k$, although they remain orthogonal.)</p>

<p>Thus, for $f\in H^\infty={\mathcal S}$, the $L^2$ expansion does converge in ${\mathcal S}$.</p>
"
"2392844","2392871","<p>To continue along the approach you've started: note that by unique factorization, you either have $c-b=1$, $c+b=a^2$ or $c-b=c+b=a$; both of these are impossible for prime $b$ and $c$ &mdash; can you see why?.</p>

<p>Alternately, you can use a sort of parity argument: exactly one of $a$, $b$, and $c$ must be $2$ (can you see why?). It trivially can't be $c$, so suppose it's $a$; then we have $4+b^2=c^2$. Now, how close can two squares be to each other?</p>
"
"2392850","2392855","<p>Any real number between $0$ and $1$ whose ternary expansion doesn't have any $1$s is in the Cantor set. So, for example, each of the following ternary expansions corresponds to a point in the Cantor set:</p>

<ul>
<li><p>$0.2020202020...$</p></li>
<li><p>$0.220022002200...$</p></li>
<li><p>$0.202200222000...$</p></li>
</ul>

<p>and so forth. It's easy to check that these don't correspond to endpoints of the Cantor set.</p>
"
"2392856","2392891","<p>When simplifying these kind of massive expressions, try renaming variables and achieving something simpler, tackling it little by little. In this particular case, set $x = (6+\sqrt{12(-1+4n)})/8$ and notice your expression is:</p>

<p>$$\frac{1}{6}(2x-1)^3-nx = - nx + \frac{4x^3}{3} - 2x^2 + x - \frac{1}{6}$$</p>

<p>Now notice:</p>

<p>$$x^3 = \frac{3n}{16}\sqrt{3(4n-1)} + \frac{27n}{16} + \frac{3}{8}\sqrt{3(4n-1)} $$</p>

<p>$$x^2 = \frac{3}{8} + \frac{3n}{4} + \frac{3}{8}\sqrt{3(4n-1)} $$</p>

<p>$$x = \frac{3}{4} + \frac{1}{4}\sqrt{3(4n-1)} $$</p>

<p>Now, set $y = \sqrt{3(4n-1)}$. Can you rewrite the expression and finish the exercise?</p>

<p><strong>Hint:</strong> Notice how the term $-nx$ cancels out with the first term from $4x^3/2$ and the first term of $-2x^2$ cancels out with the first term from $x$ (and similarly with the terms containing the lone $n$), so you only have an expression in terms of $y$ and a $-1/6$ at the end.</p>
"
"2392861","2392878","<p>Consider:</p>

<p>$$\sum_{n=0}^\infty c_n\sum_{m=0}^n {n\choose m}a^{n-m}(x-a)^m = \sum_{n=0}^\infty \sum_{m=0}^n{n\choose m}c_na^{n-m}(x-a)^m$$</p>

<p>and since:</p>

<p>$$\sum_{m=0}^n\Bigg|{n\choose m}c_na^{n-m}(x-a)^m\Bigg|&lt;\infty$$</p>

<p>because it is a finite sum, you can apply Theorem 8.3 by considering the remaining terms of the sum to be zero. Thus you can permute the sums (being careful with the indexing).</p>
"
"2392867","2392875","<p>There are really only three different locations:  a corner, a side next to a corner, and one step diagonally in from a corner.  If we can put $1$ into each of these, we can put it in any cell by using rotations and reflections.  We already have a corner.  We can subtract every number from $16$ and keep the square magic, but that doesn't help because it puts $1$ in the lower right corner. Because of the arrangement, we can add $8$ to all the numbers below $8$ and subtract $8$ from all those above, giving
$$\begin {array} {c|c|c|c} 9&amp;7&amp;6&amp;12 \\ \hline 4&amp;14&amp;15&amp;1 \\ \hline  16&amp;2&amp;3&amp;13
\\ \hline 5&amp;11&amp;10&amp;8 \end {array}$$
That gets next to the corner.  We can also rotate the $2 \times 2$ blocks by $180^\circ$ to get $$\begin {array} {c|c|c|c} 6&amp;12&amp;9&amp;7 \\ \hline 15&amp;1&amp;4&amp;14 \\ \hline  3&amp;13&amp;16&amp;2
\\ \hline 10&amp;8&amp;5&amp;11 \end {array}$$ which gets one in from the corner</p>
"
"2392868","2392876","<p>A standard way of working with these things is to just test against characteristic functions. Take $A$ and $B$ disjoint sets so that the $L^p$ norms of the characteristic functions are $|A|^{1/p}$ and $|B|^{1/p}$ respectively. </p>

<p>Now write down the parallelogram law applied to $\chi_A$ and $\chi_B$. You get (spoilers!) that </p>

<blockquote class=""spoiler"">
  <p> $$2 |A|^{2/p} + 2 |B|^{2/p} = 2 |A \cup B|^{2/p} = 2 (|A| + |B|)^{2/p}.$$</p>
</blockquote>

<p>It should be pretty quick from here to deduce the (one and only) correct value of $p$.</p>

<blockquote class=""spoiler"">
  <p> The value is $p = 2$, unsurprisingly, since this is induced by the inner product $\langle f, g \rangle = \int f \overline{g} \, d\mu$.</p>
</blockquote>
"
"2392873","2392884","<p>The variable $x$ is already fixed at that point in the statement as the person such that no roommate of this person dislikes everyone. As a result we can't define $x$ to stand for someone else. So the issue with the statement you have as a logical statement is the ""$\exists x$"" that appears after $x$ has already been fixed. We could remove this and obtain a valid statement:</p>

<p>$$\exists x\forall y(R(x,y) \to L(y,x))$$</p>

<p>However, this statement means something different than what we want; namely ""There is a person that is liked by every one of his roommates.""</p>

<p>Now this actually goes back to your original statement, which should be
$$
\forall x \exists y(R(x,y) \land \forall z \neg L(y,z))
$$
for similar reasons.</p>
"
"2392881","2393209","<p>Actually, there are nonabelian divisible groups, even finitely-generated ones (V. S. Guba, Finitely generated complete groups, Math. USSR-Izv. 29 (1987), no. 2, 233â277.). For an easier infinite example, consider $SO(n)$, $n\ge 3$: It is an infinite simple divisible group, see <a href=""https://math.stackexchange.com/questions/612649/is-so-n-mathbb-r-a-divisible-group/612653#612653"">this question</a>. </p>

<p>Anyway, even if you assume that your group is abelian, the statement you are trying to prove is false. Consider, for instance, the additive group of <a href=""https://en.wikipedia.org/wiki/Dyadic_rational"" rel=""nofollow noreferrer"">dyadic rational numbers</a>, i.e. numbers of the form
$$
\frac{a}{2^b}, \quad a\in {\mathbb Z}, \quad b\in {\mathbb N}. 
$$
These groups are not divisible, say, by 3 (I hope, that's clear),  but every nontrivial quotient is infinite (a nice exercise). </p>
"
"2392883","2393819","<p>Although your solution method is fine, I suspect that whoever gave you this exercise intended for you to use the equation of the ellipse directly instead of parameterizing the curve. Once youâve done the latter, you have a single-variable problem for which thereâs not much point in using a Lagrange multiplier: you can compute the distance of the parameterized point to the line and minimize that directly.  </p>

<p>The intended solution is likely something along these lines: The square of the distance from a point $(x,y)$ to the line is given by $$f:(x,y)\mapsto{(2x+y-10)^2\over5}.$$ You are to minimize this function subject to the constraint $g(x,y)=0$, where $g:(x,y)\mapsto\frac{x^2}4+\frac{y^2}9-1$, so form the function $F:(x,y,\lambda)\mapsto f(x,y)-\lambda g(x,y)$, set the partial derivatives of $F$ to zero and solve for $x$, $y$, $\lambda$. This will give you critical points of $f$ that youâll still have to test to see which are minima.</p>
"
"2392885","2392911","<p>To elaborate on this a bit more:</p>

<p>$$e^x \cos(x) - 1$$</p>

<p>Multiply everything through by $e^{-x}$:</p>

<p>$$e^{-x}(e^x\cos(x) - 1) = \cos(x) - e^{-x}$$</p>

<p>Notice that $h(x) = cos(x) - e^{-x}$ and $f(x) = e^x\cos(x) - 1$ have the same roots as $h(x) = e^{-x}f(x)$ and $e^{-x}$ is never $0$. So now take the deriviative of $h(x)$:</p>

<p>$$h'(x) = -\sin(x) + e^{-x}$$</p>

<p>Given any two consecutive roots of $h$ (and, well $f$), say $a$ and $b$, by Rolles Theorem there exists $c$ such that $h'(c) = 0$ so:</p>

<p>$$\begin{align}
0 &amp;= -\sin(c) + e^{-c} \\
  &amp;= \sin(c) - e^{-c} \\
  &amp;= e^c (\sin(c) - e^{-c}) \\
  &amp;= e^c \sin(c) - 1 
\end{align}
$$</p>
"
"2392896","2392910","<p>It is a theorem that if a function $g$ is holomorphic in a (deleted) neighborhood of a point and bounded in that neighborhood, then $g$ has only a <em>removable</em> singularity. (And analogously, if $|g(z)| \to \infty$ as $z \to z_0$, then $g$ has a pole there; the remaining case is that $g$ has an essential singularity.) In this case $g$ has a holomorphic extension across that point.</p>

<p>Following your comments, this deals with the issue of concluding that $f$ has zeros of multiplicity at least $3$ at every zero of $\sin^3 z$. So setting $g(z) = f(z) / \sin^3 z$ leads to a holomorphic function with a discrete set of removable singularities. Replacing $g$ with an entire extension and invoking Liouville completes the proof.</p>
"
"2392898","2392903","<p>Also there is $(0,0)$.</p>

<p>After summing of the both equations we'll get $x^3+y^3=0$, which gives $y=-x$, $x^3-2x=0$, which gives</p>

<p>$x=\sqrt2$ or $x=-\sqrt2$ or $x=0$, which gives the answer:
$$\{(\sqrt2,-\sqrt2), (-\sqrt2,\sqrt2), (0,0)\}.$$</p>
"
"2392899","2392924","<p>I'm assuming that algebra homomorphisms are required to preserve units, and that $\hom_k(A,B)$ denotes the set of $k$-algebra homomorphisms from $A$ to $B$.</p>

<p>In your first equation, $\hom_k(A,\hom_{\bar k}(\bar k,\bar k))$ doesn't make sense since $\hom_{\bar k}(\bar k,\bar k)$ is not a $k$-algebra, but the conclusion
$$
  \hom_{\bar k}(A\otimes_k \bar k,\bar k)\cong\hom_k(A,\bar k)
$$
is correct. Secondly, in general
$$
  \hom_k(A\times B,C)\neq\hom_k(A,C)\times\hom_k(B,C).
$$
Indeed given $f:A\to C$ and $g:B\to C$, if we try to define $h:A\times B\to C$ by $h(a,b)=f(a)+g(b)$, we'll run into trouble because
$$
  h((a_1,b_1)(a_2,b_2))=h(a_1a_2,b_1b_2)=f(a_1)f(a_2)+g(b_1)g(b_2)
$$
might not equal
$$
  h(a_1,b_1)h(a_2,b_2)=(f(a_1)+g(b_1))(f(a_2)+g(b_2)).
$$
In fact for the case $C=k$, we have
$$
  \hom_k(A\times B,k)=\hom_k(A,k)\sqcup\hom_k(B,k).
$$
Indeed given $f:A\times B\to k$, we have
$$
  f(1,0)f(0,1)=f((1,0)(0,1))=f((0,0))=0,
$$
$$
  f(1,0)+f(0,1)=f((1,1))=1.
$$
The first equation gives $f(1,0)=0$ or $f(0,1)=0$, and the second gives $f(0,1)=1$ or $f(1,0)=1$ respectively. In the first case define $g:B\to k$ by $g(b)=f(0,b)$. Then
$$
  f(a,b)=f((a,0)(1,0)+(0,b)(0,1))=f(a,0)\cdot0+g(b)\cdot1=g(b),
$$
so these homomorphisms correspond to $\hom_k(B,k)$, and similarly for the second case.</p>

<p>Now it is easy to see that $\hom_{\bar k}(A\otimes_k\bar k,\bar k)$ is a union of $n$ copies of $\hom_{\bar k}(\bar k,\bar k)$, where $A\otimes_k\bar k$ is a product of $n$ copies of $\bar k$.</p>
"
"2392908","2392923","<p>Let $n$ be a finite ordinal and suppose $x\in n$.</p>

<p>To prove $x$ satisfies (1), suppose $x\neq\emptyset$.  Since $x\in n$ and $n$ is a finite ordinal, $\emptyset\in n$ by (1).  Since $n$ is well-ordered by $\in$ (in particular, totally ordered), we have either $\emptyset\in x$ or $x\in\emptyset$.  The latter is impossible, and therefore $\emptyset\in x$.</p>

<p>To prove $x$ satisfies (2), note that since $n$ satisfies (3), $x\subseteq n$, and any subset of a well-ordered set is well-ordered (by the restricted order relation).</p>

<p>To prove $x$ satisfies (3), note that if $z\in y\in x$ then since $y\in x\in n$ we have $y\in n$ and since $z\in y\in n$, we have $z\in n$.  Now  $n$ is well-ordered by $\in$ and in particular $\in$ is a transitive relation on $n$, so since $x,y,z\in n$, $z\in y\in x$ implies $z\in x$.</p>

<p>To prove $x$ satisfies (4), suppose $y\in x$ is not a successor.  Then $y$ is not a successor in $n$ either, since by (3) any predecessor of $y$ in $n$ would be an element of $x$.  Since $n$ satisfies (4), we must have $y=\emptyset$.</p>

<p>To prove $x$ satisfies (5), note first that any element of a well-ordered set which is not the greatest element has a successor (the least element which is greater than it).  So it suffices to show that if $x$ has no greatest element, then it is empty.  If $x$ did not have a greatest element, then there would be no greatest element of $n$ which is less than $x$, so $x$ would not be a successor.  Since $n$ satisfies (4), this implies $x=\emptyset$.</p>
"
"2392928","2392929","<p>If $x=t+s$ and $y=t-s$, you can solve these as a system of equations for $t$ and $s$. This gives $t=\frac12(x+y)$ and $s=\frac12(x-y)$. Then you can substitute these into $z=t^2+s^2$ to get $z$ as a function of $x$ and $y$. </p>
"
"2392931","2392932","<p>It means the square made of points $(x,y)$ such that $-1 \le x \le 1$ and $-1 \le y \le 1$</p>
"
"2392933","2393679","<p>Contrary to my initial guess, we do not need that $2017$ is prime.</p>

<p><strong>Theorem.</strong>
Let $N&gt;1$ and $P,Q\in\Bbb C[X]$ such that 
$$ \tag0 P(Q(x))=P(x)^{N}\qquad\text{for infinitely many }x\in\Bbb C.$$
Then either $P$ is constant or there are $n\in\Bbb N$, $a,b,\beta\in\Bbb C$ with $a^{N-1}=b^n$ and 
$$\tag1 P(X)=a(X-\beta)^n,\quad Q(X)=b(X-\beta)^N+\beta. $$</p>

<p><em>Proof.</em>
First of all, note that the polynomial equation $(0)$  will also hold for all  $x\in\Bbb C$.
If $f$ is a polynomial and $z\in\Bbb C$,  let $v_f(z)$ denote the order of $z$ as root of $f$. So $v_f(z)=0$ for almost all $z$ and  $\sum_{z\in\Bbb C}v_f(z)=\deg f$.</p>

<p>Let $n=\deg P$, $m=\deg Q$.
As we are interested only in non-constant $P$, we can assume $n&gt;0$.
The degree of $P\circ Q$ is $nm$, that of $P^{N}$ is $Nn$. We conclude that $m=N$.</p>

<p>From $(0)$ we see that for $\alpha\in\Bbb C$ and $\beta:=Q(\alpha)$,
$$\tag2 N\cdot {v_P(\alpha)} = v_P(Q(\alpha))\cdot v_{Q-\beta}(\alpha).$$
For $\alpha$ that maximizes $v_P$ (in particular, $v_P(\alpha)&gt;0$), $(2)$ implies $v_{Q-\beta}\ge N$.
Thus $Q-\beta$ is a multiple of $(X-\alpha)^N$, i.e., 
$$\tag3Q(X)=b(X-\alpha)^N+\beta$$
with $\alpha,\beta,b\in\Bbb C$, $b\ne0$.
Then $\alpha$ is the only root of the derivative $Q'(X)=Nb(X-\alpha)^{N-1}$, hence $Q(X)-w$ with $w\ne \beta$ has $N$ distinct, simple roots and at most one of these equals $\beta$. 
Accordingly,  let $w^*$ be a root of $Q(X)-w$ with $w^*\ne\beta$.
Adapting $(2)$, we find $Nv_P(w^*)=v_P(w)$.
By infinite descent, we find that $v_P(w)=0$ for all $w\ne\beta$. In other words, 
$$\tag4P(X)=a(X-\beta)^n$$ for some $a\ne 0$.</p>

<p>Then $\beta$ is the <em>only</em> root of $P(X)^{N}$, hence the only root of $Q(X)-\beta$. Therefore  $\beta=\alpha$. By a quick comparison of leading coefficients, we arrive at $(1)$.
$\square$</p>

<p><em>Remark:</em> If we know that $P,Q$ have real coefficients, then of course $a,b,\beta$ are real.</p>
"
"2392954","2392963","<p>No.  </p>

<p>First of all, a fundamental matrix of a system of ODE's is invertible.  It can't have determinant $0$, and can't be the $0$ matrix.  </p>

<p>As for your question, consider the case $$A(t) = \pmatrix{1 &amp; 0\cr 0 &amp; -2}$$ with fundamental matrix
$$ X(t) = \exp(A t) = \pmatrix{\exp(t) &amp; 0\cr 0 &amp; \exp(-2t)\cr}$$
Then $\det X(t) = \exp(-t) \to 0$ as $t \to \infty$, but $\|X(t)\| = \exp(t)$ does not go to $0$.</p>
"
"2392959","2392965","<p>The correct formula should be </p>

<p>$$S=0.5||\operatorname{proj}_bu||\cdot||u-\operatorname{proj}_bu||$$
rather than 
$$S=0.5||b||\cdot||u-\operatorname{proj}_bu||$$</p>

<p>Edit:</p>

<p>Since the $||\operatorname{proj}_bu||=2||b||$, the answer would be $5$.</p>
"
"2392961","2392985","<p>I think the best you can do is the inclusion-exclusion formula. There are $n^{n+r}$ total sequences. For each digit $k$ there are $(n-1)^{n+r}$ sequences which don't contain $k$, so subtract those off (that is, subtract $\binom n1(n-1)^{n+r}$, since there are $\binom n1$ choices of $k$). Unfortunately we've now subtracted off the sequences that don't contain $k_1$ or $k_2$ twice, so we need to add those on again. Continuing in this manner we get
$$\sum_{a=0}^{n-1}(-1)^a\binom na(n-a)^{n+r},$$
where the term for $a$ counts all the sequences missing some particular set of $a$ digits, and multiplies by the number of sets of $a$ digits.</p>
"
"2392964","2392971","<p>$f$ is one-to-one, so it is either increasing or decreasing.  But if $f$ were decreasing, $f \circ f \circ f$ would also be decreasing, so it must be increasing.
Now if $f(x) &gt; x$ we'd have $x &lt; f(x) &lt; f(f(x)) &lt; f(f(f(x)))$, and similarly with inequalities reversed.</p>
"
"2392973","2392974","<p>No, the index variable can only appear inside the sum that uses this index variable.</p>

<p>A sum such as $\displaystyle\sum_{x=1}^3 (2x+y)$ represents a quantity that does not depend on any object named $x$. This sum is the same as $\displaystyle\sum_{b=1}^3 (2b+y)$ or $(2+y)+(4+y)+(6+y)$. It depends on the value of $y$ because $y$ is not the sum index.</p>

<p>Edit: The rule is that
$$\sum_{x}\sum_{y} q(x, y) = \sum_{x}\left(\sum_{y} q(x, y)\right)$$
If we take your last expression it would be
$$\eqalign{\sum_{y=1}^x \sum_{x=1}^3 (2x+y) &amp;= \sum_{y=1}^x \left(\sum_{x=1}^3
 (2x+y)\right)\cr
&amp;= \sum_{y=1}^x \big((2+y)+(4+y)+(6+y)\big)\cr
&amp;= \sum_{y=1}^x (12+3y)
}$$
But now we have an undefined quantity $x$ in the bounds of the sum. It is meaningless.</p>
"
"2392976","2392980","<p>No. Take $f=\chi_{\mathbb Q}$, the characteristic function of the rationals. Then every point of $\mathbb Q$ is a local maximum and every point of $\mathbb{R}\setminus\mathbb Q$ is a local minimum.</p>
"
"2392977","2392983","<p>Hint. Note that the equation is solved by
$$z=4+4i+2 \sqrt{2}(\cos(t)+i\sin(t))=4+2 \sqrt{2}\cos(t)
+i\left(4+2 \sqrt{2}\sin(t)\right)$$
with $t\in [0,2\pi)$. Hence
$$|z|^2=(4+2 \sqrt{2}\cos(t))^2
+(4+2 \sqrt{2}\sin(t))^2=40+16\sqrt{2}(\cos(t)+\sin(t))\\
=40+32\sin(t+\pi/4).$$
Hence
$$40-32\leq |z|^2\leq 40+32.$$
Can you take it from here?</p>

<p>As regards the argument consider the ratio
$$\tan(\arg(z))=\frac{\mbox{Im}(z)}{\mbox{Re}(z)}=\frac{4+2 \sqrt{2}\sin(t)}{4+2 \sqrt{2}\cos(t)}.$$
Finally you should find that the minimum argument is $\pi/12$. What is the maximum argument?</p>
"
"2392978","2392982","<p>The only stochastic matrices with a stochastic
inverse are the permutation matrices. If $A$ and $B$ are stochastic,
then all entries of $AB$ are non-negative. The only way $(AB)_{ij}$
can be zero is if the ""support"" of row $i$ of $A$ and column $j$ of $B$ are
disjoint. So if $AB=I$ we need row $1$ of $A$ to be orthogonal to columns
$2,\ldots,n$ of $B$. As these columns must be linearly independent,
then row $1$ of $A$ can only have one non-zero entry. Continuing,
all rows of $A$ have only one nonzero entry etc.</p>
"
"2392991","2392997","<p>Instead of doing the integral, you could note that $\mathbf{r}(t)$ attains $\pm \mathbf{a}$, and hence has length at least $4|\mathbf{a}|$.</p>
"
"2392992","2393688","<p>It is a rule of thumb, but there are some estimates backing that rule.</p>

<p>Given a matrix $Aââ^{nÃn}$ and a vector $bââ^n$ the linear equation system ought to be solved is $$Ax=b.$$
Now we have perturbation in both $A$ and $b$, e.g. measurement errors in the rhs or integration errors in $A$ (thinking about FEM for PDEs). So actually we solve the perturbed system 
$$\tilde{A}x=\tilde{b},$$
with $\tilde{A}=A+Î´A$, and $\tilde{b}=b+Î´b$, with the perturbation $Î´A,\ Î´b$.</p>

<p>The measure of how $x$ and $\tilde{x}=x+Î´x$ are related is the relative error 
$$\frac{\|x-\tilde{x}\|}{\|x\|}=\frac{\|Î´x\|}{\|x\|}.$$
In the following there are two intermediate results, which lead to the desired estimate.</p>

<hr>

<p><strong>Perturbation of $b$:</strong></p>

<p>Let $\tilde{x}=x+Î´x$ be the solution of the perturbed equation system $A\tilde{x}=\tilde{b}$, then it holds 
$$\frac{\|Î´x\|}{\|x\|}\leqslant \text{cond}(A)\frac{\|Î´b\|}{\|b\|}.$$
The condition number of $A$ is defined as $\text{cond}(A)=\|A\|\|A^{-1}\|$.</p>

<p>So the relative error in the solution can be estimated by the relative error in the rhs amplified with the condition number. Since that proof is quite simple I will state it for completeness. </p>

<p><em>Proof:</em> It is $$Î´x=\tilde{x}-x = A^{-1}(A\tilde{x}-Ax)=A^{-1}(\tilde{b}-b)=A^{-1}Î´b$$
Hence, we get
$$\frac{\|Î´x\|}{\|x\|}\leqslant\|A^{-1}\|\frac{\|Î´b\|}{\|x\|}\frac{\|b\|}{\|b\|} =\|A^{-1}\|\frac{\|Î´b\|}{\|b\|}\frac{\|Ax\|}{\|b\|} \leqslant\|A^{-1}\|\|A\|\frac{\|Î´b\|}{\|b\|}.$$</p>

<hr>

<p><strong>Perturbation of $A$:</strong></p>

<p>If $\|Î´A\|&lt;\|A^{-1}\|^{-1}$ then it holds
$$\frac{\|Î´x\|}{\|x\|}\leqslant \frac{\text{cond}(A)}{1-\text{cond}(A)\|Î´A\|/\|A\|}\frac{\|Î´A\|}{\|A\|}.$$
with $\tilde{x}=x+Î´x$ the solution of the perturbed system $\tilde{A}x=b$.</p>

<p>That proof is a bit more involved and need a lemma, so I will skip it here. The condition $\|Î´A\|&lt;â¦$ ensures regularity of $\tilde{A}$. (Note: Not sure if $&lt;$ or $\leqslant$.)</p>

<hr>

<p><strong>Combining both results</strong></p>

<p>Given the requirements of the theorem about perturbation in $A$ it holds 
$$\frac{\|Î´x\|}{\|x\|}\leqslant \frac{\text{cond}(A)}{1-\text{cond}(A)\|Î´A\|/\|A\|}\left(\frac{\|Î´b\|}{\|b\|}+\frac{\|Î´A\|}{\|A\|}\right),$$
with $\tilde{x}$ being the solution of the system $\tilde{A}\tilde{x}=\tilde{b}$.</p>

<p>Again I skip the proof. </p>

<hr>

<p><strong>Remark</strong></p>

<p>Note that you can ignore the denominator: 
$$\frac{\text{cond}(A)}{1-\text{cond}(A)\|Î´A\|/\|A\|} \approx \text{cond}(A),$$</p>

<p>That is because we hat the condition above, that $Î´A&lt;\|A^{-1}\|^{-1}.$ So the denominator is just some small factor $c$.</p>

<p>So you get an estimate, like in your question.
$$\frac{\|Î´x\|}{\|x\|}\leqslant [c\cdot]\text{cond}(A)\left(\frac{\|Î´b\|}{\|b\|}+\frac{\|Î´A\|}{\|A\|}\right)$$</p>

<p>So you can use that to estimate the error propagation from data to solution. But you should keep in mind, that the theorems state estimates from above. So replacing $""\leqslant""$ with $""\approx""$ will give you a very negative result. The math in physics is usually not that bad. </p>

<p>What you can see, when dealing with ill-conditioned problems, is that iterative solvers have a hard time converging. They will usually take a lot more steps to achieve an improvement of the solution guess. 
Here you can <a href=""https://en.wikipedia.org/wiki/Preconditioner"" rel=""nofollow noreferrer"">precondition</a> your system to improve convergence. 
I personally only had one (real world) problem that I could not solve due to conditioning.</p>

<p>(If you want to see the missing proofs, let me know.)</p>
"
"2392994","2393001","<p>I have posted a link with the proof in a comment. If you prefer a hint, use that the function $\ln$ is concave.</p>
"
"2392996","2392999","<p>Hint. Note that, by symmetry,
$$\int_{[-1, 1]^2}\frac{e^{x^2}}{e^{x^2} + e^{y^2}}dxdy =\int_{[-1, 1]^2}\frac{e^{y^2}}{e^{x^2} + e^{y^2}}dxdy.$$
On the other hand the following integral is quite easy,
$$\int_{[-1, 1]^2}\frac{e^{x^2}}{e^{x^2} + e^{y^2}}dxdy +\int_{[-1, 1]^2}\frac{e^{y^2}}{e^{x^2} + e^{y^2}}dxdy.$$</p>
"
"2393005","2393075","<p>$$\frac {7^2}{24^2}=\frac {1+\cos 2\alpha}{1-\cos 2\alpha} \implies$$      $$\implies 7^2(1-\cos \alpha)=24^2(1+\cos \alpha)\implies$$     $$\implies 7^2- 7^2\cos 2\alpha  = 24^2+ 24^2 \cos 2\alpha\implies$$ $$\implies  7^2-24^2= (7^2+24^2)\cos 2\alpha =25^2 \cos 2\alpha\implies$$ $$\implies -527=625\cos 2\alpha .$$</p>

<p>The missing negative sign  on the LHS of the above line is your first error.</p>

<p>Your second error is writing $\cos \frac {\alpha}{2}=\sqrt {\frac {1+\cos \alpha}{2}}\;.$ We have $|\cos \frac {\alpha}{2}|=\sqrt {   \frac {1+\cos \alpha}{2}   }\;.\;$.... If $450^o&lt;\alpha&lt;340^o$ then $225^o&lt;\frac {\alpha}{2}&lt;270^o,$ implying $\cos \frac {\alpha}{2}&lt;0.$</p>

<p>In general if $\cot x=\frac {a}{b}$ then the proportion of $\cos^2 x$ to $\sin^2 x$  is $a^2$ to $b^2$, so let $\cos^2 x=a^2y$ and $\sin^2 x=b^2y$. Since $1=\cos^2 x +\sin^2 x$, we have $y=a^2+b^2$, so $\cos^2 x =\frac {a^2}{a^2+b^2}$ and $\sin^2 x =\frac {b^2}{a^2+b^2}\;$ and therefore  $\;|\cos x|=\frac {|a|}{\sqrt {a^2+b^2}}$ and $|\sin x|=\frac {|b|}{\sqrt {a^2+b^2}}.$</p>

<p>So if $\cot \alpha =\frac {-7}{24}$ and $\cos \alpha&lt;0$ then $\cos \alpha =-\frac {7}{\sqrt {7^2+24^2}}=-\frac {7}{25}.$</p>
"
"2393009","2393016","<p>Determinants and sums of matrices do not play along very nicely, so there is no direct route to finding an expression for the determinant of $T_1 + cT_2$. However, here is something to get you started: $f(x) = \det(T_1+xT_2)$ is a polynomial with real coefficients. Is there some way you can tell that this is not the zero polynomial?</p>
"
"2393011","2393027","<p>The test function $\varphi$ is $C^\infty$ and has compact support, but its domain is not an open set. Its domain is
$$
\mathcal D=\mathbb R^n\times[0,\infty).
$$
So, $\varphi$ is compactly supported in $\mathcal D$, and thus its support could be any compact subset of $\mathcal D$.</p>

<p>Hence $\varphi$ does not have to vanish for $t=0$.</p>
"
"2393030","2393033","<p>They are  same answers.</p>

<p>I think it's better the following way.
$$4x=2x+2\pi k$$ or
$$4x=-2x+2\pi k,$$
where $k\in\mathbb Z$, which gives else the same answer:
$$\left\{\frac{\pi k}{3}|k\in\mathbb Z\right\}$$</p>
"
"2393039","2393042","<p>No, $A$ is just a proper subspace of $\mathbb{Q}^3$.
Note that $(x,x,x)\not\in A$ for any $x\in \mathbb{Q}$ and $x\not=0$. </p>
"
"2393043","2393077","<p>Let $B(x_0, \frac{1}{n})=\{x \in \mathbb{R}^n|d_2(x,x_0)  \frac{1}{n}\}$ a disk with radius $1/n$</p>

<p>Let $x \in A= \bigcap_{n=1}^{\infty} B(x_0, \frac{1}{n})$ then $d(x,x_0)&lt;\frac{1}{n},\forall n \in \mathbb{N}$ </p>

<p>Taking limits we have that $$0 \leqslant d(x,x_0) \leqslant 0 \Rightarrow d(x,x_0)=0 \Rightarrow x=x_0 \Rightarrow x \in \{x_0\}$$</p>

<p>Thus $A \subseteq \{x_0\}$</p>

<p>Also if $x \in \{x_0\}$ then $x=x_0 \Rightarrow d(x,x_0)=0&lt; \frac{1}{n}, \forall n \in \mathbb{N}$</p>

<p>Thus $x \in B(x_0, \frac{1}{n}), \forall n \in \mathbb{N} \Rightarrow x \in A \Rightarrow \{x_0\} \subseteq A$</p>
"
"2393044","2393073","<p>Take $X_n=Y_n$  together with probability $P(X_n=\frac{k}{n})=\frac1{n}$ for $k=1,\dots,n$.</p>

<p>Then $\delta(X_n,Y_n)=|\frac1{n}-\frac1{n^2}|$ so it will approach $0$ if we let $n$ grow.</p>

<p>$\text{Cov}(X_n,Y_n)=\text{Cov}(X_n,X_n)=\text{Var}(X_n)$ and will approach the positive variance of standard uniform distribution if we let $n$ grow.</p>

<p>So for $n$ large enough we will have $\delta(X_n,Y_n)&lt;\text{Cov}(X_n,Y_n)$.</p>
"
"2393047","2393060","<p>For such a linear recurrence, the general solution is $\newcommand{\la}{\lambda}a\la_1^n+b\la_2^n$. Here $\la_1&gt;|\la_2|$ and so, if $a\ne0$
$$\frac{u_{n+1}}{u_n}=\frac{a\la_1^{n+1}+b\la_2^{n+1}}
{a\la_1^n+b\la_2^n}
=\la_1\frac{1+(b/a)(\la_2/\la_1)^{n+1}}{1+(b/a)(\la_2/\la_1)^{n}}.
$$
As $|\la_2/\la_1|&lt;1$ this tends to $\la_1$. </p>

<p>Of course, you may be unlucky and have a particular solution with $a=0$,
in which case this argument fails. But here, you don't.</p>
"
"2393049","2393085","<p>I write $xRy$ for $(x,y)\in R$.</p>

<p>A relation $R$ is transitive if and only if, for all $x$, $y$, and $z$, if $xRy$ and $yRz$, then $xRz$. </p>

<p>Note that a statement of the form `if $p$, then $q$' is true if $p$ is false. </p>

<p>The point of the exercises is to notice that there are no $x$, $y$, and $z$ such that $xRy$ and $yRz$. So, it is (trivially) true that, for all $x$, $y$, and $z$, if $xRy$ and $yRz$, then $xRz$. So, the relation $R$ with which you are dealing in this exercise is transitive.  </p>
"
"2393053","2393151","<p>An alternative way to so this is to note that
$$\int_B x^2\,dx\,dy\,dz=\int_B y^2\,dx\,dy\,dz=\int_B z^2\,dx\,dy\,dz$$
where $B$ is the set $\{(x,y,z):x^2+y^2+z^2\le a^2\}$. Thererefore
each of these integrals is
$$\frac13\int_B(x^2+y^2+z^2)\,dx\,dy\,dz$$
which is really easy to do in spherical polars: it is
$$\frac13\int_0^a r^2(4\pi r^2)\,dr=\frac{4\pi a^5}{15}.$$</p>

<p>Let $B'=\{(x,y,z)\mid(x-a)^2+(y-a)^2+(z-a)^2\le a^2\}$.
This is the ball of radius $a$ centred at $(a,a,a)$. It's just
a translation of $B$, so
$$\int_{B'}f(x,y,z)\,dx\,dy\,dz=\int_{B}f(x+a,y+a,z+a)\,dx\,dy\,dz.$$
Then
$$\int_{B'}x^2\,dx\,dy\,dz=\int_B (x+a)^2\,dx\,dy\,dz
=\int_B x^2\,dx\,dy\,dz+2a\int_B x\,dx\,dy\,dz
+a^2\int_B \,dx\,dy\,dz.$$
The first you've done. The second is zero, and the last is $a^2$ times
the volume of $B$.</p>
"
"2393063","2393098","<p>Suppose $$n=2^{n_1}\cdot 3^{n_2}\dots \cdot p_r^{n_r}$$ where $p_r$ is the $r^{th}$ prime. The number of divisors of $n$ $$d(n)=(n_1+1)(n_2+1)\dots (n_r+1)$$</p>

<p>The number of factorisations is half of this unless $n$ is a square in which case it is half $d(n)+1$.</p>

<p>To find a number having $k$ factorisations, reverse this process. Look at factorisations of $2k$ and allocate the largest factors to the smallest primes. Check also factorisations of $2k-1$.</p>

<hr>

<p>Suppose $k=2$ we need $d=2k=4$ or $d=2k-1=3$.</p>

<p>If $d=4$ we have either </p>

<p>(1) the single factor $4$ which gives exponent $4-1=3$ and choose the lowest prime, $2^3=8$ with factorisations $1\cdot 8$ and $2\cdot 4$</p>

<p>(2) The factors $2\cdot 2$ which gives two exponents equal to $2-1=1$. The lowest possibility is $2^13^1=6$ and factorisations $1\cdot 6$ and $2\cdot 3$</p>

<p>If $d=3$ we have just the factor $3$ and exponent $3-1=2$ which gives $2^2=4$ with $1\cdot 4$ and $2\cdot 2$</p>

<p>This tells us that the possible lowest numbers having two factorisations are $4, 6, 8$ - obviously $4$ is the one.</p>

<hr>

<p>With $k=6$ we have $d=12$ or $d=11$</p>

<p>$d=12$ factorises </p>

<p>(1) as $12$ giving $2^{11}=2048$ </p>

<p>(2) $6\cdot 2$ giving $2^5\cdot 3=96$ </p>

<p>(3) $4\cdot 3$ giving $2^3\cdot 3^2=72$ </p>

<p>(4) $3\cdot 2 \cdot 2$ giving $2^2\cdot 3\cdot 5=60$</p>

<p>$d=11$ gives just $2^{10}=1024$</p>

<p>Clearly $60$ is the number you want.</p>

<hr>

<p>In this last case, how do we count the factors of $60$?</p>

<p>Well with the prime $2$ we have three possible factors $1,2,4$</p>

<p>With the prime $3$ we get $1,3$</p>

<p>With the prime $5$ we get $1,5$</p>

<p>We need to choose one factor associated with each of the primes, hence $3\times 2\times 2=12$ possibilities.</p>

<hr>

<p>How do we know we got all the possibilities for $12=2^2\cdot 3^1$. Well this gives $d=6$ and $k=3$ for two factor factorisations, and there is just the fourth possibility $2\cdot 2\cdot 3$ - we got them all.</p>
"
"2393069","2393101","<p>It is dangerous to take concepts from finite-dimensional cases and applying them to infinite-dimensional objects. The determinant a priori makes only sense finite dimension (the infinite products very much ruins everything).</p>

<p>The main problem with your reasoning here, is that the exponential function is not in the polynomial space and therefore, it is not an eigenfunction of your operator. If you want the exponential function to lie in your function space, then you would need to consider the ring of formal power series. However, if you do that, then $1, x, x^2, \dots$ are not a basis anymore.</p>

<p>In fact, we can directly check, that your operator has no eigenvalues except $0$. Let $(a_0, a_1, \dots)$ be an element of your polynomial function space (the $a_i$ representing the coefficients with respect to the canonical basis), then the eigenvalue equation reads</p>

<p>$$  (a_1, 2a_2, \dots ) = D(a_0, a_1, \dots) = \lambda (a_0, a_1, \dots) = (\lambda a_0, \lambda a_1, \dots).  $$</p>

<p>Thus, we have for all $n\geq 0$</p>

<p>$$ \lambda a_n = (n+1) a_{n+1}.$$</p>

<p>By induction, one proves $a_n= \frac{\lambda^n}{n!} a_0$ for $n\geq 0$. Hence, there are no eigenvalues for $\lambda \neq 0$ (polynomials have only finitely many coefficients). Thus, the only possible eigenvalue is zero. </p>

<p>For $\lambda = 0$ we have the eigenvector</p>

<p>$$ (1, 0, 0, \dots). $$</p>
"
"2393070","2393082","<p>So the vowels in ""GARDEN"" are 'A' and 'E'. Total permutations of the word GARDEN are $6! = 720.$ In half of them, A will occur before E and in the other half of the permutaions, E will occur before A. This is obvious as ""A before E"" and ""E before A"" are equally likely and exclusive(nothing other than these two can happen) events.</p>

<p>So both must have probability $1/2$.
Hence in half of the words, vowels are in alphabetical order.
Hence, answer is $720/2 = 360$.</p>
"
"2393071","2393088","<blockquote>
  <p>""So the hint was to first show that $A$ is an ordinal, which I did.""</p>
</blockquote>

<p>Then you are almost ready. </p>

<p>Ordinals are comparable when it comes to relation $\in$ so if $\alpha$ and $A$ are ordinals then: $$\alpha\in A\vee \alpha=A\vee A\in\alpha$$</p>

<p>It remains to rule out that $\alpha\in A$.</p>
"
"2393076","2393093","<p>If you use the binomial expansion you get $$\left(1+\frac 1n\right)^n=1+n\cdot \frac 1n+\frac {n(n-1)}{2!}\left(\frac 1n\right)^2+\frac {n(n-1)(n-2)}{3!}\left(\frac 1n\right)^3\dots=$$$$=1+1+\frac 1{2!}\left(1-\frac 1n\right)+\frac 1{3!}\left(1-\frac 1n\right)\left(1-\frac 2n\right)+\dots$$</p>

<p>I will leave you to conclude.</p>
"
"2393089","2393506","<p>Always use the definitions. $\cup C$ is the set of the members of the members of $C.$ That is, $x\in \cup C\iff \exists d\in C\;(x\in d).$</p>

<p>$C$ is a cover of $X $ iff $\cup C\supset X.$ If $C$ is a cover of $X$ then $D$ is a sub-cover of $C$ iff $D\subset C$ and $\cup D\supset X.$ </p>

<p>It would be more logical to say ""$D$ is a sub($X$)-cover  of $C$"" or something like that, because whether  or not $D$ is a sub-cover of $C$ depends upon the $X$ that is being covered, unless $\cup D=\cup C.$</p>

<p>With $X=\{1,2,3\}$ and $C=\{\{1\},\{2\},\{1,3\}\}$ we have</p>

<p>$$x\in \cup C\iff (x\in \{1\}\lor x\in \{2\}\lor x\in \{1,3\})\iff x\in \{1,2,3\}=X.$$ So $\cup C=X.$ So $C$ is a cover of $X.$</p>

<p>And $U=\{\{2\}\}, \{1,3\}\}\subset C$ with $\cup U=\cup C=X,\;$ so $ U$ is a sub-cover of $C.$</p>
"
"2393092","2394389","<p>Purely geometric solution: </p>

<p>Since $AB = AC$, the line $AO$ is the orthogonal bisector of segment $BC$. Choose point $E$ on the line $AO$ (and inside the triangle $ABC$) so that $\angle \, BCE = 60^{\circ}$. </p>

<p><a href=""https://i.stack.imgur.com/1iFIw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1iFIw.png"" alt=""enter image description here""></a></p>

<p>Then, by construction, triangle $BCE$ is equilateral. Thus, $$\angle \, ABE = \angle\, ABC - \angle \, CBE = 80^{\circ} - 60^{\circ} = 20^{\circ}$$
 $$\angle \, ACE = \angle\, ACB - \angle \, BCE = 80^{\circ} - 60^{\circ} = 20^{\circ}$$ However, since $O$ is the circumcenter of $ABC$ we know that $OA - OB = OC$ and $\angle \, BOA = 2 \, \angle \, BCA = 160^{\circ}$ which means that $\angle \, ABO = 10^{\circ}$ and that $$\angle \, EBO = \angle \, ABE - \angle \, ABO = 20^{\circ} - 10^{\circ} = 10^{\circ}$$ Therefore, $BO$ is the angle bisector of $\angle \, ABE$ and by the angle bisector theorem
$$\frac{AO}{OE} = \frac{AB}{BE} = \frac{AB}{BC}$$ because $BE = BC \,$ (triangle $BCE$ is equilateral). However, $BD$ is the angle bisector of angle $\angle \, ABC$ so again by the angle bisector theorem
$$\frac{AD}{DC} = \frac{AB}{BC}$$ Thus
$$\frac{AO}{OE} = \frac{AB}{BC} = \frac{AD}{DC}$$ which by Thales' intercept theorem implies that $DO$ is parallel to $CE$. Consequently, 
$$\angle \, ADO = \angle \, ACE = 20^{\circ}$$ and by angle chasing $\angle \, BDO = 100^{\circ}$.</p>
"
"2393104","2397862","<p>Assume ${\bf F}$ as given, and assume that the upper hemisphere $S: \&gt;x^2+y^2+z^2=1,\ z\geq0$ is oriented upwards. The surface $S$ has a boundary cycle $\partial S$ which is the unit circle in the $(x,y)$-plane, oriented counterclockwise.</p>

<p>We are told to compute the flux integral
$$\Phi:=\int_S{\rm curl}({\bf F})\cdot{\bf n}\&gt;d\omega\ .\tag{1}$$
This computation can be performed in three ways:</p>

<p>(i) Compute ${\bf C}:={\rm curl}({\bf F})$ as a function  of $x$, $y$, $z$, use the parametric representation
$${\bf r}(\phi,\theta):=\bigl(\cos\theta\cos\phi,\cos\theta\sin\phi,\sin\theta\bigr)$$
($\phi$ and $\theta$ are GPS coordinates) for $S$ and compute the surface integral as given:
$$\Phi=\int_0^{\pi/2}\int_0^{2\pi}{\bf C}\bigl({\bf r}(\phi,\theta)\bigr)\cdot\bigl({\bf r}_\phi\times{\bf r}_\theta)\&gt;d\phi\&gt;d\theta\ .$$
(ii) Use Stokes' theorem to convert $(1)$ into a line integral along $\partial S$; then compute this line integral:
$$\Phi=\int_{\partial S}{\bf F}\cdot d{\bf r}\ .\tag{2}$$ Going this way you don't even have to compute ${\bf C}$, but you need to parametrize $\partial S$:
$${\bf r}(\phi)=(\cos\phi,\sin\phi,0)$$
and plug this into $(2)$.</p>

<p>(iii) You have chosen a third way, namely using Gauss' theorem. This theorem deals with a three-dimensional solid $B$ and its boundary surface $\partial B$. We define $B$ to be the half ball bounded by  $S$ and the unit disc $U$ in the $(x,y)$-plane oriented downwards. Gauss' theorem then gives
$$\int_{\partial B}{\bf C}\cdot{\bf n}\&gt;d\omega=\int_B{\rm div}({\bf C})\&gt;{\rm dvol}=0\ ,$$
since ${\rm div}\circ{\rm  curl}=0$. From $\partial B=S+U$ it follows that
$$\Phi=-\int_U {\bf C}\cdot{\bf n}\&gt;{\rm d}(x,y)=\int_U C_3(x,y,0){\rm d}(x,y)\ .$$
It remains to correctly compute $C_3$, which I leave to you.</p>
"
"2393107","2393244","<p>The main term behaves like $k^{-\left(1+\frac{1}{n}\right)}$, hence it should not be difficult to tackle the problem through creative telescoping (section 1 <a href=""https://drive.google.com/file/d/0BxKdOVsjsuEwdjBEM1dpRkhMa2s/view"" rel=""nofollow noreferrer"">here</a>). If $a&gt;b&gt;0$ and $n\geq 1$ we have
$$ n(a-b)b^{n-1}\leq a^n-b^n \leq n(a-b)a^{n-1} \tag{1}$$
by simply considering the expansion of $\frac{a^n-b^n}{a-b}=a^{n-1}+\ldots+b^{n-1}$.<br> If we pick $a$ as $\frac{1}{k^{1/n}}$ and $b$ as $\frac{1}{(k+1)^{1/n}}$ we get:</p>

<p>$$\small n\left(\frac{1}{k^{1/n}}-\frac{1}{(k+1)^{1/n}}\right)\frac{1}{(k+1)^{1-1/n}}\leq \frac{1}{k(k+1)}\leq n\left(\frac{1}{k^{1/n}}-\frac{1}{(k+1)^{1/n}}\right)\frac{1}{k^{1-1/n}}$$
from which:
$$ \frac{1}{k(k+1)^{1/n}}\geq n\left(\frac{1}{k^{1/n}}-\frac{1}{(k+1)^{1/n}}\right)\tag{2} $$
leading to the claim in a straightforward way:
$$ \sum_{k\geq 1}\frac{1}{k(k+1)^{1/n}}\geq n. $$</p>
"
"2393110","2393118","<p>There is no answer to this question, because there is no unique order in 2D.</p>

<p>You might say, I will take the largest $y-x$, as this favors large $y$ and small $x$. Then D is the best.</p>

<p>But you might as well consider $y/x$ for the same reason, and conclude $A$.</p>

<p>With this particular dataset, you can find criteria for which any solution is the ""best"".</p>
"
"2393115","2393142","<p>I think that you could ask the question at <a href=""https://mathematica.stackexchange.com/"">https://mathematica.stackexchange.com/</a>. </p>

<p>This seems to be related to accuracy issues since the explicit formula in terms of $x$ is correct but for $x=100$, the result is
$$-\frac{18004190799729005916153364606626760}{9009}+$$ $$99920027994400699944002799920001
   \log \left(\frac{101}{99}\right)$$ which does not look simple (on my side, I gave up).</p>

<p>Computing each piece separately to $50$ significant decimals, I effectively obtained $0.0059908$.</p>

<p><strong>Edit</strong></p>

<p>In fact, problems happen even for quite small values of $x$. We even can get negative values for the integral. For example, using $x=11$, the result is given as $\color{red}{-1.00000}$ while it corresponds to
$$42998169600000000 \log \left(\frac{6}{5}\right)-\frac{32102724739022409728}{4095}$$ Doing the same as above, this is $0.0544852$ which is exactly the value obtained using numerical integration.</p>
"
"2393129","2393144","<p>$P^2(\mathbb{R})$ has dimension $3$, there is no way for that set being a basis for it. You meant ""(...) linearly independent and span $X$ (...)""</p>

<p>Well, that set span $X$ by definition, and it is linearly independent 'cause
$$a(x^2-4/3) + bx = 0 = 0x^2+0x+0$$
implies
$$a=b=0.$$</p>

<hr>

<p>Answering your question: the matrix shouldn't have $3\times 3$. It only needs to have $2$ linearly independent lines or columns (and it has!)</p>
"
"2393130","2393138","<p>Hint: draw lines through $P$ parallel to the sides of rectangle. Use Pythagoras theorem for the lines $PA, PB, PC, PD$ to make up the system of four equations. Then you get the answer $1$.</p>

<p>Details:</p>

<p><a href=""https://i.stack.imgur.com/CASp2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CASp2.png"" alt=""enter image description here""></a></p>

<p>$$\begin{cases} a^2+d^2=x^2 \\ a^2+b^2=11^2 \\ b^2+c^2=13^2 \\ c^2+d^2=7^2\end{cases} \stackrel{(4)-(3)+(2)}{\Rightarrow} x^2=7^2-13^2+11^2=1 \Rightarrow x=1.$$</p>
"
"2393139","2393207","<blockquote>
  <p><em>Hint:</em> Use the representation
  \begin{align*}
\cos x=\frac{e^{ix}+e^{-ix}}{2}\qquad\qquad \sin x=\frac{e^{ix}-e^{-ix}}{2i}
\end{align*}
  expand 
  \begin{align*}
y(x)=e^{2x}\cos^2(x)\sin(x)
\end{align*}
  and differentiate.</p>
</blockquote>

<p>Alternative:</p>

<blockquote>
  <p>Recall the <em><a href=""https://en.wikipedia.org/wiki/General_Leibniz_rule"" rel=""nofollow noreferrer"">general Leibniz rule</a></em> and consider
  \begin{align*}
\frac{d^n}{dx^n}\left(e^{ax}\sin(bx)\right)=\sum_{k=0}^n\binom{n}{k}\frac{d^k}{dx^k}\left(e^{ax}\right)\frac{d^{n-k}}{dx^{n-k}}\left(\sin(bx)\right)
\end{align*}</p>
</blockquote>
"
"2393141","2393553","<p>Your interpretation in (a) seems fine. Once you reject $H_0$ that all
population means are equal, the next task is to investigate what the
pattern of differences may be.</p>

<p>Specifically, part (b) investigates whether $\mu_L = \mu_M$ by looking
at a CI for the difference $\mu_L - \mu_M.$ The assumption of the ANOVA
in (a) is that all three populations have the same variance. Still assuming
the Low and Medium populations have the same variance, a 95% confidence interval (CI) based on
a two-sample ""pooled"" t procedure is $(-2.296,  6.581),$ as computed in
R statistical software below. (You can verify this by hand using a formula
in your text, or by using MATLAB.)</p>

<p>Because the CI contains $0,$ we conclude that
a $0$ difference (i.e. equal means) is believable. Also, from the output
below, the P-value for a two-sample pooled t test is about $0.31 &gt; 0.05,$
so we cannot reject $H_0: \mu_L = \mu_M.$ </p>

<pre><code>Low = c(31,36,34,37,39,34,33)
Med = c(36,31,39,32,36,30,25)

t.test(Low, Med, var.eq=T)

        Two Sample t-test

data:  Low and Med
t = 1.0519, df = 12, p-value = 0.3136
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
-2.295540  6.581254
sample estimates:
mean of x mean of y 
 34.85714  32.71429 
</code></pre>

<p>If you were do do all three possible comparisons of pairs of means
from the original data, you would have three comparisons. If each comparison
is made at the 5% level, then there is some doubt what the overall
error probability is. There are several kinds of <em>multiple comparison procedures</em>
for dealing with situations. Some names are 'Fisher LSD', 'Tukey HSD', and
'Bonferroni'. I suppose this may be the next topic in your course.</p>

<p>The Bonferroni method is to do each of the three tests at level $\alpha = .05/3 = .017.$ (Or each of three CIs at confidence level 98.3%.) This ensures that
the overall error probability is not greater than 5%.</p>

<p><em>Note:</em> If you are comparing levels Med and High it would be best to use a
Welch ('separate-variances') two-sample procedure because failure of the equal-variance assumption
can be especially dangerous when group sample sizes are unequal. To get
the Welch CI in R, omit the argument <code>var.eq</code> to obtain the 95% CI $(0.164, 11.265),$ which does <em>not</em> contain $0;$ the P-value for testing $H_0: \mu_M = \mu_H$ against the two sided alternative is $0.045 &lt; .05,$ so you could reject
at the 5% level.</p>
"
"2393148","2393240","<p>No, consider $f_n = 1_{A_n}$ where we define,</p>

<p>$$ A_n = [0,1/n] + n =  \left[n, n + \frac1n \right]. $$</p>

<p>Evidently $f_n \rightarrow 0$ pointwise and for all $\varepsilon &gt;0$ we have $\mu(\{|f_n| &gt; \varepsilon\}) \leq \mu(\{f_n \neq 0\}) = 1/n \rightarrow 0.$ So $f_n \rightarrow 0$ in measure.</p>

<p>Now let $A \subset \mathbb R$ be measurable with $\mu(A)&lt;\infty.$ Since $\mu\left(\bigcup_{n \geq N} A_n\right) = \infty$ for all $N$ (as the harmonic series diverges), there $m \in \mathbb N$ such that $A_n \setminus A \neq \varnothing$ for all $n \geq m.$ But then for all $n \geq m,$ there exists $x_n \in A_n \setminus A$ such that $f_n(x_n) = 1.$ Therefore $f_n$ does not converge uniformly to $0$ on $\mathbb R \setminus A.$</p>
"
"2393150","2393167","<p>Note that <em>both sides</em> of the first equality unfold to
$$ \Big(\frac{13}{12}\Big)^{-3} + \Big(\frac{13}{12}\Big)^{-4} + 
\Big(\frac{13}{12}\Big)^{-5} + \Big(\frac{13}{12}\Big)^{-6} + \cdots $$
The terms are just indexed differently:
$$ \underbrace{\Big(\frac{13}{12}\Big)^{-3}}_{n=1} +
 \underbrace{\Big(\frac{13}{12}\Big)^{-4}}_{n=2} + 
 \underbrace{\Big(\frac{13}{12}\Big)^{-5}}_{n=3} +
  \underbrace{\Big(\frac{13}{12}\Big)^{-6}}_{n=4} + \cdots $$
versus
$$ \underbrace{\Big(\frac{13}{12}\Big)^{-3}}_{k=3} +
 \underbrace{\Big(\frac{13}{12}\Big)^{-4}}_{k=4} + 
 \underbrace{\Big(\frac{13}{12}\Big)^{-5}}_{k=5} +
  \underbrace{\Big(\frac{13}{12}\Big)^{-6}}_{k=6} + \cdots $$</p>
"
"2393160","2393176","<p>As said in the comments, Cauchy's theorem only applies to simply connected regions.
For example, if you choose $U=\mathbb C\setminus \{0\}$, then $\frac1z$ is holomorphic in $U$,
but $U$ is not simply connected.</p>

<p>If $U$ is simply connected and does not contain $0$, then you cannot find a path that winds around $0$, so you dont have to worry about contour integrals whose path winds around $0$.</p>

<p>Note that the evaluation of the ""loop of the logarithm"" can only be nonzero, if you wind around $0$ (at least) once.</p>
"
"2393161","2393188","<p>Let $y(x)=ce^{\lambda{x}}$, plug this into the equation to give
$$c=\frac{1}{e^{\lambda}-1}$$
Hence
$$\lambda=\ln\Big(\frac{1}{c}+1\Big)$$
$$y(0)=3$$
Thus
$$c=3$$
and
$$\lambda=\ln\Big(\frac{4}{3}\Big)$$
and
$$y(x)=3e^{\ln\big(\frac{4}{3}\big)x}=3e^{\ln\big(\frac{4^{x}}{3^{x}}\big)}=\frac{4^{x}}{3^{x-1}}$$</p>
"
"2393165","2393186","<p>Clearly $(\mathbb{R}, d)$ is bounded, since $d(x,y) &lt; 1$ for every $x,y\in\mathbb{R}$.</p>

<p>Let us prove that $(\mathbb{R}, d)$ is complete.
Namely, let $(x_n)\subset\mathbb{R}$ be a Cauchy sequence. 
Given $\epsilon\in (0,1)$, let $\eta := \epsilon / (1-\epsilon)$, so that
$\eta/(1+\eta) = \epsilon$. Since $(x_n)$ is a Cauchy sequence, there exists $N\in\mathbb{N}$ such that
$$
d(x_j, x_k) &lt; \eta,
\qquad \forall j,k\geq N,
$$
i.e.
$$
\frac{|x_j - x_k|}{1+|x_j-x_k|} &lt; \frac{\epsilon}{1+\epsilon}
\qquad \forall j,k\geq N.
$$
Since the function $t\mapsto t/(1+t)$ is strictly increasing in $[0,+\infty)$,
the last condition is equivalent to
$$
|x_j - x_k| &lt; \epsilon
\qquad \forall j,k\geq N.
$$
In other words, we have proved that $(x_n)$ is a Cauchy sequence in $(\mathbb{R}, |\cdot|)$.
Since $(\mathbb{R}, |\cdot|)$ is complete, the sequence $(x_n)$ is convergent in 
$(\mathbb{R}, |\cdot|)$, i.e. there exists $x\in\mathbb{R}$ such that
$$
\lim_{n\to +\infty} |x_n - x| = 0.
$$
But this implies that
$$
\lim_{n\to +\infty} \frac{|x_n - x|}{1+|x_n-x|} = 0,
$$
hence $(x_n)$ is convergent also in $(\mathbb{R}, d)$.</p>

<p>Finally, the two metrics generate the same topology, so that $(\mathbb{R}, d)$ is not compact.</p>
"
"2393173","2393239","<p>Your solution is correct although it is strange that the system has a single solution because the following questions become trivial. Indeed, the orthogonal projection of a vector onto the zero subspace is the zero vector (there is no other choice for the orthogonal projection must belong to the subspace). Also, the orthogonal complement of the zero subspace is the whole space.</p>
"
"2393178","2393197","<p>Let $p$ be the smallest prime such that $p|n$.</p>

<p>(<strong>edit:</strong> Note that $2|n$ and $5|n$ are impossible, so $p\neq 2$ and $p\neq 5$.)</p>

<p>Let $a&gt;0$ be the smallest integer such that $10^a \equiv 1 \mod p$.
Because of $10^n \equiv 1 \mod p$ it can be shown that $a|n$.</p>

<p>Suppose $a=1$, then it follows $p=3$ and we are done.</p>

<p>Since $10^{p-1}\equiv 1\mod p$ we know that $a&lt;p$.</p>

<p>So we have $1&lt;a&lt;p$ and $a|n$, which is a contradiction to $p$ being the smallest prime that divides $n$.</p>
"
"2393190","2393204","<p>You are assuming, without justification, that $R\otimes_k R\to R(xy-yx)R$ given by $f\otimes g\mapsto f(xy-yx)g$, is an isomorphism. It isn't.</p>
"
"2393194","2393202","<p>We have $\cos (\overline{z})=\cos x \cosh y+i \sin x \sinh y$.</p>

<p>Now check for which values of $z$ the Cauchy-Riemann differential equations are valid.</p>
"
"2393198","2393222","<p>Answer to part a): each quasi-polynomials in the space above has the form $a_0e^{-t}+a_1te^{-t}+a_2t^2e^{-t}$, hence is a linear combination of $e^{-t}, te^{-t}, t^2e^{-t}$.</p>

<p>Answer to part b): solve the linear system</p>

<p>$\begin{bmatrix}
0 &amp; -2 &amp; 2 \\
0 &amp; 0 &amp; -4 \\
0 &amp; 0 &amp; 0
\end{bmatrix} * \begin{bmatrix}
a_0 \\
a_1 \\
a_2
\end{bmatrix}=\begin{bmatrix}
0 \\
0 \\
0
\end{bmatrix}$</p>

<p>.</p>
"
"2393199","2393233","<p>The rank of $\mathcal{O}(k_1)$ and $\mathcal{O}(k_2) \otimes \dots \otimes \mathcal{O}(k_n)$ is both one (the tensor product of line bundles is still a line bundle) so the coefficients $\operatorname{rank} F, \operatorname{rank} E$ in the formula for $c_1(E \otimes F)$ are both one.</p>
"
"2393201","2393203","<p>A necessary condition for the series
$$
\sum a_j
$$
to converge is $a_j\to0$ as $j\to\infty$. If you can show that $a_j\not\to0$ as $j\to\infty$, then this implies that the series diverges. See <a href=""https://en.wikipedia.org/wiki/Term_test"" rel=""nofollow noreferrer"">here</a> for more details.</p>
"
"2393227","2393639","<p>The following is partly educated guessing, but it fits.
Basically it is just about interpreting everything carefully. Here comes the TL; DR; -version.</p>

<p>Assuming that:</p>

<ul>
<li>$x\in V=\Bbb{F}_2^8$ and $\phi:V\to V$,</li>
<li>In the sum $x\oplus a$ the integer $a$ is transformed to an element of $V$ by using the base-2 digits as components. So if $a=\sum_{i=0}^7a_i2^i, a_i\in\{0,1\}$, and $x=(x_0,x_1,\ldots,x_7)\in V$, then
$$x\oplus a=(x_0+a_0,x_1+a_1,\ldots,x_7+a_7),$$ where here the sums of the components are sums in $\Bbb{F}_2$.</li>
<li>If $n\in\{0,1,2,\ldots,7\}$ the operator $D_{2^n}$ is defined as follows. If $g:V\to V$, then $D_{2^n}(g):V\to V$ is the function defined by
$$D_{2^n}(g):x\mapsto g(x\oplus 2^n)-g(x).$$
Of course, in $V$ subtraction = addition, so instead of a difference we can use a sum just as well.</li>
</ul>

<p>With this definition we do get the formula 
$$
D_1D_2D_4D_8(g):x\mapsto \sum_{a=0}^{15}g(x\oplus a).
$$
This is just a manifestation of the fact that, when viewed as above, the set
$W=\{0,1,2,\ldots,15\}$ is a subgroup of $V$. It is actually a subspace spanned by the linearly independent elements $1$, $2$, $4$ and $8$.</p>

<p>To see this we first observe that
$$
D_1(g):x\mapsto g(x\oplus1)+g(x)=\sum_{a=0}^1g(x\oplus a).
$$
Then
$$
\begin{aligned}
D_1D_2(g):x&amp;\mapsto D_2(g)(x\oplus1)+D_2(g)(x)\\
&amp;=\left(g((x\oplus1)\oplus2)+g(x\oplus1)\right)+\left(g(x\oplus2)+g(x)\right)\\
&amp;=g(x\oplus3)+g(x\oplus1)+g(x\oplus2)+g(x)\\
&amp;=\sum_{a=0}^3g(x\oplus a).
\end{aligned}
$$
Repeating the dose
$$
\begin{aligned}
D_1D_2D_4(g):x&amp;\mapsto \sum_{a=0}^3D_4(g)(x\oplus a)\\
&amp;=\sum_{a=0}^3\big(g(x\oplus a\oplus4)+g(x\oplus a)\big)\\
&amp;=\sum_{a=0}^7g(x\oplus a).
\end{aligned}
$$
And one more repetition then gives the claim.</p>

<p>To understand why $D_{2^n}$ are called derivations we need to break the functions into components. Any function $g:V\to V$ can be written using the
components
$$
g(x)=(g_0(x),g_1(x),\ldots,g_7(x)),
$$
where the components $g_i:V\to\Bbb{F}_2$ are all boolean functions in the variables $x_0,x_1,\ldots,x_7$. Any such boolean function can be uniquely written as a polynomial in the unknowns $x_i$ ranging over $\Bbb{F}_2$. Remember that we only need to use terms $x_{i_1}x_{i_2}\cdots x_{i_k}$ with
$i_1&lt;i_2&lt;\cdots&lt;i_k$ because the polynomials $x_i^2$ and $x_i$ give the same boolean function.</p>

<p>We obviously have
$$
g_i(x\oplus 2^n)=g_i(x_0,x_1,\ldots,x_{n-1},x_n+1,x_{n+1},\ldots,x_7).\tag{1}
$$ 
The following observation is the key:
$$
D_{2^n}(g_i)(x)=\frac{\partial g_i}{\partial x_n}(x).\tag{2}
$$
Everything in sight is linear, so it suffices to prove $(2)$ for each monomial $g_i(x)=x_{i_1}x_{i_2}\cdots x_{i_k}$. Here if $n$ does not appear among the indices $i_1,\ldots, i_k$ we have, by $(1)$, $g_i(x\oplus2^n)=g_i(x)$ for all $x\in V$. In this case we have the constant function zero on both sides of $(2)$. </p>

<p>On the other hand, if $i_t=n$, then
$$
\begin{aligned}
D_{2^n}(g_i)(x)&amp;=g_i(x\oplus2^n)+g_i(x)\\
&amp;=g_i(x_0,x_1,\ldots,x_n+1,\ldots,x_7)+g_i(x_0,\ldots,x_7)\\
&amp;=x_{i_1}\cdots (x_{i_t}+1)\cdots x_{i_k}+x_{i_1}x_{i_2}\cdots x_{i_k}\\
&amp;=x_{i_1}\cdots x_{i_{t-1}} x_{i_{t+1}}\cdots x_{i_k},
\end{aligned}
$$
so $(2)$ holds in this case as well. Q.E.D.</p>

<p>This then implies the last claim in the form
$$
D_1D_2D_4D_8(g)=\left(\frac{\partial^4g_i}{\partial x_0\partial x_1\partial x_2\partial x_3}\right)_{i=0}^7.
$$</p>
"
"2393232","2393249","<blockquote>
  <p>An object is moving along the curve which is derived from the intersection of the cylinder $x^2+y^2=1$ and the plane $y+z=1$. Where does the object need to be located if we want to maximize/minimize the sum $x+2y+z$? What is the min/max sum?</p>
</blockquote>

<p>So you're looking for the extreme values of $f(x,y,z) = x+2y+z$ for points $(x,y,z)$ located on the cylinder $\color{blue}{x^2+y^2=1}$ <strong>and</strong> on the plane $\color{red}{y+z=1}$; i.e. on their intersection. Introduce two Lagrange multipliers to limit the points to those on this intersection:
$$\begin{align}F(x,y,z,\lambda,\mu) &amp; = f(x,y,z)+\lambda \left( \color{blue}{x^2+y^2 -1} \right)+\mu \left( \color{red}{y+z -1} \right) \\
&amp; = x+2y+z+\lambda \left( \color{blue}{x^2+y^2 -1} \right)+\mu \left( \color{red}{y+z -1} \right)\end{align}$$ 
Now you solve the system:
$$\left\{ \begin{array}{rcl}
F_x = 0 \\
F_y = 0 \\
F_z = 0 \\
F_\lambda = 0 \\
F_\mu = 0
\end{array}\right. \iff \left\{ \begin{array}{rcl}
F_x = 0 \\
F_y = 0 \\
F_z = 0 \\
\color{blue}{x^2+y^2=1} \\
\color{red}{y+z=1}
\end{array}\right. \iff \ldots$$</p>

<p>Can you proceed?</p>

<hr>

<p>Addition after comments. In your notation with
$$\color{green}{p(x,y,z)=x+2y+z \implies \nabla p = \langle 1,2,1\rangle }$$
the function to be optimized and constraints
$$\color{blue}{j(x,y,z)=x^2+y^2-1 \implies \nabla j = \langle 2x,2y,0 \rangle}$$
and
$$\color{red}{k(x,y,z)=y+z-1 \implies \nabla k = \langle 0,1,1 \rangle}$$
the system becomes:
$$\left\{ \begin{array}{l}
\color{green}{\nabla p} = \lambda \color{blue}{\nabla j} + \mu \color{red}{\nabla k} \\
\color{blue}{j(x,y,z)=0} \\
\color{red}{k(x,y,z)=0}
\end{array}\right. \iff
\left\{ \begin{array}{l}
1=2\lambda x \\
2 = 2\lambda y + \mu \\
1 = \mu \\
\color{blue}{x^2+y^2=1} \\
\color{red}{y+z=1}
\end{array}\right.$$</p>
"
"2393238","2393305","<p>Consider the perfect pairing $\left&lt; \cdot, \cdot \right&gt; \colon V \times \bigwedge^{d-1}(V) \rightarrow \bigwedge^d(V)$ given by the wedge product $\left&lt;v, \omega \right&gt; = v \wedge \omega$. The adjugate of a linear map $T \colon V \rightarrow V$ is characterized by the property that it is the adjoint map to $\bigwedge^{d-1}(T)$ with respect to $\left&lt; \cdot, \cdot \right&gt;$. That is, we have</p>

<p>$$ \left&lt; \operatorname{adj}(T)v, \omega \right&gt; = \left&lt;v, \bigwedge\nolimits^{d-1}(T)\omega \right&gt; $$</p>

<p>for all $v \in V$ and $\omega \in \bigwedge^{d-1}(V)$. Using this definition, one can prove directly that 
$$\operatorname{adj}(T) \circ T = T \circ \operatorname{adj}(T) = \det(T) I$$
and
$$ \operatorname{adj}(\operatorname{adj}(T)) = \det(T)^{d-2} T. $$</p>

<p>I'll assume that $d$ is even and show that given any invertible map $S \colon \bigwedge^{d-1}(V) \rightarrow \bigwedge^{d-1}(V)$ we can find an invertible map $T \colon V \rightarrow V$ such that $\bigwedge^{d-1}(T) = S$. Since the pairing is perfect, there exists a (unique) map $R \colon V \rightarrow V$ which is adjoint to $S$ so that</p>

<p>$$ \left&lt; Rv, \omega \right&gt; = \left&lt; v, S\omega \right&gt; $$</p>

<p>for all $v \in V$ and $\omega \in \bigwedge^{d-1}(V)$. Note that $R$ must also be invertible. Define $T = \det(R)^{\frac{2-d}{d-1}}\operatorname{adj}(R)$. Then we have</p>

<p>$$ \operatorname{adj}(T) = \det(R)^{2 - d} \operatorname{adj}(\operatorname{adj}(R)) = \det(R)^{2-d} \det(R)^{d - 2} R = R $$</p>

<p>so</p>

<p>$$ \left&lt; v, S\omega \right&gt; = \left&lt; Rv, \omega \right&gt; = \left&lt; \operatorname{adj}(T)v, \omega \right&gt; = \left&lt; v, \bigwedge\nolimits^{d-1}(T) \omega \right&gt; $$</p>

<p>for all $v \in V$ and $\omega \in \bigwedge^{d-1}(V)$ which shows that $S = \bigwedge^{d-1}(T)$.</p>

<p>In general, one can show that $\det(R) = \det(S)$ (where $R,S$ are adjoint with respect to $\left&lt; \cdot, \cdot \right&gt;$, just like one has with an inner product). When $d$ is odd, $d - 1$ is even so the previous argument works only if $\det(R) &gt; 0$ (because we need to take an even square root) which will happen if and only if $\det(S) &gt; 0$. </p>
"
"2393243","2393300","<p>I would check this page: <a href=""https://en.wikipedia.org/wiki/Standard_probability_space"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Standard_probability_space</a> ... See the first paragraph of the section verifying the standardness, which says that any probability measure on euclidean space is standard. </p>

<p>This means that any probability measure on euclidean space is isomorphic (up to null sets) to a disjoint union of an interval and a countable atomic measure. (Isomorphism here means in the category of measure spaces with measurable measure preserving maps. Up to null sets means that we are allowed to construct this isomorphism on the complement of sets of measure zero... this is written better on wikipedia.) </p>

<p>I think I can prove this but my argument below reflects the messy state of my mind right now. I'll try to clean it up and verify details after I get some sleep... </p>

<p>A strategy for proof would be to reduce it to the case of an interval (is it easy to show that the class of standard probability spaces is closed under products?* this is stated as true on wikipedia), and then use the cumulative density function to prove this in the case of the interval. (I see now that kimchilover is discussing this. The case of the interval is a pretty standard fact from measure theoretic probability courses, it should be done in detail in Durret, if I recall correctly.)</p>

<p>(*Ahout the case of products - applying the guaranteed decomposition into interval and atomic part to the factors and using distributivity of the product, this reduces to showing that the Lebesgue measure on $I^n$ is a standard probability space. This would follow from induction after proving the existence of a measure preserving map $I \to I^2$. I think you can produce one as a the limit of a sequence of maps $f_n : I \to I^2$, where $f_n$ breaks $I$ into $2^n$ pieces and puts them in $I^2$ in a checkerboard fashion, and $f_{n+1}$ refines $f_n$ so that there is a well defined pointwise limit.)</p>

<p>Given that, I think a yes to your question is immediate  - you replace the complex numbers with the interval $J$ and some atoms (after removing a null set), chop up your parametrizing  interval $[0,1]$ into two appropriately sized intervals $I_1$ and $I_2$, and use $I_2$ to produce the atoms (again by chopping it up into intervals), and parametrize $J$ with $I_1$.</p>
"
"2393248","2393256","<p>Yes. Every isometry is a homeomorphism, then the Euler characteristic is an isometric invariant because it is a topological invariant.</p>
"
"2393250","2393260","<p>I'll also use the names Alice and Bob, with Alice going first.
<p>
I'll assume that at the end of each round, the losing player pays the winning player a dollar amount equal to the winning player's score for that round.
<p>
For simplicity, I'll assume no score is awarded if Alice and Bob choose numbers which are equally close to the value of the roll.
<p>
Also, I'll assume the rules require Bob to choose a number <em>other</em> than the one already chosen by Alice.
<p>
Claim: Alice has the advantage and should always choose $16$.
<p>
Alice is trying to maximize not her expected score, but rather, the expected value of her score <em>minus</em> Bob's score.
<p>
With Alice choosing $16$, Bob's optimal reply is $15$, but for those choices, the expected value of Alice's score minus Bob's score is $+0.5$, so Alice will win more points, on average, than Bob.
<p>
If Alice chooses any number other than $16$, Bob has a reply which can force the expected value of Alice's score minus Bob's score to be negative, so in those cases, Bob will win more points, on average, than Alice.
<p>
If Alice chooses $16$, and Bob chooses any number other than $15$, the expected value of Alice's score minus Bob's score will be greater than $0.5$, so Bob's best choice is $15$, since it minimizes Bob's expected loss.
<p>
Optimal strategies for Bob:
<p>
If Alice chooses $k$ with $k &lt; 16$, Bob's best reply is $31-k$, which yields a positive expectation for Bob.
<p>
If Alice chooses $k$ with $k &gt; 16$, Bob's best reply is $k-1$, which yields a positive expectation for Bob.
<p>
Finally, if Alice chooses $16$, all choices by Bob yield a negative expectation for Bob, but to minimize the expected loss, Bob should choose $15$.
<p>
Bottom line: You want to be Alice!</p>
"
"2393255","2393271","<p>For the first part use the fact that the complete residue system of $4$ is $\{0,1,2,3\}$. Hence you just need to compute the square values of each to get all possible quadratic residues and this can be easily done by hand.</p>

<p>For the second part, use the first part to see that a sum of squares has to be congruent to $0,1,2$ modulo $4$. So this enables to conclude that primes of the form $p=4k+3$ can not be represented as a sum of two squares. Also except 2, there isn't a prime of the form $4k+2$, as well as $4k$. </p>

<p>You can prove that each prime of the form $p=4k+1$ can be represented as a sum of two squares, but the proof isn't simple enough as it uses Gaussian Integers.</p>
"
"2393257","2393263","<p>By your last identity, </p>

<p>$$\mathcal{L}(t^{n+1}f(t)) = \mathcal{L}(t\cdot t^{n}f(t))= -\frac{d}{ds}\mathcal{L}(t^{n}f(t)).$$</p>

<p>The induction hypothesis replaces this last with</p>

<p>$$=-\frac{d}{ds}  \left( (-1)^n \frac{d^n}{ds^n} F(s) \right) = (-1)^{n+1} \frac{d^{n+1}}{ds^{n+1}} F(s). $$</p>
"
"2393267","2393546","<p>If $K$ is another minimal normal subrgoup, then $K \cap N=1$, whence $K$ and $N$ commute ($[K,N]=1$). So $K \subseteq N$ by c. And by c. also $N \subseteq K$ (replace in the entire argument $N$ by $K$, or use that $N$ is minimal normal). So $K=N$.</p>
"
"2393269","2393277","<p>Factoring out $(n-1)!$, you get: $$(n-1)!\sum_{k=0}^{n-2}\frac{1}{k!}=(n-1)!e-1-f(n),$$ where $\frac{1}{n}&lt;f(n)&lt;\frac{1}{n}+\frac{1}{n^2}+\cdots =\frac{1}{n-1}\leq 1,$ when $n&gt;1$. Since the left side is an integer, you get the value is  $$\lfloor(n-1)!e\rfloor -1.$$ </p>

<p>This increases too fast to be $n^22^n$.</p>
"
"2393278","2393297","<p><em>sketch of proof:</em></p>

<p>Define the candidate norm $\|x\| = d(x,0)$.</p>

<p><strong>First step:</strong></p>

<p>Let $y$ be given.
We have $B(y,r)=y+B(0,r)$ for all $r&gt;0$,
so
$$
 d(x+y,y) &lt; r \Leftrightarrow
 y+x \in B(y,r) \Leftrightarrow
 y+x \in y+B(0,r) \Leftrightarrow
 d(x,0) &lt; r
$$
holds for all $x\in X, r&gt;0$.
Thus $$
\tag{1}
d(x+y,y)=d(x,0)\text{.}$$
The triangle inequality for norms follows:</p>

<p>$$\|x+y\| = d(x+y, 0) \le d(x+y,y) + d(y,0) = d(x,0) + \|y\| = \|x\| + \|y\|$$</p>

<p><strong>Second step:</strong>
similarly, we can show that
$d(\alpha x,0) = \alpha d(x,0)$
for $\alpha&gt;0$.
Moreover, by (1) we have that
$$
  d(\alpha x,0)
  = d(0,-\alpha x)
 = |\alpha | d(0,x)
 = |\alpha | d(x,0)
$$
is true for all $\alpha&lt;0$.</p>

<p>Thus homogenity follows.</p>
"
"2393280","2393293","<p>Its volume is given by
$$V=\frac{4\pi r^3}{3}$$
Differentiate with respect to $r$:
$$\frac{dV}{dr}=4\pi r^2$$
By the chain rule you have
$$\frac{dV}{dt}=\frac{dV}{dr}\frac{dr}{dt}=4\pi r^2\frac{dr}{dt}=20$$
Solve for $\frac{dr}{dt}$:
$$\frac{dr}{dt}=\frac{20}{4\pi r^2}=\frac{5}{\pi r^2}$$</p>

<p>Its surface area is given by
$$A=4\pi r^2$$
Differentiate with respect to $r$:
$$\frac{dA}{dr}=8\pi r$$
Now use chain rule to get the rate at which the area changes with respect to time:
$$\frac{dA}{dt}=\frac{dA}{dr}\frac{dr}{dt}=8\pi r\frac{dr}{dt}=8\pi r\cdot\frac{5}{\pi r^2}=\frac{40}{r}.$$</p>
"
"2393289","2393298","<p>My guess is that $p(x)=ax^2+bx+c$. Am I right?</p>

<p>Your basis of $X$ and $Y$ are correct.</p>

<p>What is $X\cap Y$? It's$$\left\{ax^2+bx+c\,\middle|\,\frac43a+c=0\wedge 3a+2b+c=0\right\}.$$So, solve the system$$\left\{\begin{array}{l}\frac43a+c=0\\3a+2b+c=0.\end{array}\right.$$You'll get$$a=-\frac34c\text{ and }b=\frac58c.$$So take, for instance, the polynomial $p(x)=-6x^2+5x+8$. Then $X\cap Y=\mathbb{R}p(x)$ and $\bigl\{p(x)\bigr\}$ is a basis of $X\cap Y$.</p>
"
"2393296","2393394","<p>Left has two options from the initial $2 \times 4$ position.  He can make a move that uses a corner or make a move that takes two middle squares out of one side.  The position where he takes a corner is $\{2|0\} $because if he moves again he takes the opposite corner leaving himself two moves, while right can move to leave them each one which is equivalent to $0$.  It is taking the middle that is claimed to have value $0$.   If he takes a middle and moves again he can leave $*$ or $-2$.  If he takes a middle square and Right moves next we have a vertical L tetromino, which is shown to have value $\frac 12$ a few pages earlier.  The middle square position is therefore $\{*,-2|\frac 12\}$.  We can see this is zero because whoever moves loses.  If Right moves he moves to $\frac 12$ and Left wins because this is greater than $0$.  If Left moves he goes to $*=\{0|0\}$ and Right moves and wins.  This shows the Left options from your position are $0,\{2|0\}$  The second is reversible in that if Left moves to $\{2|0\}$ Right can immediately move back to $0$, so the whole Left option is equivalent to $0$.</p>
"
"2393324","2393332","<p>Apollonius Circle (the first type) - <a href=""http://mathworld.wolfram.com/ApolloniusCircle.html"" rel=""nofollow noreferrer"">Wolfram</a>/<a href=""https://en.wikipedia.org/wiki/Apollonian_circles"" rel=""nofollow noreferrer"">Wikipedia</a></p>

<p>$(2, 0)$ and $(-3, 0)$ are fixed points in the Complex plane. The ratio of their distances from any solution, $z\in\Bbb C$, is $2$ (ie. constant) iff $\frac{|z-2|}{|z-3|}=\left|\frac{z-2}{z-3}\right|=2$. Hence the solution set is a circle.</p>

<hr>

<p>Generalising, we can conclude equations of the form</p>

<p>$$\left|\frac{z-a}{z-b}\right|=r,\quad z\in\Bbb C$$</p>

<p>with constants $a,b\in\Bbb C, a\neq b$ and $r\in\Bbb R^+, r\neq 1$ have solution sets which are circles.</p>
"
"2393326","2393747","<p>Each complete (compact in the analytic topology) irreducible complex $n$-dimensional algebraic variety admits a triangulation making it a closed oriented  connected <em><a href=""https://www.encyclopediaofmath.org/index.php/Pseudo-manifold"" rel=""nofollow noreferrer"">pseudomanifold</a></em> of dimension $2n$. (Anosov in the article does not give a reference, just lists this as one of the examples. However: The most difficult part of the proof is the existence of a triangulation, see e.g. Hironaka's paper ""Triangulations of Algebraic Sets"", 1974, although the original proof is due to Lojasiewicz. Checking that irreducibility implies oriented pseudomanifold is a good exercise in definitions.) If $X$ is an $m$-dimensional closed oriented connected psedomanifold, then $H^m(X; {\mathbb Z})\cong {\mathbb Z}$. This is an exercise in Spanier's ""Algebraic Topology"" (p. 206); if you want to see a proof, take a look at Seifert and Threlfall ""Topology"", section 24. The rest (reducible case) follows from the Mayer-Vietoris sequence since intersections of irreducible components have real codimension $\ge 2$.  </p>
"
"2393331","2393606","<p>The function $f(x) = \dfrac{\sin x}{x}$ has $y=0$ as horizontal asymptote and has infinitely many inflection points.</p>
"
"2393336","2393543","<p>As I said in the comments, in <em>Abstract and Concrete Categories- The Joy of Cats</em> one can read the following statement, page 116 (in the 2004 edition) ""we will see that it is possible for a functor to preserve regular monomorphisms without preserving equalizers (13.6)"". </p>

<p>A way to find such a counterexample is to pick a category $\mathcal{B}$ that has many regular monos, for instance one where all monos are regular ($\mathbf{Ab}, Set$, any topos,...) and a functor that preserves monos but not equalizers.</p>

<p>The (ad hoc) example I gave is the following: pick any such category $\mathcal{B}$ (for instance $Set$) , and three arrows $A\to B$, $B\to C, B\to C$ such that the arrow $A\to B$ equalizes the two other arrows, but is not an equalizer of them (and is a monomorphism).</p>

<p>Then pick $\mathcal{A}$ to be the diagram of an equalizer (that is a category with objects $1,2,3$ and arrows that compose in the obvious way- see my comment for more detail), and choose the obvious functor $F: \mathcal{A}\to \mathcal{B}$ sending $1\to A, 2\to B, 3\to C$. </p>

<p>Clearly this functor is a counterexample to ""a functor preserving regular monos preserves equalizers""</p>
"
"2393349","2393364","<p><strong>hint</strong></p>

<p>$$f (3)-f (\frac {3}{2})=\frac {3^3}{2^2}+3$$</p>

<p>$$f (\frac {3}{2})-f (\frac {3}{2^2})=\frac {3^3}{2^4}+\frac {3}{2} $$</p>

<p>...
$$f (\frac {3}{2^n})-f (\frac {3}{2^{n+1}})=\frac {3^3}{2^{2n+2}}+\frac {3}{2^n} $$</p>

<p>sum and $n\to +\infty $.</p>

<p>You will find
$$f (3)=f (0)+\frac {27}{4}\frac {1}{1-\frac {1}{4}}+3\frac {1}{1-\frac {1}{2}} $$</p>

<p>$$=16$$</p>
"
"2393353","2393358","<p>A map $f:M\times N\to A$ (for an Abelian group $A$) is
$\Bbb Z$-bilinear if
$$f(m+m',n)=f(m,n)+f(m',n)$$
and
$$f(m,n+n')=f(m,n)+f(m,n')$$
for all $m$, $m'\in M$ and all $n$, $n'\in N$.
In other words, for fixed $m$, $n\mapsto f(m,n)$ is a $\Bbb Z$-linear
map from $N$ to $A$, and ditto for the other argument.</p>
"
"2393355","2393376","<p>Note that from the Cauchy Integral Formula</p>

<p>$$f(z)=\frac1{2\pi i}\oint_{|z'-z|=1}\frac{f(z')}{z'-z}\,dz'$$</p>

<p>we can express the first derivative of $f(z)$, $f'(z)$, as </p>

<p>$$f'(z)=\frac{1}{2\pi i}\oint_{|z'-z|=1}\frac{f(z')}{(z'-z)^2}\,dz'\tag 1$$</p>

<p>Setting $z=0$ in $(1)$ yields</p>

<p>$$f'(0)=\frac{1}{2\pi i}\oint_{|z'|=1}\frac{f(z')}{z'^2}\,dz'\tag2$$</p>

<p>Now, letting $f(z)=e^{iz}$ with $f'(0)=i$ in $(2)$ reveals</p>

<p>$$i=\frac{1}{2\pi i}\oint_{|z'|=1}\frac{e^{iz'}}{z'^2}\,dz'$$</p>

<p>whereupon solving for the integral of interest we find</p>

<p>$$\bbox[5px,border:2px solid #C0A000]{\oint_{|z'|=1}\frac{e^{iz'}}{z'^2}\,dz'=-2\pi}$$</p>
"
"2393359","2393524","<p>Since
$$
x=\frac{z+\bar{z}}{2}
\qquad
y=\frac{z-\bar{z}}{2i}
$$
the function can be written as
$$
f(z)=\frac{1}{8}\bigl((z+\bar{z})^3-(z-\bar{z})^3\bigr)=
\frac{1}{4}(3z^2\bar{z}+\bar{z}^3)
$$
Therefore
$$
\frac{f(z+h)-f(z)}{h}=
\frac{1}{4}\left(
6z\bar{z}+3\bar{z}h+3h\bar{h}+6z\bar{h}+
(3z^2+3\bar{z}^2+3\bar{z}\bar{h}+\bar{h}^2)\frac{\bar{h}}{h}
\right)
$$
and the limit for $h\to0$ exists only if $z^2+\bar{z}^2=0$, that is, $y^2=x^2$ or $y=\pm x$.</p>

<p>You can't write $(\pm x,\pm y)$: the solution consists of the points of the form $x+ix$ or $x-ix$, for $x\in\mathbb{R}$.</p>

<p>The function is differentiable at those points, but nowhere analytic (it is differentiable on no open set).</p>
"
"2393363","2393382","<p><strong>Hint</strong>:</p>

<p>Integrals of the form $$\int p(x)f(x) dx$$ where $p$ is a polynomial and $f$ some function of which you know the antiderivative $F$ are simplified by integration by parts as</p>

<p>$$\int p(x)f(x)dx=p(x)F(x)-\int p'(x)F(x) dx,$$ because the degree of $p$ has lowered. This works particularly well for functions $f$ such that you can iterate antiderivation ($e^x,\cos x,\sin x, (x+a)^\alpha$).</p>
"
"2393365","2393385","<p>The two squares differ by $2y$, so must both be odd or both even. Even squares are always divisible by $4$, so their difference will be divisible by $4$, and odd squares are of the form $8r+1$, so always differ by a multiple of $8$.</p>
"
"2393374","2395603","<p>Denote by $k$ the ground field of the algebra $A$. Since both modules are simple, every nonzero homomorphism of $A$-modules $\varphi: Ae/\text{rad}(Ae) \to D(eA/\text{rad}(eA))$ will be an isomorphism. In order to find this, we first construct some $\tilde{\varphi}: Ae \to D(eA/\text{rad}(eA))$:</p>

<p>Let $g \in D(eA/\text{rad}(eA))$ be an arbitrary linear map $eA/\text{rad}(eA) \to k$ such that $g(\overline{e}) \neq 0$. This exists since $e \notin \text{rad}(eA)$, i.e. $\overline{e} \neq 0$. We claim $eg \neq 0$: Indeed, we have
$$(eg)(\overline{e}) = g(\overline{e}e) = g(\overline{e}) \neq 0$$
by definition of $g$. This gives rise to a map $\tilde{\varphi}: Ae \to D(eA/\text{rad}(eA)), \ ae \mapsto aeg$, which is clearly $A$-linear. We claim $\text{rad}(Ae) \subseteq \ker(\tilde{\varphi})$: For $ae \in \text{rad}(Ae)$ we get for all $a' \in A$
$$\tilde{\varphi}(ae)\left(\overline{ea'}\right) = (aeg)\left( \overline{ea'}\right) = g\left(\overline{ea'ae} \right) = g(0) = 0$$,
where the second to last equality follows from $ea'ae \in eA\text{rad}(Ae) = e\text{rad}(A)e = \text{rad}(eA)e \subseteq \text{rad}(eA)$, i.e. $\overline{ea'ae} = 0$. Therefore, $\tilde{\varphi}(ae) = 0$ and thus $\text{rad}(Ae) \subseteq \ker(\tilde{\varphi})$. Hence we get an $A$-linear map on the quotient
$$\varphi: Ae/\text{rad}(Ae) \to D(eA/\text{rad}(eA)), \ \overline{ae} \mapsto \tilde{\varphi}(ae) = aeg.$$ 
We are done if $\varphi \neq 0$. This is in fact the case, since $\varphi(\overline{e})(\overline{e}) = (eg)(\overline{e}) \neq 0$.</p>

<p><strong>Follow-up Question:</strong> Is there a more <em>natural</em> isomorphism?</p>
"
"2393383","2393400","<p>The answer is correct. An easier approach is to realize the shape is an ellipse--the area of which is $\pi a b$. Setting $ x=y$ will lead to a $a  = \sqrt{2}$, setting $ x=-y$ will lead to $b  = \sqrt{2}/\sqrt{3}$, so $A = 2\pi/\sqrt{3}$.</p>
"
"2393386","2393590","<p>If you find two different limits, following two different paths respactively, then the limit does not exist.</p>

<p>It is proved that if a limit of a function exists at a given point then it is unique.</p>

<p>Thus uniqueness of the limit is a necessary condition for the existence of the limit.</p>

<p>For $\frac{xy}{|xy|}$ if you take the path $(x,x)$ i.e $x=y$ then you have that </p>

<p>$\frac{xy}{|xy|}=\frac{x^2}{|x^2|}=1 \rightarrow 0$ as $x \rightarrow 0$</p>

<p>If you take the path $(x,-x)$ ,  then $$\frac{xy}{|xy|}=\frac{-x^2}{|x^2|}= -1 \rightarrow -1$$  as $x \rightarrow 0$</p>

<p>Thus we found two paths that go to zero and the limit in the fisrt path is $1$
and in the second path is $-1$</p>

<p>If the limit existed at $(0,0)$ then the value of it, would independent of our choices of paths.</p>

<p>In simpler words it would have tha same value on all paths we choose to approximate it.</p>

<p>If you want a second way the you can do this:</p>

<p>Take polar coordinates: $$x=r \cos{t}$$ $$y=r \sin{t}$$</p>

<p>If you substitute you will have the expression $$\frac{\cos{t} \sin{t}}{|\cos{t} \sin{t}|}$$</p>

<p>If thelimit exists then its value would be independent of $t$ as $r \rightarrow 0$</p>

<p>But for $t_1= \frac{\pi}{3}$ and $t_2=\frac{\pi}{4}$ you have differend values for the above expression.Thus the limit does not exist.</p>
"
"2393431","2393440","<p>If the real part of an analytic function is constant, then C-R equations imply that the imaginary part is also constant.</p>

<p>So $f(z)^2$ is constant. So $f$ can meet only two values (the two square roots of $f(z)^2$). Since $f$ is continuous, $f$ is constant.</p>
"
"2393442","2393457","<p>If you just plug into your volume formula you have $V=(w-u)\cdot (u \times v)$.  The dot product is distributive, so $V=w \cdot (u \times v)-u \cdot (u \times v)$.  The cross product is perpendicular to the vectors that comprise it, so the last dot product is zero and we get $V=w\cdot (u \times v)$ as desired.  We don't need the assumption that $w = u \times v$.  It works for any $u,v,w$</p>
"
"2393444","2393450","<p>Formally, you get
$$
\frac{\sec^2 x }{\sqrt{1 + \tan^2 x}}
= \frac{\sec^2 x}{\sqrt{\sec^2 x}}
= \frac{\sec^2 x}{|\sec x|}
= |\sec x|.
$$  </p>
"
"2393451","2393519","<p>As pointed out by Moo,  using Wolfram we can plot $\lambda$ versus $\mu$ and observe possibility of bifurcation(change of the stability of the fixed points as the parameter varies),here the parameter is $\mu$.</p>

<p>In this dynamical system we have two fixed points whose stability can be decided from the sign of real part of the eigenvalues.</p>

<p>If the real part of the eigenvalue corresponding to a fixed point is negative then that fixed point is stable else it is unstable.</p>

<p>So from the plot you can observe for which parameter there is change of stability of the two fixed points and the point where such a change of stability occurs is the bifurcation point.</p>

<p>As from the plot  <a href=""http://www.wolframalpha.com/input/?i=plot+1%2F2+(-(+2+-+u)+-+sqrt(u%5E2+%2B+4+u+%2B+3))"" rel=""nofollow noreferrer"">fig(1)</a> and <a href=""http://www.wolframalpha.com/input/?i=plot+1%2F2+(-(+2+-+u)+%2B+sqrt(u%5E2+%2B+4+u+%2B+3))"" rel=""nofollow noreferrer"">fig(2)</a> we can see that in fig $(1)$ the real part of this eigenvalue is always negative for all set of $\mu$ so this fixed point remains stable forever but in fig$(2)$ we see that the second fixed point changes its stability as we see that as $\mu $ varies the real part of the eigenvalue changes its sign!</p>

<p>Hence the bifurcation! </p>

<p>Also <a href=""http://csc.ucdavis.edu/~chaos/courses/ncaso/Lectures/Lecture3Slides.pdf"" rel=""nofollow noreferrer"">this</a> could be a good reference.</p>
"
"2393454","2394242","<p>We already reduced the problem to calculate </p>

<p>$$Dâ=\left|\begin{matrix}
b+c - a &amp; 0 &amp; a^2 \\
0 &amp; a+c - b &amp; b^2 \\
b &amp; a &amp; -ab 
\end{matrix}\right|$$</p>

<p>If $a=0$ then </p>

<p>$$Dâ=\left|\begin{matrix}
b+c &amp; 0 &amp; 0\\
0 &amp; c - b &amp; b^2 \\
b &amp; 0 &amp; 0 
\end{matrix}\right|=0.$$</p>

<p>If $b=0$ then </p>

<p>$$Dâ=\left|\begin{matrix}
c - a &amp; 0 &amp; a^2 \\
0 &amp; a+c &amp; 0 \\
0 &amp; a &amp; 0 
\end{matrix}\right|=0.$$</p>

<p>Otherwise put $Râ_1=R_1+\frac abR_3$ and $Râ_2=R_2+\frac baR_3$. Then </p>

<p>$$Dâ=\left|\begin{matrix}
b+c &amp; \frac {a^2}b &amp; 0\\
\frac {b^2}a  &amp; a+c &amp; 0 \\
b &amp; a &amp; -ab 
\end{matrix}\right|=-ab\left|\begin{matrix}
b+c &amp; \frac {a^2}b \\
\frac {b^2}a  &amp; a+c \\
\end{matrix}\right|=-ab[(a+c)(b+c)-ab]=-ab[ac+bc+c^2]=-abc(a+b+c).$$</p>

<p>The latter formula holds also when $a=0$ or $b=0$. Finally, </p>

<p>$$D=(a+b+c)(-2)Dâ=2(a+b+c)^3abc.$$</p>
"
"2393466","2393474","<p>From $h(t) = \int_\alpha^t \frac{\gamma'(s)}{\gamma(s)-a}ds$</p>

<p>Differentiating with respect to $t$, we have</p>

<p>$$h'(t) = \frac{\gamma'(t)}{\gamma(t)-a}$$</p>

<p>$$\gamma'(t)-(\gamma(t)-a)h'(t)=0$$</p>

<p>Multiply by the integrating factor $\exp(-h(t))$,</p>

<p>$$\gamma'(t)\exp(-h(t))-(\gamma(t)-a)h'(t)\exp(-h(t))=0$$</p>

<p>which is equivalent to </p>

<p>$$\frac{d\exp(-h(t))(\gamma(t)-a)}{dt}=0$$</p>

<p>by product rule.</p>
"
"2393469","2393481","<p>Isolating for $T_{f}$ in $qf=nT_{f}T^{-1}$ gives $T_{f}=\frac{qfT}{n}$. </p>

<p>Now, $nl=TE$, so $T=\frac{nl}{E}$. </p>

<p>Therefore, $T_{f}=\frac{qf}{n}\cdot\frac{nl}{E}=\frac{qfl}{E}$.</p>

<p>It follows that $R=\frac{VT_{f}}{2} =\frac{Vqfl}{2E}$, as desired.</p>
"
"2393479","2393602","<p>Since your question is tagged with GMAT, I am going to assume you are not a mathematician...  I am not either, and all this talk of Euler's theorem and modulus will give me a headache ;)</p>

<p>So, simply put...  the trick is that <strong>only</strong> the rightmost two digits of each number being multiplied can have any effect on the rightmost two digits of the answer.  So you can just drop the ""$2$"" from $299$ because $299$ squared ends with same $2$ digits as $99$ squared.  </p>

<p>$2$nd power: $99 \times 99 = 9801, \ldots$  now you can drop the ""$98$"" because it has no effect on the rightmost two digits of the answer.</p>

<p>$3$rd power: $01 \times 99 = 99$</p>

<p>$4$th power: $99 \times 99 = 9801, \ldots$ drop the ""$98$"" again</p>

<p>$5$th power: $01 \times 99 = 99$</p>

<p>etc...  See the pattern?  Even powers will end with ""$01$"" and odd powers will end with ""$99$""</p>
"
"2393483","2393489","<p>From left exactness. As
$$0\to M\to I_0\to I_1$$
is exact, so is
$$0\to F(M)\to F(I_0)\to F(I_1).$$
So $F(M)$ is naturally isomorphic to the kernel of
$F(I_0)\to F(I_1)$, which by definition is $R^0F(M)$.</p>
"
"2393486","2393752","<p><strong>Hint:</strong></p>

<p>In the affine frame $\mathcal R= \{a_0, (\vec{a_0a_1}, \vec{a_0a_2}, \vec{a_0a_3})\}$, show the <code>vector planes</code> $\bigl\langle\,\overrightarrow{c_0c_1},\,\overrightarrow{c_0c_2}\,\bigr\rangle$ and  $\bigl\langle\,\overrightarrow{c'_0c'_1},\,\overrightarrow{c'_0c'_2}\,\bigr\rangle$ are the same.</p>

<p>For this, calculate their coordinates:
$$\overrightarrow{c_0c_1}=\begin{bmatrix}3-1\\-1-0\\\phantom{-}2-3\end{bmatrix}=\begin{bmatrix}\phantom{-}2\\-1\\-1\end{bmatrix},\quad \overrightarrow{c_0c_2}=\begin{bmatrix}1\\1\\1\end{bmatrix},\quad \text{&amp;c.}$$
and show each augmented matrix $\begin{bmatrix}\overrightarrow{c\phantom{'}_{\!0}c_1},\,\overrightarrow{c_0c\phantom{'}_{\!2}},\,\overrightarrow{c'_0c'_i}\end{bmatrix}$ has rank $2$.</p>
"
"2393493","2393516","<p>The perimeter of a regular $n$-gon inscribed in a circle of radius $R$ is
$$
P = 2\ R\ n\sin \left(\frac{\pi}{n}\right)
$$
So, $P$ is a rational multiple of $R$ iff $\sin \left(\frac{\pi}{n}\right)$ is rational.</p>

<p>Now, <a href=""https://en.wikipedia.org/wiki/Niven%27s_theorem"" rel=""nofollow noreferrer"">Niven's theorem</a> says that</p>

<blockquote>
  <p>If $\theta$ is a rational multiple of $\pi$ and $\sin(\theta)$ is rational, then $\sin(\theta)$ is $0$, $\pm 1/2$, $\pm 1$.</p>
</blockquote>

<p>Therefore, the hexagon is the only regular polygon whose perimeter is a rational multiple of $R$, which implies your claim.</p>
"
"2393497","2393733","<p>Since $H^0(A)=H^0(A-\Delta)$, letting $i:H^0(K-\Delta)\to H^0(K)$ be the natural inclusion and $\nu:H^0(A-\Delta)\otimes H^0(K-A)\to H^0(K-\Delta)$, we have $\mu_0(A)=i\circ\nu$, not injective. We also have the natural inclusion $j:H^0(A-\Delta)\otimes H^0(K-A)\to H^0(A-\Delta)\otimes H^0(K-A+\Delta)$ and then $\mu_0(A-\Delta)\circ j=i\circ \nu$. Since the latter is not injective and $j$ is injective, we get that $\mu_0(A-\Delta)$ is not injective.</p>
"
"2393507","2393523","<p><strong>TL;DR version:</strong>  you are not taking rotation into account.  This is basically irrelevant in $\mathbb{R}$, but makes a huge difference in $\mathbb{R}^d$, where $d\ge 2$.</p>

<h3>In Detail:</h3>

<p>A contracting similitude can be characterized as a map
$$ S : \mathbb{R}^{d} \to \mathbb{R}^d $$
which takes the form
$$ S(\vec{x}) = c U \vec{x} + \vec{b}, $$
where $c \in (0,1)$ is the scaling ratio of $S$, $U$ is a unitary matrix (e.g. a rotation in $\mathbb{R}^2$, and $\vec{b}\in\mathbb{R}^n$ is a translation.  In this language, the maps $S_1$ and $S_4$ which you describe can be written as
$$ S_1(\vec{x}) = \frac{1}{3} \vec{x}
\qquad\text{and}\qquad
S_4(\vec{x}) = \frac{1}{3} \vec{x} + \begin{bmatrix} \frac{2}{3} \\ 0 \end{bmatrix}. $$
In each of these cases, the we can take $U = I$ to be the identity.  For the other two maps, you are going to need to rotate.  In general, we can describe a rotation by $\theta$ in $\mathbb{R}^2$ by the matrix
$$
R_{\theta} := \begin{bmatrix}
\cos(\theta) &amp; -\sin(\theta) \\
\sin(\theta) &amp; \cos(\theta) \end{bmatrix}.
$$
To get the piece of the von Koch curve that slopes up from left to right, we require a rotation of $\frac{\pi}{3}$, then a translation by $\frac{1}{3}$, which gives the map
$$ S_2(\vec{x})
 = \frac{1}{3} R_{\pi/3} \vec{x} + \begin{bmatrix} \frac{1}{3} \\ 0 \end{bmatrix}
 = \frac{1}{3} \begin{bmatrix}
\frac{1}{2} &amp; -\frac{\sqrt{3}}{2} \\
\frac{\sqrt{3}}{2} &amp; \frac{1}{2} \end{bmatrix} \vec{x} + \begin{bmatrix} \frac{1}{3} \\ 0 \end{bmatrix}.
$$
Finally, for the last piece of the curve, we will need to rotate by $-\frac{\pi}{3}$, translate right by $\frac{1}{2}$, and translate up by $\frac{\sqrt{3}}{6}$ (the origin gets mapped to the ""top"" of the snowflake, which is the vertex of an equilateral triangle sitting on the $x$-axis, centered at $x=\frac{1}{2}$, with sides of length $\frac{1}{3}$).  This gives
$$ S_3(\vec{x})
 = \frac{1}{3} R_{-\pi/3} \vec{x} + \begin{bmatrix} \frac{1}{2} \\ \frac{\sqrt{3}}{6} \end{bmatrix}
 = \frac{1}{3} \begin{bmatrix}
\frac{1}{2} &amp; \frac{\sqrt{3}}{2} \\
-\frac{\sqrt{3}}{2} &amp; \frac{1}{2} \end{bmatrix} \vec{x}  + \begin{bmatrix} \frac{1}{2} \\ \frac{\sqrt{3}}{6} \end{bmatrix}.
$$</p>
"
"2393518","2393560","<p>Consider first fixing the movement to one particular row. You can move $m-(n-1) = m -n  + 1$ ways. The same argument holds for the number of columns: $m - n + 1$. You then multiply the two together to get the resulting:</p>

<p>$$\begin{align}
(m - n + 1)(m - n + 1) &amp;= m^2 - mn + m - nm + n^2 - n + m - n + 1 \\
                       &amp;= m^2 - 2nm + m + n^2 - n + m - n + 1 \\
                       &amp;= m^2 - 2nm + 2m - 2n + n^2 + 1
\end{align}$$</p>
"
"2393521","2393526","<p><strong>1)</strong> <p>
$\{ z\in\mathbb C~:~|z|=1\}$ is the unit circle. It contains all complex numbers $x+yi$ where $1=|x+yi|^2=x^2+y^2$. Especially $x=\pm1$ and $y=0$ or $x=0$ and $y=\pm 1$ is possible and therefore $a=-i=0+(-1)i$ is possible.
<p>
<strong>2)</strong><p>
$\mathbb C$ is a twodimensional over $\mathbb R$, since $\{1,i\}$ is a linear independent base of $\mathbb C$ over $\mathbb R$. Consider that for $x,y\in\mathbb R$ holds $x\cdot 1+y\cdot i=x+yi=0$ iff $x=y=0$.</p>
"
"2393525","2393536","<p>You write:</p>

<blockquote>
  <p>I thought the real numbers were defined as the numbers on the number line.</p>
</blockquote>

<p>This isn't really a definition of the real numbers, since ""number line"" is a bit vague, but: a key fact about the number line as generally understood is that the distance between any two points is finite. If I imagine a number with digits stretching infinitely far to the left, such a number is infinitely large, that is, infinitely far away from zero; and these don't have a place on the number line as generally understood.</p>

<p>This is not to say that we can't give a mathematically precise meaning to such objects! Indeed, one particular formalization of them - the <a href=""https://en.wikipedia.org/wiki/P-adic_number"" rel=""nofollow noreferrer""><em>$p$-adic numbers</em></a> - plays an important role in number theory and algebraic geometry (and they allow manipulations such as that in your last comment).  However, it's important to note that these are not, in fact, real numbers.</p>

<p>Put another way, while you can manipulate these expressions in an interesting way (e.g. conclude that $...99999=-1$), that does not in any way mean that they correspond to something <em>in the particular number system ""the real numbers""</em>; rather, it merely suggests that they may be interesting objects in their own right. There are lots of very interesting objects (besides the $p$-adics mentioned above) that we can make sense of, which are not real numbers:</p>

<ul>
<li><p><a href=""https://en.wikipedia.org/wiki/Complex_number"" rel=""nofollow noreferrer"">Square roots of negative numbers.</a></p></li>
<li><p><a href=""https://en.wikipedia.org/wiki/Non-standard_analysis"" rel=""nofollow noreferrer"">Infinitesimals.</a></p></li>
<li><p><a href=""https://en.wikipedia.org/wiki/Smooth_infinitesimal_analysis"" rel=""nofollow noreferrer"">Non-zero numbers which, when squared, equal zero.</a></p></li>
<li><p><a href=""https://en.wikipedia.org/wiki/Quaternion"" rel=""nofollow noreferrer"">Complex numbers but this time with even <em>more</em> square roots of negative numbers, because why quit when you're ahead.</a></p></li>
<li><p>And, dearest to my heart, <a href=""https://en.wikipedia.org/wiki/Ordinal_number"" rel=""nofollow noreferrer"">various different kinds of infinity</a>.</p></li>
</ul>
"
"2393530","2394127","<p>Golden Ratio in Regular Pentagon
The golden ratio, $\phi=\displaystyle\frac{1+\sqrt{5}}{2}$, makes frequent and often unexpected appearance in geometry. Regular pentagon - the pentagram - is one of the places where the golden ratio appears in abundance
To mention a few (some of which have been proved elsewhere, others are straightforward):</p>

<p><a href=""https://i.stack.imgur.com/mftgE.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mftgE.jpg"" alt=""enter image description here""></a>
$\frac{DE}{EX}=\frac{EX}{XY}=\frac{UV}{XY}=\frac{EY}{EX}=\frac{BE}{AE}=\phi $ 
Most recently Dao Thanh Oai posted an observation at the CutTheKntMath facebook page that in the following diagram $\frac{FB}{FA}=\phi ,$ </p>
"
"2393534","2393716","<p>The surface level answer is that if $I[w]$ is a functional acting on an appropriate function space, the stationary points of the functional (defined appropriately) satisfies a PDE, namely the <a href=""https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation"" rel=""nofollow noreferrer"">Euler-Lagrange equations</a>. But you probably know that already.</p>

<hr>

<p>A lot of modern PDE theory is concerned with the existence, uniqueness and regularity of solutions. That is, given a particular PDE with initial/boundary conditions, we ask (a) whether a solution exists, (b) whether it is unique and (c) how 'regular' it is, for example how many times we can differentiate it. The important point is that we aren't interested in explicitly writing down a solution, provided we can prove it exists.</p>

<p>To do this, we usually use the following method:</p>

<ol>
<li>Define a sufficiently general space of functions $X$ and an appropriate notion for a function $u \in X$ to be a 'weak solution' to the equation. We then (often exploiting compactness properties) prove a weak solution exists.</li>
<li>Prove that $u$ is sufficiently regular and is a solution to the PDE in the usual sense.</li>
</ol>

<p>I've omitted the question of uniqueness here, as that tends to vary in approach and is often somewhat separate from the existence part.</p>

<p>The calculus of variations is useful for two reasons:</p>

<ul>
<li><p>It provides a way of doing step 1 of the above method.</p></li>
<li><p>Moreover the weak solutions we assert exist often satisfy additional properties, which can be useful for step 2.</p></li>
</ul>

<p>In the following, we will assume we are solving the equation $Mu = 0$ where $M$ is a (often nonlinear) differential operator in some domain in $\mathbb R^n,$ which coincides with the Euler-Lagrange equation of the functional $I[w].$</p>

<p><strong>Existence:</strong> Under appropriate convexity and growth conditions, we can prove the existence of a weak solution by showing a stationary point of $I[w]$ exists. The idea is that if $I[w]$ is continuous on a compact subspace of functions, then it will attain a global maximum / minimum there. Then we can show this max/min satisfies the Euler-Lagrange equation in a weak sense.</p>

<p>In practice there are technicalities (we often can't show $I[w]$ is continuous with respect to the topology we impose on $X$), but the general idea is to show the existence of a minimum and to prove it also solves the PDE.</p>

<p><strong>Additional information:</strong> If we have a stationary point $u$ is actually a minimum, then we have $I''[u] \geq 0,$ in the sense that for all nice functions $v,$
$$ \frac{d^2}{dt^2} I[u + tv] \geq 0. $$
By differentiating under the integral sign (which we can justify with appropriate hypotheses) we obtain the second variation. Hence our solution $u$ not only satisfies the PDE $Mu = 0$ in a weak sense, but it also satisfies another differential inequality. This is useful in proving that $u$ is actually more regular (smooth) than we initially assumed, which serves as a starting point for step 2.</p>

<hr>

<p>Finally, I will mention that the equations obtained are often <em>elliptic.</em> One can show that if $I[w]$ is convex and $u$ is a local minimum, then the associated equation has an elliptic structure. This is based on analysing the second variation and is explained in detail in Evan's PDE book (section 8.1.3).</p>
"
"2393535","2394427","<p>The answer is negative even for a simple case $Q=\{0,1\}\subset\Bbb R$ and eventually constant sequences. For each natural $i$ and $n$ put $q^i_n=0$ if $n&gt;i$, and $q^i_n=1$, otherwise. Then for each $i$ a sequence $(q^i_n)$ converges to $0=\bar q^i$. But for any sequence $(n_k)$ of natural numbers and any natural number $K$ holds $\|q_{n_K}^{n_K}- \bar q^{n_K}\|=1$.</p>
"
"2393539","2393548","<p>An alternate approach is that the first card is a spade with probability $\frac{13}{52}.$ Of those case, $\frac{1}{13}$ was the spade ace, and $\frac{12}{13}$ were not the ace. So the probability is:</p>

<p>$$\frac{13}{52}\left(\frac{1}{13}\cdot\frac{3}{51}+\frac{12}{13}\cdot\frac{4}{51}\right)$$</p>

<p>If $X$ is the even that the first card is a spade ace and $Y$ is the event that the first card is another spade we get, and $Z$ is the event that the second card is an ace, this formula is:</p>

<p>$$P((X\lor Y)\land Z)=P(X\lor Y)\left(P(X\mid X\lor Y)P(Z\mid X)+P(Y\mid X\lor Y)P(Z\mid Y)\right)$$</p>

<p>This is true in general if $X$ and $Y$ are disjoint events.</p>
"
"2393542","2393549","<p>Think about it as follows:</p>

<p>Line up all 6 people, and then have the first two go on scooter 1, the next two on scooter 2, and the last two on scooter 3.</p>

<p>Now, since the people are distinguishable, you have $6!$ possible line-ups, but this of course overcounts the number of scooter arrangements in two different ways:</p>

<ol>
<li>First of all, the members of each pair can swap places (or at least it just wants to know what pair goes onto a scooter; it does not distinguish driver from passenger). So, for example, if you have line-up:</li>
</ol>

<p>$$123456$$</p>

<p>then you can swap the people of the first pair:</p>

<p>$$213456$$</p>

<p>but still get the same scooter arrangement.</p>

<p>Likewise, you can swap the members of the second and third pair. So, you need to divide by $2\cdot 2 \cdot 2$.</p>

<p>Now, if you had triplets going onto a scooter (say, we had 9 people and 3 scooters), then we can rearrange the members of each triplet in $3!$ ways. Thus, in general you would divide by $k!$, with $k$ being the number of items going into a group (though again, this is only when the group is seen as a set, i.e. has no further ordering).</p>

<p>So, in this case you really need to divide by $2! \cdot 2! \cdot 2!$, but that is just $2 \cdot 2 \cdot 2$</p>

<ol start=""2"">
<li>Second, the 3 scooters are indistinguishable, so the pairs can swap places as well (as a pair). For example, if you first have line-up </li>
</ol>

<p>$$123456$$ </p>

<p>and you swap the first pair with the second pair, you get:</p>

<p>$$341256$$  </p>

<p>but again that will lead to the very same scooter arrangement. So, swapping pairs does not change the scooter arrangement. Since we have 3 pairs (think of it as pair A, B, and C), we can rearrange those pairs in $3!$ ways.</p>

<p>Final answer:  </p>

<p>$$\frac{6!}{2!2!2!3!} = 15$$</p>

<p>And as a general formula:</p>

<p>If you want to divide $n$ items into $m$ groups of size $k$, where the groups are indistinguishable, and where the items within the group are indistinguishable (or, as here, the group is seen as a set):</p>

<p>$$\frac{n!}{{k!}^m \cdot m!}$$</p>
"
"2393544","2393554","<p>Conditional probability works regardless of chronological order. The probability $P(C\mid R)$ answers the question: Having drawn a red disc, what's the probability that it came from the bag? This is the question that was asked and it appears to have been solved correctly.</p>

<p>Note that this has a different meaning than $P(R\mid C)$, which is the probability that we get a red disc while choosing from the bag.</p>
"
"2393561","2393584","<p>$$x^2 \geqslant 0 \Rightarrow x^2+1 \geqslant 1 \Rightarrow \frac{1}{x^2+1} \leqslant 1 \Rightarrow \frac{|y|}{x^2+1} \leqslant |y|   $$</p>

<p>The same inequality is also  used  in the second example.</p>
"
"2393577","2393605","<p>Let $xyz=a$ and $x\leq y\leq z$.</p>

<p>Thus, $x$, $y$ and $z$ are roots of the equation $f(X)=a$, where
$$f(X)=X^3-6X^2+7X.$$
Now, $f'(X)=3X^2-12X+7$, which says that $X_{max}=\frac{6-\sqrt{15}}{3}$, $X_{min}=\frac{6+\sqrt{15}}{3}$ and
$$x\leq\frac{6-\sqrt{15}}{3}\leq y\leq \frac{6+\sqrt{15}}{3}\leq z.$$</p>

<p>Now, for $a=f\left(\frac{6-\sqrt{15}}{3}\right)$ we'll get a maximal value of $z$ </p>

<p>and for $a=f\left(\frac{6+\sqrt{15}}{3}\right)$ we'll get a minimal value of $x$.</p>

<p>Finally we obtain:</p>

<p>$$2-2\sqrt{\frac{5}{3}}\leq x\leq\frac{6-\sqrt{15}}{3}\leq y\leq \frac{6+\sqrt{15}}{3}\leq z\leq2+2\sqrt{\frac{5}{3}}.$$
Done!</p>
"
"2393589","2394552","<p>Erdos proved (in 1951) the stronger result that $\binom{n}{k}$ is never a perfect power if $k &gt; 3$ (assuming that $n \geq 2k$ as we may). His argument is completely elementary -- see <a href=""https://www.renyi.hu/~p_erdos/1951-05.pdf"" rel=""nofollow noreferrer"">https://www.renyi.hu/~p_erdos/1951-05.pdf</a>.</p>

<p>For $k=2$ and $k=3$, the case of possible prime powers is easily treated by divisibility arguments. The more general situation of arbitrary perfect powers for these values of $k$ was handled by Gyory in 1997 (in Acta Arithmetica), by a rather more complicated proof.</p>
"
"2393594","2393897","<p>Functoriality implies that $\operatorname{Ext}(\beta,M)^i=\operatorname{Ext}(\beta^i,M)$, so if one power of $\beta$ is zero, the same power of $\operatorname{Ext}(\beta,M)$ is zero, because $\operatorname{Ext}(\text{zero map},M)=0$.</p>
"
"2393599","2393676","<p>Observe that since $a_1\in E(Z_1)\setminus E(Z_2)$ then $Z_2+a_1$ contains a cycle $C$. Suppose $a_1=uv$. Thus the only $uv$-path in $Z_1$ is $a_1$ and the only $uv$-path $P$ in $Z_2$ is $C-a_1$. Now we can see that there exists an edge $a_2$ in $P$ that is not in $Z_1$ (or else $C$ would be a cycle in $Z_1$ which is not possible).</p>

<p>Observe that in $Z_1+a_2$ there is a unique cycle $C'$ that contains edges $a_1$ and $a_2$. Let $Q$ denote the $uv$-path resulting from $C'$ by removing the edge $a_1$.</p>

<p>Let $Z_3=Z_1-a_1+a_2$. Below we show $Z_3$ is a spanning tree of $H$.</p>

<ol>
<li><p>$Z_3$ is spanning. Clearly since no vertices are being deleted.</p></li>
<li><p>$Z_3$ is a tree. First we show $Z_3$ is connected. Let $x,y\in V(Z_3)$. We prove there is an $xy$-walk in $Z_3$. Since $Z_1$ is connected, there exists and $xy$-walk $W$ in $Z_1$. Replace any occurrence of $a_1$ in $W$ by $Q$ to obtain $W'$. Clearly $W'$ is an $xy$-walk in $Z_3$. </p></li>
</ol>

<p>Now, $Z_3$ is connected (by 2), has $|V(H)|$ vertices (by 1) and $|V(H)|-1$ edges (since $|E(Z_3)|=|E(Z_1)|-1+1=|E(Z_1)|=|V(H)|-1$), thus it is a spanning tree.</p>
"
"2393604","2393645","<p>The number $Z_{dn} $ of unique numbers when selecting $dn$ numbers with replacement from $n$ is about $(1-e^{-d})n$ on average.  Moreover, the variance is of order $n$, so $Z_{dn} = (1-e^{-d})n + O_p(\sqrt{n})$. </p>

<p>It can be shown that the error of this approximation is asymptotically normal. Here is the idea. Let the numbers be drawn one by one. Denote by $T_k$ the time of $k$th unique number appearing. Then $T_k = G_1 + \dots + G_k$, where $G_k$ are independent with $G_k\simeq \mathrm{Geo}(\frac{n-k+1}{n})$. Despite they have different distribution, the CLT (e.g. in the Lindeberg form) can be applied for $k = cn$, $c\in(0,1)$. In order to deduce the asymptotic normality for $Z$ from here, notice that $\{Z_{m}\ge k\} = \{T_k\le m\}$.</p>

<p>TLDR: you have the desired statement for any $c&lt;1-e^{-d}$. </p>
"
"2393607","2393613","<p>The prime factorization of $479001600$ is $2^{10}3^5 5^2 7\cdot 11.$  The primes $2$ and $5$ occur to even powers because their exponents are even.  The primes $3$, $7,$ and $11$ occur to odd powers.  </p>
"
"2393620","2393654","<p>The Fourier inversion theorem says that $X(t) = \hat{\hat X}(-t),$ where $\hat{\phantom{X}}$ stands for Fourier transform:
$$\hat X(f) = \int_{-\infty}^{\infty} X(t) e^{-i 2\pi ft} dt$$</p>

<p>If you compute the Fourier transform of $X(t) = \frac12 ( \delta(\omega-2\pi f) + \delta(\omega+2\pi f) )$ then you can thereafter apply the inversion theorem to justify 
$$\mathcal F\{\frac12 (e^{it(\omega-2\pi f)} + e^{-it(\omega-2\pi f)})\} = \frac12 ( \delta(\omega-2\pi f) + \delta(\omega+2\pi f) ).$$</p>

<p>Fourier transforms like these are justified in the theory of distributions ($\delta$ is a distribution which is not also a function). If $u$ is a distribution then the Fourier transform of it is defined in a <em>weak</em> sense:
$$\int \hat u(x) \ \phi(x) \ dx = \int u(x) \ \hat\phi(x) \ dx$$
Here $\phi$ is a ""nice"" function on which there is no problem to define the Fourier transform.</p>
"
"2393623","2393672","<p>Let $\Delta MNP$ be such that $P$ placed inside $\Delta ABC$, $BP=CP$ and $\measuredangle PCB=\measuredangle PBC=30^{\circ}$.</p>

<p>Also, let $BP\cap MC=\{Q\}$.</p>

<p>Thus, since $CN=NB$, $\measuredangle CNP=\frac{1}{2}\measuredangle CNB=40^{\circ}$.</p>

<p>From here $\measuredangle CPN=180^{\circ}-20^{\circ}-40^{\circ}=120^{\circ}$ and since $\measuredangle QPC=60^{\circ}$, we see that</p>

<p>$CQ$ and $PQ$ are bisectors of $\Delta CNP$.</p>

<p>Thus, $NQ$ is a bisector of $\Delta CNP$, which gives $\measuredangle CNQ=20^{\circ}$ </p>

<p>and from here $\measuredangle NQM=10^{\circ}+20^{\circ}=30^{\circ}$.</p>

<p>But $NB\perp QM$ and $BN$ is a bisector of $\angle QBM$.</p>

<p>Thus, $BN\cap QM$ is a midpoint of $QM$, which says that $NQ=NM$ and</p>

<p>$$\measuredangle NMC=\measuredangle NQM=30^{\circ}$$
and we are done!</p>
"
"2393626","2393655","<p>You proved the sequence is monotonically decreasing and bounded below by $0$ hence converges to $L$. Thus $L = \dfrac{L^2+L}{2}\implies L^2 = L \implies L(L-1) = 0 \implies L = 0, 1 \implies L = 0$ since $L \le 1/2 &lt; 1$. Thus $R = \left|\displaystyle \lim_{n \to \infty} \dfrac{1}{\frac{a_{n+1}}{a_n}}\right|= \dfrac{1}{\left|\displaystyle \lim_{n \to \infty} \dfrac{a_{n+1}}{a_n}\right|}= \dfrac{1}{\dfrac{1}{2}\left(\displaystyle \lim_{n \to \infty} a_n + 1\right)}= \dfrac{1}{\frac{1}{2}} = 2$</p>
"
"2393640","2394394","<p>The group of symmetries of the pentagram consists of all reflections and rotations mapping the pentagram to itself. Any rotation doing this will map the regular pentagon to itself (notice the small regular pentagon in the middle of the figure). Also, any reflection doing this must be a reflection about a line cutting the figure into two congruent pieces. Obviously, any such line will cut the pentagon into two congruent pieces as well. This ""proves"" that the symmetry group $G$ of the pentagram is isomorphic to a subgroup of $D_5$, the symmetry group of the pentagon (by mapping each symmetry of the former to its restriction to the pentagon). By looking at the diagram we can detect at least $10$ elements of $G$, so $G$ is actually isomorphic (equal) to $D_5$.</p>

<p>Hopefully this was sufficiently non-hand-wavy. </p>
"
"2393661","2394796","<p>The matrix that you show <em>does not</em> generate a <em>cyclic</em> code but rather an <em>extended</em> cyclic code, and your formula for the designed distance is not quite right.  That being said, the parity-check matrices that you ask about generate shorter extended cyclic codes that can be described as (extended) <em>nonprimitive</em> BCH codes.</p>
"
"2393674","2393731","<p>There are two ways to look at the ones. </p>

<p>Algebraically, we are using the properties of determinant to express the area of a triangle $T$ with vertices at $A : (x_1,y_1)$, $B : (x_2,y_2)$, $C : (x_3,y_3)$ in a simpler form:</p>

<p>$$\verb/Area/(T) = \frac12 
\left|
\begin{matrix}
x_2 - x_1 &amp; y_2 - y_1\\
x_3 - x_1 &amp; y_3 - y_1\\
\end{matrix}\right|
=
\frac12 
\left|
\begin{matrix}
0 &amp; 0 &amp; 1\\
x_2 - x_1 &amp; y_2 - y_1 &amp; 1\\
x_3 - x_1 &amp; y_3 - y_1 &amp; 1\\
\end{matrix}\right|
= \frac12
\left|
\begin{matrix}
x_1 &amp; y_1 &amp; 1\\
x_2 &amp; y_2 &amp; 1\\
x_3 &amp; y_3 &amp; 1\\
\end{matrix}\right|
$$</p>

<p>Geometrically, we can embed $\mathbb{R}^2$ as the plane $z = 1$ in $\mathbb{R}^3$. The vertices of $T$ becomes the points $A' : (x_1,y_1,1)$, $B' : (x_1,y_2,1)$, $C' : (x_3,y_3,1)$ on $\mathbb{R}^3$. 
Let $T'$ be the tetrahedron spanned by $A', B', C'$ and origin $O : (0,0,0)$.
The area of $T$ is 3 times the volume of tetrahedron $T'$.
The volume of  $T'$ is $\frac16$ of the volume of the parallelepiped $P$ with one vertex at $O$ and spanned by the 3 vectors $A', B', C'$. Since the volume of a parallelepiped can be expressed as a cross product which equals to corresponding determinant, we have:</p>

<p>$$\verb/Area/(T) = 3\verb/Volume/(T') = \frac{3}{6}\verb/Volume/(P)
= \frac12
\left|
\begin{matrix}
x_1 &amp; y_1 &amp; 1\\
x_2 &amp; y_2 &amp; 1\\
x_3 &amp; y_3 &amp; 1\\
\end{matrix}\right|
$$
In certain sense, the introduction of ones here reflect the possibility
to express geometric relations for objects living on the plane as geometric relations on $\mathbb{R}^3$. It allows one to look at plane geometry problem from a completely different angle and offer us new insight how to solve a problem.</p>
"
"2393684","2393686","<p>No. The operator $T\colon\mathbb{R}^2\longrightarrow\mathbb{R}^2$ defined by $T(x,y)=(x+y,y)$ is not normal, but $(1,0)$ is an eigenvector and $1$ is an eigenvalue.</p>
"
"2393685","2393696","<p>What you want to know is the ""domain"" of these various functions.</p>

<p>If you are only dealing with real numbers, then the domain of $\sqrt{x}$ is all non-negative reals (i.e. $x \geq 0$). </p>

<p>On the other hand, the domains of $\sin(x)$ and $\cos(x)$ is all real numbers.</p>

<p>The domain of $\tan(x)$ is everywhere $x$ is not equal to $\pi/2$ plus an integer multiple of $\pi$. In particular, $x\not=\dots, -\dfrac{3\pi}{2},-\dfrac{\pi}{2},\dfrac{\pi}{2},\dfrac{3\pi}{2},\dfrac{5\pi}{2},\dots$ (where $x$ is measure in radians). In degrees this would be $x \not= -270,-90,90,270,450,\dots$</p>

<p>As for the other trig functions, secant (i.e. $\sec(x)$) has the same domain as tangent.</p>

<p>Cosecant and cotangent (i.e. $\csc(x)$ and $\cot(x)$) have the domain of all reals not equal to an integer multiple of $\pi$ (measuring in radians). In particular, $x \not= \dots,-2\pi,-\pi,0,\pi,2\pi,3\pi,\dots$ (or in degrees $x\not=\dots,-360,-180,0,180,360,540,\dots$).</p>
"
"2393698","2393710","<p>A cluster point (or limit point) of a set can be thought of intuitively as a point with ""neighboring"" points that are arbitrarily close.  In slightly math-ier language,</p>

<blockquote>
  <p>We say that $x$ is a limit point of $E \subseteq \mathbb{R}$ if for all positive numbers $\varepsilon$, there exists some number $y \in E$ with $y\ne x$ such that $|x-y| &lt; \varepsilon$.</p>
</blockquote>

<p>In the case of your set $A$, zero is not a limit point, because it doesn't have any ""neighbors"" in $A$.  For example, if we take $\varepsilon = \frac{1}{2}$, we cannot find any point $y$ in $A$ (other than $0$) such that $|0-y| &lt; \frac{1}{2}$.</p>
"
"2393701","2393711","<p>No, for example any non-local domain is a counterexample.</p>

<p>For a non-domain example, an interesting one is $\mathbb Z[x]/(x^2-1)$ lacks nontrivial idempotents, but has distinct maximal ideals.</p>

<p>You can find this and several more examples <a href=""http://ringtheory.herokuapp.com/commsearch/commresults/?has=58&amp;lacks=4"" rel=""nofollow noreferrer"">using this search at DaRT</a>.</p>
"
"2393707","2393744","<p>Consider $x \in V$ and suppose $f(x) = 0$ for all $f \in W^{0}$. Since $V$ is finite dimensional the subspace $W$ has a basis $w_1, \ldots{}, w_n$. Note that for any $v \in \left&lt;w_1, \ldots, w_n, x\right&gt;$ we have $f(v) = 0$ for any $f \in W^{0}$ (prove this part using the fact that we have a basis). Hence $W^{0} \subset \left&lt;w_1, \ldots, w_n, x\right&gt;^{0}$ and we know the reverse holds since $W \subset \left&lt;w_1, \ldots, w_n, x\right&gt;$ so we find $W = \left&lt;w_1, \ldots, w_n, x\right&gt;$. Hence $x \in W$.</p>
"
"2393709","2393713","<p>The $p$-adic numbers are not ordered as a field.</p>

<p>The real numbers are the unique (up to isomorphism) totally ordered field that is Dedekind complete.  The $p$-adic numbers are a field for any prime $p$, but they will not be ordered and, as such, the notion of Dedekind completeness doesn't even really make sense.</p>

<p>On the other hand, we can build the $p$-adics as a <em>metric</em> completion of $\mathbb{Q}$.  From this point of view, the topologies of $\mathbb{R}$ and $\mathbb{Q}_p$ will be quite different---$\mathbb{Q}_p$ is totally disconnected (and topologically, indistinguishable from a Cantor set!).  Hence the $p$-adics differ from the reals from that point of view, as well.</p>

<p>On the other other hand, it is possible to show via Ostrowki's theorem, that the only non-trivial metric completions of the rationals are $\mathbb{R}$ and the $p$-adic completions.  Thus there is a relation between $\mathbb{R}$ and $\mathbb{Q}_p$.</p>
"
"2393712","2393725","<p>L'Hospital's rule is unnecessary here, as simple algebra gets rid of the indeterminate form. Just cancel an $x$ from the top and bottom as the author does:
$$
\lim_{x\rightarrow 0} \frac{x^3 - 7x}{x^3} = \lim_{x\rightarrow 0} \frac{x^2 - 7}{x^2}.
$$
Now, when the author writes ""$-7/0^+$"", he's saying that the limit of the numerator is -7, and the limit of the denominator is 0, but is positive on both sides. Whenever you have a finite limit for the numerator and a 0 limit for the denominator, the limit will either be nonexistent (if the signs don't match on opposite sides) or $\pm \infty$ (if they do). In this case the denominator is positive on both sides, so, since the numerator is negative, the limit will be $-\infty$. It is this sense in which $-7/0^+ = -\infty$. Similarly, in this sense $-7/0^- = \infty$, while $-7/0$ remains undefined.</p>
"
"2393719","2393749","<p>More generally, $$^X(Y^n)\cong\left(^XY\right)^n$$</p>

<p>Define $[n]=\{1,2,\dots,n\}$. Then: $Y^n\cong {}^{[n]}Y$ and the result follows from the general rule:</p>

<p>$$^X\left({}^YZ\right)\cong {}^{X\times Y}Z\equiv {}^Y\left({}^XZ\right)$$</p>
"
"2393723","2393736","<p>It looks good. Here's a proof that doesn't go through the dual space. However it's really not all that different from what you have.</p>

<p>Since $f\in L^p$, we obtain from a version of HÃ¶lder's inequality (seen <a href=""https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality"" rel=""nofollow noreferrer"">here</a> as ""Extremal equality"") a function $g\in L^q$ such that
$$\|f\|_p=\left|\int_E f(x)g(x)\,dx\right|.$$
Given $\varepsilon&gt;0$, there exists by the density of the compactly supported continuous functions in $L^q$ a compactly supported continuous function $h$ such that $\|g-h\|_q\|f\|_p&lt;\varepsilon$. Then, by HÃ¶lder's inequality and our hypothesis, we have
$$
\|f\|_p=\left|\int_Ef(x)g(x)\,dx\right|
\leq \int_E|f(x)||g(x)-h(x)|\,dx + \left|\int_E f(x)h(x)\,dx\right|
\leq \|f||_p\|g-h\|_q &lt; \varepsilon.
$$
Therefore $\|f\|_p=0$, which implies $f=0$ a.e.</p>
"
"2393737","2393754","<p>Your example seems to indicate that the $\leftrightarrow$ is <em>defined</em> as a conjunction of two conditionals.  That is, $\varphi \leftrightarrow \psi$ is seen as <em>shorthand</em> for $(\varphi \rightarrow \psi) \land (\psi \rightarrow \varphi)$</p>

<p>Some formal proof systems do indeed do this, and some will even consider the rewriting of that definition/shorthand as a step in the proof, which is what your example proof seems to be doing.</p>

<p>However, this is not the same as using any kind of logical equivalence to rewrite statements.  If you want to rewrite statements using logical equivalences, then the formal proof system needs to have the corresponding rules defined for that. If the system has not defined those equivalences as rules, then you are not allowed to do that within the system</p>

<p>OK, but maybe the proof system, has some other definitions/shorthands, like the one you propose for the $\rightarrow$, or for the $\lor$?  Well, that's possible, but again that all depends on how your proof system has defined its formal syntax, and its rules.</p>
"
"2393755","2393760","<p>After integrating both sides, we find</p>

<p>$$-e^{-x^2}=t^2+C $$</p>

<p>but for $t=0$ we should have $x (0)=1$ thus $C=-e^{-1} $.</p>

<p>finally $$x^2 (t)=-\ln \left(-t^2+\frac {1}{e}\right) $$
$$=\ln \left(\frac {1}{-t^2+\frac {1}{e}}\right) $$</p>

<p>and
$$x (t)=\sqrt {\ln \left(  \frac {1}{-t^2+\frac {1}{e}}    \right)}$$</p>

<p>if we take negative root, we will have $x (0)=-1$ instead of $1$.</p>
"
"2393756","2393761","<p>If we focus on $-1&lt;x&lt;1$ and end up with $-2&lt;3x+1 &lt; 4$, $\frac{1}{3x+1}$is unbounded.</p>

<p>Rather than focusing on $-1 &lt; x &lt; 1$, focus on a smaller interval, for example $|x| &lt; \frac14$. Hence $\delta &lt; \frac14$.</p>

<p>if $-\frac14 &lt; x &lt; \frac14$, $$-\frac34+1 &lt; 3x+1&lt; \frac34+1$$.</p>

<p>$$\frac14 &lt; 3x+1&lt; \frac74$$</p>

<p>$$\left|\frac{1}{3x+4} \right| &lt; 4$$</p>

<p>Hence $$12\delta &lt; \epsilon$$</p>
"
"2393759","2393768","<p>$$x = 1 - (1-y)^t$$
$$(1-y)^t=1-x$$
Notice that the following step does not always hold, for example if $t=2$, and $x&gt;1$, we end up with no solution for $y$. But suppose every value is proper, we then have:
$$\bigg( (1-y)^t \bigg)^{1/t}=(1-x)^{1/t}$$
$$1-y=(1-x)^{1/t}$$
$$-y=(1-x)^{1/t} -1$$
$$y=1-(1-x)^{1/t}$$</p>
"
"2393773","2393780","<p><strong>hint</strong></p>

<p>$$\int_{k}^{k+1}\frac {dt}{t^\alpha}\le \frac {1}{k^\alpha}\le \int_{k-1}^k\frac {dt}{t^\alpha}$$</p>
"
"2393774","2393821","<p>Yes, the categorical product and coproduct correspond to multiplication and addition in a fairly intuitive way.</p>

<p>Suppose we work in the category $\mathcal C$ of finite sets. For each finite set $X$, we of course have a number $|X|$, which is its cardinality. Taking the cardinality corresponds to counting the objects of $X$.</p>

<p>The coproduct $X \sqcup Y$ is simply a disjoint union. We have the relation
$$|X\sqcup Y|=|X|+|Y|.$$
This relation just says that, in order to count a set composed of two smaller sets, we just count each of the smaller sets and add them.</p>

<p>The product $X\times Y$ is the set of pairs $(x,y)$ with $x\in X$ and $y\in Y$. We have the relation
$$|X\times Y|=|X|\cdot |Y|.$$ 
This relation states that, if we wish to count the number of possible combinations of two things, we may count the number of possibilities for each item separately and multiply them. </p>

<p>This connection is fairly nice because the relations more or less express the way that addition and multiplication are related to counting - and I'd imagine that most everyone has encountered these relations implicitly while learning how (and why) to add and multiply.</p>
"
"2393776","2393859","<p>Here are some statements of results from the paper. Each result here has its own proof, some long, some short. If you can say one proof in particular that you need translated, I'll see what I can do. Keep in mind, I don't <em>actually</em> know French, so I make no guarantees as to speed.</p>

<blockquote>
  <p>Lemma 3.1: For every $(m,2k,n)$-code, and every integer $1\leq s\leq n$, we have: $t_s(m-t_s)\leq mk$.</p>
</blockquote>

<p>(very short proof)</p>

<blockquote>
  <p>Lemma 3.2: For every $(m,2k,n)$-code $X$ such that:</p>
  
  <p>$$t_s\leq \frac{m+2k-1}{k+1}-1$$</p>
  
  <p>(for all $1\leq s\leq n$), we have $t_s\leq 1$ for all $1\leq s\leq n$.</p>
</blockquote>

<p>(long proof)</p>

<blockquote>
  <p>Lemma 3.3: Every $(m,2k,n)$-code, such that $m\not\in[4,k^2+k+2]$ satisfies the condition $t_s\leq 1$ for $s\in [1,n]$.</p>
</blockquote>

<p>(fairly short proof, using 2 previous lemmas)</p>

<blockquote>
  <p>Lemma 3.4: An $(m,2k,n)$-code is trivial if and only if $t_s\leq 1$ for $1\leq s\leq n$.</p>
</blockquote>

<p>(shortish proof)</p>

<blockquote>
  <p>Lemma 3.5: If $m\geq 4$ and there exists a non-trivial $(m,2k)$-code, then, for every $m'\in[4,m]$, there exists a non-trivial $(m',2k)$-code.</p>
</blockquote>

<p>(shortish proof)</p>

<blockquote>
  <p>Lemma 3.6: If there exists a non-trivial $(m,2k)$-code, we have $N(m,k)&lt;mk$. Otherwise, we have $N(m,k)=mk$.</p>
</blockquote>

<p>(shortish proof)</p>

<p>After this, there's a paragraph saying it would be interesting to find certain spectra of numbers for which non-trivial codes exist, and then the real proof occurs in a paragraph on p. 350 beginning ""Passons maintenant...""</p>

<p>Statement of the main theorem:</p>

<blockquote>
  <p>Theorem 1.1: Let $m$ and $k$ be two integers such that $m&gt;k \geq 1$; then we have:</p>
  
  <p>i) There exists a number $N(m,k)$ such that a necessary and sufficient condition for the existence of an $(m,2k,n)$-code is that $n\geq N(m,k)$</p>
  
  <p>ii) $N(m,k)\leq mk$ and there exists a number $f(k)$ such that $f(k)\geq 4$ and $N(m,k)=mk$ if and only if $m\in [4,f(k)]$</p>
  
  <p>iii) There exists a trivial $(m,2k)$-code and its length is greater than or equal to $mk$. There exists an $(m,2k)$-code, different from a trivial code, if and only if $m\in[4,f(k)]$.</p>
  
  <p>iv) $f(k)\leq k^2+k+2$ and $f(k)=k^2+k+2$ if there exists a $PG(2,k)$.</p>
</blockquote>
"
"2393781","2394371","<p>We will show that if for all $x$, $\mu\{\omega:f(\omega)\geq x\}&gt;0$, then for all finite partitions $\{A_i\}$ of $\Omega$ into ${\scr F}$-sets, there is an $i$ such that $f$ is unbounded on $A_i$ and $\mu(A_i)$>0. The result immediately follows from this fact, since we will then have $$\inf\sum_i\left(\sup_{\omega\in A_i}f(\omega)\right)\mu(A_i)\geq\left(\sup_{\omega\in A_i}f(\omega)\right)\mu(A_i)=\infty\cdot\mu(A_i)=\infty.$$</p>

<p>Consider any finite partition $\{A_i\}$ of $\Omega$ into ${\scr F}$-sets. It is sufficient to show the result for an arbitrary partition of an arbitrary subset of $\Omega$ into two ${\scr F}$-sets, since we can consider the partitions
$$A_1|A_1^c,~~~A_1|A_2|A_1^c\cap A_2^c,~~~\cdots,~~~A_1|\cdots|A_m,$$
result applying the result to
$$\Omega,~A_1^c,~A_1^c\cap A_2^c,~\cdots\text{ and }A_1^c\cap\cdots\cap A_{m-1}^c.$$</p>

<p>Say the sets are $A$ and $A^c$. First consider the case in which $\mu(A)\neq0$ and $\mu(A^c)\neq0$. Note that $f$ must be unbounded on $A$ or $A^c$, since it is unbounded on $\Omega$. On the other hand, if one of them has measure zero, then the result is obvious. Without loss of generality, say $A$ has measure zero. Then for all $x$,
$$\mu\{\omega\in A^c:f(\omega)\geq x\}=\mu\{\omega\in\Omega:f(\omega)\geq x\}&gt;0,$$
so $f$ is unbounded on $A^c$. Note that $\mu(A^c)&gt;0$ since $A^c\supseteq\{\omega\in A^c:f(\omega)\geq1\}$.</p>
"
"2393782","2393808","<p>Hint: $f= f\cdot \chi_{\{|f|\le 1\}}+ f\cdot \chi_{\{|f|&gt;1\}}.$</p>
"
"2393783","2395571","<p>In this context, <a href=""https://en.wikipedia.org/wiki/Seifert_surface#Genus_of_a_knot"" rel=""nofollow noreferrer"">genus</a> is the minimal genus taken over all Seifert surfaces of the knot (i.e. over all oriented spanning surfaces of the knot). Ozsvath and Szabo prove (in this <a href=""https://arxiv.org/abs/math/0311496"" rel=""nofollow noreferrer"">paper</a>) that the genus of a knot is the maximum Alexander grading in which the knot Floer homology is non-trivial. If the knot Floer homology of a knot is thin, then the degree of the Alexander polynomial is equal to the maximum Alexander grading in which the knot Floer homology is non-trivial (and thus it is also equal to the genus of the knot).</p>
"
"2393784","2393799","<p>Lagrange interpolation yields
$$ {\frac {1192918183\,{x}^{9}}{90720}}-{\frac {4732108061\,{x}^{8}}{
10080}}+{\frac {106913886451\,{x}^{7}}{15120}}-{\frac {14018251313\,{x
}^{6}}{240}}+{\frac {248593167287\,{x}^{5}}{864}}-{\frac {412373959049
\,{x}^{4}}{480}}+{\frac {17052726025753\,{x}^{3}}{11340}}-{\frac {
3507552477271\,{x}^{2}}{2520}}+{\frac {322120060873\,x}{630}}+2
$$
giving the first $10$ primorials for $n=0\ldots 9$.</p>
"
"2393786","2393797","<p>(a) You can approximate a derivative by a so-called finite difference:
$$f'(4)\approx \dfrac{f(5)-f(3)}{5-3}=\dfrac{-2-4}{5-3}=-3.$$
You do not know what happens inside the interval, so this is the most useful thing you can say about the derivative at 4. If you would know function values closer to 4, the estimate of $f'(4)$ would be better.</p>

<p>(b) The integral of a derived function is the original function.
$$\int_2^{13}(3-5f'(x))\mathrm{d}x=\int_2^{13}3\mathrm{d}x-5\int_2^{13}f'(x)\mathrm{d}x=3[x]_2^{13}-5[f(x)]_2^{13}\\=3(13-2)-5(f(13)-f(2))=33-5(6-1)=8.$$</p>
"
"2393792","2393946","<p>Let $a+b+c=3u$, $ab+ac+bc=3v^2$ and $abc=w^3$.</p>

<p>Hence, $u=1$ and we need to prove that
$$(27-15\sqrt3)\left(\frac{1}{a}+\frac{1}{b}+\frac{1}{c}\right)\geq a^2+b^2+c^2$$ or
$$\frac{(27-15\sqrt3)v^2}{w^3}\geq3u^2-2v^2$$ or $f(w^3)\geq0,$ where
$$f(w^3)=(27-15\sqrt3)u^3v^2-(3u^2-2v^2)w^3.$$
We see that $f$  decreases, which says that it's enough to prove our inequality</p>

<p>for a maximal value of $w^3$, which happens for equality case of two variables.</p>

<p>Since $f(w^3)\geq0$ is homogeneous, it's enough to assume $b=c=1$, which gives
$$(27-15\sqrt3)(a+2)^3\left(2+\frac{1}{a}\right)\geq27(a^2+2)$$ or
$$(a-1-\sqrt3)^2(2(9-5\sqrt3)a^2+7(12-7\sqrt3)a+4(33-19\sqrt3))\geq0,$$
which is obvious.</p>

<p>Done!</p>
"
"2393795","2393815","<p>This is an easy consequence of the existence of the <a href=""https://en.wikipedia.org/wiki/Jordan_normal_form#Real_matrices"" rel=""nofollow noreferrer"">real Jordan normal form</a> of the matrix of the endomorfism. That matrix is similar to a block diagonal matrix, with each block being a real Jordan block. There are several cases to be considered. For instance, if you endomorfism has one and only one real eigenvalue (with multiplicity $1$) and four complex non-real eigenvalues, then the real Jordan normal form will be of the type$$\begin{pmatrix}1&amp;0&amp;0&amp;0&amp;0\\0&amp;a&amp;-b&amp;0&amp;0\\0&amp;b&amp;a&amp;0&amp;0\\0&amp;0&amp;0&amp;c&amp;-d\\0&amp;0&amp;0&amp;d&amp;c\end{pmatrix}$$and therefore the span of the first three vectors of the correspondeng basis will be invariant. If you endomorfism has one and only one real eigenvalue (with multiplicity $1$) and two complex non-real eigenvalues (each with multiplicity $2$), then either the real Jordan normal form will be like the previous one (with $c=1$ and $d=b$) or will have the form$$\begin{pmatrix}1&amp;0&amp;0&amp;0&amp;0\\0&amp;a&amp;-b&amp;1&amp;0\\0&amp;b&amp;a&amp;0&amp;1\\0&amp;0&amp;0&amp;a&amp;-b\\0&amp;0&amp;0&amp;b&amp;a\end{pmatrix},$$but again you can consider the span of the first three vectors of the corresponding basis. And so on.</p>
"
"2393800","2393806","<p>If $R$ is a ring and $J$ a two-sided ideal of $R$, the quotient ring $R/J$ consists of the equivalence classes $x + J$ for $x \in R$, where $x \sim y$ if $x - y \in J$.
This is a ring with operations $(x+J) + (y+J) = (x+y)+J$ and $(x+J)(y+J) = xy + J$.</p>

<p>In the case of $\mathbb Z / p\mathbb Z$, $p\mathbb Z$ consists of the multiples of $p$ and the equivalence relation is congruence mod $p$.  Thus $\mathbb Z/p\mathbb Z$ consists of the congruence classes mod $p$.</p>
"
"2393801","2393805","<ol>
<li>Yes, but notice that $S$ is a set of sets, i.e. every element of the set $S$ is a set. For example, $\{a\}$ is a set containing one element $a$, but the set itself is an element of the set $S$.</li>
<li>S' is a set that contains three elements $a,b,c$; while $S$ is a set that contains three elements, and each element is a set of one element, namely $\{a\},\{b\},\{c\}$.</li>
<li>No, they are not equivalent. Sigma-algebra is always a set of sets.</li>
</ol>
"
"2393810","2393889","<p>The sum evaluates to $(1-k^{2})(2K(k)/\pi)^{2}$ for $k=\sqrt{2}-1$. The value of $K$ is obtained from <a href=""https://math.stackexchange.com/a/2391675/72031"">here</a>. Using the value of $K$ we can get the desired closed form sum of the given series. </p>

<hr>

<p>Using hypergeometric transformations we can prove the formula $$\left(\frac{2K(k)}{\pi}\right)^{2}=\sum_{n=0}^{\infty}\left(\frac{1\cdot 3\cdot 5\cdots (2n-1)}{2\cdot 4\cdot 6\cdots (2n)}\right)^{3}(2kk')^{2n}\tag{1}$$ (see complete proof <a href=""http://paramanands.blogspot.com/2011/10/elementary-approach-to-modular-equations-hypergeometric-series-2.html"" rel=""nofollow noreferrer"">here</a>). Note that the above equation can be thought of as an identity involving the nome $q=e^{-\pi K'/K} $ also and then we can switch from $q$ to $-q$. In terms of Jacobi's theta functions we have $2K/\pi=\vartheta_{3}^{2}(q)$ and replacing $q$ by $-q$ leads us to $$\vartheta_{3}^{2}(-q)=\vartheta_{4}^{2}(q)=\frac{\vartheta_{4}^{2}(q)}{\vartheta_{3}^{2}(q)}\cdot\vartheta_{3}^{2}(q)=k'\cdot\frac{2K}{\pi}$$ Similarly we have $$(2kk')^{2}=4k^{2}k'^{2}=4\cdot\frac{\vartheta_{2}^{4}(q)\vartheta_{4}^{4}(q)}{\vartheta_{3}^{8}(q)}=4\cdot\frac{16q\psi^{4}(q^{2})\vartheta_{4}^{4}(q)}{\vartheta_{3}^{8}(q)}$$ where $\psi$ is one of Ramanujan's theta functions. Now changing $q$ into $-q$ interchanges $\vartheta_{3},\vartheta_{4}$ and hence the above expression becomes $$-4\cdot\frac{16q\psi^{4}(q^{2})\vartheta_{3}^{4}(q)}{\vartheta_{4}^{8}(q)} =-4\cdot\frac{\vartheta_{2}^{4}(q)/\vartheta_{3}^{4}(q)} {\vartheta_{4}^{8}(q)/\vartheta_{3}^{8}(q)}=-\frac{4k^{2}}{k'^{4}} $$ So we have the formula $$k'^{2}\left(\frac{2K}{\pi}\right)^{2}=\sum_{n=0}^{\infty}(-1)^{n}\left(\frac{1\cdot 3\cdots (2n-1)}{2\cdot 4\cdots(2n)}\right)^{3}(2k/k'^{2})^{2n}\tag{2}$$ And putting $k=\sqrt{2}-1$ we get the desired sum.</p>

<p>As indicated in a comment from Jack d'Aurizio the sum in question can also be expressed in terms of arithmetic-geometric-mean as $$\frac{2}{\operatorname {AGM} (\sqrt{2},\sqrt{1+\sqrt{2}})^{2}}\tag{3}$$ Writing in this manner helps to evaluate the sum very accurately with a few iterations of the AGM.</p>

<p>It is also interesting to know that Ramanujan used the series $(1)$ to obtain $$\frac{4}{\pi}=\sum_{n=0}^{\infty}\left(\frac{1\cdot 3\cdots (2n-1)}{2\cdot 4\cdots (2n)}\right)^{3}\cdot\frac{6n+1}{4^{n}}\tag{4}$$ and $$\frac{2}{\pi}=\sum_{n=0}^{\infty}(-1)^{n}(4n+1)\left(\frac{1\cdot 3\cdots (2n-1)}{2\cdot 4\cdots (2n)}\right)^{3}\tag{5}$$ was obtained from series $(2)$. You can get all the details <a href=""http://paramanands.blogspot.com/2012/03/modular-equations-and-approximations-to-pi-part-2.html"" rel=""nofollow noreferrer"">here</a>.</p>

<hr>

<p>The value of $K(\sqrt{2}-1)$ has been obtained via non-obvious hypergeometric transformations and it would be nice to have an approach based on direct evaluation of integral involved. </p>
"
"2393811","2393816","<p>No, it's not automatic.  For instance, consider the category $C$ with one object $X$ and two arrows $id$ and $f$ with $f\circ f=f$.  Then the map $F$ which sends both $f$ and $id$ to $f$ preserves $\circ$, but does not map $id$ to $id$.</p>
"
"2393817","2393822","<p>$$a_n \ge \sum_{k=1}^{n-1} (y_{k+1} - y_k) = y_n - y_1$$</p>
"
"2393828","2393836","<p>By CauchyâSchwarz inequality,
$$
\sum_{k=1}^n \frac{1}{n}\cdot a_k \leq \sqrt{\sum_{k=1}^n a_k^2}\cdot \sqrt{\sum_{k=1}^n \frac{1}{n^2}}
= \sqrt{\sum_{k=1}^n a_k^2}\cdot\sqrt{\frac{1}{n}}
= \sqrt{\frac{1}{n}\sum_{k=1}^n a_k^2}
$$
and you can conclude by the squeeze theorem.</p>
"
"2393845","2393848","<p>Note that $\prod_{i\in[0,1]}\{i\}$ is an infinite (actually uncountable!) product of sets, but it only contains one element; namely the function $f:[0,1]\to[0,1]$ defined by $f(i)=i$.</p>
"
"2393849","2394600","<p>You did the computation already, but I'll explain the situation in general. Suppose that functions $\phi_n$ converge to the Dirac $\delta$ in the sense of distributions, meaning 
$$
\lim_{n\to \infty}\int_{\mathbb{R}} \phi_n(x)f(x)\, dx  = f(0)
$$
for smooth compactly supported functions $f$. Then the functions $\psi_n = -\phi_n'$, with the derivative understood in the sense of distributions, satisfy
$$
\lim_{n\to \infty}\int_{\mathbb{R}} \psi_n(x)f(x)\, dx  = f'(0)
$$
The reason is integration by parts: $\int_{\mathbb{R}}(-\phi_n(x))'f(x)\, dx
= \int_{\mathbb{R}} \phi_n(x)f'(x)\, dx \to f'(0)$.</p>

<p>In your situation $\phi_n(x) = \frac{n}{2} \cos nx$ restricted to $|x|&lt;\pi/(2n)$. This is a continuous piecewise smooth function, so its distributional derivative is the same as the regular one: $\psi_n(x) = -\phi_n'(x) = \frac{n^2}{2} \sin nx$  restricted to $|x|&lt;\pi/(2n)$. </p>

<p>And since $\phi_n$ and $\psi_n$ are compactly supported, it does not matter whether $f$ is compactly supported; smoothness is enough. </p>
"
"2393850","2393871","<p>Let $A$ and $B$ be a pair of consecutive vertices and $P$ be the point in question.  If $PA$ and $PB$ are both longer than $AB$, then angle $P$ in triangle $ABP$ has to measure less than $60Â°$.  To make up a full revolution there have to be seven or more different angles of this type at $P$, each opposite a different side of the polygon.</p>

<p>So there are no candidates with fewer than seven sides.  But the center of a regular heptagon is farther from the vertices than the length of any side making seven sides a <em>sharp</em> lower bound.</p>
"
"2393851","2394168","<p>For a quiver with loops (or directed cycles), the Jacobson radical is not the ideal generated by arrows. In fact, for your quiver $Q$, the Jacobson radical of $kQ$ is zero.</p>

<p>But if by $J$ you actually mean the ideal generated by arrows, then it is not true that $kQ/I$ finite dimensional implies $J^n\subseteq I$ for some $n$.</p>

<p>Consider the simpler example of the quiver with a single loop $x$ and $I=\langle x^3-x^2\rangle$. Then $kQ/I=k[x]/(x^3-x^2)$ is $3$-dimensional, spanned by $1,x,x^2$, and (in $kQ/I$) $0\neq x^2=x^3=x^4=\dots$, so no power of $x$ is contained in $I$. </p>
"
"2393852","2397095","<p>Here's a proof of what I said in the comments (which solves your problem, but doesn't provide an explicit isomorphism). </p>

<p>Let $H\le S_n$ be a subgroup of index $n$, with $n\ge5$. Consider the action of $S_n$ on the cosets of $H$. The kernel of this action is trivial (since the only possible kernel is $A_n$, but $A_n\not\subset H$). Thus, $H$ acting on its own cosets is also a faithful action. Since $H$ is a subgroup, it fixes the trivial coset ($H$ itself). The action on the non-trivial cosets gives an injection of $H$ into $S_{n-1}$. Order considerations show this is an isomorphism. </p>
"
"2393857","2393858","<p>Let $A_k = \{a\in A : f(a)=k\}$ so by assumption $A_k$ is finite. Now $A=\bigcup_{k=1}^\infty A_k$, it follows that $A$ is a countable union of finite sets and therefore is finite or countable. </p>
"
"2393860","2393868","<p>You have all of the right ideas, but the writing is difficult to follow and you misuse a few words here and there. I'll do my best to make improvements without changing the techniques.</p>

<p>We claim that $E$ does not have any limit points. The fact that $p$ is not a limit point of $E$ is seen by noticing that, for every $r&gt;0$ we have $N_r(p)\cap E=\{p\}$. Hence $N_r(p)\cap E$ does not contain a point different from $p$. It remains to show that every $q\in X\setminus E$ is not a limit point of $E$. Given $q\in X\setminus E$, let $k$ be such that $0&lt;k&lt;d(p,q)$. Now $p\not\in N_k(q)$, so that $N_k(p)\cap E = \emptyset$. Hence $q$ is not a limit point of $E$. This completes the proof.</p>
"
"2393866","2393867","<p>Your understanding of $e$ is correct.</p>

<p>To see that $b$ is not a solution, note that $3+x^2$ is not periodic and not a constant.</p>

<p>Edit:</p>

<p>We can consider another basis of $\operatorname{span} \{ f,g\},$</p>

<p>Note that $\sin^2 x + \cos^2 x = 1$ and $\cos^2 x - \sin^2 x = \cos (2x)$</p>

<p>and we can check that they are linearly independent. </p>

<p>Hence functions in  $\operatorname{span} \{ f,g\}$ can be written in the form of $A+B\cos(2x)$. Explaining why the function has to be constaint or the period is $\pi.$</p>

<p>Credit: Carmeister to pointing out another correct solution. </p>
"
"2393870","2393960","<p>In his motivation, what you are calling ""locally constant path-integrals"" are intended to mean locally constant <em>with respect to (""proper"") variations of the <strong>paths</em></strong> (he clarifies this).</p>

<p>That said, let $\gamma_s$ be a variation of $\gamma$ such that $\gamma_0=\gamma$. Visualizing this as a map $H: I \times I \to M$ with $H(\cdot,0)=\gamma_0(\cdot)$, we know that 
$$\int_{I \times I} d(H^*\theta)=\int_{\partial (I \times I)}H^*\theta$$ 
by Stokes. The right side is $$\int_{\gamma_0} \theta-\int_{\gamma_1} \theta,$$
whereas the left side is $\int\limits_{I \times I}H^*(d\theta)=0.$</p>
"
"2393874","2394211","<p>You have a composition of two measurable maps $k_n: \Omega \to \Bbb N$ and $e:\Bbb N \to E$.</p>

<p>The first map is $k_n:\Omega \to \Bbb N$ given by $k_n(\omega) = \min\{ 1 \leq i \leq n: d_n(\omega) = d(X(\omega),e_i) \}$. Here $\Bbb N$ is given the discrete sigma algebra, i.e. all sets are measurable. This map is measurable because $$k_n^{-1}(\{i\}) = \{d_n(\omega) = d(X(\omega),e_i)\}\cap  \bigcap_{j&lt;i} \{d_n(\omega) \neq d(X(\omega),e_j)\}$$</p>

<p>and we know that the sets on the RHS are in $\cal A$, since $X$ is $(\mathcal A, \mathcal B(E))$-measurable.</p>

<p>The second map is $e: \Bbb N \to E$ given by $e(n):=e_n$, and this is measurable because all countable subsets (in particular, all subsets of $E_0$) are measurable in the Borel sigma algebra.</p>
"
"2393883","2393918","<p>Suppose a service maintains $240,000$ subscribers with an annual churn rate of $80\%.$
This service has  $192,000$ cancellations per year (offset by  $192,000$ new subscriptions if the part about ""maintains"" is true).</p>

<p>That averages to $16,000$ cancellations per month, which works out to a monthly churn rate of $(16,000/240,000)\times100\%\approx6.67\%.$</p>

<p>Not $12.55\%.$</p>

<p>A nice feature of this kind of churn calculation is that the average duration of a subscription is the reciprocal of the churn rate. </p>

<p>If you want to calculate churn by cohorts instead, then you need a model of how the cohort decays. For example, if you start with a cohort of $240,000$ and have $16,000$ cancellations from that cohort every month, you'll have an $80\%$ churn rate in the first year, and nobody from that cohort will remain after $15$ months. The monthly churn rate starts at just $6.67\%$ but rapidly increases (in the $15$th month it's $100\%.$)</p>

<p>The reciprocal relationship doesn't work out so neatly, however. If the cohort of $240,000$ were all new subscribers, their average duration is only $7.5$ months. </p>

<p>Another model is exponential decay. In this model the monthly attrition rate is constant, and the annual attrition rate is related to the monthly rate by a compounding formula as shown in the question. But the average duration of a subscription in that model depends on the <em>instantaneous</em> rate of attrition, which is related to both the monthly and annual rates by a formula of continuous compounding, and it predicts a shorter average duration than either the reciprocal of the monthly attrition or of the annual attrition. </p>

<p>For an exponential model with a small churn rate, however, the reciprocal formula for a short enough time period predicts about the same average subscription duration as the instantaneous rate does. For example, suppose the model is that the size of the cohort is $ne^{-0.03t}$ where $t$ is the number of months elapsed. This mathematical model has an average subscription duration of exactly $1/0.03=33\frac13$ months. 
After one month, this model predicts that you will lose $2.9554\%$ of your customers. 
The reciprocal formula then gives you $33.836$ months--not the same exact duration, but the difference may be a lot less than the inaccuracy of the model. </p>

<p>If we project the same exponential model to one year, it predicts you lose $30.23\%$ of customers. The reciprocal formula then gives you $3.308$ years, which is $39.69$ months. That's $6$ months longer than the mathematically precise answer for this model--but if the kind of answer you want is ""about three years"" then it's still good enough. Or you can choose your time unit so that the churn is a much smaller percentage and thereby avoid this source of disagreement with the model. </p>

<p>People who track these things for a living may be able to ignore a lot of these finer points because they are working with lower churn rates and/or they only need a rough approximation. </p>
"
"2393888","2394013","<p>Suppose $a$ is a point in $S$ that is not a limit point of $E.$</p>

<p>That means it is <b>not</b> the case that
$$
\forall\varepsilon&gt;0,\quad  \Big( [(a-\varepsilon,a+\varepsilon) \cap S] \smallsetminus \{ a \} \Big) \cap E \ne \varnothing.
$$
Thus the negation of that statement is true:
$$
\exists\varepsilon&gt;0, \quad \Big( [(a-\varepsilon,a+\varepsilon) \cap S] \smallsetminus \{ a \} \Big) \cap E = \varnothing.
$$
Thus every point $a\in S$ that is not a limit point of $E$ has an open neighborhood disjoint from $E.$ Every point in that open neighborhood fails to be a limit point of $E,$ because every point in the open neighborhood $(a-\varepsilon,a+\varepsilon)$ has its own open neighborhood that is a subset of $(a-\varepsilon,a+\varepsilon).$</p>
"
"2393899","2393905","<p>Your approach is right. Any $p$ that satisfies $f(x) \ge 0$ can be used to represent the CDF, $F(x)$. </p>

<p>In order to determine the value of $p$, we define $c = p-\frac{b}{b-a}$. Observe that $$f(x) = \frac{ab}{b-a}\left(e^{-ax}-e^{-bx}\right) + c\left(a e^{-ax}-be^{-bx}\right).$$
When $c \le 0$, $f(x)\ge 0$ for all $x \ge 0$, whereas when $c&gt;0$, $f(0) &lt;0$. Consequently, $0 \le p \le \frac{b}{b-a}$.</p>
"
"2393900","2393927","<p>Without more context it's difficult to be certain what meaning was intended, but I conjecture that $(1,-1,0)$ and $(0,1,-1)$ are to be interpreted as root vectors in the <a href=""https://en.wikipedia.org/wiki/Root_system"" rel=""nofollow noreferrer"">root system</a> of $SL_3(\mathbb C)$. Let $\mathfrak h$ be the usual Cartan subalgebra of $\mathfrak{sl}_3(\mathbb C)$, namely containing trace zero diagonal matrices. Let $r=(1,-1,0)$. Then $e=E_{1,2}$ is in the $r$-root space, in the sense that
$$
  [h,e]=\mathrm{tr}(h\,\mathrm{diag}(r))e
$$
for all $h\in\mathfrak h$. Similarly $f=E_{2,1}$ is in the $(-r)$-root space. The Lie subalgebra of $\mathfrak{sl}_3(\mathbb C)$ generated by $e$ and $f$ consists of trace zero matrices of the form
$$
  \begin{pmatrix}a&amp;b&amp;0\\ c&amp;d&amp;0\\ 0&amp;0&amp;0\end{pmatrix}.
$$
Applying the matrix exponential, the corresponding Lie subgroup of $SL_3(\mathbb C)$ is the first one you describe. Similarly for the other root.</p>
"
"2393901","2395818","<p>As you mention, the issue is that a net is not necessarily totally ordered. Let us consider an example: </p>

<p>Let $H=\ell^2(\mathbb N)$, with $\{e_n\}$ the canonical basis. Take $M=B(H)$, with the canonical trace. Let $\mathcal A$ be the family of finite subsets of $\mathbb N$, ordered by inclusion. For each $\alpha\subset\mathcal A$, define
$$
P_\alpha=\sum_{n\in\alpha} E_n,
$$
where $E_n$ is the rank-one projection onto the span of $e_n$. It is standard that $P_\alpha\nearrow I$. </p>

<p>Now you want to choose your sequence. Since you only choose by the size of the trace, say we choose 
$$
P_n=P_{\{2,4,6,\ldots,2n\}}=\sum_{k=1}^n E_{2k}.
$$
Then $\tau(P_n)=n$, and the sequence satisfies your condition. But it is not a subnet: for example $E_1=P_{\{1\}}$ is not below any $P_n$. And $$P_n\nearrow\sum_{k=1}^\infty E_{2k},$$ which is of course not the identity. </p>
"
"2393902","2394052","<p>The volume of a parallelepiped is indeed $V=xyz$. If it is inscribed into an ellipse $x^2/a^2+y^2/b^2+z^2/c^2=1$, then $z$ in the volume formula can be replaced by $z$ from the ellipse. Now the picture only shows $1/8$th part (there are $8$ octants) of the whole parallelepiped. Hence: $V_{large}=8V_{small}$. So there are two ways to find the volume of the large parallelepiped: direct or indirect:
$$Direct: \ \ V_{large}=2x\cdot 2y\cdot 2\cdot c\sqrt{1-\frac{x^2}{a^2}-\frac{y^2}{b^2}}.$$
$$Indirect: \ \ V_{large}=8V_{small}=8\cdot x\cdot y\cdot c\sqrt{1-\frac{x^2}{a^2}-\frac{y^2}{b^2}}.$$
Note: It looks one of two $c$s is excessive in the volume formula you provided. However, $f_x, f_y$ were found with one $c$.</p>

<p>Appendix:
$$f_x=yc\sqrt{1-\frac{x^2}{a^2}-\frac{y^2}{b^2}}+xyc\cdot \frac{-\frac{2x}{a^2}}{2\sqrt{1-\frac{x^2}{a^2}-\frac{y^2}{b^2}}}=yz-\frac{x^2yc^2}{a^2z}=\frac{y(a^2z^2-c^2x^2)}{a^2z}.$$
Note: After finding $f_x$ the root was replaced by $z$.</p>
"
"2393904","2394290","<p>You have
$$x[t+2]-2\beta{x}[t+1]+\beta^{2}x[t]=a\rho^{t}$$
The solution is
$$x[t]=x_{h}[t]+x_{p}[t]$$
Where the homogeneous and particular solution satisfy
$$x_{h}[t+2]-2\beta{x}_{h}[t+1]+\beta^{2}x_{h}[t]=0$$
$$x_{p}[t+2]-2\beta{x}_{p}[t+1]+\beta^{2}x_{p}[t]=a\rho^{t}$$
You have already found the homogeneous solution ans it is
$$x_{h}[t]=c_{1}\beta^{t}+c_{2}t\beta^{t}$$
For the particular one, substitute
$$x_{p}=\alpha\rho^{t}$$
this gives
$$\alpha[\rho^{2}-2\beta\rho+\beta^{2}]=a$$
Thus
$$\alpha=\frac{a}{(\rho-\beta)^{2}}$$
and the solution to the entire reccurence is
$$x[t]=c_{1}\beta^{t}+c_{2}t\beta^{t}+\frac{a\rho^{t}}{(\rho-\beta)^{2}}$$</p>
"
"2393912","2393956","<p>Note that $A$ has full column-rank, so its singular values are all positive.</p>

<p>Let $\sigma_1(A),\dots,\sigma_n(A)$ denote the singular values of $A$ from largest to smallest.  We note that $\|A\|_2 = \sigma_1(A)$ and $\|A^+\| = 1/\sigma_{n}(A)$.  Similarly, $\|A_1\|^{-1} = 1/\sigma_n(A_1)$.</p>

<p>Thus, your statement amounts to proving that $1/ \sigma_n(A) \leq 1/\sigma_n(A_1)$.  That is, 
$$
\sigma_{n} \left(\begin{bmatrix}
    A_1 \\A_2
\end{bmatrix}\right) \geq \sigma_n(A_1)
$$
It is easy to prove that this is the case using the Rayleigh-Ritz formula formula for the singular value, namely
$$
\sigma_n(M) = \min_{x \neq 0}\frac{\|Mx\|}{\|x\|}
$$
perhaps now you can see why this works.</p>

<hr>

<p>We have
$$
\sigma_{n} \pmatrix{A_1\\ A_2} = \min_{x \neq 0} \frac{\left\|\pmatrix{A_1\\A_2}x\right\|}{\|x\|} \geq
\min_{x \neq 0} \frac{\|A_1x\|}{\|x\|} = \sigma_n(A_1)
$$</p>
"
"2393922","2393929","<blockquote>
  <p>I am really confused as to why ordinals can't add/multiply just like natural numbers do, since they are essentially the same thing?</p>
</blockquote>

<p>The ordinals generalize the natural numbers in a certain sense, but that <em>does not mean</em> that every property of the natural numbers carries over to the ordinals. Weird though it may be, neither ordinal addition nor ordinal multiplication are commutative.</p>

<p>Ordinals are weird beasts. It's best at first to think of an ordinal as a particular linear order - a collection of dots laid out in a line <em>(not every linear order corresponds to an ordinal, of course, but every ordinal corresponds to a linear order)</em>. ""$\alpha+\beta$"" is what you get by putting a copy of $\beta$ after a copy of $\alpha$; ""$\alpha\cdot \beta$"" is what you get when you replace each point in $\beta$ with a copy of $\alpha$.</p>

<p>Reasoning pictorially, $1+\omega$ is $$1+\omega\quad=\quad{\large\bullet}\quad +\quad{\large\bullet}+{\large\bullet}+{\large\bullet}+{\large\bullet}+...\quad=\quad{\large\bullet}+{\large\bullet}+{\large\bullet}+{\large\bullet}+...\quad=\quad\omega,$$ while $\omega+1$ is $$\omega+1\quad=\quad {\large\bullet}+{\large\bullet}+{\large\bullet}+{\large\bullet}+...\quad+{{\large\bullet}}\quad=\quad {\large\bullet}+{\large\bullet}+{\large\bullet}+...{\color{red}+ \color{red}{\large\bullet}},$$
 and this latter does <strong>not</strong> look like $\omega$ (unlike $\omega$, it has a last element).</p>

<p>Here's another picture-argument: first, thinking about $2\omega$, we have $$2\omega\quad=\quad (2)+(2)+(2)+...\quad=\quad ({\large\bullet}+{\large\bullet})+({\large\bullet}+{\large\bullet})+...\quad=\quad {\large\bullet}+{\large\bullet}+{\large\bullet}+{\large\bullet}+...\quad=\quad\omega,$$ but $\omega2$ is $$\omega2\quad=\quad(\omega)+(\omega)={\large\bullet}+{\large\bullet}+{\large\bullet}+...{\color{\red}+ \color{red}{\large\bullet}}+{\large\bullet}+{\large\bullet}+...,$$
 and this does <strong>not</strong> look like $\omega$ (unlike $\omega$, it has an element with no immediate predecessor).</p>

<hr>

<p>In your title problem, since commutativity fails we can't argue that $$\omega+1+\omega+1=\omega+\omega+1+1=\omega+\omega+2;$$ the right answer is instead to see that $$\omega+1+\omega+1=\omega+(1+\omega)+1=\omega+\omega+1.$$ Similarly, we have to keep straight the difference between $\omega+\omega,$ which is $\omega2$, and $2\omega$. So the answer, written most snappily, is $\omega2+1$.</p>
"
"2393923","2394070","<p>$$u(x,y)=yF(y-2\sqrt{x}) \tag 1$$
Condition :</p>

<p>$1=u(s,a\sqrt {s})=a\sqrt {s}F(a\sqrt {s}-2\sqrt{s}) \quad\to\quad a\sqrt {s}F\big((a-2)\sqrt{s}\big) =1$</p>

<p>This implies $a\neq 0$ because if $a=0$ then $0=1$ which is impossible.</p>

<p>Also this implies $a\neq 2$ because if $a=2$ then $2\sqrt {s}F(0) =1\quad\to\quad \sqrt {s}=$constant, which is impossible.</p>

<p>Solution in case of $a\neq 0$ and $a\neq 2$ :</p>

<p>Let $X=(a-2)\sqrt{s} \quad\to\quad \sqrt{s}=\frac{X}{a-2} \quad\to\quad a\sqrt {s}F\big((a-2)\sqrt{s}\big) =1=a\frac{X}{a-2}F(X)$</p>

<p>The function $F(X)$ is determined :
$$F(X)=\frac{a-2}{aX}$$
Putting it into $(1)$ with $X=y-2\sqrt{x}$ leads to :
$$u(x,y)=\frac{a-2}{a}\frac{y}{(y-2\sqrt{x})}\qquad \begin{cases}a\neq 0\\a\neq 2 \end{cases}$$</p>
"
"2393925","2393996","<p><em>Probaly too long for a comment.</em></p>

<p>As  StephenG already commented, a CAS leads to
$$\left(-\mu ^2\right)^{-n} \left(\mu  (-\mu )^n \Gamma (n-1) \, _2F_2(1,1;2,2-n;\mu
   )-\pi  \mu ^n \csc (\pi  n) \Gamma (n,-\mu )+(n-1) \Gamma (n-1) \left(\pi  \mu
   ^n \csc (\pi  n)-\log (\mu ) (-\mu )^n+(-\mu )^n \psi ^{(0)}(n)\right)\right)$$ provided that $\Re(\mu )&gt;0\land \Re(n)&gt;-1$ which is effectively leading to indeterminate values if $n$ is an integer.</p>

<p>Interesting on the other side is that, if $n$ was an integer, this would reduce 
to 
$$G_{2,3}^{3,1}\left(\mu \left|
\begin{array}{c}
 -n,1-n \\
 0,-n,-n
\end{array}
\right.\right)$$ where appears the Meijer G function.</p>
"
"2393926","2393933","<p>You have found that $y' = \dfrac{x-2y}{2x-5y}\implies \dfrac{d^2y}{dx^2}= y'' = \dfrac{(1-2y')(2x-5y) - (x-2y)(2-5y')}{(2x-5y)^2} = \dfrac{2x-5y-4xy'+10yy'-2x+5xy'+4y-10yy'}{(2x-5y)^2}=\dfrac{-y+xy'}{(2x-5y)^2}= \dfrac{-y+x\cdot \dfrac{x-2y}{2x-5y}}{(2x-5y)^2}=\dfrac{x^2-4xy+5y^2}{(2x-5y)^3}$</p>
"
"2393935","2393941","<p><strong>HINT</strong> One way is to consider
$$
f(x) = a^x + (1-a)x
$$
and show that $f(x) \le 1$ for $x \in [0,1]$. Another is to look at
$$
(1+x)^k = 1 + kx + \ldots
$$
e.g. via continuous interpretation of the Binomial theorem.</p>
"
"2393938","2393953","<p>Note that the $0$ in the axioms need not be the usual $0$ in $\Bbb Z$.  In this case where we specify the usual addition it would have to be but you need to show there isn't one.  Once you find one axiom that fails you can quit.  You are right that elements do not have additive inverses.  That could be solved by taking the negatives of all your elements and adding them to the set.  Probably the easiest failure to cite is closure under addition.  Just note that $2$ and $5$ are in your set but $2+5=7$ is not and you are done.</p>
"
"2393948","2393962","<p>You must familiarize yourself with some common notation.</p>

<p>In the context of functions, $[a, b]$ is a closed interval containing all real numbers between $a$ and $b$, inclusive. This can be written as a compound inequality as $a \leq x \leq b$. Similarly $(a, b)$ is the open interval containing all real numbers $x$ strictly between $a$ and $b$. As an inequality, this is $a &lt; x &lt; b$.</p>

<p>The exercise in question speaks of a function whose domain is the interval $[-8, 6]$, that is, all $x$ for which $-8 \leq x \leq 6$. This is not a point.</p>

<p>When we speak of points, it will usually be clear in context. Domain talks about $x$-values (or whatever letter is used for the independent variable).</p>
"
"2393952","2394352","<p><strong>Example 1:</strong> Note that stochastic differential equations (SDEs) are a generalization of ordinary differential equations (ODEs); in particular, any ODE which does not have a unique solution does not have a unique (strong) solution. Consider for instance the differential equation</p>

<p>$$dX_t =2 \text{sgn}(X_t) \,  \sqrt{|X_t|} \, dt, \qquad X_0 = 0;$$</p>

<p>it does not have a unique (strong) solution since both $X_t^{(1)} :=  t^2$ and $X_t^{(2)} := - t^2$ are a solution to the equation.</p>

<p><strong>Example 2:</strong> The stochastic differential equation</p>

<p>$$dX_t = 1_{\mathbb{R} \backslash \{0\}}(X_t) \, dB_t, \qquad X_0 = 0$$</p>

<p>has more than one (strong) solution; just note that both $X_t^{(1)} := 0$ and $X_t^{(2)} := B_t$ solve the SDE.</p>
"
"2393957","2393959","<blockquote>
  <p>the definition of a logarithm is: $\;y=\log_a x\hspace{0.1cm}\Rightarrow\hspace{0.1cm} x=a^y$</p>
</blockquote>

<p>The real logarithm is only defined for $\,x \gt 0\,$.</p>

<blockquote>
  <p>Suppose we have: $\;(-8)=(-2)^3$. &nbsp;Does this mean it is equivalent to: $\,\frac{\log(-8)}{\log(-2)}=3$ ?</p>
</blockquote>

<p>Suppose we have $\,-1=(-1)^3\,$. Does this mean that $\,\frac{\log(-1)}{\log(-1)}=3\,$?</p>
"
"2393969","2394435","<p>The cohomology ring $H^*(\mathbb{C}P^n;R)\cong R[x_2]/(x_2^{n+1})$ can be calculated for any coeffient ring $R$ using the Gysin sequence of the fibration $S^1\hookrightarrow S^{2n+1}\rightarrow \mathbb{C}P^{n}$. In particular it is a truncated polynomial ring on a single degree 2 class $x_2\in H^2(\mathbb{C}P^n;R)$. Now use the fact that for any non-negative integer $r$ and any space $X$, $Sq^r$ acts on $H^r(X;\mathbb{Z}_2)$ as the squaring operation $x\mapsto x^2$. That is</p>

<p>$Sq^rx=x^2,\quad x\in H^r(X;\mathbb{Z}_2)$.</p>

<p>Now on the complex projective plane $\mathbb{C}P^1\cong S^2$ we obviously have</p>

<p>$Sq^2x_2=0$</p>

<p>since it is homeomorphic to $S^2$. For $n\geq 2$ we have that</p>

<p>$Sq^2x_2=x^2$</p>

<p>generates $H^4(\mathbb{C}P^n;\mathbb{Z}_2)\cong \mathbb{Z}_2$. This calculates the action of $Sq^2$ on $x_2$. To proceed from here we use the fact that the total Steenrod square $Sq=\sum_{r\geq 0}Sq^r=1+Sq^1+Sq^2+\dots$ is an endomorphism of the graded ring $H^*(X;\mathbb{Z}_2)=\oplus_{r\geq}H^r(X;\mathbb{Z}_2)$, together with the facts that $Sq^0x=x$, and $Sq^rx=0$ if $|x|&lt;r$ and the observation that $H^(\mathbb{C}P^n;\mathbb{Z}_2)$ vanishes in odd degrees to get, on the one hand</p>

<p>$Sq(x^k)=(Sq\,x)^k=(x+x^2)^k=\sum_{i}\binom{k}{i}(x^2)^{i}x^{k-i}=\sum_i\binom{k}{i}x^{k+i}$,</p>

<p>recalling that the binomial coefficients are calculated mod 2, and on the other</p>

<p>$Sq(x^k)=x^k+Sq^2x^k+\dots Sq^rx^k+\dots=\sum_iSq^{2i}x^k$.</p>

<p>Now match up the degree of the elements in each expression to get</p>

<p>$Sq^{2r}x^k=\binom{k}{r}x^{k+r}$.</p>

<p>In particular on $\mathbb{C}P^n$ we have</p>

<p>$Sq^2x^k=\binom{k}{1}x^{k+1}=k\cdot x^{k+1}$</p>

<p>as long as $k&lt;n$ and $Sq^2x^n=0$ for degree reasons.</p>
"
"2393970","2394186","<p>One method is to normalise your two parameters so they give answers between 0 and 1 and then apply a weighting to each when combining them. Thus $C_N$ and $T_N$ are the normalised response time and with $C_{max}$ the maximum possible score and $T_{min}$ the minimum possible response time.</p>

<p>Therefore $C_N=C/C_{max}$ and $T_N=T_{min}/T$</p>

<p>You can now attach the weighting e.g. $0.75*C_N+0.25*T_N$ . If you use percentage weightings that add up to 1 (100%) then the output from the formula will always be a number between 0 and 1 which can then be scaled and rounded appropriately.</p>
"
"2393975","2393982","<p>The third ""equation"" is in fact an algebraic identity, valid for $\,\forall x,y,z\,$:</p>

<p>$$\require{cancel}
\begin {align}
1 &amp;=(1-x)(1-y)(1-z)+x+(1-x)y+(1-x)(1-y)z \\
 &amp;= (1-x)(1-y) - \cancel{(1-x)(1-y)z} + x + (1-x)y + \cancel{(1-x)(1-y)z} \\
 &amp;= (1- \cancel{x} - \bcancel{y} + \xcancel{xy}) + \cancel{x} + (\bcancel{y} - \xcancel{xy}) \\
 &amp;= 1
\end{align}
$$</p>

<p>This leaves you with two actual equations in three unknowns.</p>
"
"2393980","2394031","<p>I think your conclusion is right. I've written a proof, please help me check if it's right.</p>

<p>Since ""local maximum values can only be countable"", we assume they are $\{a_n\}_n$. And let $F_n=\{f=a_n\}$. Then $\mathbb{R}=\bigcup_{n\geq1}F_n$.</p>

<p>Due to <strong>Baire's theorem</strong>, there is a $n_0$ such that $F_{n_0}$ is dense in an open interval (expressed as $U$).</p>

<p>Because $\{f=a_{n_0}\}$ is dense in $U$, it's easy to prove that $f(x)\geq a_{n_0}$ in $U$.</p>

<p>Assume that $x_0\in\{f=a_{n_0}\}$ is not an interior point of $\{f=a_{n_0}\}$ in $U$. In other words, $ \exists\{x_n\}_n\bigcap\{f=a_{n_0}\}=\emptyset$ such that $x_n\to x_0$. However, it can't be correct because $x_0$ is a local maximum.</p>

<p>Then we know $\{f=a_{n_0}\}$ has an interior point $x_0$ and we arrive at your conclusion. What's more, since $x_0$ is arbitrary, we know that $F_{n_0}\cap U$ is open too.</p>
"
"2393985","2393995","<p>You can download the related books from here.<a href=""http://b-ok.org/s/?q=toeplitz+operator&amp;yearFrom=&amp;yearTo=&amp;language=&amp;extension=&amp;t=0"" rel=""nofollow noreferrer"">http://b-ok.org/s/?q=toeplitz+operator&amp;yearFrom=&amp;yearTo=&amp;language=&amp;extension=&amp;t=0</a></p>

<p>I hope this link is useful.</p>
"
"2394001","2394003","<p>Because there might be a relations between the elements of the generating set that may not be preserved by the $p$ as defined. For example if $\{m_i\}$ is a generating set for $M$ and $r_i \in R$ (finitely many non-zero) is such that $\Sigma r_i m_i = 0$. Then we would expect $p(\Sigma r_i m_i)=\Sigma r_i p(m_i) = \Sigma r_i b_i$ to be zero which may not necessarily hold in $B$. Jyrki posted a nice counterexample in the comments</p>
"
"2394012","2394018","<p>Assuming that you're dealing with finite dimensional matrices, so there's no issue with convergence, the order of summation does not matter.  There is no special reason, apart from personal preference, for writing $\sum_{i,j}$ before $\sum_k$.  In other words, 
\begin{equation*}
\sum_{i,j} \sum_k W_{ik}H_{kj} = \sum_k \sum_{ij} W_{ik}H_{kj}.
\end{equation*}
Strictly speaking, writing $\sum_{i,j}\sum_k$ means you sum over $k$ first.  Here's an example of what it would look like when you write out the terms:
\begin{equation*}
\sum_{i,j=1}^n \sum_{k=1}^n W_{ik}H_{kj} = W_{11}H_{11} + W_{12}H_{21} + W_{13}H_{31} + \cdots.
\end{equation*}
Note that in the terms I wrote out, I am keeping $i$ and $j$ equal to $1$ and increasing $k$.  After I finished writing out the $k$ terms for $i=1$ and $j=1$, I'd keep $i=1$, change to $j=2$, and write out the $k$ terms again, and so on.  Writing the sums separately is no different mathematically than writing $\sum_{i,j,k}$.  As I wrote above, mathematically the order in which you sum over the indices in the example you gave does not matter.  I hope this helps.</p>
"
"2394017","2394021","<p>From the second equation, since $x^2+y^2 +1 &gt; 0$, we have $y=0$ </p>

<p>Substitute $y=0$ into the first equation, we end up with </p>

<p>$$4x(x^2-1)=0$$</p>

<p>Now we can solve for $x$.</p>
"
"2394024","2394034","<p>The author means that $N_R$ is the smallest normal subgroup such that elements of $R$ are identified with the identity <strong>when you quotient by $N_R$</strong>. </p>

<p>This is no different from saying $N_R$ is the smallest normal subgroup which contains $R$.</p>

<p>Concerning you're last question, I think you have the right idea. When we impose the relation that some expression is equal to the identity, this just means that we are taking the quotient by the smallest normal subgroup containing that expression.</p>
"
"2394028","2394061","<p>Here is a simple trick to boost the convergence: For <em>all</em> $x \in \mathbb{R}$ we have</p>

<p>$$ \arctan x = 2\arctan \left( \frac{x}{\sqrt{1+x^2}+1}\right) = 2\sum_{n=0}^{\infty} \frac{(-1)^n}{2n+1} \left(\frac{x}{\sqrt{1+x^2}+1}\right)^{2n+1}. $$</p>

<p>Of course you can mix this with other strategies to get even better convergence.</p>

<hr>

<p>Alternatively, here is an efficient series expansion found by Euler:</p>

<p>$$ \arctan x = \frac{x}{x^2+1} \sum_{n=0}^{\infty} \prod_{k=1}^{n} \left( \frac{2k}{2k+1} \frac{x^2}{x^2+1} \right) $$</p>

<p>Since there is no sign change, this may be better suited for controlling numerical error.</p>
"
"2394032","2394484","<p>Suppose $f$ has the required property and let $G_f$ be the directed graph with vertices $a_1, a_2, a_3, a_4$ and edge $a_i \to a_j$ whenever $f(a_i) = a_j$.  Each vertex of $G_f$ has outdegree 1. I make the following claims, all of which I think are simple:</p>

<ol>
<li>$G_f$ must have at least one cycle, say $C$</li>
<li>$G_f$ must be connected, and it has only one component</li>
<li>The cycle $C$ is the only cycle in $G_f$, since every vertex must be in the same component as $a_1$.  </li>
<li>The cycle $C$ is a <em>sink</em>: for each $x$, $f^k(x)$ is in $C$ for all sufficiently large $k$</li>
<li>$a_1\in C$.</li>
<li>No other vertex is in $C$.  For if $a_1 \ne f(a_1)$, then $f^k(a_1)\ne f^k(f(a_1))$ for every $k$.</li>
</ol>

<p>So we have $a_1 = f(a_1)$.  There are now only 4 topologies for $G_f$, and we can easily enumerate them by considering how many elements have $f(x) = a_1$:</p>

<p><a href=""https://i.stack.imgur.com/Oyna6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Oyna6.png"" alt=""$f(x)=a_1$ for all three $x$""></a> Only one function looks like this.</p>

<p><a href=""https://i.stack.imgur.com/KT2Vs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KT2Vs.png"" alt=""enter image description here""></a> Six functions look like this. </p>

<p><a href=""https://i.stack.imgur.com/mrQUw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mrQUw.png"" alt=""enter image description here""></a> Six functions look like this.</p>

<p><a href=""https://i.stack.imgur.com/OQMgD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OQMgD.png"" alt=""enter image description here""></a> Three functions look like this.</p>

<p>The total is 16.</p>
"
"2394033","2394062","<p>$$
S=1+y+y^2+\cdots+y^{n-1}
$$
is a geometric series; multiply $S$ by $y$ to get
$$
Sy=y+y^2+\cdots+y^{n-1}+y^n
$$
Subtract $Sy$ from $S$ to get
$$
Sy-S=y^{n}-1    
$$
Therefore
$$
y^n-1=(y-1)(1+y+y^2+\cdots+y^{n-1})
$$
Put $y=\frac{x+h}{x}$ (provided $x\not=0$) to get
$$
\frac{(x+h)^n}{x^n}-1=\left(\frac{x+h}{x}-1\right)
\left(
1+\frac{x+h}{x}+\frac{(x+h)^2}{x^2}+\cdots+\frac{(x+h)^{n-1}}{x^{n-1}}
\right)\tag{*}
$$
Multiply both sides by $x^n$ to get
$$
(x+h)^n-x^n=h(x^{n-1}+(x+h)x^{n-2}+(x+h)^2x^{n-3}+\cdots+(x+h)^{n-1})
$$</p>

<p>We wish to find the derivative of the function $f(x)=x^n$ from first principle. If $x=0$, then
$$
f'(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{h}=\lim_{h\to0}\frac{h^n}{h}=\lim_{h\to0}h^{n-1}=0
$$
If $x\not=0$, then
$$
\begin{align}
f'(x)&amp;=\lim_{h\to0}\frac{f(x+h)-f(x)}{h}\\
&amp;=\lim_{h\to0}\frac{(x+h)^n-x^n}{h}\\
&amp;=\lim_{h\to0}\frac{h(x^{n-1}+(x+h)x^{n-2}+(x+h)^2x^{n-3}+\cdots+(x+h)^{n-1})}{h}\\
&amp;=\lim_{h\to0}(x^{n-1}+(x+h)x^{n-2}+(x+h)^2x^{n-3}+\cdots+(x+h)^{n-1})\\
&amp;=x^{n-1}+xx^{n-2}+x^2x^{n-3}+\cdots+x^{n-1}\\
&amp;=\underbrace{x^{n-1}+x^{n-1}+x^{n-1}+\cdots+x^{n-1}}_{n\text{ copies}}\\
&amp;=nx^{n-1}
\end{align}
$$</p>
"
"2394045","2394078","<p>After doing a quick Google search I came across this blog post: <a href=""https://cuhkmath.wordpress.com/2015/08/14/mean-value-theorems-for-harmonic-functions-on-riemannian-manifolds/"" rel=""nofollow noreferrer"">https://cuhkmath.wordpress.com/2015/08/14/mean-value-theorems-for-harmonic-functions-on-riemannian-manifolds/</a></p>

<p>The main points of the article are as follows:</p>

<ul>
<li><p>The mean value property of harmonic functions holds on an arbitrary manifold $M$ only when for every point $p\in M$ every geodesic sphere near $p$ has constant mean curvature. Such manifolds are called <em>harmonic manifolds</em>. This coincides with the intuition I had in my comments above.</p></li>
<li><p>For any smooth function $u$ on an $n$-manifold $(M,g)$ it holds that $$\frac{1}{\mathrm{vol}(\partial B_r(p)}\int_{\partial B_r(p)}u(y)~d\sigma(y)=u(p)+\frac{\Delta u(p)}{2n}r^2 + \mathscr{O}(r^4).$$
The $\mathscr{O}(r^4)$ term is given by $B(n)r^4 + \mathscr{O}(r^6)$, where $$B(n):=\frac{1}{24(n+2)}\left(3\Delta^2u - 2\langle\nabla^2 u,\rho\rangle-3\langle\nabla u,\nabla\tau\rangle + \frac{4}{n}\tau\Delta u\right).$$
In the above $\rho(x,y)=\mathrm{tr}\left(R(\cdot,x,y,\cdot)\right)$, $R$ is the Riemann curvature tensor, and $\tau=\mathrm{tr}(\rho)$.</p></li>
<li><p>We have a similar mean value property for geodesic balls. For any smooth function $u$ it holds that $$\frac{1}{\mathrm{vol}(B_r(p))}\int_{B_r(p)}u(y)~d\mu(y)=u(p) + \frac{\Delta u(p)}{2(n+2)}r^2 + B(n+2)r^4 + \mathscr{O}(r^6).$$</p></li>
<li><p>You have similar sub-mean value properties for subharmonic functions given bounds on the Ricci and Riemann curvatures.</p></li>
</ul>

<p>The blog provides plenty of references and proofs (they seem to be correct, but I haven't had the time to work through them yet).</p>
"
"2394060","2394067","<p>According to <a href=""http://jeff560.tripod.com/operation.html"" rel=""nofollow noreferrer"">Earliest Uses of Various Mathematical Symbols</a>:</p>

<blockquote>
  <p>The convention that multiplication precedes addition and subtraction was in use in the earliest books employing symbolic algebra in the 16th century. The convention that exponentiation precedes multiplication was used in the earliest books in which exponents appeared. </p>
</blockquote>
"
"2394075","2394081","<p>Here is an approach using power series expansions.</p>

<p>It is known that for $t\in (-1,1)$
$$\frac{1}{\sqrt{1-t}} =(1-t)^{-1/2}=\sum_{n=0}^\infty 
\binom{-1/2}{n}  (-t)^{n}=
\sum_{n=0}^\infty \frac{1 }{2^{2n}}\binom{2n}{n}  t^{n},$$
and for $t\in [-1,1]$,
$$\arcsin(t) =\int_0^t\frac{ds}{\sqrt{1-s^2}}= \sum_{n=0}^\infty \frac{1 }{2^{2n}}\binom{2n}{n} \frac{ t^{2n+1}}{2n+1}.$$
Hence, the given integral is equal to
\begin{align*}I&amp;=\sum_{n=0}^\infty \frac{1 }{2^{2n}}\binom{2n}{n} \frac{1}{2n+1}\int_0^{\pi/2}\int_0^{\pi/2} \sin x (\sin x \sin y)^{2n+1} \ dx \ dy\\
&amp;=\sum_{n=0}^\infty \frac{1 }{2^{2n}}\binom{2n}{n} \frac{1}{2n+1}W_{2n+2} W_{2n+1} \\
&amp;=\sum_{n=0}^\infty \frac{1 }{2^{2n}}\binom{2n}{n} \frac{1}{2n+1}\cdot \frac{\pi}{4(n+1)}\\
&amp;=\frac{\pi}{4}\left(2\sum_{n=0}^\infty \frac{1 }{2^{2n}}\binom{2n}{n} \frac{1}{2n+1}-\sum_{n=0}^\infty \frac{1 }{2^{2n}}\binom{2n}{n} \frac{1}{n+1}\right)\\
&amp;=\frac{\pi}{4}\left(2\arcsin(1)-\int_0^1\frac{dt}{\sqrt{1-t}}\right)\\
&amp;=\frac{\pi(\pi-2)}{4}.
\end{align*}
where $W_r=\int_{0}^{\pi/2}\sin^r(t)dt $ denotes the <a href=""https://en.wikipedia.org/wiki/Wallis%27_integrals"" rel=""nofollow noreferrer"">Wallis integral</a> of order $r$.</p>
"
"2394082","2394147","<p>You can eliminate one of the dependent variables and obtain a single equation of the fourth order:</p>

<p>$$\theta=-0.1\arcsin \ddot s$$</p>

<p>$$\dot\theta=-0.1\dddot s(1-\ddot s^2)^{-1/2}$$</p>

<p>$$\ddot\theta=-0.1\ddddot s(1-\ddot s^2)^{-1/2}+0.1\ddot s\dddot s^2(1-\ddot s^2)^{-3/2}=15(1-0.01\ddot s^2)^{1/2}\frac{2s+1}{1+3s^2}.$$</p>

<p>This doesn't make the monster less BIG.</p>
"
"2394083","2394128","<p>You assumed the boxes were distinguishable, leading to $\frac{200!}{(10!)^{20}}$, ways to fill the boxes. If you make them indistinguishable, you merge the $20!$ ways of reordering the boxes into one, so that previous answer overcounts each way of filling indistinguishable boxes by a factor of $20!$. Therefore you are left with $\frac{200!}{(10!)^{20}}/20!$ ways to fill 20 indistinguishable boxes, which then must be an integer. After multiplying by $20$ it is of course still an integer.</p>
"
"2394088","2394089","<p>Yes, because by the triangle inequlity
$$ ||a|-|b||\leq |a-b|$$
and of course $|a|-|b|\leq ||a|-|b||$.</p>
"
"2394098","2394110","<p>Yes, this follows quite directly form the least upper bound property for $\mathbb R$.</p>

<p>However you're not guaranteed to have a sequence converging to the $\inf$ or $\sup$. You can for example take the set of $(t, -t)$ where $|t|\le 1$. The majorants of this set is $(x,y)$ such that $x\ge 1$ and $y\ge 1$. The least of these is $(1,1)$, but you can't make $(t_n, -t_n)\to (1,1)$ regardless of what sequence $t_n$ you choose.</p>

<p>Normally in general metric spaces one don't use the least upper bound property as much as one does in real analysis. The reason is partly the above failure, but also that it requires that the set is ordered which is normally not needed for what lub property is used for. Instead one uses the concept of completeness, the property that each Cauchy sequence is convergent.</p>
"
"2394106","2394161","<p>Maybe the Fourier transform?  Let $h_t(x)=\frac{f(x+t)-f(x)}{t}$, with $\widehat h_t(\xi)=\frac{e^{it\xi}-1}{t}\widehat f(\xi)$.  The condition $\|h_t\|_1\to 0$ implies $\|\widehat h_t\|_\infty\to 0$, which seems to imply $\widehat f(\xi)=0$, since $\widehat h_t(\xi)\to i\xi \widehat f(\xi)$.  By continuity, it would seem $\widehat f(0)=0$ as well.</p>
"
"2394107","2394942","<p>The count for successes when selecting without replacement would be a <strong>hypergeometric</strong> random variable rather than <strong>binomial</strong>, though for selection of a small enough sample from a large population this <em>approximates</em> selection with replacement (<em>if</em> removing small amount of successes does not significantly impact the proportion in the population). </p>

<p><em>However</em>, this is of no import for the expectation due to the <strong>Linearity of Expectation</strong></p>

<p>It seems counterintuitive the first time students encounter this, but expectation of a series of trials equals the series of expectations of each trial,  <em>without any influence at all</em> by whether the trials are independent or not.</p>

<p>If $X$ equals the count of successes in $n$ selections of jurors, and $X_k$ indicates a success on trial $k$-th selection, so $X=\sum_{k=1}^n X_k$, then the expectation for $X$ will be : $\mathsf E(X) ~{ =  \mathsf E(\sum_{k=1}^n X_k) \\ = \sum_{k=1}^n \mathsf E(X_k) \\ = n p}$ </p>

<p>Reguardless of how you model the selection.</p>

<hr>

<p>To get a idea of what is going on, consider the count in a sample of size two; whether selecting with or  without replacement, the expectation is:</p>

<p>$$\mathsf E(X) ~{= \mathsf E(X_1+X_2) \\ = \sum_{(x_1,x_2)\in \{0,1\}^2} (x_1+x_2)\mathsf P(X_1=x_1, X_2=x_2) \\ = \sum_{x_1=0}^1\sum_{x_2=0}^1 x_1\mathsf P(X_1=x_1, X_2=x_2) + \sum_{x_1=0}^1\sum_{x_2=0}^1 x_2\mathsf P(X_1=x_1, X_2=x_2) \\ \\ = \sum_{x_1=0}^1 x_1\sum_{x_2=0}^1 \mathsf P(X_1=x_1, X_2=x_2) + \sum_{x_2=0}^1 x_2\sum_{x_1=0}^1 \mathsf P(X_1=x_1, X_2=x_2) \\ = \sum_{x_1=0}^1 x_1\mathsf P(X_1=x_1)+ \sum_{x_2=0}^1 x_2\mathsf P(X_2=x_2) \\ = \mathsf E(X_1)+\mathsf E(X_2) \\ = 2p }$$</p>

<p>And so on for larger sample sizes. &nbsp; (Use a proof by induction if you wish.)</p>
"
"2394114","2397881","<p>I was unable to find any pre-existing operator list for 3D point groups, nor any standard for the choice of axis or order of operators. As such I ended up deriving the operators from their generators. The <a href=""https://github.com/sbliven/symmetrycad/blob/master/SymmetryCAD/point_groups.scad"" rel=""nofollow noreferrer"">OpenSCAD code</a> for the results should be easy to port to other languages if needed. Here's a summary:</p>

<h2>Chiral tetrahedral ($T$)</h2>

<p>Orientation: Given a cube oriented to the cartesian axes, we have 2-fold rotation axes through each face (i.e. along the axes) and 3-fold axes through each body diagonal.</p>

<p>Generators: 2-fold around X ($R_2$); 3-fold around (1,1,1) ($R_3$)</p>

<p>$$
R_2 =   \begin{pmatrix}
1&amp; 0&amp; 0&amp; 0\\
0&amp;-1&amp; 0&amp; 0\\
0&amp; 0&amp;-1&amp; 0\\
0&amp; 0&amp; 0&amp; 1
\end{pmatrix}
$$
$$
R_3 =    \begin{pmatrix}
0&amp; 0&amp; 1&amp; 0\\
1&amp; 0&amp; 0&amp; 0\\
0&amp; 1&amp; 0&amp; 0\\
0&amp; 0&amp; 0&amp; 1
\end{pmatrix}
$$</p>

<p>Operators (12):</p>

<p>$$
T = \{e,
R_2, R_3,
R_2 R_3, R_3 R_2, R_3 R_3,
R_2 R_3 R_3, R_2 R_3 R_2, R_3 R_2 R_3, R_3 R_3 R_2,
R_3 R_3 R_2 R_3, R_3 R_2 R_3 R_3 \}
$$</p>

<h2>Full tetrahedral ($T_d$)</h2>

<p>Orientation: Same as $T$, with 6 additional mirror planes containing opposing edges of the cube.</p>

<p>Generators: Same as $T$, plus a reflection across x=y ($M_{x=y}$)</p>

<p>$$
M_{x=y} = \begin{pmatrix}
0&amp; 1&amp; 0&amp; 0\\
1&amp; 0&amp; 0&amp; 0\\
0&amp; 0&amp; 1&amp; 0\\
0&amp; 0&amp; 0&amp; 1
\end{pmatrix}
$$</p>

<p>Operators (24):</p>

<p>All operators in $T$ plus mirrored versions
$$
T_d = T \cup \left\{r\cdot M_{x=y}:r\in T\right\}
$$</p>

<h2>Pyritohedral ($T_h$)</h2>

<p>Orientation: Same as $T$, with 6 additional mirror planes containing opposing edges of the cube.</p>

<p>Generators: Same as $T$, plus a reflection across xz ($M_{xz}$)
$$
M_{xz} = \begin{pmatrix}
1&amp; 0&amp; 0&amp; 0\\
0&amp;-1&amp; 0&amp; 0\\
0&amp; 0&amp; 1&amp; 0\\
0&amp; 0&amp; 0&amp; 1
\end{pmatrix}
$$</p>

<p>Operators (24):</p>

<p>All operators in $T$ plus mirrored versions
$$
T_h = T \cup \left\{r\cdot M_{xz}:r\in T\right\}
$$</p>

<h2>Chiral Octohedral ($O$)</h2>

<p>Orientation: Given a cube oriented to the cartesian axes, we have 4-fold rotation axes through each face (i.e. along the axes), 3-fold axes through each body diagonal, and 2-fold rotations through the edge midpoints</p>

<p>Generators: 4-fold around X ($R_4$); 3-fold around (1,1,1) ($R_3$)
$$
R_4 = \begin{pmatrix}
1&amp; 0&amp; 0&amp; 0\\
0&amp; 0&amp;-1&amp; 0\\
0&amp; 1&amp; 0&amp; 0\\
0&amp; 0&amp; 0&amp; 1
\end{pmatrix}
$$</p>

<p>Operators (24):</p>

<p>$$
O = \{e,
R_3, R_4,
R_3 R_3, R_3 R_4, R_4 R_3, R_4 R_4,
R_3 R_3 R_4, R_3 R_4 R_4, R_4 R_3 R_3, R_4 R_4 R_3, R_4 R_4 R_4,
R_3 R_3 R_4 R_4, R_3 R_4 R_3 R_3, R_3 R_4 R_4 R_3, R_3 R_4 R_4 R_4, R_4 R_3 R_3 R_4, R_4 R_4 R_3 R_3,
R_3 R_3 R_4 R_3 R_3, R_3 R_3 R_4 R_4 R_3, R_4 R_4 R_3 R_3 R_4, R_3 R_4 R_4 R_3 R_3, R_4 R_3 R_3 R_4 R_4,
R_3 R_4 R_4 R_3 R_3 R_4\}
$$</p>

<h2>Full octohedral ($O_h$)</h2>

<p>Orientation: Same as $O$, with additional mirror planes along the axes and containing opposing edges of the cube.</p>

<p>Generators: Same as $O$, plus a reflection across xz ($M_{xz}$)</p>

<p>Operators (48):</p>

<p>All operators in $O$ plus mirrored versions
$$
O_h = O \cup \left\{r\cdot M_{xz}:r\in O\right\}
$$</p>

<h2>Chiral icosahedral ($I$)</h2>

<p>Orientation: Orient an icosahedron such that each axis is along a 2-fold axis.</p>

<p>Generators: 2-fold around X ($R_2$), plus a 5-fold rotation around the $(\phi,0,1)$ axis ($R_5$).</p>

<p>$$
f = \frac{5-\sqrt{5}}{5+\sqrt{5}} \\
g = \frac{3+\sqrt{5}}{4} \\
\phi = \frac{1+\sqrt{5}}{2} \\
R_5 = \begin{pmatrix}
1-\frac{f}{2}&amp;         -\frac{\phi \sqrt{f}}{2}&amp; \frac{\phi f}{2}&amp;   0\\
\frac{\phi \sqrt{f}}{2}&amp; 1-f (\frac{1}{2}+g)&amp;    -g \sqrt{f}&amp;0\\
\frac{\phi f}{2}&amp;       g \sqrt{f}&amp;      1-f g&amp;     0\\
0&amp;0&amp;0&amp;1
\end{pmatrix}
$$</p>

<p>Operators (60):</p>

<p>Use the decomposition $I = T\times C_5$:</p>

<p>$$
I = \left\{R_5^i \cdot r:r\in T, i \in [0\ldots4] \right\}
$$</p>

<h2>Full icosahedral ($I_h$)</h2>

<p>Orientation: Same as $I$, with mirror planes</p>

<p>Generators: Same as $I$, plus XZ reflection ($M_{xz}$)</p>

<p>Operators (120):</p>

<p>$$
I_h = I \cup \left\{r \cdot M_{xz}:r\in I \right\}
$$</p>
"
"2394119","2394129","<p>Rewrite like this</p>

<p>$$x^2+y^2+z^2-2x-2y+2 =0$$</p>

<p>then</p>

<p>$$x^2-2x +1 +y^2-2y+1+z^2 =0$$</p>

<p>so</p>

<p>$$(x-1)^2+(y-1)^2+z^2 =0$$ </p>

<p>thus $x=y=1$ and $z=0$ so $x^3+y^3+z^3 = 2$.</p>
"
"2394145","2394308","<p>Yes, you can rewrite it with vectorization and Kronecker products:</p>

<p>$$\|{\bf ZXd - y}\|^2 \to \|{\bf M_1M_2}\text{vec}({\bf X}) - \text{vec}({\bf y})\|^2$$</p>

<p>where $$\cases{{\bf M_1} = \text{Multiplication from left by }{\bf Z}\\{\bf M_2} = \text{Multiplication from right by }{\bf d}}$$</p>

<p>Now let us construct these matrices</p>

<p>$$ {\bf AXB} \to ({\bf B}^T\otimes {\bf A})\text{vec}({\bf X})$$</p>

<p>Now for $\bf M_1$: $\cases{\bf A=Z\\\bf B=I} \to {\bf M_1} = ({\bf Z}^T\otimes {\bf I})$.</p>

<p>And for $\bf M_2$: $\cases{\bf A=I\\\bf B=d} \to {\bf M_2} = ({\bf I}^T\otimes {\bf d})$.</p>

<p>So our vectorized system becomes: $$\begin{align*}\min_{\text{vec}({\bf X})}&amp;\|{\bf M_1 M_2 } \text{vec}({\bf X})-\text{vec}({\bf y})\|^2= \\\min_{\text{vec}({\bf X})}&amp;\|\underset{\text{The }{\bf A}\text{ you are looking for}}{\underbrace{({\bf Z}^T\otimes {\bf I})({\bf I}^T\otimes {\bf d})}{ \text{vec}({\bf X})}}-\text{vec}({\bf y})\|^2\end{align*}$$</p>
"
"2394156","2394224","<p>Consider that $x\sin(x^4+y)$ is an odd function in $x$ and therefore that for any given $y\in [-1, 1]$, $$\int_{-\sqrt{1-y^2}}^{\sqrt{1-y^2}} x\sin(x^4+y)\,\mathrm{d}x = 0$$ which tells us that $$\iint_D x\sin(x^4+y)\,\mathrm{d}x\,\mathrm{d}y = \int_{-1}^1\int_{-\sqrt{1-y^2}}^{\sqrt{1-y^2}} x\sin(x^4+y)\,\mathrm{d}x\,\mathrm{d}y = 0$$ Therefore, we only need worry about the $1$. $$\iint_D 1\,\mathrm{d}x\,\mathrm{d}y = \pi$$</p>
"
"2394163","2394187","<p>$$\left(x-\frac{1}{x}\right)^{1/2}+\left(1-\frac{1}{x}\right)^{1/2} = x\iff \left(x-\frac{1}{x}\right)^{1/2} = x-\left(1-\frac{1}{x}\right)^{1/2}.$$ Squaring</p>

<p>$$x-\frac{1}{x} = x^2+1-\frac{1}{x}-2x\left(1-\frac{1}{x}\right)^{1/2}.$$ Simplifying</p>

<p>$$x = x^2+1-2x\left(1-\frac{1}{x}\right)^{1/2}.$$ That is</p>

<p>$$2x\left(1-\frac{1}{x}\right)^{1/2}=x^2-x+1.$$ Squaring again</p>

<p>$$4x^2\left(1-\frac{1}{x}\right)=x^4-2x^3+3x^2-2x+1.$$ Thus, we get</p>

<p>$$x^4-2x^3-x^2+2x+1=0.$$ That is</p>

<p>$$(x^2-x-1)^2=0.$$ This must be easy to solve.</p>
"
"2394180","2394182","<p>Suppose that we are working with real matrices, and we  can write $A=U^TDU$</p>

<p>Then $A^T=U^TD^T(U^T)^T=U^TDU=A$</p>
"
"2394181","2394222","<p>It's an upper triangular Toeplitz matrix. Probably ""Toeplitz"" is the first word to use in your search for further information. </p>

<p><a href=""https://math.stackexchange.com/questions/786108/explicit-formula-for-inverse-of-upper-triangular-toeplitz-matrix-inverse"">Here</a>, for instance, is a reference about inverting such matrices, and <a href=""https://link.springer.com/article/10.1007/s40574-015-0031-3"" rel=""nofollow noreferrer"">here</a> is an article about factorizing them.</p>
"
"2394192","2395349","<p>Note that the radius of convergence is $R$ and $R_1 &lt;R$. Thus $f$ is absolutely convergent when $|x-x_0|\le R_1$. In particular, when $|x-x_0| = R_1$, </p>

<p>$$\sum_{n=1}^\infty |a_n| R_1^n &lt;\infty \Rightarrow |a_n | R_1^n \to 0.$$</p>

<p>Thus there is $k_0$ so that $|a_n| \le R_1^{-n}$ for all $n\ge k_0$. Now for all $r&lt;R_1$ and $|x-x_0|\le r$, if $k \ge k_0$, </p>

<p>$$\begin{split}
\left| f(x) - \sum_{n=0}^k a_n (x-x_0)^n\right| &amp;= \left| \sum_{n=k+1}a_n (x-x_0)^n\right|\\
&amp;\le \sum_{n=k+1}^\infty |a_n| |x-x_0|^n \\
&amp;\le \sum_{n=k+1}^\infty R_1^{-n} r^n \\
&amp;= \left(\frac{r}{R_1}\right)^{k+1} \sum_{n=0}^\infty \left( \frac{r}{R_1}\right)^n \\
&amp;=\left(\frac{r}{R_1}\right)^{k+1} \frac{1}{1-(r/R_1)} \\
&amp;=\left(\frac{r}{R_1}\right)^{k+1} \frac{R_1}{R_1 -r}. 
\end{split}$$</p>
"
"2394194","2394200","<p>Hint. Note that the domain of integration is symmetric with respect to the line $y=0$, hence your integral is just
$$\iint_{T} [35xy^2 + 7e^x y^3] dx dy=\iint_{T} [35xy^2] dx dy=
2\cdot 35\int_{y=0}^1 y^2\left(\int_{x=y^2+1}^{2}x\, dx\right) dy$$
because $7e^x y^3$ is an odd function in the variable $y$ and $35xy^2$ is an even one.</p>
"
"2394216","2394309","<p>you answered in a comment: ""I thought there is something more specific in scalar meaning then just minimal dimension count.""</p>

<ol>
<li><p>""Scalars"" are usually understood as elements of a field (in the sense of rationality field where rational operations: addition, subtraction, multiplication, division (not by zero) can be performed (commutatively). So not in the sense of vector field).</p></li>
<li><p>""Dimension"" as algebraic concept is a property of a vector space over some field. You cannot talk about a dimension without specifing the field over which the space is defined.</p></li>
</ol>

<p>You say that a scalar is monodimensional. This should be restated: a real number can be thought of as a vector of a vector space over the reals and this vector space is monodimensional.
But this is a fact of algebra: every field can be though of as a monodimensional vector space over itself.
This pragmatically speaking means that every linear operaton on a monodimensional vector space are of the type ""multiplication of a vector by a scalar"": for whatever linear operator on the vector space there is a unique scalar that multiplicatively coincides with it.</p>

<p>As an example: in $\mathbb{R}^2$ thought simply as an abelian group where its elements can be commutatively added, it is possible to introduce a structure of vector space over the reals and in such a case it becomes a bidimensional vector space. Here you can find $\mathbb{R}$-linear operators that cannot be given by scalars (think of a general matrix), but only some of them are isomorphic to the field (that is, can be identified with the scalars, think of matrix like $aI$, where $a\in\mathbb{R}$ and $I$ is the identity matrix: $aIx=ax; (aI+bI)x=(a+b)x: aIbIx=abx$, where $x\in\mathbb{R}^2$). Here remember that by $\mathbb{R}$-linear operator is meant an operator that preserve the addition of vectors and the multiplication of a vector by a scalar: $A(x+y)=Ax+Ay$ and $Acx=cAx$</p>

<p>At the same time in the abelian additive group $\mathbb{R}^2$ can also be introduced the structure of vector space over a (rationality) field of operator on $\mathbb{R}^2$ that preserve the addition of vectors and the rotation of vectors: $B(x+y)=Bx+By$ and $B\Theta x=\Theta Bx$, where $\Theta=\begin{bmatrix}\cos \theta&amp;-\sin\theta\\\sin\theta&amp;\phantom-\cos \theta\end{bmatrix}$</p>

<p>You can easily prove that such operators are of the type $B=\begin{bmatrix}b_1&amp;-b_2\\b_2&amp;\phantom-b_1\end{bmatrix}=|B|\begin{bmatrix}\cos\beta&amp;-\sin\beta\\\sin\beta&amp;\phantom-\cos\beta\end{bmatrix}$ where $|B|=\sqrt{b_1^2+b_2^2}$ and $\beta=\arctan \frac{b_2}{b_1}$</p>

<p>In addition these operators form a rationality field: all rational operations can be performed (commutatively) giving always (with the exception of division by zero) and uniquely an operator in that same field. By multiplication it is meant the composition.</p>

<p>You now also see that such operators can be identified with the elements of $\mathbb{R}^2$: identifying the operator $B$ with the vector $b=(b_1, b_2)$, you can write $Bx=bx; (B+C)x=(b+c)x; BCx=bcx$. These formulae introduce the structure of field on $\mathbb{R}^2$: that is elements of $\mathbb{R}^2$ can be called scalars and they can be used in rational operations. In particular multiplying to elements of $\mathbb{R}^2$ means interpreting them as scalars, that is as operators that preserve additions and rotations, composing them, and reinterpreting the result as vector (analogously for the division).</p>

<p>You also notice that the multiplication by a scalar is preserved by such operators, because $B(Cx)=C(Bx)$. So they are linear operator on $\mathbb{R}^2$ over the new field.
So this new structure of vector space we are introducing in $\mathbb{R}^2$ over itself is monodimensional, even though scalars are identifiable with vectors (as happend with the real line) but these have two real components. </p>

<p>Usually this particular field (whose elements have 2 real components) representing only those $\mathbb{R}$-linear operators that preserve the rotations is called complex field and its elements complex numbers, and the special $\mathbb{R}$-linear operators they represent are called $\mathbb{C}$-linear operators.</p>

<hr>

<p><strong>EDIT:</strong> (answering your question in the comment section)</p>

<p>Scalars let define which, among the operators that preserve additions of the elements of the abelian group (what is going to be a vector space), are linear (that is, also preserve multiplication by scalars). So linearity depends on scalars. After that, scalars can be (field-homomorphically) identified with some linear operators: these are all those linear operators that commute with all the linear operators. On the other hand, an ""ordered set of components of a vector"" (meaning a numerical representation of the vector, or ""numerical vector"") gives under (in general non-linear) operator the vector itself: this operator is called ""coordinate system"" and is usually defined on a subset of the cartesian power of a field (usually the same field over which the vector space is defined, but it is not a need) into the vector space. A component of a vector happens to be a ""scalar"" only for a fortuitous case. More exactly, it is an element of the field on which the space is defined but NOT a scalar, that is, it does not represent a linear operator even though it is in the field, that is, it does  change while the scalar does not change under coordinate-system changes (I can give a proof here). So it should not be called scalar to avoid confusion even though it in the the same field over which the space is defined.</p>

<p>(do not say that a scalar has a dimension. Vector spaces have dimension that depends on the scalars. The set of scalars (a field) is always a monodimensional vector space over itself and a monodimensional subspace of whatever vector space over such a field. This is true even if scalar are ""complex"" entities not simple reals, made up of more than one ""component"". The fact that a vector, or in general an element of a set, has $n$ component does not mean that it is an element of a vector space of dimension $n$: in this reasoning no mention has been made of the field of scalar, so $n$ cannot be said a dimension. Moreover coordinate systems need not be injective: you can have more numerical vectors representing one and the same vector. You can have ""redundant"" components. Or you can have components that are not from the same set of the scalars of the vector space. In the example made of the vector space $\mathbb{R}^2$ over the complex scalars, a vector of it has $2$ real components where it is chosen a real cartesian coordinate system, or it has only one complex component if a complex cartesian coordinate system is chosen, or it can have more than $2$ real components in a real non injective (that is, ""redundant"") coordinate system, etc. etc.)</p>
"
"2394227","2394361","<p>Let $A \in M_n(\mathbb{C})$ be a matrix such that $0$ is a simple eigenvalue of $A$ (that is, the algebraic multiplicity of $0$ is one). Let $q \neq 0$ such that $Aq = 0$ and $p \neq 0$ such that $A^{*}p = 0$. Assume that $p \perp q$. Then 
$$q \in \operatorname{span} \{ p \}^{\perp} = \ker(A^{*})^{\perp} = \operatorname{Im}(A) $$</p>

<p>so write $q = Av$ for some $v \neq 0$. Then $A^2v = Aq = 0$ and $q,v$ are linearly independent so $\dim \ker(A^2) \geq 2$ which implies that the algebraic multiplicity of $0$ is $\geq 2$, a contradiction.</p>

<p>For your situation, apply the above to $A - \lambda I$ and conclude that $p,q$ can't be perpendicular.</p>
"
"2394230","2394401","<p>Let $y\in R $</p>

<p>we look for $x\in R $ such that</p>

<p>$$e^x-e^{-x}-2y=0$$
or
$$e^{2x}-2ye^x-1=0$$
put $t=e^x&gt;0$
then
$$t^2-2yt-1=0$$
hence
$$t=e^x=y+\sqrt {1+y^2} $$</p>

<p>and finally
$$x=\ln (y+\sqrt {y^2+1} )$$</p>
"
"2394232","2394257","<p>If the elements were real, then as a set $A$ would be a plane in $\mathbb R^3$. So there is a homeomorphism of $A$ to $\mathbb Q \times \mathbb Q$. Hence $A$ has the subspace topology of $\mathbb Q \times \mathbb Q$ in $\mathbb R^3$ which is the same as the subspace topology of $\mathbb Q \times \mathbb Q$ in $\mathbb R^2$ which has a product subspace topology. So the answer to your question is no, there is no $B$. </p>
"
"2394250","2395948","<p>It seems to me that your construction should work.</p>

<p>Let $\mathbf{x}_1,\ldots,\mathbf{x}_k$ be such that $f(\mathbf{x}_i) = T$ and $\mathbf{y}_1,\ldots,\mathbf{y}_l$ the remaining vectors such that $f(\mathbf{y}_j) = F$.  We will say that $\mathbf{x}_i = \langle x_{i1},\ldots,x_{in} \rangle$ and similarly for the $\mathbf{y}_j$'s.</p>

<p>To show to your formula realizes $f$, we need to show that it works on all the $\mathbf{x}_i$'s as well as the $\mathbf{y}_j$'s.  It's pretty clear that $\gamma_i(\mathbf{x}_i) = T$ and so $\alpha(\mathbf{x}_i) = T$ for any $i$, as is desired.</p>

<p>To prove correctness w.r.t the $\mathbf{y}_j$'s on the other hand does require monotonicity.  Fix $\mathbf{y}_j$ and let $\mathbf{x}_i$ be arbitrary.  By monotonicity, $\mathbf{x}_i \not\leq \mathbf{y}_j$ and so there is a $p$ such that $x_{ip} = T$ and $y_{jp} = F$.  $A_{p}$ must then appear as a conjunct in $\gamma_i$.  Thus, $\gamma_i(\mathbf{y}_j) = F$, and since $i$ was arbitrary, this holds for every disjunct in $\alpha$.  Thus, $\alpha(\mathbf{y}_j) = F$.</p>
"
"2394252","2394287","<p>To find the integral like this, insight of potential differentiation involved is necessary. In this case, the derivative of ln(x) is 1/x with the understanding of the chain rule where the factor x in the question is just another part of the result from applying the rule to ln(3x^2-1). </p>

<p>Let u = ln(3x^2-1), then du/dx = (1/3x^2-1)*6x </p>

<p>or dx = (3x^2-1)/(6x) du
â«[5x/(3x^2â1)]dx = â«[5x/(3x^2â1)][(3x^2â1)/6x]du = â«[5/6]du = 5u/6 </p>

<p>=(5/6)ln(3x^2-1)</p>

<p>ps. first use and answer on this site</p>
"
"2394261","2396491","<p>Using spherical coordinates, the radius $r$ should be $\frac{a}{2 \cos \phi}&lt;r&lt;a$ and indeed the angle $0&lt;\phi&lt;\pi/3$. So you get </p>

<p>$$
2 \pi \int_0^{\pi/3}\sin\phi\bigg[\frac{r^3}{3}\bigg]_{\frac{a}{2 \cos \phi}}^{a} \rm d \phi 
= 2 \pi \int_0^{\pi/3}\frac{a^3\sin\phi}{3} \rm d \phi -2 \pi \int_0^{\pi/3} \frac{a^3\sin\phi}{24 \cos^3 \phi} \rm d \phi \\
= 2 \pi \frac{a^3}{3} \bigg[ 1 - \cos\pi/3 -\frac{1}{16} ( \cos^{-2} \phi)_0^{\pi/3}  \bigg] \\
= 2 \pi \frac{a^3}{3} \bigg[ 8/16 -\frac{1}{16} (4-1 )\bigg]\\
= 2 \pi \frac{a^3}{3} \frac{5}{16} = \pi {a^3} \frac{5}{24}
$$</p>

<p>Compare this to the standard result (which uses cylindrical coordinates) which is (see my comment above):  </p>

<p>$$
V = \dfrac{\pi (a/2)^2}{3} (3a -a/2 ) = \dfrac{5 \pi a^3}{24} 
$$</p>

<p>Here you go!</p>
"
"2394269","2394272","<p>If you separate the real and imaginary parts of $\frac{1}{2}\frac{df}{dx}-\frac{1}{2i}\frac{df}{dy}=0$ (remembering that $f = u+iv$, where $u$ and $v$ are real valued), you get exactly the CR equations.</p>
"
"2394274","2394358","<p>The keypad has $13$ characters.  To form a code, we must form a sequence of length $4$ using those characters.  We have $13$ choices for each entry, so the number of such sequences is $13^4$, as you found.  </p>
"
"2394282","2394289","<p>So the current region is a little annoying to integrate over. However, note that $-D=\{(-x,-y):(x,y)\in D\}$ is disjoint from $D$ (except at the boundary, which won't affect the integral) and $D\cup (-D)=[-1,1]\times [-1,1]$. Let $R=[-1,1]\times [-1,1]$.</p>

<p>Then,
$$\frac{5}{2} \iint_{-D} x + xy f(x^2+y^2) \,dA
=\frac{5}{2} \iint_{D} -x + xy f(x^2+y^2) \,dA.
$$
Let this value be denoted by $I_{-D}$. Similarly let $I_D=\frac{5}{2} \iint_{D} x + xy f(x^2+y^2) \,dA$ and let $I_R=\frac{5}{2} \iint_{R} x + xy f(x^2+y^2) \,dA$. 
Then $I_D-I_{-D} = 5 \iint_D x \,dA.$</p>

<p>On the other hand, $I_D+I_{-D}=I_R$. 
As long as we can work out $I_R$, we can solve for $I_D$.</p>

<p>Now, because the region, $R$, is symmetric about the $x$-axis, replacing $y$ by $-y$ and integrating over the region will yield the same result. However when we substitute in $-y$, we get 
$$\frac{5}{2} \iint_D x -xy f(x^2+y^2) \,dA.$$
Then we have 
$$2I_R = \frac{5}{2} \iint_R x -xy f(x^2+y^2) \,dA + \frac{5}{2} \iint_R x +xy f(x^2+y^2) \,dA = 5 \iint_R x \,dA, $$
or
$$I_R = \frac{5}{2} \iint_R x \,dA = 0, $$
since $R$ is symmetric around the $y$-axis, so positive $x$ values cancel
with negative $x$ values.</p>

<p>Therefore $I_D+I_{-D}=0$, so $-I_D=I_{-D}$.</p>

<p>Together with $I_D-I_{-D} = 5\iint_D x\,dA$, we have $I_D=\frac{5}{2}\iint_D x\, dA$. </p>
"
"2394292","2394304","<p>As $z_k^6=-1$, you get
$$
z^6+1=z^6-z_k^6=(z-z_k)(z^5+z^4z_k+â¦+zz_k^4+z_k^5)
$$
as in the binomial theorems or geometric sums.</p>

<hr>

<p>Note that also
$$
z^6+1=(z^2+1)(z^4-z^2+1)=(z^2+1)((z^2+1)^2-3z^2)
\\
=(z^2+1)(z^2+\sqrt3z+1)(z^2-\sqrt3z+1)
$$
which allows to compute the integral using partial fraction decomposition and the arcus tangent.</p>
"
"2394295","2394529","<p>This is a classical variation on <a href=""https://en.wikipedia.org/wiki/Gauss_circle_problem"" rel=""nofollow noreferrer"">Gauss circle problem</a>. It is reasonable to expect that the number $N(R)$ of lattice points in the region $x^2+y^2+z^2+w^2\leq R^2$ is close to the volume of such region, namely $\frac{\pi^2}{2}R^4$. On the other hand, if we consider a unit hypercube centered at each lattice point, the union of such hypercubes belongs to the region $x^2+y^2+z^2+w^2\leq\left(R+1\right)^2$, hence</p>

<p>$$ N(R) = \frac{\pi^2}{2}R^4+O(R^3)\tag{1} $$
is simple to prove. To improve the error term is non-trivial: you may have a look at Voronoi's technique for improving the error term in the original Gauss circle problem.<br></p>

<p>The $2$-dimensional and $4$-dimensional cases share much more. By denoting as</p>

<p>$$ r_2(n)=\left|\left\{(a,b)\in\mathbb{Z}^2:a^2+b^2=n\right\}\right| $$
$$ r_4(n)=\left|\left\{(a,b,c,d)\in\mathbb{Z}^2:a^2+b^2+c^2+d^2=n\right\}\right| \tag{2}$$
explicit formulas for $r_2$ and $r_4$ can be derived from <a href=""https://en.wikipedia.org/wiki/Lambert_series"" rel=""nofollow noreferrer"">Lambert series</a> and the <a href=""https://en.wikipedia.org/wiki/Jacobi_triple_product"" rel=""nofollow noreferrer"">Jacobi triple product</a>.<br>
We have:
$$ r_2(n) = 4\left(\sum_{\substack{d\mid n \\ d\equiv 1\!\!\pmod{4}}}\!\!\!\!1-\sum_{\substack{d\mid n \\ d\equiv 3\!\!\pmod{4}}}\!\!\!\!1\right)=4(\chi_4*1)(n)\tag{3}$$</p>

<p>$$ r_4(n) = 8 \sum_{\substack{d\mid n\\ 4\nmid d}} d \tag{4} $$
and a simple proof of $(4)$ can be found <a href=""http://web.maths.unsw.edu.au/~mikeh/webpapers/paper18.pdf"" rel=""nofollow noreferrer"">here</a>. In explicit terms, we clearly have</p>

<p>$$ N(R) = 1+\sum_{n=1}^{R^2}r_4(n) = 1+8\sum_{n=1}^{R^2}\sum_{\substack{d\mid n\\ 4\nmid d}} d.\tag{5} $$</p>

<p>By denoting $\sigma(n)=\sum_{d\mid n}d$, <a href=""https://math.stackexchange.com/questions/2349132/power-summation-over-the-divisor-function/2349407#2349407"">the classical result</a> 
$$ \sum_{n\leq x}\sigma(n) = \frac{\pi^2}{12}x^2 +O(x) \tag{6}$$
leads to:
$$\begin{eqnarray*}N(R) &amp;=&amp; 8\left(\frac{\pi^2}{12}R^4+O(R^2)\right)-8\sum_{m=1}^{\lfloor R^2/4\rfloor}\sum_{\substack{d\mid 4m\\ 4\mid d}}d\\ &amp;=&amp;\frac{2\pi^2}{3}R^4+O(R^2)-32\sum_{m=1}^{\lfloor R^2/4\rfloor}\sigma(m)\\&amp;=&amp;\frac{\pi^2}{2}R^4+O(R^{\color{red}{2}})\tag{7}\end{eqnarray*}$$
improving $(1)$.</p>

<hr>

<p>Despite the idea of exploiting the explicit formula $(5)$ (which can be seen as an algebraic interplay between $S^3$ and the quaternion group $\mathbb{H}$) is quite natural, it looks like $(7)$ is an actual achievement. <a href=""https://www.jstor.org/stable/2003508?seq=1#page_scan_tab_contents"" rel=""nofollow noreferrer"">In the literature</a> I found the bound $N(R)=\frac{\pi^2}{2}R^4+O(R^2\log\log R)$ and Walfisz' statement about the optimality of $(7)$, but no proof of $(7)$ at all. I guess we have it now.</p>
"
"2394297","2394395","<p>Let $a = x^2, b = y^2, c=z^2$, where $x, y, z &gt; 0$. By C-S, we have 
$$ \sum_{cyc} \frac{x^3 z}{2 y^3 x + 3y^2z^2} \sum_{cyc} \frac{2yx + 3z^2}{zx} \geq \left( \sum_{cyc} \frac{x}{y} \right)^2.$$
But 
$$ \sum_{cyc} \frac{2yx + 3z^2}{zx} = 5 \sum_{cyc} \frac{x}{y}.$$
Combining them we get
$$ \sum_{cyc} \frac{x^3 z}{2 y^3 x + 3y^2z^2} \geq \frac{1}{5} \sum_{cyc} \frac{x}{y}.$$
It remains to apply AM-GM.</p>
"
"2394299","2394364","<p>Let us start $$y=A e^{-k t} \sin (t \omega )$$ $$y'=A e^{-k t} (\omega  \cos (t \omega )-k \sin (t \omega ))$$ $$y''=A e^{-k t} ((k^2-\omega^2 )  \sin (t \omega )-2 k \omega  \cos (t \omega ))$$ Replace in the equation $$my''+\lambda y'+\omega^2y=0$$ Forgetting the $A e^{-k t}$ common factor, you then have
$$\sin (t \omega ) \left(k^2 m-\lambda  k-(m-1) \omega ^2\right)+\omega  (\lambda -2    k m) \cos (t \omega )=0$$ SInce this has to be true for all $t$, you have $$\lambda -2    k m=0 \tag 1$$ $$k^2 m-\lambda  k-(m-1) \omega ^2=0 \tag 2$$ So, from $(1)$ $$k=\frac \lambda {2m}\tag 3$$ Plug in $(2)$ to get another condition between $\lambda$, $\omega$ and $m$.</p>
"
"2394313","2394326","<p>$$x&lt;\frac { 1 }{ x } \\ x-\frac { 1 }{ x } &lt;0\\ \frac { { x }^{ 2 }-1 }{ x } &lt;0\\ \frac { x\left( x-1 \right) \left( x+1 \right)  }{ { x }^{ 2 } } &lt;0\\ x\left( x-1 \right) \left( x+1 \right) &lt;0\\ x\in \left( -\infty ;-1 \right) \cup \left( 0;1 \right) $$</p>
"
"2394317","2394349","<p>With $|x|=y\sqrt{n}$, we have $$\frac{|x|}{(n+x^2)^2}=\frac1{n^{3/2}}\frac{y}{(1+y^2)^2}\le\frac{C}{n^{3/2}}$$ with some positive constant $C$, since $y/(1+y^2)^2$ is continuous for $y&gt;0$ and $\rightarrow0$ as $y\rightarrow\infty$, so it must be bounded. Thus the series is uniformly convergent on the whole real line.</p>
"
"2394328","2394333","<p>The key is that the infinite sum $\sum x^n $ converges to $\frac 1{1 - x}$ under certain conditions on $x$, and differentiating the resulting inequality gives that $\sum nx^{n-1}$ is convergent to the derivative of $\frac 1{1 - x}$, under the same conditions. Multiplying this by $x$ gives the sum $\sum nx^{n}$, which is the result you are looking for with $x = \frac 12$, which does fall under the set for which the first equality holds.</p>
"
"2394334","2394356","<p>Your observations are correct. You can construct a sequence in $A\subset C$ that is converging to $z$ in the following way. Take $b^i \in A$ with $b_i^i \leq z_i+1/m$ and you get a set $\{b^1,\ldots,b^n\}\subset A$. Now take the minimum of this set with respect to your ordering and call this element $x_m$. In this way you get a sequence $(x_m)_{m\in \mathbb{N}}$ and this sequence converges to $z$ since </p>

<p>$$\Vert x_m-z\Vert\leq \sqrt{n\cdot(1/m)^2}=\sqrt{n}\cdot\dfrac{1}{m}$$ </p>

<p>which goes to zero as $m$ goes to infinity. Since $C$ is compact you have that $z \in C$.</p>

<p>Note that you don't actually need that $C$ is compact you only need it to be closed nor to you need convexity.</p>
"
"2394337","2394447","<p>For now, I'll assume all the labels are the same, so you have a vector of length $n$ and want to cycle through all $n!$ permutations.</p>

<p>It is best explained by example. Suppose your current permutation is</p>

<p>$$(3,2,6,5,4,1)$$</p>

<ul>
<li>Start from the right, the end of the vector, in this case a $1$. </li>
<li>While the element to the left is bigger than the current element, move to the left. In this case you move to the left to the $4$, the $5$, and then the $6$. At that point you stop because the next element $2$ is smaller than $6$. In this way you have identified a decreasing tail $6,5,4,1$ and the next element, a $2$, that does not belong to it.</li>
<li>Now you have to swap the $2$ with the next-higher element that lies somewhere to its right. This is simple to find, because we know all those elements in this tail are in decreasing order. In this case the next-higher element is the $4$. After the swap we get the intermediate result, $(3,4,6,5,2,1)$.</li>
<li>As a final step, reverse the order of the tail, i.e. of all the elements to the right of the one you just swapped. The result is:</li>
</ul>

<p>$$(3,4,1,2,5,6)$$</p>

<p>That's it. If you keep applying this procedure, you will generate the permutations in lexicographical order. If you repeat the procedure again, you get:</p>

<p>$$(3,4,1,2,6,5)$$</p>

<p>Note that now we did a swap, but the tail was of length $1$ so there was nothing to do in the final step. Applying the procedure again, we get the intermediate result $(3,4,1,5,6,2)$ and finish with the next permutation:</p>

<p>$$(3,4,1,5,2,6)$$</p>

<p>The next permutations are then</p>

<p>$$(3,4,1,5,6,2)\\(3,4,1,6,2,5)\\(3,4,1,6,5,2)\\(3,4,2,1,5,6)\\(3,4,2,1,6,5)\\(3,4,2,5,1,6)\\(3,4,2,5,6,1)\\(3,4,2,6,1,5)$$</p>

<p>If in the second step you find that whole permutation is decreasing, i.e. is just a tail with no element to its left, then you have reached the end. You can however let it ""overflow"" by reversing the tail to start again at the first permutation.</p>

<hr>

<p>You can easily extend this to deal with multiple labels. Apply the procedure to the last set of elements with the same label. If that worked normally, you are done. However if the permutation overflowed, then you have to ""transfer a carry"" to the next set, i.e. also apply the procedure to the next label's set of elements.</p>
"
"2394339","2394344","<p>Without loss of generality let the vertices be $0,1,2,\dots,n-1$, and assume $0$ is one of the vertices. We need to select the other $k-1$ vertices. This can be done by selecting positive integers $a_1,a_2,\dots, a_{k-1}$ such that $a_i\geq 2$ and $a_1+a_2+\dots+a_{k-1}\leq n-2$ and selecting the vertices of the form $a_1+a_2+\dots+a_i$.</p>

<p>The number of ways to do this is equal to the number of solutions in integers greater than $2$ to $a_1+a_2+\dots+a_{k-1}\leq n-2$. This is clearly the number of solutions to $a_1+a_2+\dots+a_k=n$ with each $a_i\geq 2$. This is equal to the number of solutions to $a_1+a_2+\dots+a_k=n-2k$ with $a_i\geq 0$. By star and bars there are $\binom{n-k-1}{k-1}$ solutions to this.</p>

<p>Hence the final answer is $\frac{\binom{n-k-1}{k-1}}{\binom{n-1}{k-1}}$</p>
"
"2394343","2394360","<p>Note that
\begin{align*}
\int_{x=0}^1\int_{y=0}^x \ln(x^2+y^2)\,dxdy&amp;=2\int_{x=0}^1\int_{y=0}^x \ln(x)\,dxdy+
\int_{x=0}^1\int_{y=0}^x \ln(1+(y/x)^2)\,dxdy\\
&amp;=2\int_0^1\ln(x)\left(\int_0^x dy\right)\,dx+\int_{x=0}^1x\left(\int_{t=0}^1 \ln(1+t^2)\,dt\right) dx\\
&amp;=2\int_0^1x\ln(x) \,dx+\frac{1}{2}\int_{0}^1 \ln(1+t^2)\,dt\\
&amp;=\left[x^2\ln(x)-\frac{x^2}{2}\right]_0^1+\frac{1}{2}\left[t\ln(1+t^2)-2t+2\arctan(t)\right]_0^1\\
&amp;=-\frac{3}{2}+\frac{\ln(2)}{2}+\frac{\pi}{4}.
\end{align*}</p>
"
"2394345","2394375","<p>There's no way to prove it, since the hypothesis can be true, but the conclusion false.
<p>
For example, suppose $P(x),Q(x)$ are always false. </p>
"
"2394347","2394351","<p>Your reasonings for a) and b) are correct.</p>
"
"2394392","2394405","<p>Let $A=\{1,2,3,\cdots,n\}$ WLOG.</p>

<p>Consider $(1,1), (2,2), (3,3), \cdots, (n,n)$. The relation can include any of these without making it antisymmetric or symmetric. There are $n$ of these pairs.</p>

<p>Consider $(a,b)$ and $(b,a)$ for $a \ne b$. If a relation is symmetric, then either both of them are included, or none of them is included, which gives $2$ possibilities. If a relation is anti-symmetric, then either none of them is included, or one of them is included, which gives $3$ possibilities. There are $\dfrac12(n^2-n)$ pairs of pairs of this kind.</p>

<p>The number of symmetric relations is $2^n 2^{\frac 1 2 (n^2-n)}$, while the number of anti-symmetric relations is $2^n 3^{\frac 1 2 (n^2-n)}$, and the number of symmetric and anti-symmetric relations is $2^{n^2}$.</p>

<p>Hence, the number of relations that are neither symmetric nor anti-symmetric is $2^{n^2} - 2^n \left[ 2^{\frac12(n^2-n)} + 3^{\frac12(n^2-n)} - 1 \right]$.</p>

<p>So, the required probability is $\dfrac{2^{n^2} - 2^n \left[ 2^{\frac12(n^2-n)} + 3^{\frac12(n^2-n)} - 1 \right]}{2^{n^2}}$.</p>

<p>Note that the probability $\to 1$ as $n \to \infty$, so if $A$ is an infinite set, you could say that the probability is $1$.</p>
"
"2394402","2397546","<p>If P in 3P44 stands for a base 5 digit, then there are only five possibilities for P: 0, 1, 2, 3, 4. Let's say P is 0. Then 3P44 is 399 in decimal, which is clearly greater than 246. Any of the other allowable values for P give numbers larger than 246 still.</p>

<p>For what it's worth, 399 decimal in base 13 is 249. I've also thought about negative and balanced bases.</p>
"
"2394407","2394426","<p>You seem to be looking for the following identity which can be found in Hiriart-Urruty and LemarÃ©chal's book ""Fundamentals of Convex Analysis,"" ex. 5.2.6:</p>

<p>$$
N_C(z) = C^\circ \cap \{z\}^\perp,
$$</p>

<p>where $C^\circ$ is the polar of $C$. Can you take it from there?</p>
"
"2394408","2394470","<p>Yes, Eve can find $b$.  Let's make sure we're on the same page, though, by clearly declaring the problem.</p>

<p>Alice and Bob have publicly declared the prime $p$ and a primitive element (generator) $g$ for the multiplicative group $\mathbb{Z}_p^*$.  Alice chooses an integer power $a$ in secret, and Bob does the same choosing $b$.  Alice computes $A= g^a \mod p$ and Bob computes $B= g^b \mod p$, and they both exchange the results of their computations (but not $a$ or $b$).  Eve the eavesdropper sees all of their transmissions.</p>

<p>The point is that--by Euler's Theorem--<strong>their choices of exponents only matter</strong> mod $\varphi(p)$.  But $\varphi(p)=p-1$, and your problem tells you something very strong about this.  Eve knows $b \mod r$ and $b \mod s$, and $r$ and $s$ are distinct primes, so Eve can use any simple implementation of the Chinese Remainder Theorem and recover $b \mod 2rs$.  Since this is also $b \mod p-1$, Eve has recovered $b$ and hence the secret key.</p>

<p>In this attack, she may not recover Bob's $b \in \mathbb{Z}$, but by my remarks above it doesn't matter.  Getting $b$ right up to multiples of $p-1$ is good enough. </p>
"
"2394409","2394434","<p>Let $k$ be a field, and let $R=k[x,y]$.
<p>
Let $M$ be the set of monomials $m \in R\;$such that  $\deg(m,x) &lt; \deg(m,y)$.
<p>
Define subrings $A,B$ of $R$ by
$$A = k[M]$$
$$B = A[x]$$
Then $B\;$is finitely generated over $A$.
<p>
Also, $B=R$, so $B\;$is noetherian.
<p>
But $A\;$is not noetherian, since the ideal of $A\;$generated by $M$ is not finitely generated.</p>
"
"2394414","2394418","<p>I would probably call this ""rounding to the nearest (integer) multiple of $n$.""  That is, if $n$ is some number (probably I would want it to be a positive integer, or perhaps the reciprocal of an integer, but there is no reason that it should be---any positive real number could work), then we can round a real number $x$ to the nearest multiple of $n$ as follows:</p>

<ol>
<li>Set $x_n$ to be $x$ modulo $n$.  That is, take $m$ to be the largest integer such that $mn \le x$, then set $x_n = x - mn$.</li>
<li>If $x_n &lt; n/2$, round $x$ down to $mn$.  If $x_n &gt; n/2$, round $x$ up to $(m+1)n$.  If $x_n = n/2$, round up or down according to some rule (always round up, always round down, banker's rounding, flip a coin, etc).</li>
</ol>
"
"2394421","2394425","<p>Yes, that is right. Your argument is correct.</p>
"
"2394423","2396908","<p>Refering to thread linked the definition should be
\begin{equation}
W_{\text{loc}}^{k,p}(U)=\{u:U\longrightarrow\mathbb{R}:u\in W^{k,p}(V)\text{ for all }V\subset\subset U\}.
\end{equation}</p>
"
"2394428","2394436","<p>Hint: if $\lim a_n = 0$ then there exists $N$ such that for all $n\ge N$, $|a_n| &lt; 1/2$. Also if $\text{card} (A) &lt; 1/2$, then $A = \varnothing$.</p>
"
"2394431","2394466","<p><strong>Hint:</strong> note that for $\varepsilon &gt; 0$:</p>

<p>$$ | (1+\varepsilon)^x |\leq (1+\varepsilon)^{\pi/2} &lt; (1+\varepsilon)^2.$$ </p>
"
"2394444","2394944","<p>Because both $(w_1,\dots ,w_k)$ and $(u_1,\dots ,u_m)$ are bases of same subspace $V\cap W$, each basis must be linearly independent.</p>

<p>Hence there is a unique set of scalars $c,d$ for each basis to represent a linear combination of any vector in the subspace. Let this vector be $x$.</p>

<p>$$c_1w_1+\dots +c_kw_k=x=d_1u_1+\dots +d_mu_m$$</p>

<p>The set of scalars $d$ could be, but is not constrained to, the set of scalars $a$. Again using the unique representation of vectors,</p>

<p>$$d_1u_1+\dots +d_mu_m=-a_1u_1-\dots -a_mu_m-b_1v_1-\dots -b_jv_j$$</p>

<p>(The linear combination of vectors $u\in V\cap W$ and $v\in V$, is in subspace $V\cap W$, by analogy to the similar proof for vector $w\in W$ being in subspace $V\cap W$.)</p>
"
"2394445","2394478","<p>I think you're misunderstanding the problem.  The problem is not to show that functions $\{\mathbf{u}(\mathbf{x},t), P(\mathbf{x},t)\}$ exist which satisfy the Navier-Stokes equations;  that much is relatively easy.  The problem is to show that given <strong>any</strong> function $\mathbf{u}^\circ(\mathbf{x})$, there exists functions $\{\mathbf{u}(\mathbf{x},t), P(\mathbf{x},t)\}$ that satisfy the Navier-Stokes equations and satisfy
$$
\mathbf{u}(\mathbf{x},0) = \mathbf{u}^\circ(\mathbf{x}).
$$
This function $\mathbf{u}^\circ(\mathbf{x})$ is called the <em>initial data</em>;  it basically tells you what the system is doing at the moment you start your stopwatch.</p>

<p>For your function, you have $\mathbf{u}(\mathbf{x},0) = (1,1,1)$.  I haven't gone carefully through your math, but even if this were a solution of the Navier-Stokes equations, it would only be a drop in the bucket.  You'd then have to find a solution with $\mathbf{u}(\mathbf{x},0) = (e^{-(x^2+y^2 + z^2)}, 0, 0)$.  And then $\mathbf{u}(\mathbf{x},0) = e^{-(x^2+y^2 + z^2)}(\sin(z^2+x), x^2, \tanh(y))$.  And then <em>every other conceivable function</em> that I could give you for the initial data.  </p>

<p>Obviously, it's impossible to write down a solution for every conceivable function, for the simple reason that there are infinitely many such functions and you don't have an infinite amount of time.  Rather, when mathematicians are trying to prove existence of solutions to PDEs, they try to find a general solution in the form of integrals involving the initial data, and then prove that these integrals are well-behaved no matter what the initial data is.  Sometimes, they have to impose certain conditions on the initial data to make sure that their integrals are well-behaved;  these are the conditions that you ran into in your last post.</p>
"
"2394452","2394509","<p>A <em>vector field</em> is just a function $\mathbf{F}:\mathbb{R}^n\to\mathbb{R}^n$. (Or the domain can be some subset $D$ of $\mathbb{R}^n$, not all of it, of course. To avoid making this remark repetitively, I'll just stick with $\mathbb{R}^n$, as an example). From an intuitive/geometrical point of view, it is best visualized as having a vector attached to (or starting from) each point of the domain $D$. Given a multivariable function $f:\mathbb{R}^n\to\mathbb{R}$, its gradient $\nabla f$ is precisely that kind of a function, as it acts from $\mathbb{R}^n$ to $\mathbb{R}^n$, so it is a vector field.</p>

<p>What the book says is that not all vector fields can be obtained in this way. Intuitively, it should make sense: the fact that the components of $\nabla f$ are derived from the same original function $f$ should result in some relationships among them. If you put random functions as components, most likely they wouldn't be related in that special way. It's like siblings: being the descendants of the same parents they have some things in common, something that random people wouldn't.</p>

<p>In more mathematical terms, one of such special relationships is the condition of equality of mixed partial derivatives. Say, for a function of two variables $f:\mathbb{R}^2\to\mathbb{R}$, we must have $\displaystyle \frac{\partial^2f}{\partial y\partial x}=\frac{\partial^2f}{\partial x\partial y}$, assuming these derivatives are continuous. Therefore, if $\displaystyle \mathbf{F}=\nabla f=\left\langle\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}\right\rangle=\langle M,N\rangle$ is its gradient, then we have to have
$$\color{blue}{\frac{\partial M}{\partial y}}=\frac{\partial^2f}{\partial y\partial x}=\frac{\partial^2f}{\partial x\partial y}=\color{blue}{\frac{\partial N}{\partial x}}.$$</p>

<p>So being the gradient is very special! This condition allows you to construct vector fields that are not gradients. Pretty much anything you would put into a vector field at random almost surely wouldn't be a gradient. For example, $\mathbf{F}(x,y)=\langle 2x+3y,4x+5y\rangle$ isn't a gradient, because
$$\frac{\partial M}{\partial y}=3\neq4=\frac{\partial N}{\partial x}.$$</p>
"
"2394456","2394909","<p>I'll start with some critique. First of all, your normal vector isn't quite correct: from the equation of the plane $-2x+z=0$, we get the normal vector $\mathbf{n}=\langle-2,0,1\rangle$ (or it could be its opposite, but this one gives the upward orientation, consistent with the counterclockwise orientation of the curve $C$). Fortunately, it doesn't affect your solution because the first component of curl is zero.</p>

<p>Second, it is a really bad habit to drop differentials, representing the variables of integration, from integral notation! For example, the last line of your computation should be written as
$$\iint_S (2x-xz)\,dx\,dy=\iint_S (2x-2x^2)\,dx\,dy=\int_{-\pi/2}^{\pi/2}\int_0^{2\cos\theta} (2r\cos\theta-2r^2\cos^2\theta)r\,dr\,d\theta.$$</p>

<p>Third, you must be much more clear regarding domains of integration. The ""equality""
$$\iint_S \operatorname{curl}F\cdot\mathbf{n}\,dS=\iint_S (2x-2x^2)\,dx\,dy$$
is wrong because the domains of integration in these two integrals are <strong>NOT</strong> the same and thus cannot be denoted by the same letter $\color{red}{S}$. If $S$ stands for the portion of the plane cut out by the paraboloid (or cylinder), then it's rightfully used in the first integral, but not in the second. The second one represents a double integral over a region $D$ in the $xy$-plane after you effectively parameterized the surface $S$. And this region $D$ is the disk $(x-1)^2+y^2=1$, that you correctly found. And to integrate over this $D$, it certainly makes sense to switch to polar coordinates.</p>

<p>In the end of the day, you did get a correct double integral in polar coordinates (also see above), so you can finish solving this problem by evaluating that integral. (I presume you can do that, and you don't need us to give you the answer.)</p>

<p>Now, a very short <strong>main answer</strong> to your <strong>main question</strong>: <strong><em>YES</em></strong>, we are allowed to choose any such surface. :-)</p>
"
"2394459","2394474","<p>Yeah, that's typically the natural action to consider. But then you have to <em>prove</em> that it is a valid left action. </p>

<p>In general, to prove something is a left $R$-module, it just has to have <em>some</em> action of $R$ defined on the left. You get to pick that action, and usually it turns out to be a very natural choice. In the general case, part of the exercise is deciding what the ""natural action"" is. </p>
"
"2394467","2394481","<p>Note that
\begin{align*}
&amp;\begin{bmatrix}
A &amp; B \\ B^T &amp; C
\end{bmatrix}
\begin{bmatrix}
D &amp; -DBC^{-1} \\ -C^{-1}B^TD &amp; C^{-1}+C^{-1}B^TDBC^{-1}
\end{bmatrix}
\\ &amp;=
\begin{bmatrix}
AD - BC^{-1}B^TD &amp; -ADBC^{-1} + BC^{-1} + BC^{-1}B^TDBC^{-1} \\
B^TD + -CC^{-1}B^TD &amp; -B^TDBC^{-1}+CC^{-1}+CC^{-1}B^TDBC^{-1}
\end{bmatrix}
\\ &amp;=
\begin{bmatrix}
(A-BC^{-1}B^T)D &amp; (I-(A-BC^{-1}B^T)D)BC^{-1} \\
0 &amp; I
\end{bmatrix}
\\ &amp;=
\begin{bmatrix}
I &amp; 0 \\
0 &amp; I
\end{bmatrix}.
\end{align*}
If instead you choose to multiply in the reverse order, then you will have similar cancellations using $D=(A-BC^{-1}B^T)^{-1}$ and the fact that $C$ is invertible.</p>
"
"2394497","2394508","<p>For the main question, the answer is no, not necessarily. 
<p>
For example, let $f = x^2y^2 + z^4$.
<p>
Then $f$ is irreducible in $\mathbb{R}[x,y,z]$.
<p>
It's easily seen that 
$$V(f) = \{(x,y,z) \in \mathbb{R}^3 \mid z=0,\;\text{and at least one of}\;x,y\;\text{is zero}\}.$$
<p>
Consider the ideal $I = I(V(f))$
<p>
Then $xy\in I$, but $x \notin I$ and $y \notin I$, so $I$ is not a prime ideal of $\mathbb{R}[x,y,z]$.
<p>
Hence $V(f)$ is not irreducible.</p>
"
"2394500","2394628","<p>Let $E$ denote the interior of the ellipse and $\partial E$ the boundary (i.e. the ellipse itself). You're trying to calculate</p>

<p>$$\color{blue}{\oint_{\partial E}\mathbf{F}\cdot d\mathbf{r}}$$</p>

<p>Your first hope is that Green's theorem applies, so you can instead calculate</p>

<p>$$\int_E(Q_x-P_y)\,dA$$</p>

<p>The problem is that <em>the hypotheses of Green's theorem are not satisfied</em>, for the functions $P$ and $Q$ are not continuously differentiable throughout the region $E$. As you observed, there is no way we may continuously extend the field to be defined at the point $(1,0)$, so that point is a singularity.</p>

<p>The way to proceed in such cases is to put a little closed curve around the singularity but still lying entirely inside the region $E$ of interest; in this case a unit circle will be convenient (the calculation will show you why). So let $D$ be the unit disk centered at $(1,0)$, and let $\partial D$ be the unit <em>circle</em> centered there, which has parametrization $C(t)=(\cos t +1, \sin t)$ for $t\in[0,2\pi)$. </p>

<p>You can easily check by direct calculation that </p>

<p>$$\color{red}{\oint_{\partial D}\mathbf{F}\cdot d\mathbf{r}}=\int_0^{2\pi}(\cos^2t+\sin^2t)\,dt=2\pi$$</p>

<p>Now, consider the region $E-D$, the complement of the disk in the elliptical region. In <em>this</em> region, $P$ and $Q$ <em>are</em> continuously differentiable everywhere, so we <em>may</em> use Green's theorem, which says</p>

<p>$$\int_{E-D}(Q_x-P_y)\,dA=\oint_{\partial(E-D)}\mathbf{F}\cdot d\mathbf{r}\tag{$\star$}$$</p>

<p>The left side vanishes, because $Q_x-P_y=0$ in the region $E-D$. What about the right side? What is the ""boundary"" $\partial(E-D)$ of the elliptical annular region $E-D$? It is $\partial E - \partial D$, where we interpret the negative sign on $\partial D$ to mean reversing the standard counterclockwise orientation.</p>

<p>So $(\star)$ says</p>

<p>$$0=\color{blue}{\oint_{\partial E}\mathbf{F}\cdot d\mathbf{r}}-\color{red}{\oint_{\partial D}\mathbf{F}\cdot d\mathbf{R}}$$</p>

<p>The blue integral is the one the problem asks for, and the red integral we calculated above.</p>

<hr>

<p>To summarize, the strategy for calculating line integrals of <em>curl-free</em> fields around closed curves that encircle a single singularity is to replace the original closed curve with an easier one (but still contained within the original one). Green's theorem in the region <em>between</em> the curves guarantees the line integral around the easier curve equals the line integral around the original curve. </p>

<p>What makes a good candidate for an ""easier curve""? Well, you're trying to find a curve whose parametrization will make the integrand simple enough to calculate. In your problem, integrating around the <em>ellipse</em> gives a ""hard"" integrand. But integrating around a <em>circle centered at the singularity</em> gives an ""easy"" integrand. </p>

<p>Why a circle in this case? Because your field is closely related to the pullback of the length element of a circle of radius $r$ (the length element is $ds=r d\theta$); thus it was, in some sense, <em>tailor-made</em> to be integrated around a circle. It is not always so easy to spot what sort of curve will make the line integral easier, but you should remember the general form $(-\frac{y}{r^2}, \frac{x}{r^2})$, which is $\frac{1}{r}$ times the circle's unit tangent vector. The line integral of the unit tangent vector around the circle gives $2\pi r$, and the factor $\frac{1}{r}$ scales the result to $2\pi$. (In other words, you are just integrating $d\theta=\frac{1}{r}ds$ around the circle.) So the <em>radius</em> of the circle you use around the singularity doesn't matter. (This make sense, because the region between two concentric circles centered at the singularity doesn't contain a singularity and the field is conservative there, so by the above argument, the line integral of the field around them should be the same.)</p>
"
"2394502","2394541","<p>The approach in the OP is solid.  I thought it might be instructive to present a way forward using polar coordinates.</p>

<p>First, let $z=re^{i\theta}$. Next, we see that $\bar z = re^{-i\theta}$ and therefore</p>

<p>$$\frac{\bar z}z=e^{-i2\theta}$$</p>

<p>In order for $\bar z/z$ to be in Quadrant 3, we must have $-2\theta \in (\pi+2n\pi,3\pi/2+2n\pi)$ for some integer $n$.  This implies that $\theta \in (-3\pi/4-n\pi,-\pi/2-n\pi)$.  </p>

<p>Given that $z$ is in the third quadrant, $n$ must be even.  Taking $n=0$, we have $\theta \in (-3\pi/4,-\pi/2)$, which occurs when $y&lt;x&lt; 0$.</p>
"
"2394503","2394636","<p>I'll give you the start of the calculation</p>

<p>$$\renewcommand{\arraystretch}{2}  \begin{array}{rcl}J = \displaystyle  \left|\frac{\partial  \left(x , y , z\right)}{\partial  \left(r , a , b\right)}\right|&amp;=&amp;\displaystyle  \left|\begin{array}{ccc}\frac{\partial  x}{\partial  r}&amp;\frac{\partial  x}{\partial  a}&amp;\frac{\partial  x}{\partial  b}\\
\frac{\partial  y}{\partial  r}&amp;\frac{\partial  y}{\partial  a}&amp;\frac{\partial  y}{\partial  b}\\
\frac{\partial  z}{\partial  r}&amp;\frac{\partial  z}{\partial  a}&amp;\frac{\partial  z}{\partial  b}
\end{array}\right|\\
&amp;=&amp;\displaystyle  \left|\begin{array}{ccc}\sin  \left(a\right) \cos  \left(b\right)&amp;r \cos  \left(a\right) \cos  \left(b\right)&amp;{-r} \sin  \left(a\right) \sin  \left(b\right)\\
\frac{1}{\sqrt{2}} \sin  \left(a\right) \sin  \left(b\right)&amp;\frac{1}{\sqrt{2}} r \cos  \left(a\right) \sin  \left(b\right)&amp;\frac{1}{\sqrt{2}} r \sin  \left(a\right) \cos  \left(b\right)\\
\cos  \left(a\right)&amp;{-r} \sin  \left(a\right)&amp;0
\end{array}\right|
\end{array}$$</p>

<p>In the second row of this determinant, you can factor $\frac{1}{\sqrt{2}}$. In the second
column, you can factor $r$, and in the last column, you can factor
$r \sin  \left(a\right)$. It gives</p>

<p>$$J = \frac{1}{\sqrt{2}} {r}^{2} \sin  \left(a\right) \left|\begin{array}{ccc}\sin  \left(a\right) \cos  \left(b\right)&amp;\cos  \left(a\right) \cos  \left(b\right)&amp;{-\sin } \left(b\right)\\
\sin  \left(a\right) \sin  \left(b\right)&amp;\cos  \left(a\right) \sin  \left(b\right)&amp;\cos  \left(b\right)\\
\cos  \left(a\right)&amp;{-\sin } \left(a\right)&amp;0
\end{array}\right|$$</p>

<p>It should now be easy to finish this calculations.</p>

<p>Edit: the rules I used are the following</p>

<p>$$\left|\begin{array}{ccc}X a&amp;X b&amp;X c\\
d&amp;e&amp;f\\
g&amp;h&amp;i
\end{array}\right| = X \left|\begin{array}{ccc}a&amp;b&amp;c\\
d&amp;e&amp;f\\
g&amp;h&amp;i
\end{array}\right|$$</p>

<p>and</p>

<p>$$\left|\begin{array}{ccc}X a&amp;b&amp;c\\
X d&amp;e&amp;f\\
X g&amp;h&amp;i
\end{array}\right| = X \left|\begin{array}{ccc}a&amp;b&amp;c\\
d&amp;e&amp;f\\
g&amp;h&amp;i
\end{array}\right|$$</p>

<p>The same rule works with any row or any column instead of
the first row or the first column. It also works for smaller or
larger determinants.</p>
"
"2394513","2396962","<p>This is the text you're referring to:</p>

<blockquote>
  <p>Rotations for a group, as the result of combining two rotations is a third rotation. The same must therefore be true of rotors. Suppose that $R_1$ and $R_2$ generate two distinct rotations. The combined rotations take $a$ to</p>
  
  <p>$$a\mapsto R_2(R_1(a)R_1^\dagger)R_2^\dagger=R_2R_1aR_1^\dagger R_2^\dagger$$</p>
  
  <p>We therefore define the product rotor
  $R=R_2R_1$</p>
  
  <p>so that the result of the composite rotation is described by $RaR^\dagger$, as usual. The product $R$ is a  new rotor, and in general it will consist of geometric products of an even number of unit vectors[...]</p>
</blockquote>

<p>There isn't anything mentioned here about ""rotating rotors,"" just compositions of rotations, and the meaning of the passage seems completely clear: to compute the composite of rotations, you just multiply them.</p>

<p><em>(from the comments)</em></p>

<blockquote>
  <p>how does a rotor behave under a rotation of the reference frame?</p>
</blockquote>

<p>Well, that is a completely different topic from composition of rotations! </p>

<p>If $C$ is the rotor that converts from the old basis $\{e_1,\ldots, e_n\}$ onto the new basis $\{f_1,\ldots f_n\}$, then</p>

<p>$CRe_iR^\dagger C^\dagger = CRC^\dagger f_iCR^\dagger C^\dagger$, and so $CRC^\dagger$ expresses the rotation in the new basis of $f_i$'s.</p>

<p>Personally I don't like the idea of using the phrase ""rotating a rotor."" Conceptually there is supposed to be a divide here between ""space"" and ""operators."" That is, the space is the set that models what is being transformed, and the operators are what do the transforming. </p>

<p>""Rotating a rotor"" makes me think the two things are being muddled together. (Although, it might be a good mnemonic for remembering how an orthogonal change of basis can be used.) Maybe something like this is what led to your question.</p>
"
"2394527","2394548","<p>How about</p>

<p>$$\lvert \sqrt{a_n} - \sqrt{\ell}\rvert^2 \leqslant \lvert\sqrt{a_n} - \sqrt{\ell}\rvert\cdot(\sqrt{a_n} + \sqrt{\ell}) = \lvert (\sqrt{a_n} - \sqrt{\ell})(\sqrt{a_n} + \sqrt{\ell})\rvert = \lvert a_n - \ell\rvert,$$</p>

<p>which follows from $\lvert \sqrt{a_n} - \sqrt{\ell}\rvert \leqslant \sqrt{a_n} + \sqrt{\ell}$?</p>
"
"2394531","2394534","<p>Imagine that the dot at the center is a light bulb. Then every point on the cup-shaped part of the figure has a shadow point on the line. (That's what ""projection"" means in this context.)</p>

<p>The points on the cup shaped figure correspond to the points on the unit interval using the ""bending"" Courant and Robbins describe.</p>
"
"2394543","2394549","<p>It's very likely to be a typo (on both lines). It most likely meant $n=5.7(10^{14})$.</p>

<p>We see on the next line of calculation that this interpretation would make sense, given the result.</p>

<p>$$5.7(10^{14}) Ã1.67(10^{-27}) = 10^{-12}$$</p>
"
"2394546","2394639","<p>EDIT: After the OP has been edited, there is no hope to find a closed form solution, since you are likely to obtain at least fourth degree powers in $\cos\theta$ when squaring :(</p>

<p>First try to isolate the $\theta$:
$$\frac{c^2-a^2}{pq}=\frac{1-2a\cos \theta}{\sqrt{q^2 \sin^2\theta+p^2\cos^2\theta}}$$
now take squares in both sides and substitute the $\sin\theta$:
$$\left(\frac{c^2-a^2}{pq}\right)^2=\frac{1-4a\cos \theta+4a^2\cos^2\theta}{q^2 \sin^2\theta+p^2\cos^2\theta}=\frac{1-4a\cos \theta+4a^2\cos^2\theta}{q^2 +(p^2-q^2)\cos^2\theta}$$
Let us call $k$ the left hand side for brevity, multiply:
$$kq^2+k(p^2-q^2)\cos^2\theta=1-4a\cos \theta+4a^2\cos^2\theta$$
and group:
$$\left(k(p^2-q^2)-4a^2\right)\cos^2\theta + 4a\cos \theta+(kq^2-1)=0$$</p>

<p>Can you take it from here? It is a standard second degree equation in $\cos\theta$. </p>
"
"2394547","2394869","<p>Your claim is incorrect, even for finite dimensional spaces: let $H=\mathbb{R}^2$ and pick $A$ to be the operator corresponding to the matrix $\left(\begin{matrix} 1 &amp; 0 \\ 0 &amp; 0 \end{matrix}\right)$. Then $A$ is selfadjoint (since the matrix is symmetrical), nonnegative (since the eigenvalues $0$, $1$ are both nonnegative), and, trivially, has finite trace. However, $\overline{AH}=AH=\text{span}\{e_1\}\neq\mathbb{R}^2=H$. </p>

<p>Even simpler (but perhaps a little too trivial) is the operator $Ax:=0_H$ for any Hilbert space $H$.</p>
"
"2394559","2394570","<p>Correct, once you keep in mind that $f(x_1,\ldots,x_n)$ is a shorthand for $f ((x_1,\ldots,x_n))$, etc..</p>
"
"2394565","2394596","<p>I think there are some points you aren't completely understanding here.
First of all, there is no number line involved here, $X$ is an arbitrary set, with some distance function defined on it, specifically, $X$ is not necessarily linearly ordered, so something like ""the smallest $q$"" doesn't really make sense in this context. </p>

<p>Secondly, if we do assume $X$ is a subset of $\mathbb R$, or linearly ordered, then $N_r(p)$ contains all the points <em>whose distance from p</em> is less than the distance of the nearest $q\in X$ to $p$ (which is probably what you meant).</p>

<p>What we are trying to prove is that $N_r(p)$ contains <em>no</em> points besides $p$, meaning that there aren't any points closer to $p$ than $q$. So if </p>

<blockquote>
  <p>the interval within the radius $r$ contains $p$ and some other points smaller than $q$</p>
</blockquote>

<p>those points would be inside $N_r(p)$, and then $N_r(p)$ <em>wouldn't</em> be a neighborhood of $p$ contained in $E$.
Basically, we don't ignore other points in $X$ because we only care about $q$, we ignore them because we specifically chose $q$ such that there <em>aren't</em> any other points between them.</p>
"
"2394568","2394580","<p>Let $x \in A$. By the mean value property, and for a suitably small $r$, we have : $$
u(x) = \frac 1{|B(x,r)|} \int_{B(x,r)} u(y)dy
$$
From what we know, since $u(y) \leq u(x)$ for all $y \in \Omega$, we have that:
$$
u(x) \geq \frac 1{|B(x,r)|} \int_{B(x,r)} u(y)dy
$$</p>

<p>I encourage you to prove a converse to this statement : if equality happens above, and we assume that $x \in A$, then indeed $u$ is constant on $B(x,r)$. Think about how you would do this.</p>

<p>Once this is true, then $A$ will contain an open neighbourhood of $x$, namely the entire ball of radius $r$, and hence would be open. </p>
"
"2394572","2394599","<p>First let us count the number of each letter there are:</p>

<p>$A$-2, $C$-1, $E$-1, $H$-1, $M$-1, $S$-4, $T$-2, $U$-1</p>

<p>For the first and last character to both be vowels, one of three situations will occur:</p>

<ul>
<li><p>Neither $A$ is used as a first or last character</p></li>
<li><p>Exactly one $A$ is used as a first or last character</p></li>
<li><p>Both $A$'s are used as first and last characters</p></li>
</ul>

<p>Let us count each case individually.</p>

<p>In the scenario that neither $A$ is used as a first or last character, that implies that the characters are an $E$ and a $U$.  Pick which of the two was the first character and the remaining will be the last character.  We then choose the locations of the two $A$'s simultaneously and then the four $S$'s and the two $T$'s, or equivalently worded we use multinomial coefficients to find the arrangements of the word <code>aachmsssstt</code>.  This gives us: $2\cdot \frac{11!}{2!4!2!}$</p>

<p>In the scenario that exactly one $A$ is used as a first or last character, first select whether the $A$ is the first character or the last character.  Then, pick which of the remaining vowels takes the other spot.  Then, for whichever vowel is left, we arrange <code>achmssssttx</code> where <code>x</code> is the remaining vowel.  This gives us $2\cdot 2\cdot \frac{11!}{4!2!}$</p>

<p>In the scenario that both $A$'s are used as first and last characters, arrange <code>cehmssssttu</code> for the middle.  This gives us $\frac{11!}{4!2!}$</p>

<p>Combining all of these together, this gives us a final total of:</p>

<p>$$6\cdot \frac{11!}{4!2!}$$</p>

<p>Their given answer then appears to be correct, though it isn't perfectly clear how they arrived at their answer.</p>

<hr>

<p>An alternate approach:</p>

<p>First select the positions for the four vowels collectively.  Two of them must be the front and back, so that amounts to selecting two more of the center eleven spaces to be used by vowels.  This gives $\binom{11}{2}=\frac{11!}{2!9!}$ options.  Then, select the arrangement of the vowels within those four spaces.  This can be accomplished in $\frac{4!}{2!1!1!}=4\cdot 3$ ways.</p>

<p>We can then separately arrange all of the consonants in $\frac{9!}{4!2!}$ ways and then thread the two strings together.</p>

<p>This gives us a total of $\frac{11!}{2!9!}4\cdot 3\cdot \frac{9!}{4!2!}=\frac{4\cdot 3\cdot 11!}{4!2!2!}$ ways.</p>
"
"2394582","2394586","<p>Try to use $$(\sqrt{x^2+1}-x)(\sqrt{x^2+1}+x)=1$$</p>
"
"2394585","2394652","<p>We can use generating functions.</p>

<p>Take your example of writing n as a sum of $1$s and $2$s, represent the number of possible $1$s as</p>

<p>$$x^{0\cdot 1}+x^{1\cdot 1}+x^{2\cdot 1}+x^{3\cdot 1}+\cdots$$</p>

<p>What is $x$? You might ask, well it isn't anything, it has no value beyond the formality of allowing us to write out a list of possible $1$s in our sum. E.g. $x^{3\cdot 1}$ denotes the possibility of a sum having three $1$s.</p>

<p>We can do the same with the number of possible $2$s by listing</p>

<p>$$x^{0\cdot 2}+x^{1\cdot 2}+x^{2\cdot 2}+x^{3\cdot 2}+\cdots$$</p>

<p>Now since we are interested in all combinations of $1$ and $2$ added, if we multiply these two lists as if they were series then each resulting term will represent a possible sum, this is called a <a href=""https://en.m.wikipedia.org/wiki/Generating_function"" rel=""noreferrer"">generating function</a></p>

<p>$$(x^{0\cdot 1}+x^{1\cdot 1}+x^{2\cdot 1}+x^{3\cdot 1}+\cdots)(x^{0\cdot 2}+x^{1\cdot 2}+x^{2\cdot 2}+x^{3\cdot 2}+\cdots)\tag{1}$$</p>

<p>So the expansion will have terms like your first example $x^{2\cdot 1+4\cdot 2}=x^{10}$ and the coefficient in front of this term will be the number of times the sum of $10$ can be achieved using $1$s and $2$s.</p>

<p>We are able to use the expansion $(1-u)^{-1}=1+u+u^2+u^3+\cdots$ and the difference between two squares $a^2-b^2=(a-b)(a+b)$ to help us express the product $(1)$ as</p>

<p>$$\frac{1}{(1-x)(1-x^2)}=\frac{1}{(1-x)^2(1+x)}\tag{2}$$</p>

<p>which can be written using partial fractions</p>

<p>$$\begin{align}\frac{1}{(1-x)^2(1+x)}&amp;=\frac{1}{2(1-x)^2}-\frac{1}{4(1-x)}+\frac{1}{4(1+x)}\\[1ex]
&amp;=\frac{1}{2}\sum_{n\ge 0}\binom{n+1}{1}x^n - \frac{1}{4}\sum_{n\ge 0}x^n+\frac{1}{4}\sum_{n\ge 0}(-1)^nx^n\\[1ex]
&amp;=\sum_{n\ge 0}\frac{2(n+1)+1+(-1)^n}{4}x^n\end{align}$$</p>

<p>Hence in the simple case of $1$s and $2$s we can derive a formula</p>

<p>$$\text{required count using $1$s and $2$s}=\frac{2(n+1)+1+(-1)^n}{4}$$</p>

<p>So for example for a sum of $n=10$ there are $(2\cdot 11+1+1)/4=24/4=6$ possible sums.</p>

<p>It is possible to derive formulas for larger sets of possible parts (not just parts of size $1$ and $2$) but the maths becomes more long-winded. In general it is better to use the series products like $(1)$ to derive a $2$ dimensional recurrence (a bit like Pascal's Triangle) to calculate relevant coefficients.</p>

<p>An example of the generating function for sums involving $1$s, $2$s and $3$s is</p>

<p>$$\dfrac{1}{(1-x)(1-x^2)(1-x^3)}$$</p>

<p>derived in the same way as for your example.</p>
"
"2394590","2394616","<p>By the definition of the natural numbers, given a number $n&lt;x&lt;n+1$, it is enough to find <em>any</em> inductive set that doesn't contain $x$ to prove that $x\notin \mathbb N$.</p>

<p>By induction on $n$: </p>

<p>for the case where $n=1$, take the inductive set $\{1,2,...\}$, there is no $1&lt;x&lt;2$ in that set.</p>

<p>For $n&gt;1$: given $x$ such that $n&lt;x&lt;n+1$, by the induction hypothesis we know that $x-1$ isn't a natural number because $n-1&lt;x-1&lt;n$, therefore there is an inductive set $S$ such that $x-1\notin S$. Then by the definition of an inductive set, the set $S-\{x\}$ obtained by removing $x$ from $S$ is also an inductive set, and doesn't contain $x$. </p>
"
"2394604","2394676","<p>This is generally true with $\phi$ integrable and $f$ decreasing.  One approach is to use Riemann sums for proof.  Another is to use Riemann-Stieltjes integration.  </p>

<p>Defining $\Phi(x) = \int_a^x \phi(t) \, dt$ we have $\Phi$ absolutely continuous, and it is a general property of Riemann-Stieltjes integrals that (with $f$ monotone),</p>

<p>$$\int_a^b \phi(x)f(x) \, dx = \int_a^b f \, d\Phi. $$ </p>

<p>Applying integration by parts we obtain</p>

<p>$$\int_a^b \phi(x)f(x) \, dx = \int_a^b f \, d\Phi = \Phi(b)f(b) - \Phi(a)f(a) - \int_a^b \Phi \, df.$$</p>

<p>Since $\Phi(a) = 0$ it follows that</p>

<p>$$\tag{*}\int_a^b \phi(x)f(x) \, dx =  \Phi(b)f(b) - \int_a^b\Phi \,df.$$</p>

<p>Since $\Phi$ is continuous it is bounded on $[a,b]$.  Let $A = \inf_{x \in [a,b]}\Phi(x)$ and $B = \sup_{x \in [a,b]}\Phi(x)$.</p>

<p>Since $A \leqslant \Phi(x) \leqslant B$ for all $x \in [a,b]$ and $f$ is decreasing we have</p>

<p>$$\tag{**}A(f(a) - f(b)) \leqslant -\int_a^b \Phi\, df \leqslant B(f(a) - f(b)).$$</p>

<p>From (*) and (**) it follows that</p>

<p>$$Af(a) - Af(b) + \Phi(b) f(b) \leqslant \int_a^b \phi(x) f(x) \, dx \leqslant Bf(a) - Bf(b) + \Phi(b) f(b),$$</p>

<p>and</p>

<p>$$Af(a) + (\Phi(b) - A)f(b) \leqslant \int_a^b f(x) h(x) \, dx \leqslant Bf(a) -(B- \Phi(b))f(b).$$</p>

<p>Since $A \leqslant \Phi(b) \leqslant B$ and $f$ is non-negative, we have $(\Phi(b)-A)f(b) \geqslant 0$ and $(B- \Phi(b))f(b) \geqslant 0$.</p>

<p>Therefore,</p>

<p>$$Af(a) \leqslant \int_a^b \phi(x) f(x) \, dx \leqslant Bf(a).$$</p>
"
"2394608","2394611","<p>Using Stokes Theorem:
$$
\int_{\partial\Sigma} y^2 dx + x dy = \iint_{\Sigma} d\left[y^2 dx + x dy\right]= \iint_{\Sigma} \left[-2y dx\, dy + dx\, dy\right]=\iint_{\Sigma} \left[-2y + 1\right] dx\, dy
$$</p>
"
"2394620","2394695","<p>Let $(1,0) \equiv a = e^{2x}\cos x$ and $(0,1) \equiv b = e^{2x}\sin x$,</p>

<p>$$\frac{\partial e^{2x}\cos x}{\partial x} = 2e^{2x}\cos x - e^{2x}\sin x = 2a-b$$</p>

<p>$$\frac{\partial e^{2x}\sin x}{\partial x} = 2e^{2x}\sin x + e^{2x}\cos x = 2b+a$$</p>

<p>So,</p>

<p>$$(1,0) \rightarrow (2,-1) \text{ and } (0,1) \rightarrow (1,2)$$</p>

<p>And,</p>

<p>$$D = \begin{pmatrix}2 &amp; 1 \\ -1 &amp; 2\end{pmatrix}$$</p>

<p>Note that $D+I = \begin{pmatrix}3 &amp; 1 \\ -1 &amp; 3\end{pmatrix}$ is invertible and its inverse is $$(D+I)^{-1} = \frac{1}{10}\begin{pmatrix}3 &amp; -1 \\ 1 &amp; 3\end{pmatrix}$$</p>

<p>Finally, the required answer is,</p>

<p>$$(D+I)^{-1}\begin{pmatrix}1\\0\end{pmatrix} = \frac{1}{10}\begin{pmatrix}3\\1\end{pmatrix}$$</p>

<p>Therefore,</p>

<p>$$y = \frac{1}{10}(3a + b)$$</p>

<p>P.S. Sorry I studied Linear Algebra long time back. I could have possible written a better answer.</p>
"
"2394630","2397274","<p>I'm expanding on Eric's hint:</p>

<p>I think the idea is like this:</p>

<p>Fix $i&gt;0.$ </p>

<p>(Following sloppy conventions, various equalities below are actually natural isomorphisms.)</p>

<ol>
<li>Let $\ldots \to P_1 \to P_0 \to M$ be a projective resolution of $M$. Let $C_* = \ldots \to P_1 \to P_0$, so $C_*$ is quasi-isomoprhic to $M$, and a complex of projectives.</li>
<li>Then $Tor^i(M,N) = H_i(C_* \otimes N)$.</li>
<li>We want to show that $Tor^i(M,N)$ is torsion. This is equivalent to $Tor^i(M,N) \otimes_R K = 0$.</li>
<li>So we wish to show that $0 = Tor^i(M,N) \otimes K$:</li>
<li>$Tor^i(M,N) \otimes K = H_i(C_* \otimes N) \otimes K = H_i(C_* \otimes N \otimes K)$. Since $K$ is flat, $\otimes K$ is exact, since exact functors ""commute"" with taking homology, we can bring the tensor product inside.</li>
<li>Now $C_* \otimes_R N \otimes_R K = (C_* \otimes_R K) \otimes_K (N \otimes_R K)$, by associativity of tensor products and what is usually called cancellation for tensor products: $(A \otimes_R B) \otimes_B C= A \otimes_R C$, for $B$ an $R$ algebra. </li>
<li>However, $H_i((C_* \otimes_R K) \otimes_K (N \otimes_R K))= Tor^i_K(M \otimes_R K, N \otimes_R K)$. This is zero because $K$ is a field - in particular to compute this Tor we can take the projective resolution of $M \otimes_R K$ to be the complex consisting of $M \otimes_R K$ in degree $0$.</li>
</ol>
"
"2394635","2394648","<p>The substitution $x\rightarrow 1/x$ makes that 
$$I(\alpha)=\int_{\frac{1}{n}}^{n}\frac{x^\alpha\,dx}{(x^\alpha+1)(1+x^2)},$$ adding both equations gives 
$$2\,I(\alpha)=\int_{\frac{1}{n}}^{n}\frac{dx}{1+x^2}=\arctan n-\arctan\frac1n=2\arctan n- \frac{\pi}2,$$
i.e. $$I(\alpha)=\arctan n- \frac{\pi}4.$$</p>
"
"2394650","2394669","<p>$\mathbb Z$ is one of the seven <a href=""https://en.wikipedia.org/wiki/Frieze_group#Descriptions_of_the_seven_frieze_groups"" rel=""noreferrer"">frieze groups</a>. It is the symmetry group of the simplest frieze:</p>

<p><a href=""https://i.stack.imgur.com/bdgCx.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bdgCx.png"" alt=""enter image description here""></a></p>

<p>Such decorative friezes occur very frequently in architecture and art. 
(Perhaps not with feet!)</p>
"
"2394661","2396296","<p>In a somewhat trivial, but perhaps significant, way, the answer is ""no"": compact connected Riemann surfaces of genus >1 are not diffeomorphic to open subsets of $\mathbb C$"". Namely, open subsets of $\mathbb C$ are not compact. If we try to dodge this by taking closures, then the uniformization theorem will require that we identify some points on the boundaries, so the map (from a non-Euclidean polygon to the Riemann surface) will definitely not be one-to-one.</p>

<p>Possibly the genuine intent of the question is somewhat different from this, or can be refined to take into consideration such ""trivial counter-examples"", to approach the real issue?</p>
"
"2394668","2395893","<p><em>Please note that if this answer is not as rigorous as you may like then I urge you to fill in the details.</em></p>

<p>I'm thinking you can rearrange $(1)$ into the form</p>

<p>$$\sum_{m=j_2-j}^{j_1} \frac{(j_1+m)!}{(j_2-j+m)!(j+j_1-j_2)!}\frac{(j_2+j-m)!}{(j_1-m)!(j-j_1+j_2)!}\stackrel{?}{=}
\frac{(j+j_1+j_2+1)!}{(j_1+j_2-j)!(2j+1)!}$$</p>

<p>or</p>

<p>$$\sum_{m=j_2-j}^{j_1} \binom{j_1+m}{j_2-j+m}\binom{j_2+j-m}{j_1-m}\stackrel{?}{=}
\frac{(j+j_1+j_2+1)!}{(j_1+j_2-j)!(2j+1)!}\tag{*}$$</p>

<p>Using $(3)$ on the left hand side summation</p>

<p>$$\sum_{m=j_2-j}^{j_1} (-1)^{j_2-j+m}\binom{j_2-j_1-j-1}{j_2-j+m}(-1)^{j_1-m}\binom{j_1-j_2-j-1}{j_1-m}$$</p>

<p>gives</p>

<p>$$(-1)^{j_1+j_2-j}\sum_{m=j_2-j}^{j_1} \binom{j_2-j_1-j-1}{j_2-j+m}\binom{j_1-j_2-j-1}{j_1-m}$$</p>

<p>then call $x=j_2-j+m$ and $c=j_1+j_2-j$ so we have from $(2)$</p>

<p>$$(-1)^{j_1+j_2-j}\sum_{x=2(j_2-j)}^{j_2+j_1-j} \binom{j_2-j_1-j-1}{x}\binom{j_1-j_2-j-1}{c-x}=\binom{-2j-2}{j_1+j_2-j}\tag{**}$$</p>

<p>Now use $(3)$ on the right hand side of $(\text{**})$</p>

<p>$$\begin{align}(-1)^{j_1+j_2-j}\sum_{x=2(j_2-j)}^{j_2+j_1-j} \binom{j_2-j_1+j+1}{x}\binom{j_2-j_1+j-1}{c-x}=&amp;(-1)^{j_1+j_2-j}\binom{j+j_1+j_2+1}{j_1+j_2-j}\\[1ex]\implies\sum_{x=2(j_2-j)}^{j_2+j_1-j} \binom{j_2-j_1+j+1}{x}\binom{j_2-j_1+j-1}{c-x}&amp;=\frac{(j+j_1+j_2+1)!}{(j_1+j_2-j)!(2j+1)!}\end{align}$$</p>

<p>which is the right hand side of $(\text{*})$.</p>

<p>Notice I am playing fast and loose with limits on the Vandermonde summation. I'll leave that to you to make sure they obey $(2)$.</p>
"
"2394674","2394792","<p>If $(Y_k)$ is i.i.d. standard exponential, that is, with PDF $$f(y)=e^{-y}\mathbf 1_{y&gt;0}$$ then, for every $n\geqslant1$, $T_n=Y_1+\cdots+Y_n$ has PDF $$f_n(t)=\frac{t^{n-1}}{(n-1)!}e^{-t}\mathbf 1_{t&gt;0}$$ hence, for every $n\geqslant2$, $$E\left((T_n)^{-1}\right)=\frac1{n-1}$$ and, for every $n\geqslant3$, $$E\left((T_n)^{-2}\right)=\frac1{(n-1)(n-2)}$$ hence $$\mathrm{Var}\left((T_n)^{-1}\right)=\frac1{(n-1)^2(n-2)}$$ Now, each of your random variables $X_k$ is distributed like $Y_k/\theta$ hence your $\bar X_n$ is distributed like $$\frac1{n\theta}T_n$$ hence, for every $n\geqslant3$, $$\mathrm{Var}\left((\bar X_n)^{-1}\right)=\frac{n^2\theta^2}{(n-1)^2(n-2)}$$</p>
"
"2394682","2394751","<blockquote>
  <p>The condition $f(1)=2$ is unnecessary.  The only functions $f:\mathbb{Q}\to\mathbb{C}$ sastisfying $$f(xy)=f(x)\,f(y)-f(x+y)+1\tag{$\star$}$$
  are the constant function $f\equiv 1$ and the function $f(x)=x+1$ for all $x\in\mathbb{Q}$.</p>
</blockquote>

<p>First, as ajotaxe observed, $f(0)=1$.  If $f(1)=1$, then we obtain from $(\star)$ that
$$f(x)=f(x\cdot 1)=f(x)\,f(1)-f(x+1)+1=f(x)-f(x+1)+1$$
for all $x\in\mathbb{Q}$.  This means $f(x+1)=1$ for all $x\in\mathbb{Q}$, which leads to $f\equiv 1$.  From now on, we assume that $f(1)\neq 1$.  </p>

<p>By plugging in $x:=1$ and $y:=-1$ into $(\star)$, we  have
$$f(-1)=f(1)\,f(-1)\,.$$
Because $f(1)\neq 1$, we must have $f(-1)=0$.  Putting $y:=-1$ yields
$$f(-x)=-f(x-1)+1\,.\tag{*}$$
Now, plugging in $x:=1$ and $y:=-2$ into $(\star)$, we obtain
$$f(-2)=f(1)\,f(-2)-f(-1)+1=f(1)\,f(-2)+1\,.$$
Using $(*)$, we have $f(-2)=-f(1)+1$, so the equation above translates to
$$-f(1)+1=-\big(f(1)\big)^2+f(1)+1\,.$$
This gives
$$\big(f(1)\big)^2-2\,f(1)=0\,.$$
That is, $f(1)=0$ or $f(1)=2$.</p>

<p>If $f(1)=0$, then $y:=1$ into $(\star)$ leads to
$$f(x)=-f(x+1)+1\,.\tag{$\Box$}$$
This proves that $f(n)=\frac{1+(-1)^n}{2}$ for every integer $n$, and from $(\Box)$, we get
$$f(x+2)=f(x)$$
for all $x\in\mathbb{Q}$.  Now, if we plug in $y:=n$ into $(\star)$, where $n$ is an even integer, then
$$f(xn)=f(x)\,f(n)-f(x+n)+1=f(x)-f(x)+1=1$$
for every $x\in\mathbb{Q}$.  Thus, substituting $x:=\frac{1}{2}$ and $n:=2$ in the previous equation, we obtain $$0=f(1)=f\left(\frac{1}{2}\cdot 2\right)=1\,,$$
which is absurd.  Hence, $f(1)\neq 0$, and so $f(1)=2$ as required.</p>

<p>Now, $(\star)$ with $y:=1$ implies that
$$f(x+1)=f(x)+1$$
for all $x\in\mathbb{Q}$.  Ergo, we can see that $f(n)=n+1$ for every integer $n$.  Consequently, for a rational number $x=\frac{p}{q}$, where $p,q\in\mathbb{Z}$ with $q&gt;0$, we have
$$p+1=f(p)=f\left(\frac{p}{q}\cdot q\right)=f\left(\frac{p}{q}\right)\,f(q)-f\left(\frac{p}{q}+q\right)+1=(q+1)\,f\left(\frac{p}{q}\right)-f\left(\frac{p}{q}\right)-q+1\,.$$
Hence,
$$f(x)=f\left(\frac{p}{q}\right)=\frac{p+q}{q}=\frac{p}{q}+1=x+1\,,$$
as desired.</p>

<p><strong>P.S.:</strong> It is a very interesting question what happens if the domain extends to $\mathbb{R}$.  All I know is that, if a nonconstant function $f:\mathbb{R}\to\mathbb{Q}$ satisfies $(\star)$, then
$$f(x+r)=f(x)+r$$
and
$$f(rx)=r\,f(x)-r+1$$
for all $x\in\mathbb{R}$ and $r\in\mathbb{Q}$.</p>
"
"2394683","2394713","<p>1) An essentially bounded function is exactly what you described. The $f$ you gave is not essentially bounded, because for any real number $M$, there exists $\varepsilon&gt;0$ such that $f(x)&gt;M$ for all $x\in (0,\varepsilon)$. This shows that it is not essentially bounded because $(0,\varepsilon)$ has positive measure.</p>

<p>2) You wrote in part (1) that ""the inequality holds on some set $E$ such that $(0,1)\setminus E$ has zero measure"". This means precisely that the inequality holds almost everywhere. Your example doesn't apply because that $f$ is not essentially bounded. Consider the function $g:(0,1)\to\mathbb{R}$ given by
$$
g(x) = \begin{cases}
n &amp; \text{if}\ x=1/n\ \text{for some}\ n\in\mathbb{N},\,n\geq 2 \\
0 &amp; \text{otherwise}.
\end{cases}
$$
Then $g(x)=0\leq 1$ for all $x\in(0,1)\setminus\{1/n\mid n\in\mathbb{N},\,n\geq 2\}$ and $\{1/n \mid n\in\mathbb{N},\,n\geq2\}$ is a measure zero set. Thus $g=0$ a.e., which is another way of saying that the equivalence class of $g$ is equal to the equivalence class of the zero function $0$ under the ""equal a.e."" equivalence relation.</p>

<p>3) Yes, by definition $L_\infty(0,1)$ is the space of all essentially bounded functions from $(0,1)$ into $\mathbb{R}$. Some authors consider the quotient of this space by the ""equal a.e."" equivalence relation, but in practice the two are the same.</p>

<p>4) Do you mean $\|f\|_p=0$ but $f\ne0$? Consider the function $g$ given in (2). Then $\|g\|_p=0$, but $g\ne0$. Now we do have $g=0$ a.e., so some would write $g=0$ and it would be understood that the equality here is with respect to the ""equal a.e."" equivalence relation.</p>
"
"2394685","2394699","<p>Use also $x=A^{-1}b$. This should lead to a bound related to the condition number of the matrix $A$.</p>
"
"2394686","2394727","<p>One way to create thorns is with absolute value of a periodic function, such as cosine. Such a ""thorns"" function can be $$\frac{1}{|\cos x|+1}$$ I did not want my thorns to be divergent. If that's not a problem, use $|\cos x|^{-1}$. You can change periodicity of the thorns by multiplying $x$ by a factor, and amplitude of the thorns by multiplying the whole expression by a constant. Now, to apply it on top of the cosine function, you can add (thorns will point always up), subtract (thorns will point down), or multiply (thorns will point away from the axis)</p>

<pre><code>plot ((1/(abs(cos(x*5))+1)+cos(x/2)), for 0&lt;x&lt;50
plot ((-1/(abs(cos(x*5))+1)+cos(x/2)), for 0&lt;x&lt;50
plot ((1/(abs(cos(x*5))+1)*cos(x/2)), for 0&lt;x&lt;50
</code></pre>
"
"2394703","2394710","<p>By the definition of a category, $Hom(A_1,A_1)$ must contain the identity morphism on $A_1$. By the definition of initial object, there is only one morphism from $A_1\to A_1$, so that's all there is.</p>

<p>Now $fg$ is a morphism from $A_1\to A_1$, and guess what: there's only one thing it could be!</p>
"
"2394716","2394724","<p>Hint: If $y = \frac{x+1}{2}$  then $\frac{x}{2}-\lfloor\frac{x+1}{2}\rfloor = y-\lfloor y\rfloor-1/2$. Can you the supremum and infimum?</p>
"
"2394721","2394750","<p>There is a quick little approximation for CIs for sequences of bernoulli trials.</p>

<p>$$ \dfrac{n_F}{n} \pm z_{1-\alpha/2}\sqrt{\dfrac{n_Mn_F}{n}}$$</p>

<p>Here, $z_{1-\alpha/2}$ is the $1-\alpha/2$ quantile from the standard normal distribution, $n_F$ is the number of observed females, $n_M$ is the number of observed males, and $n$ is the sum of both males and females.</p>

<p>So, doing some plugging and chugging yields...</p>

<p>$$ \dfrac{260}{1200} \pm \dfrac{1.96}{1200} \sqrt{\dfrac{260\cdot (1200-260)}{1200}} $$</p>

<p>So the CI is approximately $[0.19,0.24]$</p>

<p>It may be worth noting that this estimate is called a Wald Interval and is known to be a bad estimate over all.  Agresti-Coulli intervals for binomial proportions are better, but I doubt that is what your instructor is asking.</p>
"
"2394736","2394777","<p>You need <strong>one</strong> solution of 
$$f'' + \frac{\phi}{2} f = 0$$ 
so that $$(-2\frac{f'}{f})' = -2\frac{f''}{f}+2\frac{f'^2}{f^2}=\phi+\frac{1}{2}(-2\frac{f'}{f})^2$$</p>

<p>and with $ g' = \frac{C}{f^2},\frac{g''}{g'} = -2\frac{f'}{f}$ you get</p>

<p>$$S(g) = \phi$$</p>
"
"2394740","2394825","<p>Any finite strictly totally ordered set has a unique strict order-isomorphism to a unique initial segment of $\mathbb N$. More precisely: If $(I; \prec)$ is a strict finite total order there is a unique $n \in \mathbb N$ (namely $n = \operatorname{card}(I)$) with a unique strict order isomorphism</p>

<p>$$
\pi \colon (I; \prec) \to ( \{1,2, \ldots, n \}; &lt;),
$$
given by </p>

<ul>
<li>$\pi(\min(I; \prec)) = 1$ and </li>
<li>$\pi(\min(I \setminus \pi^{-1}\{1, 2, \ldots, k \}; \prec)) = k+1$. (*)</li>
</ul>

<p>Now use the regular Recursion Theorem.</p>

<p>(*) On the surface it seems like I'm using the Recursion Theorem for $(I; \prec)$ to define $\pi$ but I really don't. The existence of $\pi$ follows easily by picking any bijection $f \colon I \to \{1, 2, \ldots, n\}$ together with a permutation $\sigma \colon \{1,2, \ldots, n \} \to \{ 1,2, \ldots, n\}$ such that for all $i,j \in I$
$$
i \prec j \iff \sigma(f(i)) &lt; \sigma(f(j)).
$$
The existence of $\sigma$ can be proved by the regular Recursion Theorem.</p>
"
"2394761","2394919","<p>In order to prove that $(1)$ is finite you just have to compute the radius of convergence of a power series, namely
$$ an\,\phantom{}_2 F_1\left(1-c,1-n; 2;-a\right). $$
Since by the ratio test such radius of convergence equals one, the exchange of $\int$ and $\sum$ is allowed for any $a\in(0,1)$ and the $a=1$ case can be studied as a separate instance.</p>

<p>The exchange of $\int$ and $\sum$ in the $a\in(0,1)$ case then leads to the identity</p>

<p>$$ \int_{0}^{+\infty}cx^{-1-c}\left(1-\left(\frac{1+(1-a)bx}{1+bx}\right)^n\right)\,dx=cb^c\sum_{k\geq 1}(-1)^{k+1}a^k\binom{n}{k} B(c,k-c) $$
wher $B$ is Euler's Beta function. Both sides equals
$$ \frac{\pi nac b^c}{\sin(\pi c)}\phantom{}_2 F_1\left(1-c,1-n;2;a\right)=\frac{\pi n c b^c a(1-a)^{c+n}}{\sin(\pi c)}\phantom{}_2 F_1\left(c+1,n+1;2;a\right)$$
where the last identity is a consequence of <a href=""https://en.wikipedia.org/wiki/Hypergeometric_function#Transformation_formulas"" rel=""nofollow noreferrer"">Euler's transformations</a>.</p>
"
"2394767","2395624","<p>The family $\mathfrak{M} = \{ M(S,V) : S \in \mathfrak{S}, V \in \mathfrak{V}\}$ need not consist of open sets. Some authors define a neighbourhood of a point $x$ as an open set containing $x$, but the other convention - more widespread according to my experience - is that a neighbourhood of $x$ is a set that contains an open set containing $x$, so every superset (open or not) of a neighbourhood is again a neighbourhood.</p>

<p>As your example shows, the sets $M(S,V)$ are in general not open, even if $V$ is open. And defining an open neighbourhood base in the $\mathfrak{S}$-topology is not entirely straightforward, one would have to demand that $V$ is a uniform neighbourhood of $f(S)$ rather than just $f(S) \subset V$.</p>

<p>What we need to get a (translation-invariant) topology from $\mathfrak{M}$ is the compatibility condition between neighbourhood filters. Generally, that condition is</p>

<p>$$\bigl(\forall U \in \mathscr{V}(x)\bigr)\bigl(\exists V \in \mathscr{V}(x)\bigr)\bigl(\forall y \in V\bigr)\bigl(U \in \mathscr{V}(y)\bigr).$$</p>

<p>Here, for a translation-invariant topology induced by $\mathfrak{M}$, that is equivalent to the condition that for all $M(S,V)$ there is an $M(S',V')$ such that for all $f \in M(S',V')$ there is an $M(S'',V'')$ such that $f + M(S'',V'') \subset M(S,V)$. Since $M(S,U) + M(S,W) \subset M(S, U+W)$, we can make a simple choice: If $V'$ is a neighbourhood of $0$ in $F$ with $V' + V' \subset V$, then</p>

<p>$$M(S,V') + M(S,V') \subset M(S, V' + V') \subset M(S,V).\tag{$\ast$}$$</p>

<p>Using $(\ast)$ and $-M(S,V) = M(S, -V)$, we can also see that the $\mathfrak{S}$-topology makes $F^T$ a topological group (Hausdorff iff $F$ is Hausdorff and $\bigcup \mathfrak{S} = T$). However, in general it doesn't make $F^T$ a topological vector space. Although for every scalar $\lambda$ the map $f \mapsto \lambda f$ is continuous, the map $\lambda \mapsto \lambda f$ is not continuous at $0$ if there is an $S\in \mathfrak{S}$ such that $f(S)$ is unbounded. The subspace</p>

<p>$$\mathscr{B}_{\mathfrak{S}} = \bigl\{ f \in F^T : \bigl(\forall S\in \mathfrak{S}\bigr)\bigl(f(S)\text{ is bounded}\bigr)\bigr\}$$</p>

<p>is a topological vector space in the subspace topology, and a subspace $E\subset F^T$ is a topological vector space in the $\mathfrak{S}$-topology if and only if $E \subset \mathscr{B}_{\mathfrak{S}}$.</p>
"
"2394770","2394774","<p>You've misunderstood your teacher. <em>One</em> way to prove $p\implies q$ is to <em>assume</em> $\lnot q$ and from that assumption prove $\lnot p$. This is called proof by contraposition. (The contrapositive of $p\implies q$ is $\lnot q\implies\lnot p$.)</p>

<p>Of course, that is not the <em>only</em> way to prove $p\implies q$. You could also give a <em>direct</em> proof: assume $p$ and from it deduce $q$.</p>
"
"2394778","2395520","<p>There are two conventions for the meaning of the word <em>conformal</em>.</p>

<ol>
<li>conformal = biholomorphic, and</li>
<li>conformal = locally biholomorphic.</li>
</ol>

<p>Here, the first convention is used, since the assertion becomes wrong in the second convention:</p>

<p>$$f_n \colon z \mapsto \frac{1}{n}\bigl(e^{nz}-1\bigr)$$</p>

<p>is, for every $n &gt; 0$, a locally biholomorphic function with $f_n(0) = 0$ and $f_n'(0) = 1$. For large enough $n$, the rectangle $R_n = \{ z \in \mathbb{C} : 0 \leqslant \operatorname{Re} z \leqslant 2\log n/n, \lvert \operatorname{Im} z\rvert \leqslant 2\pi/n\}$ is contained in the unit disk. The map $g_n \colon z \mapsto \frac{1}{n} e^{nz}$ maps $R_n$ onto the annulus $A_n = \bigl\{ w\in \mathbb{C} :\frac{1}{n} \leqslant \lvert w\rvert \leqslant 2\bigr\}$, and $A_n - \frac{1}{n}$ contains the unit circle.</p>

<p>Thus the problem asks to prove that the image of a <a href=""https://en.wikipedia.org/wiki/De_Branges&#39;s_theorem#Schlicht_functions"" rel=""nofollow noreferrer"">schlicht</a> function does not contain the entire unit circle. That it can contain all of the unit circle with the exception of a single point is shown by the Koebe functions</p>

<p>$$f_{\alpha} \colon z \mapsto \frac{z}{(1 - \alpha z)^2}$$</p>

<p>with $\lvert \alpha\rvert = 1$. We have $f_{\alpha}(D) = \mathbb{C} \setminus \{ -\overline{\alpha}\cdot t : t \geqslant 1/4\}$.</p>

<p>Now to prove that the image of a schlicht function cannot contain the whole unit circle, we assume that $f$ were a schlicht function with $f(D) \supset \partial D$. Since $f(D)$ is simply connected, it then follows that $f(D) \supset \overline{D}$. By the maximum modulus principle, $g = f^{-1}\lvert_D$ maps the unit disk into itself. Also, $g(0) = 0$ and $g'(0) = 1$. The Schwarz lemma says that then $g = \operatorname{id}_D$, and consequently $f = \operatorname{id}_D$, but that contradicts $f(D) \supset \partial D$.</p>
"
"2394786","2394800","<p>Consider the function $g(t)=e^t-(2t+1)$. We see that $g(0)=0$. Assume that there exists $c\in(0,\log 2)$ such that $g(c)=0$. Now, apply Rolle's theorem (which is only a special case of MVT). You should arrive at a contradiction quickly.</p>
"
"2394788","2395666","<p>The second component of the right adjoint $N_0$ amounts to taking the restriction to the kernels; and in particular, the kernel of $\pi_A:A\times C\to A$ is simply $(0,1):C\to A\times C$. Let us denote $\kappa : Ker(\sigma_0)\to C$ the restriction of $(1_A,b)^{\sharp}$ through the kernels, i.e. the only map such that
$$(0,1)\circ \kappa=(1_A,b)^{\sharp}\circ \ker(\sigma_0).$$
Then $\kappa$ is the second component of the image of $(1_A,b)^\sharp$ by the right adjoint. Composing both sides by $\pi_C$ and $\eta_B$ gives you
$$\kappa\circ \eta_B=\pi_C\circ (0,1)\circ \kappa\circ \eta_B=\pi_C\circ (1_A,b)^{\sharp}\circ \ker(\sigma_0)\circ \eta_B.$$</p>

<p>Your map $(1_A,b)^{\sharp}$ is defined by the adjunction, so that applying the right adjoint and composing with $\eta$ sould give you back $(1_A,b)$. In particular, the second component of this is exactly the left-hand side in the equation above, which must then be equal to $b$, and then the right-hand side must be equal to $b$.</p>
"
"2394789","2394854","<p>Here are two examples.</p>

<hr>

<p>(Example 1): Consider $R := k[x_{1},x_{2},\{x_{1}/x_{2}^{n} \;:\; n &gt; 0\}]$. A basis for $R$ as a $k$-vector subspace of $k(x_{1},x_{2})$ is the collection $\{x_{1}^{i_{1}}x_{2}^{i_{2}}\}_{i_{1} \ge 0 , i_{2} \ge 0} \cup \{x_{1}^{i_{1}}x_{2}^{i_{2}}\}_{i_{1} &gt; 0 , i_{2} &lt; 0}$ of monomials.</p>

<p>Claim: The ring $R$ is not Noetherian (so it is not isomorphic to a localization of $k[x_{1},x_{2}]$).</p>

<p>Proof: For $n &gt; 0$, let $\mathfrak{a}_{n}$ be the ideal of $R$ generated by $x_{1}/x_{2}^{n}$. We show that the inclusion $\mathfrak{a}_{n} \subset \mathfrak{a}_{n+1}$ is not an equality for all $n &gt; 0$. Suppose there exists $r \in R$ such that $r(x_{1}/x_{2}^{n}) = x_{1}/x_{2}^{n+1}$. This implies $r = 1/x_{2}$ using the multiplication law in the fraction field $k(x_{1},x_{2})$; this is a contradiction since $1/x_{2} \not\in R$.</p>

<hr>

<p>(Example 2): Consider $R := k[x_{1},x_{2},\frac{x_{1}}{x_{2}}] = k[x_{2},\frac{x_{1}}{x_{2}}]$. Since $R$ is abstractly isomorphic to the polynomial ring $k[x,y]$, the canonical inclusion $k^{\times} \to R^{\times}$ is an isomorphism.</p>

<p>We have the following description of the units of a localization of a UFD:</p>

<blockquote>
  <p>Let $A$ be a UFD, let $W$ be a set of irreducible elements of $A$ that are pairwise coprime, and let $S$ be the multiplicative subset of $A$ generated by $W$. Then the canonical inclusion $$ \textstyle A^{\times} \oplus \bigoplus_{w \in W} \mathbb{Z} \to (S^{-1}A)^{\times} $$ is an isomorphism (of abelian groups).</p>
</blockquote>

<p>This shows that the inclusion $k[x_{1},x_{2}] \subset R$ is not of the form $k[x_{1},x_{2}] \to S^{-1}(k[x_{1},x_{2}])$ for a nontrivial multiplicative subset $S$ of $k[x_{1},x_{2}]$.</p>
"
"2394801","2394860","<p>Suppose we consider a sum $f(x) = \sum_{j=1}^\infty f_j(x)$ to be defined inductively.  I'll want to choose these so that for some increasing sequence $n_k$, 
$|\Delta_{n_k}(f)| &gt; n_k^{-\epsilon}$.  In fact, I'll take $$\left|\Delta_{n_k}\left(\sum_{j=1}^{k}f_j
\right)\right| &gt; 2 n_k^{-\epsilon}$$
and require 
$$ \sup_{x \in [0,1]}|f_j(x)| \le 2^{-j} n_k^{-\epsilon} \ \text{for}\ j &gt; k $$
Given $f_1, \ldots, f_{k-1}$ and $n_1, \ldots, n_{k-1}$, let $B_k$ be the bound on $\sup_{x \in [0,1]} |f_k(x)|$ arising from these.
Take $n_k$ large enough that $B_k &gt; 2 n_k^{-\epsilon}$.   Let $g$ be a continuous function whose graph consists of narrow triangles of height $B_k$ centred at the multiples of $1/n_k$ (so that $R_{n_k}(g) = B_k$), narrow enough that $\int_0^1 g \; dx &lt; B_k - 2 n_k^{-\epsilon}$.  Thus $\Delta_{n_k}(g) &gt; 2 n_k^{-\epsilon}$.  Take $f_k = \pm g$, the sign chosen to be the same as that of $\Delta_{n_k}\left(\sum_{j=1}^{k-1} f_j\right)$, so that $\Delta_{n_k}\left(\sum_{j=1}^k f_j \right) \ge 2 n_k^{-\epsilon}$. </p>
"
"2394804","2394826","<p>If you have proven that if $V_{p^k} = V_{p^{k+1}}$ then $V_{p^m} = V_{p^k}$ for $m\geq k$ you're almost there. You have to notice that $V_{p^k}$ is a vector space. It is a subspace of $V$ so it can maximal be of dimension $n$. So it has to stop ""growing"" at some point. And it will do so latest for $n$....</p>
"
"2394807","2394823","<p>Studying remainders has to be the place to start. So we have $2$ numbers divisible by three (=remainder $0$), $2$ numbers with remainder $1$ and $3$ numbers with remainder $2$. </p>

<p>The remainder-$0$ numbers can be inserted at any point within or after a sequence of the other numbers that otherwise qualifies; they will not there ""break"" a qualifying sequence and will not ""fix"" a non-qualifying sequence.</p>

<p>So we can focus on the other $5$ numbers first: </p>

<ul>
<li><p>If we start with a rem $1$ number, another $1$ must follow but then we have only rem $2$ numbers left, and we cannot avoid a running total divisible by $3$.</p></li>
<li><p>If we start with a rem-$2$ number, another $2$ must follow, then $1$ and $2$ to avoid $6$, finally $1$.</p></li>
</ul>

<p>So only a remainder sequence matching  $22121$ is suitable.  We have $3!=6$ options for placing the remainder-$2$ numbers, $2!=2$ for the rem-$1$s. Then we can insert the rem-$0$ numbers in $\binom 62 = 15$ patterns and $2$ orders.</p>

<p>So we have $3!\cdot 2! \cdot \binom 62 \cdot 2 =360$ options.</p>
"
"2394810","2394893","<p>I assume that $P_4(\mathbb{C})$ is the vector space of the polynomials of the complex variable $X$ of degree $\leq 4$ and $p',p'',\cdots$ are the derivatives of the polynomial $p$. </p>

<p>The matrix of $N$ is $5\times 5$ and is nilpotent.
Indeed, $N^5=0$ and $N^4\not= 0$ (because $N^4(X^4)\not= 0$).</p>

<p>Then the jordan form of $N$ is $J_5$, the nilpotent jordan block of dimension $5$. A basis that ""Jordanizes"" $N$ is $\{N^4(X^4),N^3(X^4),N^2(X^4),N(X^4),X^4\}$.</p>
"
"2394815","2394993","<p>What I said in the comments is true, and goes by the name</p>

<p><strong>PrÃ¼fer's First Theorem</strong>: An abelian $p$-group $G$ with bounded exponent (an integer $k$ such that $g^k=1$ for all $g\in G$) is a direct sum of cyclic subgroups.</p>

<p>The proof is by induction on $k=p^e$, the base case $e=1$ being the vector-space case.</p>

<p>For the inductive step, write $pG=\oplus_\alpha\langle g_\alpha\rangle$, and choose $h_\alpha$ in $G$ with $ph_\alpha=g_\alpha$. Then the $h_\alpha$ generate a subgroup of $G$ (call it $H$) that is a direct sum of $\langle h_\alpha\rangle$. Let $L$ be a subgroup of $G$, maximal with respect to having trivial intersection with $H$. Then $L$ is also a direct sum of cyclic subgroups (by the vector-space case), and you can show $G=H\oplus L$.</p>

<p>Reference: <em>Fundamentals of the Theory of Groups</em>, by Kargapolov and Merzljakov, $\S$10.</p>
"
"2394827","2394887","<p><strong>Hint about your instructor's hint:</strong>  The hint suggests that you look at $[H:G]$ by considering $[G[\sqrt{a}]:G]$ and $[G[\sqrt[3]{b}]:G]$ which both equal $[H:G]$.  Now since $\sqrt{a}\not\in\Bbb{Z}$ and $\sqrt[3]{b}\not\in\Bbb{Z}$, $x^2-a$ and $x^3-b$ are irreducible over $\Bbb{Z}$.  Looking at each of those polynomials over $G$ should give you a possibility for $[H:G]$ and considering them together will give the answer.</p>

<blockquote class=""spoiler"">
  <p>On the one hand considering $H=G[\sqrt{a}]$, we see $[H:G]|2$, but $H=G[\sqrt[3]{b}]$ tells us $[H:G]|3$.  Together these imply $[H:G]=1$</p>
</blockquote>
"
"2394835","2394856","<p>For independent events $A,B$ (<em>such as each attempt at firing the gun after having spun the chamber</em>) we have the probability of both occurring as $Pr(A\cap B)=Pr(A)\cdot Pr(B)$</p>

<blockquote class=""spoiler"">
  <p> More generally if $A_1,A_2,\dots,A_k$ are all mutually independent we have $Pr(A_1\cap A_2\cap \dots \cap A_k)=Pr(A_1)Pr(A_2)\cdots Pr(A_k)$</p>
</blockquote>

<p><em>N.B.  It is not true in general that $Pr(A\cap B)=Pr(A)Pr(B)$.  This is only true for independent events and is false for dependent events.</em></p>

<p>The answer of approximately $0.8024$ is the answer to a slightly different question.  That of if there happened to have been <em>two</em> bullets in the chambers instead of one where we still ask the probability of death within 4 tries.</p>

<blockquote class=""spoiler"">
  <p> $1-(\frac{4}{6})^4=\frac{65}{81}\approx 0.802469$</p>
</blockquote>

<p>For our question, we ask instead the related but different question of what is the probability that he <em>survives</em> four attempts.  That is, he needs to have missed the bullet four times in a row.  Each time he has a $\frac{5}{6}$ chance of landing on a chamber that doesn't contain a bullet.  Using what we know about independent events the probability of not getting a bullet four times in a row and thus surviving is...</p>

<blockquote class=""spoiler"">
  <p> $(\frac{5}{6})^4$</p>
</blockquote>

<p>So the probability that he doesn't survive is then $1$ minus that which is...</p>

<blockquote class=""spoiler"">
  <p> $1-(\frac{5}{6})^4\approx 0.5177$</p>
</blockquote>
"
"2394840","2394857","<p>Some concerns about your attempt: </p>

<ul>
<li>the $\varepsilon$ is not used in the sequel of the proof.</li>
<li>It is not clear that $w\gt a$ and it has to be justified.</li>
</ul>

<p>Here is a sketch of solution (the details have to be filled to not spoil the exercise).
Using the assumption $f(a)=0$ and the fundamental theorem of calculus, we have 
$$\tag{*}    f(x)=\int_a^x f'(t)  \mathrm dt  $$
hence by the triangle inequality and $ \left\lvert f'(t)\right\rvert\leqslant C \left\lvert f(t)\right\rvert$, we get
$$\tag{**}   \left\lvert f(x)\right\rvert \leqslant C\int_a^x \left\lvert f(t)\right\rvert\mathrm dt\leqslant C(x-a)\sup_{s\in (a,b)} \left\lvert f(s)\right\rvert.$$
Now, going back to $(*)$ we have 
$$\left\lvert f(x)\right\rvert \leqslant \int_a^x\left\lvert f'(t)\right\rvert\mathrm dt\leqslant C\int_a^x\left\lvert f(t)\right\rvert\mathrm dt       $$
and using (**) with $t$ instead of $x$, we get that 
$$\left\lvert f(x)\right\rvert\leqslant\frac 12  C^2(x-a)^2\sup_{s\in (a,b)} \left\lvert f(s)\right\rvert.$$
This suggests that for any $n$, there exists $c_n$ such that for any $x\in (a,b)$, $ \left\lvert f(x)\right\rvert  \leqslant c_n\left(x-a\right)^n$, where $c_n$ has to be determined.   We will find that $c_n=C^n/n! \sup_{s\in (a,b)} \left\lvert f(s)\right\rvert$ does the job.</p>
"
"2394847","2394863","<p>Of course, we can use the fundamental theorem of calculus in a more direct way. Let $0 &lt; \lvert h\rvert &lt; 1-\frac{1}{n}$ and $x\in [0, 1]$. Then, \begin{align*} \frac{f_n(x+h)-f_n(h)}{h} &amp;= \frac{n}{2h}\left[\int_{x+h-1/n}^{x+h+1/n} f(t)\,\mathrm{d}t-\int_{x-1/n}^{x+1/n} f(t)\,\mathrm{d}t\right] \\ &amp;= \frac{n}{2h}\left[\int_{x+1/n}^{x+h+1/n} f(t)\,\mathrm{d}t-\int_{x-1/n}^{x+h-1/n} f(t)\,\mathrm{d}t\right] \end{align*} so $$\lim_{h\to 0} \frac{f_n(x+h)-f_n(h)}{h} = \frac{n}{2}[f(x+1/n)-f(x-1/n)]$$ by the FTC. Therefore, $f_n'(x) = \frac{n}{2}[f(x+1/n)-f(x-1/n)]$ for $x\in [0, 1]$.</p>
"
"2394852","2394934","<p>First, one should have in mind that for a finite-degree separable field extension $L/K$, the <em>trace</em> <em>pairing</em> $\langle x,y\rangle={\mathrm tr}^L_K xy$ is <em>non-degenerate</em>. Here the trace is Galois trace, which, notably, does not depend on the extension being Galois. Rather, ${\mathrm tr}x$ is the sum, in a Galois closure of $L$ over $K$, of all the conjugates of $x$... which, by Galois theory, lies in the ground field $K$, in fact.</p>

<p>Thus, for example, for a finite (unavoidably separable, because characteristic is $0$) extension $k/\mathbb Q_p$, trace is non-degenerate, and is ""sum of conjugates"". An often unasked question amounts to comparison of the ""global trace"" (of number field $K/\mathbb Q$) to the <em>local</em> traces $K_v/\mathbb Q_p$, where $v$ runs through places/primes of $K$ lying over $p$. A very useful point, too-often mistakenly suppressed because it's ""not sufficiently elementary"", is that $K\otimes_{\mathbb Q} \mathbb Q_p\approx \bigoplus_{v/p} K_v$. In this context, it is not at all surprising that ""the global trace is the sum of the local traces"".</p>

<p>And, yes, for number field extensions $K/k$ and for local field extensions $K/k$, the <em>inverse</em> different is the fractional-ideal inverse of $\{x\in K: {\mathrm tr}x\mathfrak o_K\subset \mathfrak o_k\}$. In both cases, in addition to giving a duality by the trace pairing, there are strong connections to ramification.</p>

<p>(Also, I'd suggest that K. Iwasawa be mentioned in this context, since, after all, he gave an ICM talk in 1950 about doing this sort of thing. Not to mention Matchett's 1946 thesis under Artin about similar matters. ""It was in the air."")</p>
"
"2394853","2394904","<p>First, a general observation: if a finite measure $\mu$ on $\mathbb{R}$ has no atoms, then the diagonal $\{(x,x)\in\mathbb{R}^2\}$ has zero measure with respect to the product measure $\mu\times \mu$. To see why, partition $\mathbb R$ into $n$ intervals of measure $1/n$, and observe that the diagonal is covered by $n$ squares, each of which gets product measure $1/n^2$.  </p>

<p>So we integrate over $x\ne y$; by symmetry, it suffices to integrate over $x&lt;y$. Write $x,y$ in base-3 as </p>

<p>$$x = 0.\underset{n \text{ digits}}{\underbrace{\cdots}} 0\cdots , \quad y = 0.\underset{\text{same digits}}{\underbrace{\cdots}} 2\cdots $$
where $n$ is a nonnegative integer. Note that $|x-y|\ge 3^{-n-1}$. Also, the measure of all pairs $(x,y)$ as above is 
$$
2^{n}(1/2)^{2(n+1)} = \frac14\cdot 2^{-n}
$$
because there are $2^n$ choices of $n$ digits of $x$, and because fixing the first $(n+1)$ digits of a number restricts it to a subset of measure $(1/2)^{n+1}$. </p>

<p>Putting it all together, 
$$
\iint_{x&lt;y} |x-y|^{-s}\,d\mu(x)\,d\mu(y) \le \sum_{n=0}^\infty 3^{s(n+1)} \frac14\cdot 2^{-n}  = \frac{3^s}{4} \sum_{n=0}^\infty (3^s/2)^n
$$
which converges when $3^s&lt;2$.</p>
"
"2394866","2394874","<p>Looks like a sign error in the very last line.</p>

<p>The top multiplies out to be $a^2-pa-pu-u^2$, but then $-pu-u^2=q$, so that it is equal to the denominator.</p>

<p>In the other example you gave,</p>

<p>$$ \frac{a^{2}-ab-b^{2}u-b^{2}u^{2}}{a^{2}-ab+b^{2}}  \\ = \frac{a^{2}-ab-b^{2}(u+u^{2})}{a^{2}-ab+b^{2}}=\frac{a^2-ab-b^2(-1)}{a^2-ab+b^2} =1 $$.</p>

<p>You didn't make any mistakes. You simply failed to use the relation $u^2+u+1=0$.</p>
"
"2394867","2395019","<p>Define
$$f(x) = \sin x - x + \frac{1}{6}x^3$$
and
$$g(x) = x - \frac{1}{6}x^3 + \frac{1}{120}x^5 - \sin x$$
We want to show $f(x) &gt;0$ and $g(x) &gt; 0$ for $x &gt; 0$.</p>

<p>Notice that $f$ and its first four derivatives vanish at $x=0$.  If $f$ has a positive zero, say $f(a_0) = 0$, then by Rolle's Theorem $f'$ has a zero $a_1$ with $0 &lt; a_1 &lt; a_0$.  Repeating this argument, we have $0 &lt; a_4 &lt; a_3 &lt; a_2 &lt; a_1 &lt; a_0$ with $f^{(n)}(a_n) = 0$.  Since $f^{(4)}(x) = \sin x$, we must have $\pi \le a_4$, so $\pi &lt; a_0$.  This shows $f(x)$ does not change sign for $0&lt;x&lt;\pi$.  Since $-x + (1/6) x^3 &gt; 3/2$ and is increasing for $x &gt; 3$ while $\sin x \ge -1$, we see that $f(x) &gt; 0$ for all $x &gt; 0$.</p>

<p>To see that $g(x) &gt; 0$ for $x&gt;0$, observe that $g(0) = g'(0) = 0$, $g''(x) = f(x)$, and $f(x) &gt; 0$ for $x&gt;0$, as we just showed.</p>
"
"2394875","2394900","<p>Except for what I assume to be a typo on the answer to <code>3</code>, what you've written is correct (or more accurately, not wrong), although it seems you've missed out the actual negation of the statements which is what you're after.</p>

<p>Essentially, all you need is the rule</p>

<p>$$\neg((\forall x)\quad P(x))\iff((\exists x)\quad \neg P(x))$$</p>

<p>as well as the more basic $\neg\neg P\iff P$.</p>

<hr>

<p>Let</p>

<p>$$S_1\equivÂ¬((âx)\quad F(x,\text{me}))$$</p>

<p>$$S_2\equiv(âx)\quad F(x,\text{Fred})$$</p>

<p>$$S_3\equiv(âx)\quad((ây)\quad F(x,y))$$</p>

<p>Then you're after $\neg S_1,\neg S_2,\neg S_3$.</p>

<p>As has been noted in the comments, the first is simple</p>

<p>$$\neg S_1\equiv\neg\neg((âx)\quad F(x,\text{me}))$$</p>

<p>$$\neg S_1\equiv(âx)\quad F(x,\text{me})$$</p>

<p>Your working for the second is helpful and we get</p>

<p>$$\neg S_2\equiv\neg((âx)\quad F(x,\text{Fred}))$$</p>

<p>$$\equiv\negÂ¬((âx)\quadÂ¬F(x,\text{Fred}))$$</p>

<p>$$\equiv(âx)\quadÂ¬F(x,\text{Fred})$$</p>

<p>Finally, as has been pointed out by @fleablood, the last is slightly harder, but is just applying the above rule twice</p>

<p>$$\neg S_3\equiv\neg((âx)\quad((ây)\quad F(x,y)))$$</p>

<p>$$\equiv(âx)\quad\neg((ây)\quad F(x,y))$$</p>

<p>$$\equiv(âx)\quad((ây)\quad \neg F(x,y))$$</p>
"
"2394885","2394917","<p>In the category of pointed sets, $A\to A\amalg B\to B$ is always exact, but coproducts and products are not isomorphic.  What you can say is that if $A\to A\amalg B\to B$ is always exact, then the natural map $A\amalg B\to A\times B$ always has trivial kernel.  Indeed, the kernel of $A\amalg B\to A\times B$ is just the pullback of the kernels of the two component maps $A\amalg B\to A$ and $A\amalg B\to B$, which by assumption are the inclusions $i:A\to A\amalg B$ and $j:B\to A\amalg B$.  Now suppose $f:C\to A$ and $g:C\to B$ are maps such that $if=jg$.  Composing $if=jg$ with the map $(1,0):A\amalg B\to A$ we find $f=(1,0)if=(1,0)jg=0$.  Similarly, $g=0$.  This shows that the pullback of $i$ and $j$ is $0$, as desired.</p>

<p>As for a unital example, I believe you can take the category of all monoids with the property that $xy=1$ implies $x=1$ and $y=1$.  Such monoids are closed under limits and coproducts, and $A\to A\amalg B\to B$ is always exact for them.  The map $A\amalg B\to A\times B$ is not typically an isomorphism for such monoids though: it is surjective and has trivial kernel, but it is not injective.</p>
"
"2394888","2394895","<p>First, some definitions:  </p>

<p>The ""solutions of a polynomial equation"" in, say, the variable $x$ is the set of values of $x$ that make that equation true.  For example, the solutions of 
$$
x^3 -5x^2 -2x +24 = 0
$$
are $\{x=-2, x=3, x=4\}$ and we know this because 
$$
x^3 -5x^2 -2x +24 = (x-(-2))(x-3)(x-4)
$$
Second definition:  The ""negatives of the solutions"" is the set of values each of which is the negative of one of the values in the set of solutions.  In our example, the negatives of the solutions would be 
$$\{x=+2, x=-3, x=-4\}$$</p>

<p>And now you can see what happens to make the ""constant term"", that is, the term not involving $x$, [which is 24 in our example] equal to the product of the negatives of the solutions.  Each of the solutions $-2, 3$, and $4$ appears in our product of factors as being <strong>subtracted from</strong> x.  So the constant term is the product of the negatives of the solutions.</p>

<p>By the way, this theorem, at least in this simple form, holds only for one-variable polynomial equations.  </p>
"
"2394896","2394903","<p>You have</p>

<p>$$\lim_{a\to -\infty} \int_a^0 e^x \sin x \; dx = \left.\lim_{a\to -\infty}\frac{e^x(\sin x - \cos x)}{2}\right|_a^0  $$ $$=\lim_{a\to -\infty}\frac{e^0(\sin 0 - \cos 0)}{2}-\frac{e^a(\sin a - \cos a)}{2}  =-\frac{1}{2}-0.$$</p>
"
"2394899","2394912","<p>If $|f(t)|\leq C|t|^p$, it <em>might</em> also be the case that $|f(t)|\leq D|t|^{p+1}$, because $|t|^{p+1}\leq |t|^p$ near $t=0$. But it <em>must</em> be the case that $|f(t)|\leq C|t|^k$ for $k&lt;p$ because then $|t|^p\leq |t|^k$ near $t=0$. The statement ""$f(t)$ is $O(p)$"" does not say $p$ is the <em>sharpest</em> bound you can get, but it does imply a minimal degree of sharpness. As a result, it is best to read the statement as guaranteeing a minimal degree of vanishing, not specifying the best possible bound. That is why your book tells you to call this property vanishing to ""at least"" order $p$: they don't want you to make the mistake of thinking the order is exactly $p$.</p>

<p>Just look at a concrete example. Say you know $|f|$ is bounded by a constant times $t^2$. This means $|f|$ is also bounded by a constant times $|t|$, because $t^2&lt;|t|$ for small nonzero $t$. But you don't know whether you can do better and bound $|f|$ by a higher-order power.</p>

<p>The general point is that if $a&lt;b$ and you are interested in some $c&lt;b$, it <em>might</em> be true that $a&lt;c$, but it certainly doesn't have to be true.</p>
"
"2394915","2398257","<p>The answer is yes. If $pd_R(M)&lt;\infty$, using flatness, one has $pd_{\hat{R}}(M\otimes_R\hat{R})&lt;\infty$. Tensoring $R\to \hat{R}$ by $M$, one has a map $M\to M\otimes_R\hat{R}$ and using the map $M\times \hat{R}\to M$ by $(m,a)\mapsto am$, we get a map $M\otimes_R\hat{R}\to M$, which gives a splitting $M\otimes_R\hat{R}=M\oplus N$ fro some $N$. Since direc summand of a module of finite projective dimension must have fnite projective dimension, we are done.</p>
"
"2394929","2394939","<p>You were correct to use the <a href=""https://en.wikipedia.org/wiki/Hypergeometric_distribution"" rel=""nofollow noreferrer"">hypergeometric distribution</a> to solve the problem since the flower bulbs are selected without replacement.  The <a href=""https://en.wikipedia.org/wiki/Binomial_distribution"" rel=""nofollow noreferrer"">binomial distribution</a> is used when selections are made with replacement.  In particular, the probability of success must be the same for each outcome, which is not the case here since selecting a yellow flower changes the probability that the next flower you select is also yellow.</p>
"
"2394937","2394941","<p>You always start with the same diagram (for questions involving $3$ sets). Draw three circles which represent sets $A,B,C$ which intersect each other and form a total of $8$ regions (including the outside, which is $(A\cup B\cup C)'$).</p>

<p><a href=""https://i.stack.imgur.com/YtNwI.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YtNwI.gif"" alt=""enter image description here""></a></p>

<p>Then you locate the region described by what you're after - $(A-C)\cap(C-B)$. First mark regions $A-C$ and $C-B$. Then mark their intersection (which is empty as pointed out in the comments - so there is no region to mark in this case).</p>
"
"2394938","2394945","<p>Not that there is nothing wrong with doing what you did for the question, but all you need to say is:</p>

<blockquote>
  <p>Because $(-1, 0) \in R$, and $(0, 1) \in R$ but $(-1, 1) \notin R$, $R$ is not transitive. Hence $R$ is not an equivalence relation.</p>
</blockquote>

<p>Otherwise, everything is correct. But keep in mind there are tradeoffs between comprehensiveness and conciseness.</p>
"
"2394943","2394956","<p>I suspect by $E_\lambda$ you mean the eigenspace of $\lambda$, which can be described by
$$
E_\lambda :=\{x \mid x=0\ \text{or}\ x\ \text{is an eigenvector of $A$ with eigenvalue $\lambda$}\}
= \{x \mid Ax=\lambda x\}.
$$</p>

<p>Then $x\in E_\lambda$ iff $Ax=\lambda x$ iff
$$
0=\lambda x-Ax=\lambda I_nx-Ax = (\lambda I_n-A)x
$$
iff $x$ is in the null space of $\lambda I_n-A$.
Therefore $E_\lambda = \mathsf{N}(\lambda I_n-A)$.</p>
"
"2394946","2394990","<p>Hint: find the pointwise limit of this sequence of functions on $[0,\pi]$. If a sequence is uniformly convergent, then its limit is a continuous function. Is it continuous in this case?</p>
"
"2394947","2394984","<p>You can use the Divergence Formula for $\vec{F} = \langle x,y,z \rangle$ and you will have:</p>

<p>$$\frac 13 \iint _A \vec{F}\cdot \vec{n} \, dA = \frac 13 \iiint _V \operatorname{div} \vec{F} \, dV = \iiint_V dV = V$$</p>

<p>In a similar manner you can use the same trick for $\vec{G} = \langle x,0,0 \rangle$</p>

<p>$$\iint _A \vec{G}\cdot \vec{n} \, dA =  \iiint _V \operatorname{div} \vec{G} \, dV = \iint_V dV = V$$</p>

<p>But keep in mind the formulae aren't really that simple as in $\mathbb{R}^2$, as after all it depends on the normal vector. So sometimes we have to make the right choice for the vector field in order to get an easier computation problem.</p>
"
"2394955","2394998","<p>Let $\Pi$ denote the projection from $\mathbb R^2\to\mathbb R$, so $\mu=\Pi^*\pi$.  Then $\Pi(\operatorname{supp}(\pi))\subseteq\operatorname{supp}(\mu)$.  (As can be seen from the fact that $\operatorname{supp}(\mu)$ is the complement of the union of the open sets $O$ such that $\mu(O)=0$.  Since $\Pi$ is continuous, the inverse images $U=\Pi^{-1}(O)$ of those open sets are open, and hence the complement of $\operatorname{supp}(\pi)$ is at least as large as the union of those $U$, that is, of the complement of $\Pi^{-1}(\operatorname{supp}(\mu))$.)</p>

<p>But equality need not hold.  Let $r_n$ be an enumeration of the rationals  let $s_n = (r_n, n)$, let $S=\{s_n:n\in\mathbb N\}\subset \mathbb R^2$ and let $\pi = \sum_n 2^{-n} \delta_{s_n}$ be supported on $S$. It is easy to see that $S$ is closed (the points in $S$ are all at least distance 1 from each other) so $S=\operatorname{supp}(\pi)$.  So $\Pi(\operatorname{supp}(\pi)) = \mathbb Q $ but the support of $\mu$ is $\mathbb R$.</p>
"
"2394968","2395062","<p>The typical way to set up a ""foundation"" is via an axiomatic approach, the most famous example, of course, being <a href=""https://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory#Axioms"" rel=""nofollow noreferrer"">ZFC</a>. For a variety of philosophical and technical reasons, this will usually be a axiomatization in (some variant of) first-order logic. </p>

<p>While many categorical approaches to characterizing concepts are not presented in a first-order manner, it's not unusual that they <em>can</em> be presented in a first-order manner. So the characterization of categorical products in terms of representability is not patently first-order but the characterization of categorical products in terms of universal arrows (""universal properties"") is given a fairly standard axiomatization of the notion of a category. So you simply take the universal properties as axioms. <a href=""https://ncatlab.org/nlab/show/fully+formal+ETCS"" rel=""nofollow noreferrer"">This nLab page</a> describes an axiomatization of ETCS (the elementary theory of the category of sets) which includes axiomatizing categorical products, or in this particular axiomatizations case, pullbacks and a terminal object.</p>

<p>Simplifying the axioms there for pullbacks to products (which are just pullbacks along arrows into the terminal object) and using their notation and conventions produces (if I didn't make a mistake):
$$\begin{align}a=s(a),b=s(b)\vdash\exists_h\exists_k\forall_{h'}\forall_{k'}&amp;((t(h')=a\land t(k')=b \land s(h')=s(k'))\\&amp;\Rightarrow \exists!_j(c(h,j,h')\land c(k,j,k')))\end{align}$$
The postulated $h$ and $k$ correspond to the categorical product, in particular to the projection $\pi_1$ and $\pi_2$. This particular axiomatization uses an arrows-only formulation of category theory so there is no product object, instead the identity arrow $s(h) = s(k)$, corresponding to the source of $h$ and $k$, represents the object.</p>

<p>In something like <a href=""https://ncatlab.org/nlab/show/FOLDS"" rel=""nofollow noreferrer"">FOLDS</a>, you would have an axiom like:
$$\begin{align}A,B:\mathsf{Ob}\vdash\ &amp;\exists P:\mathsf{Ob}.\exists h:\mathsf{Hom}(P,A).\exists k:\mathsf{Hom}(P,B).\\&amp;\forall X:\mathsf{Ob}.\forall h':\mathsf{Hom}(X,A).\forall k':\mathsf{Hom}(X,B).\\&amp;\exists!j:\mathsf{Hom}(X,P).h'=h\circ j \land k' = k\circ j\end{align}$$</p>

<p>See the <a href=""https://ncatlab.org/nlab/show/ETCC"" rel=""nofollow noreferrer"">ETCC nLab page</a> for references to similar approaches for axiomatizing the category of categories or the 2-category of categories. These have been less influential than ETCS which itself is not <em>that</em> influential.</p>
"
"2394972","2395254","<p>Each qubit can be thought of as a vector in $\mathbb{C}^2$, and a system of many qubits lives in the tensor product of some copies of $\mathbb{C}^2$. For example, a three qubit system lives in $\mathbb{C}^2 \otimes \mathbb{C}^2 \otimes \mathbb{C}^2$. This means that any operator you want to apply to this system has to be a map $\mathbb{C}^8 \to \mathbb{C}^8$.</p>

<p>Gates such as $X: \mathbb{C}^2 \to \mathbb{C}^2$ can only be applied to a single qubit, and somehow you have to ""lift"" them into position so you can apply them to the correct qubit in a three qubit system. This is done by taking a tensor product with the identity operator $I$, for example the operator $I \otimes X \otimes I$ will apply the $X$ gate to the middle qubit only.</p>

<p>If you have some gate, perhaps called $C: \mathbb{C}^4 \to \mathbb{C}^4$, which operates on two qubits at once, you only have to lift in the identity operator for the one qubit which is left out. For example, $I \otimes C$ will apply the $C$ gate to the second and third qubit, and leave the first alone.</p>
"
"2394977","2394996","<p>First off, there's a big difference between row vectors and column vectors.  That's because formally, under the hood, row vectors aren't vectors at all.  They're covectors, also called linear forms or linear functionals, and what this means is that they actually represent a linear map $f:U\to \mathbb{R}$ in the same way an $n\times m$-dimensional matrix can be used to define a linear map $g: \mathbb{R}^m\to \mathbb{R}^n$ (where $U$ is the vector space containing $\bf{V}$).</p>

<p>This map is really easy to define, in normal Euclidean space it's just the dot product, or equivalently matrix multiplication:</p>

<p>\begin{equation*}
\begin{bmatrix}
   a &amp; b &amp; c
\end{bmatrix}
\begin{bmatrix}
    e \\
    f \\
    g
\end{bmatrix} = ae + bf + cg
\end{equation*}</p>

<p>Note that it doesn't make sense to multiply row vectors with row vectors, or column vectors with column vectors.  If you've seen physics, this is the motivation for the Bra-Ket notation: in the expression $\langle a | b \rangle$, $\langle a |$ is a row vector and $| b \rangle$ is a column vector.  Together, $\langle a | b \rangle = \langle a |\text{ } | b \rangle$.</p>

<p>This might seem a little silly right now, since covectors very clearly correspond to normal vectors, but that's not always the case (for example, in the infinite-dimensional spaces studied in functional analysis).</p>

<p>On the other hand, the form $$a{\bf\hat{i}} + b {\bf\hat{j}} + c {\bf\hat{k}}$$ is exactly the same as $$\begin{bmatrix}a\\ b\\ c\end{bmatrix}$$</p>

<p>I suppose the former notation is more succint in handwriting, but I've always found column vectors to be more illustrative -- you can easily tell the difference between vectors and scalars this way.  That said, the choice between the two is ultimately a matter of convenience.</p>
"
"2394978","2395569","<p>Even without an additive inverse of $\omega,$ it is not safe to give
it mutiplicative distribution over addition:
$$
1 = \omega \cdot 0 = \omega \cdot (0+0) = \omega \cdot 0 + \omega \cdot 0 = 1 + 1 = 2.
$$</p>

<p>Suppose you allow associativity of multiplication involving $\omega.$ Then
$$
1 = \omega \cdot 0 = \omega \cdot (0 \cdot 0) = (\omega \cdot 0) \cdot 0
= 1 \cdot 0 = 0.
$$</p>

<p>If $1 + \omega = \omega$ then we must give up either the additive inverse of
$\omega$ or associativity of addition with $\omega$, or else
$$
0 = \omega + (-\omega) = (1 + \omega) + (-\omega)
 = 1 + (\omega + (-\omega)) = 1 + 0 = 1.
$$</p>

<p>What about the equation $x \cdot 0 = 2$?
Is $x = 2\cdot \omega$ the solution of this equation?
We cannot use associativity to prove this, so do we just take it as a
definition that $(a\cdot\omega) \cdot 0 = a$ for all $a$?</p>

<p>What about $(x + x)\cdot 0 = 2$? Can we still rewrite the left-hand side as $x\cdot 0 + x\cdot 0$? Even when it might turn out that $x=\omega$?</p>

<p>What about $x \cdot (0 + 0) = 2?$
We already ran into trouble with this one; do we therefore have to make special rules restricting the operations we can perform on arbitrary polynomials, just in case we decide to put one equal to some positive number when ordinary arithmetic would reduce it to the zero polynomial?</p>

<p>So it's not just ""properties of $\omega$"" we have to give up;
we have trouble even with arithmetic we used to do with ordinary numbers.
I realize all of the above doesn't answer the main question (whether there's a foolproof way to tell which properties are ""safe"" for $\omega$),
but I think that question may not even come close to covering all of the difficulties caused by adjoining the definition $\omega \cdot 0 = 1$
to ordinary arithmetic.</p>
"
"2394979","2394987","<p>A set of vectors $A$ is <strong>linearly independent</strong> if whenever $x_1,\ldots,x_n$ are in $A$ and $c_1,\ldots,c_n$ are scalars satisfying
$$
c_1x_1+\cdots+c_nx_n=0,
$$
then $c_1=\cdots=c_n=0$.</p>

<p>To show that $T$ is linearly independent, we fix vectors $x_1,\ldots,x_n$ in $T$ and $c_1,\ldots,c_n$ scalars satisfying
$$
c_1x_1+\cdots+c_nx_n = 0.
$$
Then in particular $x_1,\ldots,x_n$ are in $S$. Hence the fact that $S$ is linearly independent implies $c_1=\cdots=c_n=0$. This completes the proof.</p>
"
"2394980","2395081","<p>I have no reference, but it seems to me fairly straightforward that monadic FOL would be decidable: If you only have monadic predicates, and given that any sentence in monadic FOL would have only a finite number of monadic predicates (say $n$), then you can distinguish at most $2^n$ different 'kinds' of objects in terms of them having or not having the property as expressed by the predicate for each of the $n$ predicates. And without being able to express identity (which requires an at least 2-place relation), you cannot express that there are at least two of a certain 'kind'. Ergo: if there are no models for a sentence with $2^n$ objects in its domain, then there are no models either with more than $2^n$ objects in its domain. So, to see whether some sentence is a monadic FOL valid sentence, just check see if there is a model for its negation with $2^n$ objects: if ao, then the sentence is not valid, but if not, then it is.</p>
"
"2394999","2395011","<p>For the equation to be zero for all values of $t$, the coefficients of $\cos(tw)$ and $\sin(tw)$ must be zero. That means that the constants $\lambda$, $m$ etc. must satisfy certain constraints, such as, $\lambda = 2 k m$. If, for example, $$m =\frac{w^2}{k^2+w^2},$$ and $\lambda = 2 k m$ then $m y''+\lambda y' + w^2 y =0$.</p>
"
"2395000","2395005","<p>Rank of the matrix in RREF form is equal to the number of pivot column, which is equal to the number of non-zero rows. </p>

<p>Nullity of the matrix is equal to number of column $-$ rank of the matrix.</p>

<p>But number of rows is equal to number of columns for our square matrix.</p>

<p>Hence nullity of the matrix is equal to number of rows $-$ number of non-zero rows, which is the number of zero rows.</p>
"
"2395006","2395013","<p>Of course we have $\int_{0}^{n}\sqrt[4]{1+x^4}\,dx &gt; \int_{0}^{n}x\,dx=\frac{n^2}{2}$, hence the given series is $&lt;\frac{\pi^2}{3}\approx 3.29$.<br></p>

<p>Since $a&gt;b&gt;0$ implies $a^n-b^n&gt; n(a-b)b^{n-1}$, by choosing $n=4, a=\sqrt[4]{1+x^4}$ and $b=x^{1/4}$ we have
$$ \int_{0}^{n}\sqrt[4]{1+x^4}-x\,dx &lt; \int_{0}^{n}\frac{dx}{4x^{3/4}}=n^{1/4}$$
so a lower bound for the given series is provided by
$$ \sum_{n\geq 1}\frac{1}{\frac{n^2}{2}+n^{1/4}}\approx 1.68. $$</p>
"
"2395017","2395250","<p>The proof in Wikipedia is muddled.  The final integration by parts is unnecessary for deriving the Cauchy form of the remainder $R_n$ from the integral form.  Furthermore, the statement that "" ... the last integral can be solved immediately ... "" with reference to </p>

<p>$$\tag{*}\int_a^x \frac{f^{(n+2)}(t)}{(n+1)!}(x - t)^{n+1} \, dt$$</p>

<p>is, at best, ambiguous.  Notice, also, that this is the integral form for the remainder $R_{n+1}$ not $R_n$.</p>

<p>Assuming $f^{(n+1)}$ is continuous and since $(x - t)^{n} \geqslant 0$ for $t \in [a,x]$, we can apply the (second) mean value theorem for integrals to obtain $K \in [a,x]$ such that</p>

<p>$$R_n = \int_a^x \frac{f^{(n+1)}(t)}{n!}(x - t)^{n} \, dt = \frac{f^{(n+1)}(K)}{n!} \int_a^x (x - t)^{n} \, dt \\ = \frac{f^{(n+1)}(K)}{(n+1)!}  (x - a)^{n+1}. $$</p>

<p>This is the correct Cauchy form of the remainder $R_n$ and cannot be obtained from (*) without reversing the final integration by parts in the Wikepedia proof.</p>
"
"2395020","2395022","<p>You just have to apply the <a href=""https://en.wikipedia.org/wiki/Rational_root_theorem"" rel=""noreferrer"">rational root theorem</a> in the correct way.<br>
A polynomial $p(x)=x^m+\ldots+6\in\mathbb{Z}[x]$ has a rational root iff it vanishes at some $x\in\{\pm 1,\pm 2,\pm 3,\pm 6\}$. Your polynomial does not, so it has no rational root. </p>

<p>Tsemo Aristide's answer is even more powerful: by Eisenstein's criterion (wrt $p=2$) your polynomial is irreducible over $\mathbb{Q}$.</p>
"
"2395026","2395032","<p>Note that
$$
J-\lambda I_n = 
\begin{bmatrix}
0 &amp; 1 \\
&amp; 0 &amp; 1 \\
&amp; &amp; \ddots &amp; \ddots \\
&amp; &amp; &amp; 0 &amp; 1 \\
&amp; &amp; &amp; &amp; 0
\end{bmatrix}.
$$
Thus $\dim(E_\lambda)=\dim(N(J-\lambda I_n))=1$.</p>
"
"2395039","2396159","<p>The inital conditions aren't correct as you observed. Data is prescribed along $x = r, t = \frac{-r}{3}.$ Thus, the characteristics and initial conditions should be listed as
\begin{align}
\frac{dx}{ds} = 3t;&amp; \;\; x(r,0) = r\\
\frac{dt}{ds} = 1;&amp; \;\; t(r,0) = \frac{-r}{3}\\
\frac{du}{ds} = u;&amp; \;\; u(r,0) = 1 + \cos{(r)}.
\end{align}</p>

<p>From there, we can solve the system, try to write $r$ and $s$ as functions of $x$ and $t,$ i.e. $r = r(x,t),\; s = s(x,t),$ and then ultimately $u(x,t) = u(r(x,t),s(x,t)).$</p>
"
"2395042","2396494","<p>After playing around with the above summations without any results, I realize that we can simply derive the mean and variance of X via $f_X(x)$.</p>

<p>By using Mathematica, we obtain the mean and variance as follows:
<a href=""https://i.stack.imgur.com/3xNeo.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3xNeo.jpg"" alt=""enter image description here""></a></p>

<p>where $a = a_2$, $b = a_1$, and $c = b$ in the original post.</p>
"
"2395043","2395049","<p>If the characteristic polynomial has roots in $F_p$, then you are in one of the situations described.</p>

<p>If $J =\begin{bmatrix} a &amp; 0 \\
0 &amp; b \end{bmatrix}$ then $J^{p-1}=\begin{bmatrix} a^{p-1} &amp; 0 \\
0 &amp; b^{p-1} \end{bmatrix}=I_2$.</p>

<p>If $J=\begin{bmatrix} a &amp; 1 \\
0 &amp; a \end{bmatrix}$ prove that $J^p$ is diagonal, and hence $J^{p(p-1)}=I_2$.</p>

<p>The other situation is if the characteristic polynomial doesn't have roots in $F_p$. Since the characteristic polynomial is quadratic, there exists an extension $F_p \hookrightarrow K$ of degree 2 where the characteristic polynomial has a root, and hence two roots. </p>

<p>Show that the two roots are different (hint: derivative) and hence, in this algebraic extension the Jordan form is
$$$J =\begin{bmatrix} \alpha &amp; 0 \\
0 &amp; \beta \end{bmatrix} \Rightarrow \\J^{p^2-1}=\begin{bmatrix} \alpha^{p^2-1} &amp; 0 \\
0 &amp; \beta^{p^2-1} \end{bmatrix}=I_2 \,.$$</p>
"
"2395044","2395054","<p>The OEIS sequence links to <a href=""http://www.kvant.info/zkm_sol/0001/0001.pdf"" rel=""nofollow noreferrer"">this Russian paper</a>, wherein the question is defined more clearly:</p>

<blockquote>
  <p>Ð ÑÑÑÐ°Ð½Ðµ ÐÐ½ÑÑÑÐ¸Ð¸, Ð³Ð´Ðµ Ð¿ÑÐ°Ð²Ð¸Ñ Ð¿ÑÐµÐ·Ð¸Ð´ÐµÐ½Ñ ÐÐ¸ÑÐ°ÑÐ»Ð¾ÑÐµÑ, Ð¿ÑÐ¸Ð±Ð»Ð¸Ð·Ð¸Ð»Ð¾ÑÑ
  Ð²ÑÐµÐ¼Ñ Ð½Ð¾Ð²ÑÑ Ð¿ÑÐµÐ·Ð¸Ð´ÐµÐ½ÑÑÐºÐ¸Ñ Ð²ÑÐ±Ð¾ÑÐ¾Ð². Ð ÐÐ½ÑÑÑÐ¸Ð¸ 20 000 000 Ð¸Ð·Ð±Ð¸ÑÐ°ÑÐµÐ»ÐµÐ¹, Ð¸Ð· ÐºÐ¾ÑÐ¾ÑÑÑ ÑÐ¾Ð»ÑÐºÐ¾ Ð¾Ð´Ð¸Ð½ Ð¿ÑÐ¾ÑÐµÐ½Ñ (Ð°ÑÐ¼Ð¸Ñ ÐÐ½ÑÑÑÐ¸Ð¸) Ð¿Ð¾Ð´Ð´ÐµÑÐ¶Ð¸Ð²Ð°ÐµÑ ÐÐ¸ÑÐ°ÑÐ»Ð¾ÑÐµÑÐ°. ÐÐ½ ÑÐ¾ÑÐµÑ
  Ð±ÑÑÑ Ð´ÐµÐ¼Ð¾ÐºÑÐ°ÑÐ¸ÑÐµÑÐºÐ¸ Ð¸Ð·Ð±ÑÐ°Ð½Ð½ÑÐ¼. &lt;ÐÐµÐ¼Ð¾ÐºÑÐ°ÑÐ¸ÑÐµÑÐºÐ¸Ð¼ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ð°Ð½Ð¸ÐµÐ¼> ÐÐ¸ÑÐ°ÑÐ»Ð¾ÑÐµÑ
  Ð½Ð°Ð·ÑÐ²Ð°ÐµÑ Ð²Ð¾Ñ ÑÑÐ¾: Ð²ÑÐµÑ Ð¸Ð·Ð±Ð¸ÑÐ°ÑÐµÐ»ÐµÐ¹ ÑÐ°Ð·Ð±Ð¸Ð²Ð°ÑÑ Ð½Ð° Ð½ÐµÑÐºÐ¾Ð»ÑÐºÐ¾ ÑÐ°Ð²Ð½ÑÑ Ð³ÑÑÐ¿Ð¿, Ð·Ð°ÑÐµÐ¼
  ÐºÐ°Ð¶Ð´ÑÑ Ð¸Ð· ÑÑÐ¸Ñ Ð³ÑÑÐ¿Ð¿ Ð²Ð½Ð¾Ð²Ñ ÑÐ°Ð·Ð±Ð¸Ð²Ð°ÑÑ Ð½Ð° Ð½ÐµÐºÐ¾ÑÐ¾ÑÐ¾Ðµ ÐºÐ¾Ð»Ð¸ÑÐµÑÑÐ²Ð¾ ÑÐ°Ð²Ð½ÑÑ Ð³ÑÑÐ¿Ð¿, Ð·Ð°ÑÐµÐ¼ ÑÑÐ¸ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ Ð³ÑÑÐ¿Ð¿Ñ ÑÐ½Ð¾Ð²Ð° ÑÐ°Ð·Ð±Ð¸Ð²Ð°ÑÑ Ð½Ð° ÑÐ°Ð²Ð½ÑÐµ Ð³ÑÑÐ¿Ð¿Ñ Ð¸ ÑÐ°Ðº Ð´Ð°Ð»ÐµÐµ; Ð² ÑÐ°Ð¼ÑÑ
  Ð¼ÐµÐ»ÐºÐ¸Ñ Ð³ÑÑÐ¿Ð¿Ð°Ñ Ð²ÑÐ±Ð¸ÑÐ°ÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð¸ÑÐµÐ»Ñ Ð³ÑÑÐ¿Ð¿Ñ â Ð²ÑÐ±Ð¾ÑÑÐ¸ÐºÐ°, Ð·Ð°ÑÐµÐ¼ Ð²ÑÐ±Ð¾ÑÑÐ¸ÐºÐ¸ Ð²ÑÐ±Ð¸ÑÐ°ÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð¸ÑÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð² ÐµÑÑ Ð±Ð¾Ð»ÑÑÐµÐ¹ Ð³ÑÑÐ¿Ð¿Ðµ Ð¸ ÑÐ°Ðº Ð´Ð°Ð»ÐµÐµ;
  Ð½Ð°ÐºÐ¾Ð½ÐµÑ, Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð¸ÑÐµÐ»Ð¸ ÑÐ°Ð¼ÑÑ Ð±Ð¾Ð»ÑÑÐ¸Ñ Ð³ÑÑÐ¿Ð¿ Ð²ÑÐ±Ð¸ÑÐ°ÑÑ Ð¿ÑÐµÐ·Ð¸Ð´ÐµÐ½ÑÐ°. ÐÐ¸ÑÐ°ÑÐ»Ð¾ÑÐµÑ
  ÑÐ°Ð¼ Ð´ÐµÐ»Ð¸Ñ Ð¸Ð·Ð±Ð¸ÑÐ°ÑÐµÐ»ÐµÐ¹ Ð½Ð° Ð³ÑÑÐ¿Ð¿Ñ. ÐÐ¾Ð¶ÐµÑ Ð»Ð¸ Ð¾Ð½ ÑÐ°Ðº Ð¾ÑÐ³Ð°Ð½Ð¸Ð·Ð¾Ð²Ð°ÑÑ Ð²ÑÐ±Ð¾ÑÑ, ÑÑÐ¾Ð±Ñ
  ÐµÐ³Ð¾ Ð¸Ð·Ð±ÑÐ°Ð»Ð¸ Ð¿ÑÐµÐ·Ð¸Ð´ÐµÐ½ÑÐ¾Ð¼? (ÐÑÐ¸ ÑÐ°Ð²ÐµÐ½ÑÑÐ²Ðµ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð² Ð¿Ð¾Ð±ÐµÐ¶Ð´Ð°ÐµÑ Ð¾Ð¿Ð¿Ð¾Ð·Ð¸ÑÐ¸Ñ.)</p>
</blockquote>

<p>Translation:</p>

<blockquote>
  <p>In the country of Anchuria, where President Miraflores rules, it is the time of new presidential election. In Anchuria, only one percent of the 20,000,000 voters support Miraflores, namely the army of Anchuria. He wants to be democratically elected.  Miraflores demands: all voters be split into several equal groups, then each of these groups be again divided into a number of equal groups, and those last groups be again divided into equal groups and so on; in the smallest groups, select a representative of the group - the elector, then the electors choose representatives for voting in an even larger group and so on; finally, representatives of the largest groups choose the president. Miraflores himself divides voters into groups. Can he organize elections so that he is elected president? (With equality of votes, the opposition wins.)</p>
</blockquote>

<p>(Disclaimer: I used Google Translate with some corrections by myself.)</p>

<p>Now, we can address your questions:</p>

<blockquote>
  <p>In particular, must all of the rounds have an equal number of voters?</p>
</blockquote>

<p>Yes, but not necessarily with other rounds.</p>

<blockquote>
  <p>Are rounds allowed to end in ties?</p>
</blockquote>

<p>Yes, in which case the opposition wins.</p>
"
"2395050","2395058","<p>I'm not sure how you get any of the equalities in your work, except for $B=BI_n$. Since $S$ is just some invertible matrix, how do you get $A=SAS^{-1}=AI_n$ and $BI_n=SBS^{-1}$? I suspect you are commuting $SAS^{-1}$ into $ASS^{-1}$ and similarly $SBS^{-1}$ into $BSS^{-1}$, but remember that matrix multiplication is not commutative. For example, if
$$
S:=\begin{pmatrix}
2 &amp; 0 \\ 0 &amp; 1
\end{pmatrix}
\quad\text{and}\quad
B:=\begin{pmatrix}
1 &amp; 1 \\ 1 &amp; 1
\end{pmatrix},
$$
then
$$
SBS^{-1} = \begin{pmatrix}
2 &amp; 0 \\ 0 &amp; 1
\end{pmatrix}
\begin{pmatrix}
1 &amp; 1 \\ 1 &amp; 1
\end{pmatrix}
\begin{pmatrix}
1/2 &amp; 0 \\ 0 &amp; 1
\end{pmatrix}
= \begin{pmatrix}
2 &amp; 2 \\ 1 &amp; 1
\end{pmatrix}
\begin{pmatrix}
1/2 &amp; 0 \\ 0 &amp; 1
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 2 \\ 1/2 &amp; 1
\end{pmatrix}
$$
is not equal to $B$. Also, when you write $AI_n=B$, you seem to be assuming that $A$ is equal to $B$. But this isn't true, even for similar matrices. For instance, if
$$A:=\begin{pmatrix}
0 &amp; 1 \\ 0 &amp; 0
\end{pmatrix}
\quad\text{and}\quad
B:=\begin{pmatrix}
0 &amp; 0 \\ 1 &amp; 0
\end{pmatrix}
\quad\text{and}\quad
S:=\begin{pmatrix}
0 &amp; -1 \\ 1 &amp; 0
\end{pmatrix}
,$$
then $A\ne B$ are similar because $S$ is invertible and
$$
SBS^{-1} = 
\begin{pmatrix}
0 &amp; -1 \\ 1 &amp; 0
\end{pmatrix}
\begin{pmatrix}
1 &amp; 0 \\ 0 &amp; 0
\end{pmatrix}
\begin{pmatrix}
0 &amp; -1 \\ 1 &amp; 0
\end{pmatrix}^{-1}
=
\begin{pmatrix}
1 &amp; 0 \\ 0 &amp; 0
\end{pmatrix}
\begin{pmatrix}
0 &amp; 1 \\ -1 &amp; 0
\end{pmatrix}
=\begin{pmatrix}
0 &amp; 1 \\ 0 &amp; 0
\end{pmatrix}
= A.
$$</p>

<hr>

<p>Now, onto the question.
We need to prove that $\sim$ is reflexive ($A\sim A$ for all $A\in\mathsf{M}_n$), symmetric ($A\sim B$ implies $B\sim A$), and transitive ($A\sim B$ and $B\sim C$ implies $A\sim C$).</p>

<p>I'll let you do reflexivity.</p>

<p>For symmetry, suppose $A\sim B$. Then there exists an invertible matrix $S$ such that $A=SBS^{-1}$. Hence $S^{-1}$ is invertible and $B=S^{-1}A(S^{-1})^{-1}$, so $B\sim A$.</p>

<p>For transitivity, assume $A\sim B$ and $B\sim C$. Then there are invertible matrices $S$ and $T$ such that $A=SBS^{-1}$ and $B=TCT^{-1}$. Then $ST$ is invertible and
$$
A = SBS^{-1} = S(TCT^{-1})S^{-1} = (ST)C(ST)^{-1},
$$
so $A\sim C$.</p>
"
"2395052","2395065","<p>Every natural number is an ordinal, but not conversely. If I can define something for all <em>ordinal</em> numbers, that gets me further than if I just define it for all <em>natural</em> numbers.</p>

<p>Sometimes this doesn't matter, but in the Baire hierarchy (and the related <a href=""https://en.wikipedia.org/wiki/Borel_hierarchy"" rel=""nofollow noreferrer"">Borel hierarchy</a>) it really does. It's a good exercise to find a sequence of functions $f_0, f_1, f_2, f_3, ...$ such that</p>

<ul>
<li><p>$f_i$ is of Baire class $i$,</p></li>
<li><p>The sequence $(f_i)_{i\in\mathbb{N}}$ converges pointwise to some function $g$, but</p></li>
<li><p>$g$ is not of any finite Baire class.</p></li>
</ul>

<p><em>(HINT: Try to build an appropriate $g$ first, then figure out what the sequence of $f_i$s should be. We can build $g$ ""in pieces"": have $g\upharpoonright [0, 1]$ be Baire class $0$, $g\upharpoonright [1, 2]$ be Baire class $1$ but not Baire class $0$, $g\upharpoonright [2,3]$ be Baire class $2$ but not Baire class $1$, etc. - can $g$ be Baire class $n$ for any finite $n$?)</em></p>

<p>This function $g$ is intuitively ""Baire class $\omega$;"" getting a precise notion of this, and going beyond $\omega$, is accomplished by extending the definition of Baire classes to arbitrary (not just finite) ordinals.</p>

<hr>

<p>Interestingly, it turns out that there <em>is</em> a limit to how far up the ordinals we have to go before we hit the point of diminishing (in fact, zero) returns: if $f$ is Baire class $\alpha$ for some ordinal $\alpha$, then $f$ is Baire class $\omega_1$ (where $\omega_1$ is the first uncountable ordinal) - in fact, $f$ is Baire class $\gamma$ for some countable $\gamma$. So while we can define ""Baire class $\omega_{\omega_7+3}+32$,"" this doesn't get us anything that we didn't get at stage $\omega_1$ anyways. (The same thing happens with the Borel hierarchy.)</p>
"
"2395053","2395057","<p>It is not enough that $a,b$ are nonnegative, it is also necessary that $a,b$ are less than $n$.  With both of these conditions, then $a\equiv b$ implies that $a=b$.  The reason is that $a\equiv b$ means that $a-b$ is an integer multiple of $n$.  However, this integer multiple cannot be $n$ or greater (else $a\ge b+n$, impossible), nor $-n$ or smaller (else $a+n\le b$, also impossible).  Hence the only integer multiple of $n$ available is $0n$.</p>
"
"2395077","2395400","<p>It seems that the assumption of uniform strong convergence gives that $g^\delta\to f$ as $\delta$ goes to zero. Indeed, we know that if $h_n\to h$ weakly, then $\left\lVert h\right\rVert\leqslant \liminf_{n\to + \infty}       \left\lVert h_n\right\rVert$. Apply this fact to $h_n=f_n-g_n^{\delta}$ for a fixed $\delta$, one gets 
$$\left\lVert f-g^{\delta}   \right\rVert_1 \leqslant 
\liminf_{n\to + \infty}       \left\lVert f_n-g_n^{\delta}\right\rVert_1\leqslant \sup_{n\geqslant 1}     \left\lVert f_n-g_n^{\delta}\right\rVert_1
 $$<br>
and the latter quantity goes to zero as $\delta$ goes to zero. </p>
"
"2395083","2395104","<p>To expand a bit on my comment.</p>

<p>Suppose you work on an arbitrary smooth manifold with local coordinates $(x_1, x_2)$. These coordinates define a basis of each tangent space given by the operators $\frac{\partial}{\partial x_1}$ and $\frac{\partial}{\partial x_2}$. For simplicity of the notation, call these operators $\mathbf{b}_{1}$ and $\mathbf{b}_{2}$, respectively. In an arbitrary smooth manifold, this is all that can be done: there is not enough structure to calculate the ""second derivatives"" $\frac{\partial\mathbf{b}_i}{\partial x_j}$.</p>

<p>The extra data that is needed is supplied by an affine connection or a covariant derivative (each determines the other). The covariant derivative is perhaps simplest to understand in terms of the <a href=""https://en.wikipedia.org/wiki/Christoffel_symbols"" rel=""nofollow noreferrer"">Christoffel symbols</a> $\Gamma^k_{i,j}$, which specify the ""second derivatives"" of the standard basis vectors:</p>

<p>$$\frac{\partial \mathbf{b}_i}{\partial x_j}=\sum_{k=1}^n\Gamma^k_{i,j}\mathbf{b}_k$$</p>

<p>The coefficients $\Gamma^k_{i,j}$ just tell how to express the partial derivative of $\mathbf{b}_i$ with respect to $x_j$ as a linear combination of the basis vectors $\mathbf{b}_k$.</p>

<p><a href=""http://www.physicspages.com/2013/12/22/christoffel-symbols-in-terms-of-the-metric-tensor/"" rel=""nofollow noreferrer"">This page</a> works out the Christoffel symbols for planar polar coordinates.</p>
"
"2395097","2395124","<p>If $M^2=M$, then the minimal polynomial of $M$ divides $\lambda(\lambda-1)$. Split your work into cases. </p>

<p>Note that it cannot be that $A$ has minimal polynomial $\lambda$ while $B$ has minimal polynomial $\lambda-1$ (or the other way around), for then they have different rank.</p>

<ul>
<li>Case one: $A$ and $B$ both have minimal polynomial $\lambda$</li>
</ul>

<p>Then each matrix is the zero matrix, and they are obviously similar.</p>

<ul>
<li>Case two: $A$ and $B$ both have minimal polynomial $\lambda-1$</li>
</ul>

<p>Then each matrix is the identity matrix, and they are obviously similar.</p>

<ul>
<li>Case three: $A$ and $B$ both have minimal polynomial $\lambda(\lambda-1)$</li>
</ul>

<p>Then $A$ and $B$ are each diagonalizable with eigenvalues $0$ and $1$, with rank equal to the number of $1$'s along their diagonalizations. So we can bring each matrix into a diagonal form where all the $1$'s on the main diagonal precede all the $0$'s. So there are change-of-basis matrices $P$ and $Q$ such that $$PAP^{-1}=QBQ^{-1}\implies A=(P^{-1}Q)B(P^{-1}Q)^{-1}$$</p>

<p>Hence $A$ and $B$ are similar.</p>
"
"2395101","2395282","<p>Let $g$ a generator of $\mathbb{F}_p^\times$. Define the cubic character $\chi_3(g^a) = \zeta_3^a,\chi_3(0)=0$ and the trivial character $\chi_0(x) = 1_{x \in \mathbb{F}_p^\times}$. Then $$\frac{1}{3}(\chi_3(x)+\overline{\chi_3(x)}+\chi_0(x)) = 1_{x \in (\mathbb{F}_p^\times)^3}$$</p>

<p>Define the Jacobi sum
$$J(\chi,\chi) =\sum_{x\in \mathbb{F}_p} \chi(x)\chi(1-x)=\sum_{x\in \mathbb{F}_p} \chi(ax)\chi(1-ax)=\chi(a)^2\sum_{x\in \mathbb{F}_p} \chi(x)\chi(a^{-1}-x), \quad a \in \mathbb{F}_p^\times$$</p>

<p>Thus 
$$\sum_{y \in \mathbb{F}_p} 1_{y^2-1 \in (\mathbb{F}_p^\times)^3} = \frac{1}{3} \sum_{y \in \mathbb{F}_p} (\chi_3(y^2-1)+\overline{\chi_3(y^2-1)}+\chi_0(y^2-1))$$
$$ =\frac{1}{3}( \chi_3(2^{-1})J(\chi_3,\chi_3)+\overline{ \chi_3(2^{-1})J(\chi_3,\chi_3)}+J(\chi_0,\chi_0))$$
If $p \equiv 1 \bmod 3$ then $\mathbb{F}_p$ contains a 3rd root of unity so $x^3 = c$ has $0$ or $3$ solutions.</p>

<p>Therefore, together with $J(\chi_0,\chi_0))=\sum_{y \in \mathbb{F}_p^\times} 1_{1-y \in \mathbb{F}_p^\times} = p-2$,</p>

<p>the point at $\infty$ and the 2 solutions $y^2-1= 0$ we find
$$\# E(\mathbb{F}_p) = 1+2+3 \sum_{y \in \mathbb{F}_p^\times} 1_{y^2-1 \in (\mathbb{F}_p^\times)^3} = p+1+\chi_3(2^{-1})J(\chi_3,\chi_3)+\overline{ \chi_3(2^{-1})J(\chi_3,\chi_3)}$$</p>
"
"2395105","2395110","<p>Due to the Cauchy-Schwarz inequality, we have
$$
\|f\chi_R\|_1 \leq \|f\|_2\|\chi_R\|_2 = (2R)^{1/2}\|f\|_2 &lt; \infty.
$$</p>
"
"2395107","2395915","<p>Taking a geometric approach is a bit tedious, but it seems to give the desired result. I'll frequently use the volumes and surfaces of <a href=""https://en.wikipedia.org/wiki/N-sphere#Volume_and_surface_area"" rel=""nofollow noreferrer"">N-spheres and N-balls</a> throughout this computation.</p>

<p>The object you are integrating is a ratio of two quadratic forms, so we can write it compactly using slightly different notation.</p>

<p>$$\int_{\mathbb R^N}\frac{\vec x^TA\vec x}{\vec x^T\vec x}\delta(\vec x)d\vec x$$</p>

<p>Where $A$ is a diagonal matrix whose diagonal entries are $\alpha_i$, and $\delta$ is a multivariate Dirac delta function. Ordinarily, we could evaluate the integral using the properties of the Dirac delta (often, the equality below is used as a definition):</p>

<p>$$\vec 0\in D\implies\int_Df(\vec x)\delta(\vec x)d\vec x=f(\vec 0)$$</p>

<p>However, in this case $f(0)$ is undefined. This means that we can't generally consider this integral to be well-defined. Luckily, since the rational function of integration is bounded and continuous almost everywhere, we can still integrate it over regions containing the origin, but we'll have to treat the delta a bit more carefully (and perhaps a bit less rigorously).</p>

<p>To begin with, consider the surface of an $N$-sphere or radius $R$ centered at the origin, which I'll denote $\Sigma(R)$, with an associated $N-1$ dimensional volume differential $d\sigma$. We know the volume of this region is the surface volume of an $N$-sphere.</p>

<p>$$\int_{\Sigma(R)}d\sigma=S_{N-1}R^{N-1}$$</p>

<p>Where $S_{N-1}=\frac{2\pi^{N/2}}{\Gamma(N/2)}$ is the sphere surface constant. Since $\vec x\cdot\vec x= R^2$ everywhere in $\Sigma(R)$, we can write the following integral.</p>

<p>$$\int_{\Sigma(R)}\vec x\cdot\vec x\ d\sigma=S_{N-1}R^{N+1}$$</p>

<p>Using Euclidean coordinates:</p>

<p>$$\int_{\Sigma(R)}(x_1^2+x_2^2+...+x_N^2)d\sigma=S_{N-1}R^{N+1}$$</p>

<p>$$\int_{\Sigma(R)}x_1^2\ d\sigma+\int_{\Sigma(R)}x_2^2\ d\sigma+...+\int_{\Sigma(R)}x_N^2\ d\sigma=S_{N-1}R^{N+1}$$</p>

<p>Due to symmetry, we know each of the integrals must have the same value.</p>

<p>$$\int_{\Sigma(R)}x_i^2\ d\sigma=\frac{S_{N-1}}{N}R^{N+1}\tag{1}$$</p>

<p>With this identity, we can integrate our function on $\Sigma(R)$.</p>

<p>$$\int_{\Sigma(R)}\frac{\vec x^TA\vec x}{\vec x^T\vec x}d\sigma$$</p>

<p>The denominator is is the constant $R^2$ everywhere in $\Sigma(R)$.</p>

<p>$$=\frac{1}{R^2}\int_{\Sigma(R)}\vec x^TA\vec x\ d\sigma$$</p>

<p>$$=\frac{1}{R^2}\int_{\Sigma(R)}(\alpha_1x_1^2+\alpha_2x_2^2+...+\alpha_Nx_N^2)d\sigma$$</p>

<p>Now we just have a linear combination of the integrals from $(1)$.</p>

<p>$$\int_{\Sigma(R)}\frac{\vec x^TA\vec x}{\vec x^T\vec x}d\sigma=\frac{\sum_{i=1}^N\alpha_i}{N}S_{N-1}R^{N-1}\tag{2}$$</p>

<p>This essentially tells us that the average value of the rational function on $\Sigma(R)$ is the constant $\frac{\sum_{i=1}^N\alpha_i}{N}$ for any $R&gt;0$. Now if we let $B(R)$ be a ball of radius $R$ centered at the origin, we can integrate the rational function on $B(R)$ by switching to polar coordinates.</p>

<p>$$\int_{B(R)}\frac{\vec x^TA\vec x}{\vec x^T\vec x}d\vec x=\int_0^R\int_{\Sigma(r)}\frac{\vec x^TA\vec x}{\vec x^T\vec x}d\sigma\ dr$$</p>

<p>Using $(2)$:</p>

<p>$$=\frac{\sum_{i=1}^N\alpha_i}{N}\int_0^RS_{N-1}r^{N-1}dr$$</p>

<p>The integral is simply the volume of an $N$-sphere, which we can write using the volume constant $V_N=\frac{\pi^{N/2}}{\Gamma(N/2+1)}$.</p>

<p>$$\int_{B(R)}\frac{\vec x^TA\vec x}{\vec x^T\vec x}d\vec x=\frac{\sum_{i=1}^N\alpha_i}{N}V_Nr^N\tag{3}$$</p>

<p>We can think of the Dirac delta as the limit of a sequence of symmetric distributions centered about the origin that integrate to $1$. In particular, consider the following set of distributions:</p>

<p>$$\Delta_R(\vec x)=\begin{cases}
\frac{1}{V_NR^N} &amp; \vec x\in B(R) \\
0                &amp; \vec x\notin B(R)
\end{cases}$$</p>

<p>Each of these has the properties mentioned above, and additionally they become increasingly localized around the origin as $R\to 0^+$. Because of this, I'll claim that the following definition is equivalent to the other definitions of the multivariate Dirac delta. (this can be justified more thoroughly, but I'll just note that both will converge to $f(\vec 0)$, provided $f$ is continuous there.)</p>

<p>$$\int_Df(\vec x)\delta(\vec x)d\vec x\equiv\lim_{R\to 0^+}\int_Df(\vec x)\Delta_R(\vec x)d\vec x$$</p>

<p>Importantly, this definition no longer requires that $f$ be continuous at $\vec 0$. We can therefore use it to evaluate the integral of interest.</p>

<p>$$\int_{\mathbb R^N}\frac{\vec x^TA\vec x}{\vec x^T\vec x}\delta(\vec x)d\vec x=\lim_{R\to 0^+}\int_{\mathbb R^N}\frac{\vec x^TA\vec x}{\vec x^T\vec x}\Delta_R(\vec x)d\vec x$$</p>

<p>$$=\lim_{R\to 0^+}\frac{1}{V_NR^N}\int_{B(R)}\frac{\vec x^TA\vec x}{\vec x^T\vec x}d\vec x$$</p>

<p>Noticing that the integral above appears in $(3)$:</p>

<p>$$=\lim_{R\to 0^+}\frac{\sum_{i=1}^N\alpha_i}{N}$$</p>

<p>$$\int_{\mathbb R^N}\frac{\vec x^TA\vec x}{\vec x^T\vec x}\delta(\vec x)d\vec x=\frac{\sum_{i=1}^N\alpha_i}{N}$$</p>

<p>The reason this result is ""not rigorous"" is because treating the Dirac delta as a limit of distributions is very problematic here. If $f$ is continuous at the origin then the equality</p>

<p>$$\int_Df(\vec x)\delta(\vec x)d\vec x=\lim_{R\to 0^+}\int_Df(\vec x)\Delta_R(\vec x)d\vec x$$</p>

<p>holds or a wide class of well-behaved distributions. However, when we allow $f$ to be discontinuous at the origin, different distributions will give different results. In general, I believe the following can be derived.</p>

<p>$$\liminf_{\epsilon\to 0^+}\{f(\vec x):|\vec x|&lt;\epsilon\}\le\lim_{R\to 0^+}\int_Df(\vec x)\Delta_R(\vec x)d\vec x\le\limsup_{\epsilon\to 0^+}\{f(\vec x):|\vec x|&lt;\epsilon\}$$</p>

<p>Thus a different choice of distributions could have resulted in a result of $\min\{\alpha_i\}$ or $\max\{\alpha_i\}$. If we arbitrarily limit ourselves to spherically symmetric distributions, the definition is no longer inconsistent, but we now aren't talking about the standard delta function, but an entirely new function that no longer ""sees"" $f(0)$, but instead the spherically symmetric average of $f$ in the vicinity of $0$. This function is equivalent to the delta for continuous functions and defined for all locally bounded almost everywhere continuous function. By using it, we aren't computing the integral you posed (which is ill-defined), but rather a very similar looking, but arbitrarily chosen integral. Whether this result is useful at all depends on where the integral comes from.</p>
"
"2395109","2395126","<p>As you have claimed, an element of order $2$ in $A_n$ is a product of even number of disjoint transposition. Afterall, in general, order of element in $S_n$ is the $lcm$ of the lengths of cycle in its cycle decomposition. And in $A_n$, elements consists of even no. of transpositions. So you're right.</p>

<p>Since $(12)(34)=(1324)^2$. Write $\sigma$ as product of pairs of disjoint transposition and conclude that it can be written similarly as squares. e.g. $(12)(34)(56)(78)=((1324)(5768))^2$</p>
"
"2395123","2395134","<p>if a is paired with c and if b is paired with d and if ab .there follows that since 30 is 5th which  from the right must be paired with 12 which  is 5th from the left . </p>
"
"2395129","2395160","<p>Yes, your proof is correct.  A few comments:</p>

<p>Note that because $(2 + \sqrt{3})(2 - \sqrt{3}) = 1$, we have $$-i \ln (2 \pm \sqrt{3}) = i \ln (2 \mp \sqrt{3}).$$</p>

<p>Another method of solution would be to use the angle addition identity to write $$ \sin (a+bi) = \sin a \cos bi + \cos a \sin bi = \sin a \cosh b + i \cos a \sinh b,$$
from which we conclude that $(a,b) \in \mathbb R^2$ must satisfy $$\sin a \cosh b = 2, \\ \cos a \sinh b = 0.$$  Since $\sinh b = 0$ if and only if $b = 0$, and $\cos a = 0$ if and only if $a \in \frac{\pi}{2} + \pi k$ for $k \in \mathbb Z$, we have two cases.  The first case is impossible:  if $b = 0$, then $\cosh b = 1$ and $\sin a = 2$ has no solutions for $a \in \mathbb R$.  So we must have instead $$\cosh b = 2 \csc \left(\frac{\pi}{2} + \pi k\right) = 2(-1)^k.$$  Since $\cosh b \ge 1$, it follows that $k$ must be even, and we get $b = \pm \cosh^{-1} 2$, and our solution set is $$a+bi = \frac{\pi}{2} + 2\pi m \pm i \cosh^{-1} 2, \quad m \in \mathbb Z.$$  It is worth observing that $\cosh^{-1} 2 = \ln(2 + \sqrt{3}).$</p>
"
"2395133","2395162","<p>The easiest trick to implement (but not necessarily to think of) is to write $tI - A$ as the product
$$
tI - A = \pmatrix{tI - A &amp; 0\\0&amp;I}\pmatrix{I &amp; 0\\-C&amp;I} \pmatrix{I &amp; 0\\0&amp;tI - D}
$$
and it suffices to determine that the matrices in this product have determinants $\det(tI - A),1,\det(tI - D)$ (in that order).  We could prove those formulas using induction, if you like.  In particular, the formulas for 
$$
\det \pmatrix{I &amp; 0\\0&amp;tI - D}, \det\pmatrix{I &amp; 0\\C&amp;I}
$$
are very nicely proven using the hint as it's given.</p>
"
"2395138","2395176","<p>Two polynomials are considered equal if they have matching coefficients. </p>

<p>So $1x^2 + 0x + 1 = a^2x^2 + 2abx + b^2$ if and only if</p>

<p>\begin{align}
   a^2 &amp;= 1 \\
   2ab &amp;= 0 \\
   b^2 &amp;= 1
\end{align}</p>

<p>This implies that $|a| = |b| = 1$. Hence $2ab = \pm 2 \ne 0$. So there is no solution.</p>
"
"2395149","2395178","<p>What you need to do is for each $x\in X$ and each
$\newcommand{\ep}{\varepsilon}\ep&gt;0$ there is $\ep'&gt;0$ with
$B_{d'}(x,\ep')\subseteq B_d(x,\ep)$ and there is $\ep''&gt;0$ with
$B_{d}(x,\ep'')\subseteq B_{d'}(x,\ep)$.</p>

<p>Both really should be straightforward.</p>
"
"2395151","2395181","<p>Notably, all we can say about the entries of $E$ is that they all have magnitude at most $5$.  Of course, this means that the Frobenius norm satisfies $\|E\| \leq (0.5) \cdot n = \frac n2$ (and $\alpha = \frac n2$ is indeed the lowest possible upper bound).  We easily get the result that all singular values of $E$ are at most $\frac n2$.  Moreover, we could note that in general,
$$
\|E\| = \sqrt{\sigma_1^2(E) + \cdots + \sigma_n^2(E)} \leq \frac n2
$$
we can't say much more about the singular values of $E$ directly.</p>

<p>Bhatia's <em>Matrix Analysis</em> provides a useful result:</p>

<blockquote>
  <p><strong>(using problem III.6.5)</strong>  For any two operators $A, E$ and any two indices $i, j$ such that $i + j = n +1$, we have
  $$
\sigma_{i+j-1}(A + E) \leq \sigma_i(A)+ \sigma_j(E)\\
\sigma_{i}(A + E) \geq |\sigma_{i + j-1}(A) - \sigma_j(E)|
$$</p>
</blockquote>
"
"2395154","2395193","<p>Let $X = \mathbb{R}^2$ and $Y = \{(0,y) \mid y \in \mathbb{R}\}$. It's not hard to see that $X/Y$ is isomorphic to $\mathbb{R}$. Let $p : X \to X/Y$ denote the quotient map. </p>

<p>Now suppose $(x_n)$ is Cauchy in $\mathbb{R}$ and observe that the the sequence $((x_n,n))$ is not Cauchy in $\mathbb{R}^2$. However, $p(x_n,n) = x_n$ and we know that $(x_n)$ is Cauchy.</p>
"
"2395177","2395190","<p>Hint: compute the limit of
$$f(t e^{i \frac{\pi}4{}})$$
when $t&gt;0$ and $t \to 0^+$ ?</p>
"
"2395180","2395183","<p>I mean, it's literally the reverse triangle inequality, like you said.</p>

<p>Given $x$, since $f$ is continuous, for all $\epsilon&gt;0$ there exists a $\delta&gt;0$ such that $|x-y|&lt;\delta \Rightarrow |f(x)-f(y)|&lt;\epsilon$.</p>

<p>But $\left||f(x)|-|f(y)|\right|\le |f(x)-f(y)|$ so we are done.</p>
"
"2395198","2395232","<blockquote>
  <p><strong>Definition.</strong> A function $f:I\to\mathbb{R}$ is defined near a point $c\in\mathbb{R}$ if there is $\delta&gt;0$ such that
  $$
\{x:0&lt;|x-c|&lt;\delta\}\subseteq I
$$</p>
  
  <p><strong>Defintion.</strong> Let $f:I\to\mathbb{R}$ be defined near $c\in I$. $f$ is continuous at $x=c$ if for every $\epsilon&gt;0$ there exists a corresponding $\delta&gt;0$ such that for all $x$
  $$
|x-c|&lt;\delta\Rightarrow |f(x)-f(c)|&lt;\epsilon
$$</p>
</blockquote>

<p>Notice that there is $\delta&gt;0$ (you need to take $\delta\leq p$) such that
$$
(c-\delta,c)\cup(c,c+\delta)\subseteq (c-p,c+p)
$$
so $f$ is defined near $c$. Let $\epsilon&gt;0$ be arbitrary. We must find $\delta&gt;0$ such that for all $x$
$$
|x-c|&lt;\delta\Rightarrow |f(x)-f(c)|&lt;\epsilon
$$
Let's approximate $|f(x)-f(c)|$.</p>

<p>$$
|f(x)-f(c)|\leq B|x-c|
$$
We want to make $|f(x)-f(c)|&lt;\epsilon$ so we need to make $B|x-c|&lt;\epsilon$ which forces $|x-c|&lt;\frac{\epsilon}{B}$. This suggests that we need to take $\delta=\frac{\epsilon}{B}$. Then
$$
|x-c|&lt;\delta=\frac{\epsilon}{B}\Rightarrow
|f(x)-f(c)|\leq B|x-c|&lt;B\frac{\epsilon}{B}=\epsilon
$$</p>
"
"2395203","2395214","<p>Since $f$ is continuous, there is $x_h \in [c,c+h]$ such that $m_h=f(x_h)$.</p>

<p>With $h \to0$, we get $x_h \to c$. Then, by continuity, we derive $m_h \to f(c)$.</p>
"
"2395225","2395245","<p>For any point $x\in \partial B(0, 1)$, let $\{x_n\}_{n=1}^{\infty}\subset B(0, 1)$ such that $x_n\to x$. We will show that $\{f(x_n)\}$ is Cauchy. For $\epsilon &gt; 0$, we consider $\delta_1(\epsilon) &gt; 0$ such that $\|x-y\| &lt; \delta_1(\epsilon)$ implies $\|f(x)-f(y)\| &lt; \epsilon$ for all $x, y\in B(0, 1)$. Now, as $\{x_n\}$ is Cauchy, for any $\delta &gt; 0$ we have some $N(\delta)\in \mathbb{N}$ such that $m, n\geq N$ implies $\|x_n-x_m\| &lt; \delta$. Then, for $m, n\geq N(\delta_1(\epsilon))$, we have $\|x_n-x_m\| &lt; \delta_1(\epsilon)$, and so $$\|f(x_n)-f(x_m)\| &lt; \epsilon$$ Therefore, we let $\tilde{f}(x) = \lim_{n\to \infty} f(x_n)$. We will show that this is well-defined, i.e. for any $\{y_n\}_{n=1}^{\infty}\subset B(0, 1)$, $y_n\to x$ implies $f(y_n)\to \tilde{f}(x)$. As $x_n\to x$ and $y_n\to x$, for any $\epsilon &gt; 0$, we have some $N\in \mathbb{N}$ such that $\|x-x_n\| &lt; \frac{\delta_1(\epsilon)}{2}$ and $\|x-y_n\| &lt; \frac{\delta_1(\epsilon)}{2}$ for $n\geq N$. Then, by the triangle inequality, $$\|x_n-y_n\| = \|(x-x_n)-(x-y_n)\|\leq \|x-x_n\|+\|x-y_n\| &lt; \delta_1(\epsilon)$$ which implies that $\|f(x_n)-f(y_n)\| &lt; \epsilon$ for $n\geq N$. This implies that $f(x_n)-f(y_n)\to 0$, or $\lim_{n\to \infty} f(y_n) = \tilde{f}(x)$. Therefore, we can define $\tilde{f}(x) = f(x)$ on $B(0, 1)$ and $\tilde{f}(x) = \lim_{x'\to x} f(x')$ on $\partial B(0, 1)$ as the continuous extension of $f$ to $\overline{B(0, 1)}$. Note that as $\overline{B(0, 1)}$ is compact, $\tilde{f}$ is actually uniformly continuous.</p>
"
"2395233","2395249","<p>The key element is that your set of positions in a room for a solver to be picked is exactly the set of all possible positions $\{1st,2nd,3rd,4th\}$.</p>

<p>The probabilities of one particular solver to be 1st,2nd,3rd or 4th in its room sum to 1. Then the number of the rooms don't play a role (it is independant from the solver's rank in the room). Said otherwise, you could shuffle the numbers written on the doors after the student have been assigned without changing the probability of a student to be picked.</p>

<p>In your second problem, ""A student is selected for the next task if he is the 1st solver of the 1st room or the 2nd solver of the 2nd room or the 1st solver of the 3rd room or the 2nd solver of the last room."", the set of positions is not the set of all possible positions $\{1st,2d,3d,4th\}$ anymore and the probability to pe picked varies among students (for the best student, it is 0.5, for the two worst ones, it is 0).</p>
"
"2395237","2395535","<p>Let's just deal with the case of two curves, $A$ and $B$. </p>

<p>It might be possible to represent the combined curve as a single cubic, or it might not. If $A$ and $B$ were designed independently, it's highly unlikely that they will be joinable in this way. But some design systems have a function for splitting a curve into several pieces, and if $A$ and $B$ were produced by this sort of splitting process, then it <strong>will</strong> be possible to join them back together into a single cubic.</p>

<p>If the joined curve can't be represented as a single cubic, then it can't be represented by a single polynomial curve of higher degree, either. In other words, increasing degree doesn't help.</p>

<p>Your question was ""is joining always possible?"". The answer is no.</p>

<p>A related question could be ""is it sometimes possible?"" The answer is yes.</p>

<p>I don't know if you're interested in the ""sometimes"" question. If you are, leave a comment, and I will supply more details.</p>

<p>There are several approaches to the ""sometimes"" question. One simple technique is to compute derivatives of orders one, two, and three of each curve at the common end-point. If these derivatives are all equal, then the two cubics are the same, and can be joined into one. Just having the same tangent or the same first derivative is not enough.</p>
"
"2395241","2395257","<p>Take any $x\in \ A\cap (B -C)$ then $x\in A$ and $x\in B-C$ (so $x\in B$ and $x\notin C$).</p>

<p>Now $x\in A\cap B$ (since $x\in A$ and $x\in B$) and $x\notin A\cap C$ (since $x\notin C$). </p>

<p>So $x\in \ (A\cap B) - (A \cap C)$. </p>

<p>Since $x$ is arbitrary we have $A\cap (B-C)\subseteq \ (A\cap B) - (A \cap C)$.</p>
"
"2395248","2395381","<p>Observe that for $n \geq 1$
$$A_n(x) := \sum_{k=1}^{\infty} \frac{k^n}{k!} x^k = \left(x\frac{d}{dx}\right)^n e^x = p_n(x) e^x$$
where $p_n(x)$ is a polynomial with integer coefficients.</p>

<p>Therefore $a_n = A_n(1)$ is an integer times $e$. In the same way $b_n$ is an integer times $e^{-1}$.</p>

<p>Thus $a_n b_n$ is an integer.</p>

<hr>

<p>Elaborating according to a question in the comments.</p>

<p>Deriving the Maclauring expansion of $e^x$ we get
$$
\frac{d}{dx}e^x 
= \frac{d}{dx} \sum_{k=0}^{\infty} \frac{1}{k!}x^k
= \sum_{k=1}^{\infty} \frac{k}{k!}x^{k-1}
$$
(Notice the change of lower sum limit since the term for $k=0$ vanishes.)</p>

<p>Multiplying with $x$ then gives
$$
x\frac{d}{dx}e^x 
= \sum_{k=1}^{\infty} \frac{k}{k!}x^{k}
$$</p>

<p>Deriving once again gives
$$
\frac{d}{dx}x\frac{d}{dx}e^x 
= \sum_{k=1}^{\infty} \frac{k^2}{k!}x^{k-1}
$$
and after multiplication by $x$
$$
x\frac{d}{dx}x\frac{d}{dx}e^x 
= \sum_{k=1}^{\infty} \frac{k^2}{k!}x^{k}
$$
Generally we can write
$$
\left(x\frac{d}{dx}\right)^n e^x 
= \sum_{k=1}^{\infty} \frac{k^n}{k!}x^{k}
$$</p>

<p>Now lets look at the left hand sides.</p>

<p>For $n=0$ we have
$\left(x\frac{d}{dx}\right)^0 e^x = e^x = p_0(x) e^x$
where $p_0(x)=1$.</p>

<p>For $n=1$ we have
$$x\frac{d}{dx} e^x = x e^x = p_1(x) e^x$$
where $p_1(x)=x$.</p>

<p>For $n=2$ we have
$$\left(x\frac{d}{dx}\right) e^x = x \frac{d}{dx}(x e^x) = x(x+1) e^x = p_2(x) e^x$$
where $p_2(x)=x(x+1) = x^2+x$.</p>

<p>It's obvious, and can easily be shown using induction, that every $p_n(x)$ will have (non-negative) integer coefficients.</p>
"
"2395252","2395316","<p>Well, the determinant is real, so why not. Find some real matrix with the same eigenvalues (a simple rotation would do), then the eigenvectors of both, then change basis to bring one to the other.</p>

<p>In short, if your matrix is $A$ and $B=\begin{pmatrix}1&amp;0&amp;-1\\0&amp;0&amp;1\\0&amp;i&amp;1\end{pmatrix}$, then $B^{-1}AB=\begin{pmatrix}1&amp;0&amp;0\\0&amp;1&amp;1\\0&amp;-1&amp;1\end{pmatrix}$</p>
"
"2395264","2395267","<p>I would say that the typical notation for the function you describe (also known as the <a href=""https://en.wikipedia.org/wiki/Floor_and_ceiling_functions"" rel=""nofollow noreferrer""><strong>floor</strong></a> function) is $\lfloor x\rfloor$, but $[x]$ is not uncommon either.</p>

<hr>

<p>However, I would recommend $\lfloor x\rfloor$, because $[x]$ is sometimes (not that common) used as the ""fractional"" part of $x$, i.e. $[x] = x-\lfloor x\rfloor$</p>
"
"2395270","2395278","<blockquote>
  <p>If I graph this function I get the line $f_{(x)}=x+2$, but it that with the point $(2,4)$ not included?</p>
</blockquote>

<p>You're thinking in the right direction:
$$f_1(x) = \frac{x^2-4}{x-2}$$
and
$$f_2(x) = x+2$$
are identical for all $x$ <strong>except</strong> at $x=2$, since $f_2(2) = 2+2= 4$ but $f_1$ does not exist there. You cannot divide by $0$ and the maximal domain for $f_1$ would be $\mathbb{R} \setminus \left\{ 2 \right\}$.</p>

<p>See also: <a href=""https://math.stackexchange.com/q/2093466/159845"">Why aren&#39;t the functions $f(x) = \frac{x-1}{x-1}$ and $f(x) = 1$ the same?</a></p>
"
"2395277","2396236","<blockquote>
  <p>If one wants to model the joint distribution of 10
  consecutive words in a natural language with a vocabulary V of size 100,000, there are potentially
  $100000^{10} â 1 = 10^{50} â 1$ free parameters</p>
</blockquote>

<p>Why is this? Well, to specify the joint distribution of 2 words is a table of $|V|^2$ numbers (probabilities of joint appearance). For each new word, you add 1 new dimension to the table. Hence, for a set of $n$ words you need to specify $|V|^n$ values, minus $1$ (because probability distributions sum to $1$).
So, ouch! That's a lot.</p>

<blockquote>
  <p>When modeling continuous variables, we obtain generalization more easily (e.g. with smooth classes of functions like multi-layer neural networks or Gaussian mixture models) because the function to be learned can be expected to have some local smoothness properties.</p>
</blockquote>

<p>The thing about discrete distributions is that they can be exceptionally ""jagged""; i.e. things can 
In language, for instance, there is no reason why one word should statistically appear in similar contexts to, say, the one next to it, alphabetically. 
Hence the explosion of parameters above. Continuous distributions, by assumption, don't have this issue.</p>

<p>More concretely, our problem above had $|V|^n-1$ parameters to characterize the joint distribution of $n$ variables, in the discrete case. Let's suppose we have an RV $X_i$ that takes values in $\mathbb{R}^d$ rather than $V$. At first glance, this seems to be <em>harder</em> since the number of possible values in $\mathbb{R}^d$ is larger than $|V|$ (even for $d=1$). However, what if we think that the joint distribution of $X=(X_1,\ldots,X_n)$ is well approximated by a Gaussian mixture model? Then we need only specify $k$ (number of Gaussians), $W$ (vector of weights, $|W|=k$), the means $\mu_j\in\mathbb{R}^d$, and the covariances $\sigma\in\mathbb{R}^{d\times d}$. This is only on the order of $k+k(nd+n^2d^2)$, roughly speaking. This is comparatively quite small! Much of the reason for this is that large patches of space are assumed to have probabilities smoothly varying compared to their neighbors; hence, one requires many fewer parameters to characterize large patches of space. (Even the largest deep neural networks have nowhere near close to ${\sim}10^{50}$ parameters! Hence why we prefer to do NLP in ""continuous spaces"" by embedding them).</p>
"
"2395279","2395295","<p>No, for example for $A=72$, the equation $(p-1)(q-1)=A$ has $4$ prime solutions:
$$(p,q)\in\{ (2, 73),(3, 37), (5, 19), (7,13)\}.$$
For $A=1080$ there are $6$ prime solutions:
$$(p,q)\in\{ (3, 541), (5, 271), (7,181), (11,109), (19,61), (31,37)\}.$$</p>
"
"2395285","2395290","<p>The expression</p>

<p>$$24,69,214,747,2820,...$$</p>

<p>Is already a ""mathematical"" expression. But that is probably not what you mean. You probably want a general expression for the $n$-th element in your sequence, and if you want that, the answer....</p>

<p><strong>does not exist</strong>.</p>

<hr>

<p>You didn't give enough information about the sequence for us to answer your question. For example, the sequence could be</p>

<p>$$24,69,214,747,2820,0,0,0,0,0,0\dots$$</p>

<p>in which case the mathematical expression for the sequence would be</p>

<p>$$a_n=\begin{cases}24&amp;n=1\\ 69&amp;n=2\\214&amp;n=3\\ 747 &amp; n=4 \\ 2820&amp; n=5\\ 0&amp;n\geq 6\end{cases}$$</p>

<p>Given your information, there is nothing that prevents the sequence being like that. Or, being $$24,69,214,747,2820,1,2,3,4,5,6,7,8\dots$$
or
$$24,69,214,747,2820,93485,34598345,23489234,23497234,4589567$$</p>

<p>or whatever other sequence you want to think of.</p>

<hr>

<p>If you want a <strong>polynomial</strong> that fits your data, then the lowest degree polynomial is</p>

<p>$$f(x)=36x^4â312x^3+1022x^2â1377x+655$$</p>

<p>(for this polynomial, you have $f(1)=24,f(2)=69,f(3)=214, f(4)=747, f(5)=2820$).</p>

<p>However, there are <strong>infinitely many</strong> other polynomials that fit your data, if you allow the polynomials to have degree $5$.</p>
"
"2395299","2395355","<p>Considering $$f=\frac{x}{y^2+1}-\lambda  \left(\frac{x^2}{4}+\frac{y^2}{9}-1\right)$$ as you wrote $$f'_x=\frac{1}{y^2+1}-\frac{\lambda  x}{2}=0\tag 1$$ $$f'_y=-\frac{2 x y}{\left(y^2+1\right)^2}-\frac{2 \lambda  y}{9}=0\tag 2$$ $$f'_\lambda =\frac{x^2}{4}+\frac{y^2}{9}-1=0\tag 3$$ From $(1)$, eliminate $x$ to get $$x=\frac{2}{\lambda  \left(y^2+1\right)}$$ Replace in $(2)$ and simplify to get $$f'_y=-\frac{2 y \left(\lambda ^2 \left(y^2+1\right)^3+18\right)}{9 \lambda 
   \left(y^2+1\right)^3}$$ making $y=0$ an obvious solution. Now, you are then left with $(3)$.</p>
"
"2395302","2395341","<p>Well, first change to the variables $u=xy,\;v=x/y$ (the Jacobian is $1\over2v$), and the thing becomes
$$\int\limits_1^\infty\int\limits_{1/u}^u{1\over2u^3v^2\ln u}dv\;du$$
Now take the inner integral and get
$$\int\limits_1^\infty{u^2-1\over2u^4\ln u}du$$
Now is the time for some complex machinery. A common trick with introducing a parameter would do. Say, we have $u^t$ instead of $u^2$ in the numerator; that would be $F(t)$. Then $F'(t)=\int\limits_1^\infty{u^t\over2u^4}du={1\over2(3-t)}$, and $F(0)=0$, so we may reconstruct $F(2)$ which we need.</p>
"
"2395305","2395315","<p>How many functions from $[1,1095]$ to $[1,100]$ are surjective?
Let $N$ be this number then your probability is equal to
$$p=\frac{N}{100^{1095}}=\frac{100!S(1095,100)}{100^{1095}}.$$
Where $S(n,k)$ is a <a href=""https://en.wikipedia.org/wiki/Stirling_numbers_of_the_second_kind"" rel=""nofollow noreferrer"">Stirling number of the second kind</a>.</p>

<p>If you are looking for an approximation value of $p$ then for $k$ fixed 
$$k!S(n,k)=\sum_{j=0}^k (-1)^j\binom{k}{j}(k-j)^n\approx k^n-k(k-1)^{n}.$$
and therefore 
$$p\approx \frac{k^n-k(k-1)^{n}}{k^{n}}=1-k(1-1/k)^{n}\approx1-ke^{-n/k}\approx 0.9982442$$
where $k=100$ and $n=1095$. Note that the correct value of $p$ is $0.9983396$.</p>
"
"2395307","2395375","<p>Look at your equation, and imagine you solve it in exact arithmetic. For $r=r_{max}$, it is constant in $\theta$. If it is a constant $C$ for $r=r_i$,
$$
F(r_{i-1},\theta_i)= \frac{(h_r+h_\theta) F(r_{i},\theta_i) - h_r F(r_{i},\theta_{i-1})}{h_\theta} = \frac{(h_r+h_\theta) C - h_r C}{h_\theta}=C,
$$
so all of your solution would be $C.$ The differences occur because you introduce rounding errors in every step and divide them by a small number, $h_\theta$. I'd imagine the following form is mathematically equivalent, but more stable:
$$
F(r_{i-1},\theta_i)= F(r_{i},\theta_i)+h_r\,\frac{F(r_{i},\theta_i) - F(r_{i},\theta_{i-1})}{h_\theta}
$$
I wonder what would be the physical meaning of such an equation in polar coordinates, though. It's easy to see that its general solution is $F(r,\theta)=f(r-\theta)$, meaning it's constant along the lines $r=\theta+c.$ Those lines are Archimedean spirals, and it's clear you get a singularity, an eddy, at $r=0.$</p>
"
"2395308","2395331","<p>Using,</p>

<p>$$e^{x} &gt; x+1$$</p>

<p>For all $x \neq 0$, we have $e^{-1/x}&gt;(-1/x)+1$. Hence for $x&gt;0$,</p>

<p>$$\frac{e^{-1/x}-1}{x^{2/3}}&gt;-\frac{1}{x^{5/3}}$$</p>

<p>At the same time $e^{-1/x}&lt;1$ for $x&gt;0$. So we have that for $x&gt;0$,</p>

<p>$$0&gt;\frac{e^{-1/x}-1}{x^{2/3}}=f(x)&gt;-\frac{1}{x^{5/3}}$$</p>

<p>By convergence of $\int_{1}^{\infty} \frac{1}{x^{5/3}} dx$ the comparison test for integrals ensures the convergence of $\int_{1}^{\infty} f(x) dx$.</p>
"
"2395318","2395362","<p>You may assume $v_{n+1}=0$. Write $\Lambda$ for the volume form (""det"" in your question). Then
$$\Lambda\left({v_1+v_2\over2},v_2,\dots,v_n\right)={1\over2}\Lambda(v_1,v_2,\ldots,v_n)+{1\over2}\Lambda(v_2,v_2,\ldots,v_n)\ .$$
Here the second term on the right hand side is $=0$ since it contains two equal entries. It follows that the ratio you are interested in is always $={1\over2}$.</p>
"
"2395329","2395333","<p>If $y$ values are all negative, $-y$ values are all positive. Just handle the problem of $$-y=-A \exp(Bx)$$</p>

<p>$$\ln(-y) = Bx+\ln(-A)$$</p>
"
"2395367","2395384","<blockquote>
  <p>Is the quotient space the set $Q=\mathbb{R}$, such that $U\subset Q$ is open if and only if $U\times\{0\}\cup U\times\{1\}$ is open in $\mathbb{R}\times\{0,1\}$?</p>
</blockquote>

<p><strong>No</strong>. The quotient space, by definition, is the set $X/_\sim$ where $X$ is the original space (in your case, $X=\mathbb R\times\{0,1\}$. This means $Q=\{[(x,y)]| (x,y)\in X\}$.</p>

<p>In $Q$, a set $U\subset Q$ is open if $q^{-1}(U)$ is open in $X$.</p>

<hr>

<p>Now, for $x\neq 0$, you have $[(x,0)] = [(x,1)] = \{(x,0),(x,1)\}$, and you can certainly see that the local neighborhood around the point $[(x,0)]\in Q$ is homeomorphic to the local neighborhood around the point $x\in R$, but that does not mean that the two sets are equal! In fact, you get into trouble around $x=0$, since $[(0,0)] = \{(0,0)\}$ and $[(0,1)]=\{(0,1)\}$</p>

<hr>

<p>So, the idea is to prove that those two points do not have disjoint neighborhoods.</p>

<hr>

<p>Take any neighborhood $O_0$ containing $[(0,0)]$. Then, $q^{-1}(O)$ is an open set around $(0,0)$, which means that it contains some ""interval"" (i.e. there exists some $\epsilon_0 &gt; 0$ such that $(-\epsilon_0, \epsilon_0)\times\{0\}\subset q^{-1}(O_0)$. </p>

<p>From this, it should be easy to show that $I_0 = \{[(x, 0])| |x|&lt;\epsilon_0\}$ must be a subset of $O_0$</p>

<p>If you now repeat the process on a neighborhood $O_1$ containing $[(0,1)]$, you will find that $I_1 = \{[(x,0)]; |x&lt;\epsilon_1\}$ is a subset of $O_1$, and thus $O_0\cap O_1\neq\emptyset$</p>
"
"2395376","2395380","<p>Hint. Note that
$$0=3x^8-12x^4 +1 =3(x^8-4x^4+4)-12 +1=3(x^4-2)^2-11.$$
Hence
$$x^4=2\pm\sqrt{\frac{11}{3}}=\frac{6\pm\sqrt{33}}{3} \quad (\mbox{both positive real numbers})$$
Now you may find $x$ easily. </p>
"
"2395377","2395387","<p>First determine an expression for the vector $\vec{PQ} $ by taking the difference between the given position vectors. </p>

<p>$\vec{PQ} = (1+t)\underline i + (-t)\underline j + (t-2) \underline k$</p>

<p>Now find an expression for the magnitude (length) of that vector in terms of $t$. </p>

<p>$|\vec{PQ}| = \sqrt{(1+t)^2 + t^2+ (t-2)^2} = \sqrt{3t^2-2t+5} = \sqrt{3(t-\frac 13)^2 + \frac{14}{3}}$</p>

<p>You want to mimimise that expression, and it's easier with completing the square as I did above. Note that the square part $(t - \frac 13)^2 \geq 0$ for real $t$, so the magnitude is minimised when the square part is zero, i.e. when $t =\frac 13$ and the minimal magnitude is $\sqrt{\frac{14}{3}}$</p>
"
"2395390","2395411","<p>$\textbf{Hint}$: Use the chain rule.</p>

<p>\begin{align*} T(u,v)=T(u(x,y),v(x,y)) \Rightarrow Z_y = \frac{\partial T}{\partial y} = \begin{pmatrix} T_u &amp; T_v \end{pmatrix} \cdot \begin{pmatrix}  u_y \\ v_y\end{pmatrix} \end{align*}</p>
"
"2395398","2395420","<p>He certainly consider the pointwise infimum of $\liminf$, that is, for any $\omega\in \Omega$, $\left(  \inf_n X_n\right) \left(\omega\right)$ denotes the infimum of the sequence $\left(X_n\left(\omega\right)\right)_{n\geqslant 1}$ and  $\left( \liminf_nX_n\right)  \left(\omega\right)$ denotes the $\liminf$  of the sequence $\left(X_n\left(\omega\right)\right)_{n\geqslant 1}$.          </p>
"
"2395406","2395426","<p><strong>Q2</strong></p>

<p>Suppose the contrary, that is, assume that there is no finite subcover that covers $[0,1]$. Then, at least one of the intervals $[0,1/2]$ and $[1/2, 1]$ can't be covered with a finite subcover. Pick this interval and call it $I_1$. This is the base of an inductive process.</p>

<p>Now assume that $I_n$ has length $1/2^n$ and can't be covered with a finite subcover. Divide $I_n$ into two intervals of the same length. One of these intervals can't be covered with a finite subcover. Pick this one and call it $I_{n+1}$ This new interval has length $1/2^{n+1}$. This completes the inductive process, which yields a sequence of intervals $I_n$. Each interval of the sequence is contained and has a length that is a half of the preceeding one. And none of them can be covered with a finite subcover.</p>

<p>Since $I_{n+1}\subset I_n$, $\bigcap_{n=1}^\infty I_n$ is not empty. And since the length of $I_n$ tends to $0$ it has only one point. That point is in some open set from the cover, and this open set alone covers some $I_m$. Contradiction.</p>

<p><em>Remark</em>: this idea can be used to show that a finite cartesian product of closed, bounded invervals is compact.</p>
"
"2395409","2396071","<p>First of all note that $\bigcap_{i=1}^N A_i=A_N$ since $A_i\downarrow$.</p>

<p>Let $x\in A_N$. Since $\bigcap_iK_i=\emptyset$, there exists a $j\leq N$ such that $x\notin K_j$. Together with $A_j\supset A_N$ follows $x\in A_j\setminus K_j$ which shows the inclusion $A_N\subset \bigcup_iA_i\setminus K_i$</p>
"
"2395424","2395551","<p>$$1 + x + \frac{1}{2!}x^2 + \frac{1}{3!}x^3 + \frac{1}{4!}x^4 +\cdots\\\times
\\ 0+ x + 0 - \frac{1}{3!} x^3  +    0  +  \frac{1}{5!} x^5+\cdots$$</p>

<p>Collect the products of coefficients leading to a common power and you get</p>

<p>$$x^0\to1\cdot0\\
x^1\to1\cdot0+1\cdot1\\
x^2\to\frac1{2!}0+1\cdot1+1\cdot0\\
x^3\to\frac1{3!}0+\frac1{2!}1+1\cdot0-1\frac1{3!}\\
x^4\to\frac1{4!}0+\frac1{3!}1+\frac1{2!}0-1\frac1{3!}+1\cdot0\\
x^5\to\frac1{5!}0+\frac1{4!}1+\frac1{3!}0-\frac1{2!}\frac1{3!}-1\cdot0+1\cdot\frac1{5!}$$</p>
"
"2395441","2395566","<p>Your initial idea just needs a little tweak: instead of multiplying by $\rm P+Q$, let's multiply by $\rm P-Q$:
$$\rm (P^2+Q^2)(P-Q)=P^3-P^2Q+Q^2P-Q^3=0.$$</p>

<p>If $\rm \det(P^2+Q^2)\neq0$, then $\rm P^2+Q^2$ is invertible, and we can multiply by its inverse:
$$\rm (P^2+Q^2)(P-Q)=0 \implies P-Q=(P^2+Q^2)^{-1}0=0,$$
contradicting the given condition that $\rm P\neq Q$.</p>
"
"2395447","2395456","<p>By <a href=""https://en.wikipedia.org/wiki/Rule_of_Sarrus"" rel=""nofollow noreferrer"">Sarrus rule</a>,
\begin{align*}
\det(A)&amp;=x^{1+9+5}+x^{2+4+7}+x^{3+8+6}-x^{7+9+3}-x^{6+4+1}-x^{5+8+2}\\
&amp;=x^{15}+x^{13}+x^{17}-x^{19}-x^{11}-x^{15}\\
&amp;=x^{11}(x^2+x^6-x^8-1) \\
&amp;=-x^{11}(x^6-1)(x^2-1)\\
&amp;=-x^{11}(x^2+x+1)(x^2-x+1)(x-1)^2(x+1)^2.
\end{align*}</p>
"
"2395448","2395461","<p>Let $A,M$ denote their current ages and let $C$ denote the number of years that have elapsed since the prior time under discussion.  That is, $C$ is the difference between their two ages.</p>

<p>Then:  $$A=24\quad \quad A-C=M\quad \quad A = 2\times (M-C)$$</p>

<p>It is easy to solve this system to get $M=18$.</p>
"
"2395449","2395467","<p>First of all, it should be easy to prove the statement if $y_0=0$, because $$y=2r\|y\| \cdot \frac{1}{2r\|y\|}y$$</p>

<p>which means that if $Ax' = \frac{1}{2r\|y\|}y$, then setting $x=2r\|y\| x'$ means that $$Ax=A(2r\|y\| x')=2r\|y\| Ax' = 2r\|y\| \cdot \frac{1}{2r\|y\|}y = y$$</p>

<hr>

<p>So, all you need is to prove that for every $y\in B(0,r)$, the equation $Ax=y$ has a solution.</p>

<p>Take $y\in B(0,r)$ and define $y_b=y_0 + y$. Then clearly $y_b\in B(y_0, r)$, so there exists some $x_b$ such that $Ax_b=y_b$. And you also know of some $x_0$ such that $Ax_0=y_0$.</p>

<p>Now you know that $y=y_0-y_b$, and you know that $y_0=Ax_0$ and you know that $y_b=Ax_b$, and you know that $A$ is linear.</p>

<p>I'll leave it to you to put the pieces together.</p>
"
"2395463","2397321","<p>As has been pointed out in the comments, the rotation matrix that you built rotates about the origin, but you need to rotate the rectangle about some other point. A systematic way to deal with this would be to first translate the center of rotation to the origin, rotate, and then translate the result into its final position.  </p>

<p>Since youâre specifying the rectangle positions by their centers, weâll rotate about the center of the rectangle. So, we first translate by $(-1,-2)$, rotate 90Â° conterclockwise, as youâve done, and translate to the final position: $$\begin{bmatrix}1&amp;0&amp;4\\0&amp;1&amp;5\\0&amp;0&amp;1\end{bmatrix}\begin{bmatrix}0&amp;-1&amp;0\\1&amp;0&amp;0\\0&amp;0&amp;1\end{bmatrix}\begin{bmatrix}1&amp;0&amp;-1\\0&amp;1&amp;-2\\0&amp;0&amp;1\end{bmatrix}=\begin{bmatrix}0&amp;-1&amp;6\\1&amp;0&amp;4\\0&amp;0&amp;1\end{bmatrix}.\tag1$$ We check: $$\begin{bmatrix}0&amp;-1&amp;6\\1&amp;0&amp;4\\0&amp;0&amp;1\end{bmatrix}\begin{bmatrix}2\\4\\1\end{bmatrix}=\begin{bmatrix}2\\6\\1\end{bmatrix}.$$ Iâll leave the other corners for you to verify.  </p>

<p>You can save having to do one of the matrix multiplications by skipping the first translation. Applying the rotation to the rectangle rotates it about the originâits lower-left cornerâso the necessary translation to then apply is the one that takes this new center to the target. The rotated center is $(-2,1)$, so the necessary translation is now $(4,5)-(-2,1)=(6,4)$, which, not coincidentally, is the last column of the matrix derived in (1). To see why this might be so, you can write and multiply these matrices in block form: $$\left[\begin{array}{c|c}I_2&amp;\mathbf c_2 \\ \hline \mathbf 0^T&amp;1 \end{array}\right]\left[\begin{array}{c|c}R &amp; \mathbf 0 \\ \hline \mathbf 0^T&amp;1\end{array}\right]\left[\begin{array}{c|c}I_2&amp;-\mathbf c_1 \\ \hline \mathbf 0^T&amp;1\end{array}\right]=\left[\begin{array}{c|c}R &amp; \mathbf c_2-R\mathbf c_1 \\ \hline \mathbf 0^T&amp;1 \end{array}\right]=\left[\begin{array}{c|c}I_2 &amp; \mathbf c_2-R\mathbf c_1 \\ \hline \mathbf 0^T&amp;1 \end{array}\right]\left[\begin{array}{c|c}R &amp; 0 \\ \hline \mathbf 0^T&amp;1 \end{array}\right].$$ Here, $I_2$ is the $2\times2$ identity matrix, $R$ is the $2\times2$ rotation matrix, and $\mathbf c_1$ and $\mathbf c_2$ are the respective rectangle centers. In the process, weâve also shown that a translate-rotate-translate sequence is equivalent to a rotate-translate sequence.  </p>

<p>Now, thereâs a quicker way to build the transformation matrix that youâre looking for by using an important property of transformation matrices: their columns are the images of the basis vectors, but thatâs getting ahead of where you appear to be in this subject.</p>

<p>Note that since you havenât specified how the vertices of the original rectangle correspond to those of the target, there are several other rigid motions (a.k.a. isometries) that will map one onto the other. For instance, you could rotate the rectangle in the opposite direction. That will also change the translation component of the transformation. Or, you could reflect the rectangleâflip it overâin various ways to get it aligned the right way.</p>
"
"2395471","2395803","<p>If by $\otimes_\pi$ you mean the projective tensor product then the answer is positive for $p=1$ as you remarked and otherwise it is false. Indeed, take $p=2$ and $X=L_2$. Then $L_2(L_2)$ is a Hilbert space but $L_2\otimes_\pi L_2$ is the dual of the space of compact operators on $L_2$, hence it is non-reflexive.</p>
"
"2395481","2395627","<p>I hope this explains why you need to make a perfect square.</p>

<p>Starting with your original problem ....</p>

<p>$$\begin{align}
\int{\frac{x+1}{x^2-x+1}dx} &amp;= \int{\frac{x+1}{x^2-\frac{1}{2}x-\frac{1}{2}x+1}dx}\\
&amp;= \int{\frac{x+1}{x^2-\frac{1}{2}x-\frac{1}{2}x+\frac{1}{4} + \frac{3}{4}}dx}\\
&amp;= \int{\frac{x+1}{x(x-\frac{1}{2})-\frac{1}{2}(x-\frac{1}{2}) + \frac{3}{4}}dx}\\
&amp;= \int{\frac{x+1}{(x-\frac{1}{2})^2 + \frac{3}{4}}dx}\\
&amp;= \int{\frac{x+1}{\frac{3}{4}\cdot \frac{4}{3}(x-\frac{1}{2})^2 + \frac{3}{4}}dx}\\
&amp;= \frac{1}{\frac{3}{4}}\int{\frac{x+1}{\frac{4}{3}(x-\frac{1}{2})^2 + 1}dx}\\
&amp;= \frac{1}{\frac{3}{4}}\cdot\frac{\frac{4}{3}}{\frac{4}{3}}\int{\frac{x+1}{\frac{2^2}{\sqrt{3}^2}(x-\frac{1}{2})^2 + 1}dx}\\
&amp;= \frac{4}{3}\int{\frac{x+1}{\bigg(\frac{2}{\sqrt{3}}(x-\frac{1}{2})\bigg)^2 + 1}dx}\\
&amp;= \bigg(\frac{2}{\sqrt{3}}\bigg)^2\int{\frac{x-\frac{1}{2}+\frac{1}{2}+1}{\bigg(\frac{2}{\sqrt{3}}(x-\frac{1}{2})\bigg)^2 + 1}dx}\\
&amp;= \frac{\bigg(\frac{2}{\sqrt{3}}\bigg)^2}{\color{red}{\frac{2}{\sqrt{3}}}}\int{\frac{\color{red}{\frac{2}{\sqrt{3}}}(x-\frac{1}{2}+\frac{3}{2})}{\bigg(\frac{2}{\sqrt{3}}(x-\frac{1}{2})\bigg)^2 + 1}dx}\\
&amp;= \frac{2}{\sqrt{3}}\int{\frac{\color{green}{\frac{2}{\sqrt{3}}(x-\frac{1}{2})}+\sqrt{3}}{\bigg(\color{green}{\frac{2}{\sqrt{3}}(x-\frac{1}{2})}\bigg)^2 + 1}dx}\\
\end{align}$$ </p>

<p>Now, just make the substitution $\displaystyle u = \frac{2}{\sqrt{3}}(x-\frac{1}{2})$</p>
"
"2395496","2395508","<p>You might find the relevant sections of Lawson and Michelsohn's <em>Spin Geometry</em> a useful resource during the course (maybe not so much as a precursor). In particular, it includes a discussion of the Atiyah-Singer index theorem and its applications to questions about positive scalar curvature. It also includes a very important and well-known theorem by Gromov and Lawson which states that surgery in codimension at least three preserves the existence of positive scalar metrics.</p>

<p>As for learning about scalar curvature, any book on Riemannian geometry will do. For example, take a look at the books by do Carmo, Lee, Petersen, etc. </p>
"
"2395499","2395555","<p>For the first part, let $F_t : [0, 1] \times X \to X$ be a transformation given by the formula $F_t(p) = (1-t) \cdot p$. It is continuous, $F_0 = \operatorname{id}_X$ and $F_1(p) = 0$ for all $p \in X$. </p>

<p>Let $G_t : [0, 1] \times X \to X$ be a transformation given by $G_t( p ) = t \cdot \pi_x(p)$ where $\pi_x : X \to I$ is the orthogonal projection. Then $G_t$ is continuous, $G_0(p) = 0$ for all $p \in X$ and $G_1[X] = I$. </p>

<p>Clearly the composition of these two transformations is a deformation retraction of $X$ onto $I$.</p>

<p>Intuitively, we pull everything to $0$ in the first phase and expand it to $I$ in the second phase.</p>

<hr>

<p>For the second part, suppose $H : [0, 1] \times X \to X$ is a strong deformation retraction onto $I$. Since </p>

<p>$$V = \left\{ (x, y) \in X : x &gt; \frac{1}{2} \right\}$$</p>

<p>is an open neighbourhood of the point $\left&lt; 1, 0 \right&gt;$ in $X$, the preimage $G = H^{-1}[V]$ is open. Moreover $[0, 1] \times \{ \left&lt; 1, 0 \right&gt; \} \subseteq G$ because $H$ is a strong deformation. By compactness of $[0, 1]$, there is open neighbourhood $U \subseteq X$ of $\left&lt; 1, 0 \right&gt;$ such that $[0, 1] \times U \subseteq G$. </p>

<p>Let $p \in U \setminus I$, specifically $p \in U \cap I_n$ for some $n \in \mathbb{N}$, where $I_n$ is the segment with slope $\frac{1}{n}$. Now $H \big[ [0, 1] \times \{ p \} \big]$ is a connected subset of $X$, and also a subset of $V$, so it must be a subset of the connected component of $H( 0, p ) = p$ in $V$ which is $V \cap I_n$. So $H(1, p) \in V \cap I_n$, thus $H(1, p) \notin I$ which is a contradiction.</p>

<p>Intuitively, if $I$ is fixed by $H$ throughout time, a small neighbourhood of $\left&lt; 1, 0 \right&gt;$ must stay close to $\left&lt; 1, 0 \right&gt;$ during the transformation, but it can't end up being moved to $I$ without going through the origin, which is far from $\left&lt; 1, 0 \right&gt;$.</p>
"
"2395504","2395511","<p>Hint: Find $n$ so that $1-\frac{1}{n}\leq a&lt;1-\frac{1}{n+1}$</p>
"
"2395506","2395512","<p>You have say
$$f(x)=\frac{g(x)}{(x-r)(x-s)}.$$
Then
$$f(x)=\frac{r^{-1}s^{-1}g(x)}{(r^{-1}x-1)(s^{-1}x-1)}
=\frac{r^{-1}s^{-1}g(x)}{(1-ux)(1-vx)}$$
where $u=1/r$, $v=1/s$.</p>
"
"2395518","2395536","<p>Let $v=(1,0)^T$. Then $Tv=(0,1)^T$. Thus $\{v,Tv\}$ is a basis of $F^2$.</p>

<p>Conclusion: $v$ is a cyclic vector.</p>
"
"2395519","2395526","<p>split the cases:</p>

<p><strong>Nonnegative $x$</strong>:</p>

<p>For nonnegative values of $x$, you have $f(|x|)=f(x)$ which means that $f(|x|)\leq|f(x)|$ if and only if $f(x)\leq |f(x)|$ which is always true.</p>

<p><strong>negative $x$</strong>:</p>

<p>For negative values of $x$, the inequality $f(|x|)\leq |f(x)|$ translates to
$$f(-x) \leq |f(x)|$$ which, for example, is true for odd functions like $\sinh$ since if $f(-x)=-f(x)$, then $f(-x)=-f(x)\leq |f(x)|$.</p>
"
"2395529","2395761","<p>Perron-Frobenius theorem asserts more than what you said in your first bullet point. In fact, when $A$ is irreducible (this includes the special case where $A$ is positive), $r=\rho(A)$, the spectral radius of $A$, is a <strong>simple</strong>, <strong>positive</strong> and <strong>dominant</strong> eigenvalue of $A$, and $A$ has a positive left-eigenvector and a positive right-eigenvector for this eigenvalue. (It follows that even when $A$ has more than one positive eigenvalues, $\rho(A)$ is the only eigenvalue that has a positive left/right-eigenvector. So, in your first bullet point, we are not talking about just any positive eigenvalue of $A$, but specifically the spectral radius $\rho(A)$.)</p>

<p>This means $A/r$ has a Jordan decomposition $A/r=P(1\oplus J)P^{-1}$, where $J$ is a Jordan normal form whose eigenvalues all have magnitudes <strong>strictly smaller</strong> than $1$. Let the first column of $P$ be $v$ and the first row of $P^{-1}$ be $w$. Then $v$ and $w$ are respectively right- and left-eigenvectors of $A$ for the eigenvalue $1$ and $w^Tv=1$.</p>

<p>It is well known that we can always make the super-diagonal of any non-trivial Jordan block arbitrarily small via a similarity transform using a diagonal matrix. That is, for any $\epsilon&gt;0$, we can always pick a diagonal matrix $D$ (e.g. $D=\operatorname{diag}(1,\epsilon,\epsilon^2,\ldots,\epsilon^{n-1})$) such that $1\oplus J=D(1\oplus B)D^{-1}$ for some bidiagonal matrix $B$ whose diagonal entries (i.e. eigenvalues) are exactly those of $J$ and whose super-diagonal entries all lie inside $[0,\epsilon]$. Therefore, when $\epsilon$ is sufficiently small, the induced $\infty$-norm of $B$ (i.e. $\|B\|_\infty=\max_i\sum_j|b_{ij}|$) is strictly smaller than $1$. So, we see that $\lim_{k\to\infty} B^k=0$ and in turn
$$
\lim_{k\to\infty} (A/r)^k=PD(1\oplus 0)D^{-1}P^{-1}=P(1\oplus 0)P^{-1}=vw^T.
$$</p>
"
"2395531","2395541","<p>The question is: if(!) $|x-a|&lt;|a|$, then $x$ and $a$ have the same sign.</p>

<p>We have $|x-a|&lt;a \iff -|a|&lt;x-a&lt;|a|$.</p>

<p>Case 1: $a&gt;0$. Then we have $0&lt;x&lt;2a$ and we are done.</p>

<p>Case 2: $a&lt;0$. Then we have $2a&lt;x&lt;0$ and we are done.</p>
"
"2395537","2395604","<p>The estimate on p. 4 does not depend on the properties of $N_0$, only on the Tauberian condition $n|a_n|\to 0$ as $n\to\infty$. </p>

<p>Let $b_n=n|a_n|$. Then $b_n\to 0$, and hence the averages ${1\over N}(b_1+\cdots+b_N)$ will $\to 0$ as well. So if $N$ is large enough, ${1\over N}(b_1+\cdots+b_N)&lt;\epsilon$, or $\sum_{k=0}^Nn|a_n|&lt;N\epsilon$.</p>
"
"2395547","2395564","<p><strong>Hint</strong> Use HÃ¶lder's inequality for series.</p>

<p>Edit: since this answer was accepted, I think @cmi found the solution, so here
is my version: HÃ¶lder's inequality gives</p>

<p>$$\sum _{n = 1}^{\infty } {a}_{n}^{\frac{1}{4}} \frac{1}{{n}^{\frac{4}{5}}}  \leqslant  {\left(\sum _{n = 1}^{\infty } {\left({a}_{n}^{\frac{1}{4}}\right)}^{p}\right)}^{\frac{1}{p}} {\left(\sum _{n = 1}^{\infty } {\left(\frac{1}{{n}^{\frac{4}{5}}}\right)}^{q}\right)}^{\frac{1}{q}}$$</p>

<p>when $\frac{1}{p}+\frac{1}{q} = 1$ and $p , q  \geqslant  1$. We choose $p = 4$ and
$q = \frac{4}{3}$. It gives</p>

<p>$$\sum _{n = 1}^{\infty } {a}_{n}^{\frac{1}{4}} \frac{1}{{n}^{\frac{4}{5}}}  \leqslant  {\left(\sum _{n = 1}^{\infty } {a}_{n}\right)}^{\frac{1}{4}} {\left(\sum _{n = 1}^{\infty } \frac{1}{{n}^{\frac{16}{15}}}\right)}^{\frac{3}{4}}$$</p>

<p>Both series on the RHS of this inequality converge, hence the series on the LHS also
converges.</p>
"
"2395570","2395652","<p><strong>First case</strong>: 
$ 100 \leq n $. 
In this case we $\color{Green}{\text{claim}}$ that<br>
$\color{Green}{\text{the last digit is equal to zero}}$ , 
conversly every integer with the last digit equal to zero 
has the above property.</p>

<hr>

<p>Let </p>

<p>$$ 100 \leq 
n=\overline{  a_m  a_{m-1}  ...  a_1  a_0  }= 
a_m10^m + a_{m-1}10^{m-1} + ... + a_110 + a_0  \  
; \ \ \   
\text{i.e.} \ \ 
2 \leq m $$ </p>

<p>with $a_m \neq 0$, 
also on the otherhand let </p>

<p>$$n ^ {\prime}=\overline{  a_m  a_{m-1}  ...  a_1  }= 
a_m10^{m-1} + a_{m-1}10^{m-2} + ... + a_210 + a_1  \  .$$ </p>

<hr>

<p>then one can see easily that: </p>

<p>$$\color{Blue} {n=10n^{\prime}+a_0} ,$$ </p>

<p>also notice that </p>

<p>$$ a_0 &lt; 10 
\ \ \ \ \ \ \ \ \ \ \ \ 
\text{and} 
\ \ \ \ \ \ \ \ \ \ \ \ 
10 \leq n^{\prime},$$ </p>

<p>so we can conclude that: </p>

<p>$$a_0  &lt; n^{\prime} 
\ \ \ \ \ \ \ \ 
,\text{i.e.}   
\ \ \ \ \ \ \ \ \ \ \ \ 
0  \leq  \dfrac  {a_0}  {n^{\prime}}  &lt; 1 .$$</p>

<hr>

<p>Now notice that: </p>

<p>$$       \dfrac{n}{n^{\prime}}     = 
\dfrac{    10    n^{\prime}    +    a_0}    {    n^{\prime}    } = 
\dfrac{    10    n^{\prime}            }    {    n^{\prime}    } + 
\dfrac{                             a_0}    {    n^{\prime}    } = 
10 + 
\dfrac{                             a_0}    {    n^{\prime}    } 
\ \ \ \ \ \ \ \ 
\Longrightarrow 
\\ 
\color{Red} 
{       10 \leq \dfrac{n}{n^{\prime}} &lt; 10+1=11       } 
, $$ </p>

<p>so we must have: $\color{Red} 
{       \dfrac{n}{n^{\prime}}=10       }$ , 
i.e. $\color{Blue} {n=10n^{\prime}+0}$ ; 
which implies that 
$\color{Green}{a_0=0}$ . </p>

<hr>

<hr>

<hr>

<hr>

<p><strong>Second case</strong> : $n &lt; 100$, which can be done by a simple calculation! </p>
"
"2395574","2396054","<p>This is possible.</p>

<p>Consider the check matrix</p>

<p>$$H=\begin{pmatrix}1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;0\\0&amp;1&amp; 2 &amp;3 &amp;4 &amp;5&amp; 6 &amp;7&amp; 8 &amp;9&amp; 10&amp;0\\0^2&amp;1^2&amp; 2^2 &amp;3^2 &amp;4^2 &amp;5^2&amp; 6^2 &amp;7^2&amp; 8^2 &amp;9^2&amp; 10^2&amp;1\end{pmatrix}.$$ </p>

<p>We need to prove that any three columns are linearly independent over $\Bbb{F}_{11}$. If the triple of columns does not include the last column, then this follows from a Vandermonde argument as in the check matrix you used to produce a $[12,9,4]_{13}$ code. On the other hand, if the triple includes the last column and any two others we can expand the resulting determinant along that last column. We end up with a 2x2 Vandermonde determinant, and the claim follows again.</p>
"
"2395577","2395579","<p>$X_1$ is the span of the single vector $x_1$ so has basis $\{ x_1 \}$ and hence is one dimensional.</p>

<p>Edit: To answer the second question that you added - $X_1$ is a closed proper subspace of $X$ since $\mbox{dim}(X) = \infty$ so we are able to apply Riesz' lemma to the subspace $X_1$ of $X$ to find a vector $x_2 \not \in X_1$ with the desired distance property. Note that $X_1$ contains exactly one vector of norm $1$ whilst $X$ contains infinitely many.</p>
"
"2395581","2396117","<p>You solve the system and get</p>

<p>$\left[x= -\dfrac{3-2 \lambda}{2 \lambda+3},\;y= -1,\;z= -\dfrac{3 (2 \lambda+1)}{2 \lambda+3}\right]$</p>

<p>then you use the constraint</p>

<p>$x^2+y^2+z^2-2x+2y+6z+9=0$</p>

<p>and substitute. Simplify</p>

<p>$(2 \lambda-3) (2 \lambda+9)=0$</p>

<p>$\lambda=\dfrac{3}{2};\;\lambda=-\dfrac{9}{2}$</p>

<p>Substitute again in the solutions and get</p>

<p>$x= 2,\;y= -1,\;z= -4$ we have a maximum $27$</p>

<p>and</p>

<p>$x = 0,\; y = -1,\; z = -2$ we have a minimum $3$</p>

<p>hope this helps</p>
"
"2395583","2395989","<p>Completing the square:
$$f(x)=2x(1-x) = -2 (x-\tfrac 1 2)^2 +\tfrac 1 2$$
We can see more clearly what $f$ does to $X=x-\tfrac 1 2$ by defining
$$F(X) = f(X+\tfrac 1 2) - \tfrac 1 2$$
so
$$F(X)=-2X^2$$
which gives
$$F^{\circ n}(X) = -2^{2^n-1}X^{2^n}$$
$$f^{\circ n}(x) = -2^{2^n-1}(x-\tfrac 1 2)^{2^n}+\tfrac 1 2$$</p>

<p>I get the integral over $x\in[0,1]$ to be $\tfrac 1 2 - \frac 1 {(2^n+1)2^n}$, so there might be an arithmetic mistake somewhere.</p>
"
"2395586","2395608","<p>There are always infinitely many solutions. Suppose $M$ is a $k\times n$ matrix. Choose $a\in\mathbb{R}$, let $I$ denote the identity matrix and let $D_1 = a I_{k}$ and let $D_2 = -a I_{n}$. Then we get
$$
D_1M + MD_2 = aI_kM + M(-a)I_n = aI_kM-aMI_n = aM - aM = 0.
$$</p>
"
"2395597","2395610","<p>as $\lim _{ x\rightarrow 0 }{ x\log { x } =0 } $$$\lim _{ x\rightarrow 0 }{ \frac { 1-{ x }^{ x } }{ x\log { x }  }  } =\lim _{ x\rightarrow 0 }{ \frac { 1-{ e }^{ x\log { x }  } }{ x\log { x }  }  } \overset { L'Hospital }{ = } \ =\lim _{ x\rightarrow 0 }{ \frac { -{ x }^{ x }\left( \log { x+1 }  \right)  }{ \left( \log { x+1 }  \right)  }  } =-\lim _{ x\rightarrow 0 }{ { e }^{ x\log { x }  } } =-1$$</p>
"
"2395602","2395790","<p>The general recipe for answering such questions is based on a <a href=""https://en.wikipedia.org/wiki/Cut-elimination_theorem"" rel=""nofollow noreferrer"">cut-elimination theorem</a>. The authors state in footnote 28 that the $Cut$ rule is ""admissible"" for this calculus, which is another way to say that it can be eliminated.[*] To decide whether $\bigcirc A \to \bigcirc B \vdash \bigcirc (A \to B)$ is provable, it therefore suffices to ask, ""Does it have a cut-free proof?""</p>

<p>Now, any cut-free proof of $\bigcirc A \to \bigcirc B \vdash \bigcirc (A \to B)$ must necessarily end in either ${\to}L$ or ${\bigcirc}R$, as in one of the following proof attempts:
$$
 \frac{\vdash \bigcirc A \qquad \bigcirc B \vdash \bigcirc (A \to B)}{\bigcirc A \to \bigcirc B \vdash \bigcirc (A \to B)} {\to}L
\qquad
 \frac{\bigcirc A \to \bigcirc B \vdash A \to B}{\bigcirc A \to \bigcirc B \vdash \bigcirc (A \to B)} {\bigcirc}R
$$
The first possibility can be immediately eliminated because it is not possible to prove $\vdash \bigcirc A$ for arbitrary $A$ (take $A$ to be any atomic formula). The second possibility can likewise be eliminated through a bit more case analysis of cut-free proofs: eventually we would be required to prove $\bigcirc B \vdash B$, but this is not possible for general $B$.</p>

<p>Therefore, $\bigcirc A \to \bigcirc B \vdash \bigcirc (A \to B)$ is not provable for general $A$ and $B$.</p>

<p>[*] technically, a rule is <em>admissible</em> for a sequent calculus if given derivations of the premises, we can construct a derivation of the conclusion without using the rule itself. In particular, the $Cut$ rule is admissible if given cut-free derivations of $\Gamma \vdash B$ and $B, \Delta \vdash C$, we can always construct a cut-free derivation of $\Gamma,\Delta \vdash C$.</p>

<hr>

<p><strong>Edit</strong>: Of course, the extra bit of reasoning which is missing from this argument is the proof of cut-elimination itself which the authors claim in the footnote. That is usually done on a case-by-case basis for a particular sequent calculus, and is a good exercise (you can find an example for another sequent calculus in these <a href=""https://www.cs.cmu.edu/~fp/courses/15317-f09/lectures/10-cutelim.pdf"" rel=""nofollow noreferrer"">lecture notes by Frank Pfenning</a>).</p>
"
"2395611","2397215","<p>Check that $N_{G/S}(T/S) = N_G(T)/S$ and $Z_{G/S}(T/S) = Z_G(T)/S$ and use the third isomorphism theorem.</p>

<p>I'll write out the details of why $Z_{G/S}(T/S) = Z_G(T)/S$.  The inclusion '$\supseteq$' is clear.  Conversely, let $gS \in G/S$, and suppose $gS$ commutes with every element of $T/S$.  This means that for all $t \in T$, $gtg^{-1} \in S$.  Then $gtg^{-1}$ commutes with everything in $G$, so </p>

<p>$$gt = (gtg^{-1})g = g(gtg^{-1})$$</p>

<p>which implies $t = gtg^{-1}$.  Thus $g \in Z_G(T)$, and $gS \in Z_G(T)/S$.</p>
"
"2395620","2395720","<p>Since the two points are moving independently on  opposite sides of $[0,1]^2$ we have to compute the integral
$$J:=\int_0^1\int_0^1\sqrt{1+(x-y)^2}\&gt;dxdy\ .$$
This can be done in elementary terms; the result is, according to Mathematica,
$${2\over3}-{\sqrt{2}\over3}+{\rm arsinh}(1)\doteq1.07664\ .$$</p>
"
"2395628","2395664","<p>It might be easier if you think of it in a different way. Instead of trying to sum up all the whisky you draw out of the jar, try to find how many dips it'll take for the volume of whisky in the jar to have decreased by the volume of the shot glass.</p>

<p>Notice that $V_{J}$ and $V_{S}$ are both proportional to $H$ by a constant factor, and hence they're proportional to each other. If the volume in the jar after $n$ dips is $V_{J,n}$, then you can find $H_{n}$ and hence $V_{S,n}$.</p>

<blockquote class=""spoiler"">
  <p> $V_{S,n} = \pi r_{S}^{2} H_{n} = \pi r_{S}^{2} \frac{V_{J,n}}{\pi r_{J}^{2}} = \frac{r_{S}^{2}}{r_{J}^{2}} V_{J,n}$$</p>
</blockquote>

<p>The change in volume in the jar is going to just be the volume held in the straw, $V_{S,n}$ and this will be proportional to the volume in the jar before the straw is dipped in, $V_{J,n}$. So the volume left in the jar after a dip will be a fixed fraction of the volume before the dip, hence you can express $V_{J,n+1}$ in terms of $V_{J,n}$:</p>

<blockquote class=""spoiler"">
  <p> $V_{J,n+1} = \left(1-\frac{r_{S}^{2}}{r_{J}^{2}}\right) V_{J,n}$</p>
</blockquote>

<p>You can use this to find a formula for $V_{J,n}$ in terms of $V_{J,0}$.</p>

<blockquote class=""spoiler"">
  <p> $V_{J,n} = \left(1-\frac{r_{S}^{2}}{r_{J}^{2}}\right)^{n} V_{J,0}$</p>
</blockquote>

<p>Then you just want the smallest integer $n$ where $V_{J,0}-V_{J,n} \ge 4\ \mathrm{cl}$.</p>
"
"2395631","2395660","<p>Physicists like to use the notation $\operatorname d \vec S$, i.e. they assign a direction to the area element, so it becomes a vector and the scalar product is ""well-defined"". This direction is simply the unit normal. Hence
$\operatorname d \vec S= \hat n \operatorname d S$. I guesse some authors simply ommit the arrows as usual in the mathematics literature.</p>

<p>In summary, this are equivalent notations.</p>
"
"2395633","2395690","<p>$$f(X|Z) = \frac{f(Z|X)f(X)}{f(Z)}$$</p>

<p>$$f(Z|X) \sim \mathcal{N}(HX,Q)$$</p>

<p>$$f(X) \sim \mathcal{N}(\mu, \Sigma)$$</p>

<p>$$f(Z) \sim \mathcal{N}(H\mu,H\Sigma H^{T}+Q)$$</p>

<p>I assumed that $X$ and $W$ are independent. Note that $Z$ is sum of two normal random variables which will be a random variable. Take expectation on both sides to get the mean of $Z$ and similarly for variance.</p>
"
"2395642","2395659","<p>$s=2m(m+n)d \implies s/2=m(m+n)d&gt;m^2$ so $\color{blue}{m&lt;\sqrt {s/2}}$</p>

<p>Also, $s/2=m(m+n)d \implies \color{blue}{m \mid s/2}$</p>

<p>So those are two outcomes of the given equation, not derived from each other (in either direction).</p>
"
"2395657","2395667","<p><strong>Hint</strong></p>

<blockquote>
  <p>$$m^2-n^2+4n-4$$</p>
</blockquote>

<p>Notice that $n^2-4n+4 = \left(n-2\right)^2$ by completing the square, so you have:</p>

<p>$$m^2-n^2+4n-4 = m^2-\left(n-2\right)^2$$</p>

<p>Now use the formula $a^2-b^2=(a-b)(a+b)$; what are $a$ and $b$ in your case?</p>

<blockquote>
  <p>$$(2+y)^2-9z^6$$</p>
</blockquote>

<p>Use the same formula but with $a=2+y$ and $b=3z^3$ since $\left( 3z^3 \right)^2 = 9z^6$.</p>
"
"2395662","2395669","<p>Please note that $i\sin\left(\pi\left(\frac12+m\right)\right)=i\sin\left(\pi\frac{2m+1}2\right)=i\sin\left(m\pi+\frac\pi2\right)=i(-1)^m$.</p>
"
"2395672","2395712","<p>We can prove that
$$c-l\geq\left(1-\frac{\sqrt3}{2}\right)c$$ or
$$\frac{\sqrt3}{2}c\geq\frac{1}{2}\sqrt{2a^2+2b^2-c^2}$$ or
$$3c^2\geq2a^2+2b^2-c^2$$ or
$$2c^2-a^2-b^2\geq0.$$ 
The equality occurs for $a=b=c$, </p>

<p>which says that $1-\frac{\sqrt3}{2}$ is a maximal value of $k$ for which the inequality
$$c-l\geq kc$$
is true for all triangle whith $c\geq b\geq c$. </p>

<p>By the way, for $c\rightarrow0^+$ in $c-l\geq\alpha$ we obtain $0\geq\alpha$, which gives $\alpha=0$ is a maximal $\alpha$,</p>

<p>for which your inequality is true.</p>
"
"2395685","2395697","<p>You are wrong about when the equality $|x|-|y|=|x-y|$ holds. It holds when (and only when) of of these conditions holds:</p>

<ul>
<li>$x\geqslant y\geqslant0$;</li>
<li>$x\leqslant y\leqslant0$.</li>
</ul>

<p>I shall now use this to solve your problem. The equality holds if and only if$$3x^2-5\geqslant2x^2+3\geqslant0\text{ or }3x^2-5\leqslant2x^2+3\leqslant0.$$The secons possibility cannot take place, of course. On the other hand\begin{align}3x^2-5\geqslant2x^2+3&amp;\iff x^2\geqslant8\\&amp;\iff x\geqslant\sqrt8\vee x\leqslant-\sqrt8.\end{align}</p>
"
"2395686","2395704","<p>Aplying the $n-$th root test you can find that $R=\frac{1}{\lim_{n \rightarrow \infty}\sqrt[n]{a_n}}=5$</p>
"
"2395688","2395701","<p>This is a Geometric series, for $|r|&lt;1:$ $$\sum_{m=0}^\infty r^m= \frac{1}{1-r}$$ Take $r=-e^{-\xi}$, then $\xi&gt;0$ so $|r|&lt;1$ and thus $$\sum_{m=0}^\infty r^m=\sum_{m=0}^\infty (-e^{-\xi})^m=\sum_{m=0}^\infty (-1)^me^{-m\xi}=\frac{1}{1-(-e^{-\xi})}=\frac{1}{1+e^{-\xi}}$$</p>
"
"2395699","2396858","<p><strong>TL;DR</strong> You are correct.</p>

<p>So, in <a href=""https://math.stackexchange.com/questions/793755/given-that-xy-yx-what-could-x-and-y-be?noredirect=1&amp;lq=1"">the question you were already given the link to</a>, it is stated that $$f(x) = -\frac x{\ln x}W\left(-\frac{\ln x}x\right),$$</p>

<p>however, it is bit imprecise, because $W$ is multivalued function, with its branches denoted with $W_0$ and $W_{-1}$ and thus, if you were to plot $f$ this way in some software that has <a href=""https://en.wikipedia.org/wiki/Lambert_W_function"" rel=""nofollow noreferrer"">Lambert W-function</a> implemented, you would get the same graph as by plotting $x^y-y^x=0.$</p>

<p>Thus, the first step would be to eliminate the line $y=x$ to get your $g$.</p>

<p>We will use <a href=""https://en.wikipedia.org/wiki/Lambert_W_function#Identities"" rel=""nofollow noreferrer"">identities</a> </p>

<p>\begin{align}
W_0(xe^x) = x,&amp;\ x\geq -1,\\
W_{-1}(xe^x) = x,&amp;\ x\leq -1,
\end{align}</p>

<p>to identify which branch in the above definition of $f$ we need to use:</p>

<p>\begin{align}
f(x) = x &amp;\iff -\frac x{\ln x}W\left(-\frac{\ln x}x\right) = x\\
&amp;\iff W\left(te^t\right) = t,\ x = e^{-t}
\end{align}</p>

<p>By the above identities, we need to use $W_{-1}$ when $t\geq -1$ and $W_{0}$ when $t\leq -1$ to avoid $f(x) = x$. Substituting back with $x$, we get the following definition of $f$:</p>

<p>$$f(x) = \begin{cases}
-\frac x{\ln x}W_{-1}\left(-\frac{\ln x}x\right),\ 0&lt;x&lt; e,\\
-\frac x{\ln x}W_{0}\left(-\frac{\ln x}x\right),\ x\geq e,
\end{cases}$$</p>

<p>but here is the thing, $W_{-1}\left(-\frac{\ln x}x\right)$ is not defined on whole $(0,e)$. The reason is that $W_{-1}$ is only defined on $[-1/e,0)$, so we need to solve $-1/e \leq -\ln x/x &lt; 0$. We get that $x&gt;1$ so the correct definition of $f$ is </p>

<p>$$f(x) = \begin{cases}
-\frac x{\ln x}W_{-1}\left(-\frac{\ln x}x\right),\ \color{blue}{1&lt;x&lt; e},\\
-\frac x{\ln x}W_{0}\left(-\frac{\ln x}x\right),\ x\geq e.
\end{cases}$$</p>

<p>Finally, we can now give precise definition of $g$ as well (using $x^y = e^{y\ln x}$):</p>

<p>$$g(x) = \begin{cases}
\exp\left(-xW_{-1}\left(-\frac{\ln x}x\right)\right),\ 1&lt;x&lt; e,\\
\exp\left(-xW_{0}\left(-\frac{\ln x}x\right)\right),\ x\geq e.
\end{cases}$$</p>

<hr>

<p>Let us analize what is happening on $(1,e)$. We want to prove that $-xW_{-1}\left(-\frac{\ln x}x\right)$ is strictly decreasing on $(1,e)$. Let $u(x) = W_{-1}\left(-\frac{\ln x}x\right)$. By differentiating $-xu(x)$ we get $$-u(x) + \frac{(\ln x - 1)u(x)}{(1+u(x))\ln x}$$ and we want to prove it is strictly negative on $(1,e)$. Having in mind that $0&lt;\ln x&lt; 1$ and $u(x)\leq -1$ on $(1,e)$, we can manipulate the expression to get </p>

<p>$$-u(x) + \frac{(\ln x - 1)u(x)}{(1+u(x))\ln x} &lt; 0\iff u(x)+\frac1{\ln x} &gt; 0.$$</p>

<p>Now, to see this last thing is true involves some trickery which I will skip. It is not as hard as is very tedious. I can write it down if somebody would be interested in that.</p>

<p>We conclude that $-xu(x)$ is strictly decreasing on $(1,e)$ and so is $g(x)$.</p>

<hr>

<p>Now, the analysis on $[e,+\infty)$ is much easier. If we denote with $v(x) = W_0\left(-\frac{\ln x}{x}\right)$, then $-1 \leq v(x) &lt; 0$ and $v(x)$ is strictly increasing on $(e,+\infty)$ since both $W_0$ and $-\ln x/x$ are. Calculating derivative of $-xv(x)$ we get </p>

<p>$$-v(x) + \frac{(\ln x - 1)v(x)}{\ln x(1+v(x))}$$</p>

<p>and like the above case, we get that $$-v(x) + \frac{(\ln x - 1)v(x)}{\ln x(1+v(x))}&gt; 0 \iff v(x)\ln x &gt; -1$$ which is true on $(e,+\infty)$ since $v(x)&gt; -1$ and $\ln x&gt; 1$. Thus $-xv(x)$ is strictly increasing on $[e,+\infty)$ and so is $g$.</p>

<hr>

<p>Having done that, it immediately follows that $g$ has minimum at $x = e$, which is easy to calculate: $g(e) = \exp(-e W_0(-e^{-1})) = \exp(-e\cdot (-1)) = e^e.$</p>

<p>Finally, let us check the limits at boundaries:</p>

<p>$$\lim_{x\to 1^+} g(x) = \exp(\lim_{x\to 1^+} (-xu(x))) = \exp(\lim_{x\to 1^+} (-xW_{-1}(-\frac{\ln x}x))) = +\infty$$ because $\lim_{x\to 0^-}W_{-1}(x) = - \infty.$</p>

<p>To verify that $g(x)\sim x$ at $+\infty$:</p>

<p>$$\lim_{x\to+\infty}\frac{g(x)}{x} = \lim_{x\to+\infty}\frac{\exp(-xv(x))}{x} = \exp(\lim_{x\to+\infty}(-xv(x) - \ln x))$$</p>

<p>and to calculate last limit, we can use Taylor expansion of $W_0$ at $0$:</p>

<p>$$\exp(\lim_{x\to+\infty}(-xv(x) - \ln x)) = \exp(\lim_{x\to+\infty}(-x(-\frac{\ln x}{x}-\frac{\ln^2x}{x^2}+\ldots) - \ln x)) = e^0 = 1.$$</p>

<p>However, $\lim_{x\to +\infty}(g(x)-x) = +\infty$.</p>
"
"2395700","2395711","<p><strong>Hint</strong>: rewrite
$$\sin x \log x = \frac{\sin x}{x} \; x \log x$$
and use the well-known limits (fill in the gaps):
$$\lim_{x \to 0} \frac{\sin x}{x} = \ldots \quad \mbox{and} \quad \lim_{x \to 0^+} x\log x = \ldots$$</p>
"
"2395703","2395742","<p>Yes, it would still be an exact sequence. You mention that $\operatorname{im}(h')\cong\operatorname{im}(h)$ and $\ker(i)\cong \ker(i')$, but that these isomorphisms are not actually equalities; that's true, but these isomorphisms are actually induced by $\phi$ and its inverse, so in the end their composition is actually induced by the identity on $W'$, i.e. $\operatorname{im}(h')$ is really equal to $\ker(i')$.</p>

<p>To be more precise, you can prove the chain of equalities
$$\operatorname{im}(h')=\phi(\operatorname{im}(h))=\phi(\ker(i))=\ker(i').$$
The first one is a consequence of the equality $h'=\phi\circ h$, the second one comes from the exactness of the first sequence and the last one from the equality $i'=i\circ \phi^{-1}$.</p>
"
"2395707","2395721","<p>$$y=e^{y\ln (x)} $$</p>

<p>$$\ln (y)=y\ln (x) $$</p>

<p>$$x=e^{\frac {\ln (y)}{y}} $$</p>

<p>differentiating wrt $x $, we find</p>

<p>$$1=x \frac {1-\ln (y)}{y^2}y'$$</p>

<p>thus</p>

<p>$$y'=\frac {y^2}{x (1-\ln (y))} $$</p>
"
"2395722","2395734","<p>$$\mathbb{E}[X]=\mathbb{E}[X|X&gt;0]Pr(X&gt;0)+\mathbb{E}[X|X=0]Pr(X=0)$$</p>

<p>Hence </p>

<p>$$\mathbb{E}[X]=\mathbb{E}[X|X&gt;0]Pr(X&gt;0)+0$$</p>

<p>That is 
$$\mathbb{E}[X]=\mathbb{E}[X|X&gt;0]Pr(X&gt;0)$$</p>

<p>Final conclusion:</p>

<p>$$\mathbb{E}[X|X&gt;0]=\frac{\mathbb{E}[X]}{Pr(X&gt;0)}$$</p>

<p>Remark:</p>

<p>$$Pr(X&gt;0, X=x)=Pr(X=x)$$ is not true when $x=0$.</p>
"
"2395728","2396445","<p>Yesï¼it is just the definition of the weak solution.  You can use many methods to show the existence of such weak solutions, say Galerkin method, which is exactly what Evans used in his book.  For a vector-valued function $u\in L^p(0,T;X)$, the weak derivative $u'$ is defined as the distribution $T(u)$ such that 
$$
\int_0^T T(u)vdt=-\int_0^T u v' dt\quad \forall v\in C_c^\infty((0,T)).
$$
Then we define $W^{1,p}(0,T;X)=\{u\in L^p(0,T;X); u'\in L^p(0,T;X)\}$. $ u'\in L^p(0,T;X)$ if and only if  there exits a $L^p$ function $u'$ such that for any $f\in X'$, the function $t\to \langle f, u(t)\rangle$ is absolutely continuous, and $\langle f, u'\rangle=\frac{d}{dt}\langle f, u(t)\rangle$. </p>
"
"2395729","2395731","<p>You read those chains of inequalities as if they were all written on one line. They're on multiple lines only for typographic convenience.</p>

<p>The same is true of chains of equalities, but in that case the alternate reading just happens to be true.</p>
"
"2395737","2398757","<p>$$\newcommand{\ai}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\Ga}{\Gamma}
\newcommand{\ga}{\gamma}	
\newcommand{\de}{\delta}
\newcommand{\De}{\Delta}
\newcommand{\e}{\epsilon}
\newcommand{\lam}{\lambda}
\newcommand{\La}{\Lamda}
\newcommand{\om}{\omega}
\newcommand{\Om}{\Omega}
\newcommand{\si}{\sigma}
\newcommand{\vp}{\varphi}
\newcommand{\rh}{\rho}
\newcommand{\ta}{\theta}
\newcommand{\Ta}{\Theta}
\newcommand{\W}{\mathcal{O}}
\newcommand{\ps}{\psi}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\ms}[1]{\mathscr{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\cd}{\cdots}
\newcommand{\s}{\subset}
\newcommand{\es}{\varnothing}
\newcommand{\cp}{^\complement}
\newcommand{\bu}{\bigcup}
\newcommand{\ba}{\bigcap}
\newcommand{\co}{^\circ}
\newcommand{\ito}{\uparrow}
\newcommand{\dto}{\downarrow}		
\newcommand{\ti}[1]{\tilde{#1}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\ov}[1]{\overline{#1}}
\newcommand{\no}[1]{\left\lVert#1\right\rVert}
\newcommand{\du}{^\ast}
\newcommand{\pf}{_\ast}
\newcommand{\is}{\cong}
\newcommand{\n}{\lhd}
\newcommand{\m}{^{-1}}
\newcommand{\ts}{\otimes}
\newcommand{\ip}{\cdot}
\newcommand{\op}{\oplus}
\newcommand{\xr}{\xrightarrow}
\newcommand{\xla}{\xleftarrow}
\newcommand{\xhl}{\xhookleftarrow}
\newcommand{\xhr}{\xhookrightarrow}
\newcommand{\mi}{\mathfrak{m}}
\newcommand{\w}{\wedge}
\newcommand{\X}{\mathfrak{X}}
\newcommand{\pd}{\partial}
\newcommand{\dx}{\dot{x}}
\newcommand{\dr}{\dot{r}}
\newcommand{\dy}{\dot{y}}
\newcommand{\dth}{\dot{theta}}
\newcommand{\pa}[2]{\frac{\pd #1}{\pd #2}}
\newcommand{\na}{\nabla}
\newcommand{\dt}[1]{\frac{d#1}{d t}\bigg|_{ t=0}}
\newcommand{\ld}{\mathcal{L}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\bh}{\mathbb{H}}
\newcommand{\lix}{\lim_{x\to\infty}}
\newcommand{\li}{\lim_{n\to\infty}}
\newcommand{\infti}{\sum_{i=1}^{\infty}}
\newcommand{\inftj}{\sum_{j=1}^{\infty}}
\newcommand{\inftn}{\sum_{n=1}^{\infty}}	
\newcommand{\snz}{\sum_{n=-\infty}^{\infty}}	
\newcommand{\ie}{\int_E}
\newcommand{\ir}{\int_R}
\newcommand{\ii}{\int_0^1}
\newcommand{\sni}{\sum_{n=0}^\infty}
\newcommand{\ig}{\int_{\ga}}
\newcommand{\io}{\text{ i.o.}}
\newcommand{\aut}{\text{Aut}}
\newcommand{\out}{\text{Out}}
\newcommand{\inn}{\text{Inn}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\V}{\mathbf{V}}	
\newcommand{\II}{\mathbf{I}}
\newcommand{\wh}{\Rightarrow}
\newcommand{\eq}{\Leftrightarrow}
\newcommand{\eqz}{\setcounter{equation}{0}}
\newcommand{\se}{\subsection*}
\newcommand{\ho}{\text{Hom}}
\newcommand{\ds}{\displaystyle}
\newcommand{\tr}{\text{tr}}
\newcommand{\id}{\text{id}}
\newcommand{\im}{\text{im}}
\newcommand{\ev}{\text{ev}}$$</p>

<p>I suppose the sectional curvature $R$ actually means Riemann curvature instead, since sectional curvatures are usually defined on planes in tangent spaces intead of on $\ts^4T_xM .$</p>

<p>First note that $\om^i\w\om^j\w \Om^i_j$ is a $4$-form, and this is where the four inputs $i,j,k,l$ come from. By the shuffle definition of wedge product, we have $$\om^i\w\om^j(e_m,e_n)=\om^i(e_m)\om^j(e_n)-\om^i(e_n)\om^j(e_m)=\e^{ij}_{mn},$$ where $\e^{ij}_{mn}=1$ when $(m,n)=(i,j),$ $-1$ when $(m,n)=(j,i),$ and $0$ otherwise. </p>

<p>We know that $\Om^i_j(X,Y)=-R(X,Y,e_i,e_j),$
so for $i,j,k,l$ distinct we have
\begin{align*}
0=-&amp;\om^i\w\om^j\w \Om^i_j(e_i,e_j,e_k,e_l)\\=&amp;\sum_{\si\in Sh_{2,2}}(-1)^\si\om^i\w\om^j(e_{\si(i)},e_{\si(j)})R(e_{\si(k)},e_{\si(l)},e_i,e_j)\\
=&amp;R(e_k,e_l,e_i,e_j)\\
=&amp;R(e_i,e_j,e_k,e_l),
\end{align*}
where $Sh_{2,2}$ means $(2,2)$ shuffles, i.e., $\si(i)&lt;\si(j),\si(k)&lt;\si(l).$ 
$\newcommand{\np}{\wedge\!\!\!\!\!\!\bigcirc}$
Now, we have
\begin{align*}
W=R-\frac{1}{n-2}(Ric-\frac{S}{n}g)\wedge\!\!\!\!\!\!\bigcirc g-\frac{S}{2(n-2)}g\np g.
\end{align*}
By definition of Kulkarni-Nomizu product $\wedge\!\!\!\!\!\!\bigcirc,$ we have $T\np g(e_i,e_j,e_k,e_l)=0$ for any 2-tensor $T$ and distinct $i,j,k,l.$ This yields the desired result
$$W(e_i,e_j,e_k,e_l)=0.$$</p>
"
"2395741","2395748","<p>It's a common mistake.</p>

<p>Your pdf is</p>

<p>$$f(x| \theta) = \frac{1}{3}e^{-\frac{1}{3}(x-\theta)}1_{\{x\geq \theta\}}$$</p>

<p>Use this pdf in the likelihood (along with the indicator) and see for yourself that the MLE estimate of $\theta$ is $\min\limits_{i=1}^{n} X_i$.</p>

<p><strong>Edit</strong>:</p>

<p>$$L(\theta|x_{1},\ldots,x_{n}) = \left(\frac{1}{3}\right)^n e^{-\frac{1}{3n}(\bar{x}-\theta)}\prod_{i=1}^{n}1_{\{x_i\geq \theta\}}$$</p>

<p>Now, $\theta$ should satisfy $x_i \geq \theta \ \forall i$ otherwise likelihood will be zero. Therefore, $\theta \leq \min\limits_{i=1}^{n}x_i$. As, $\theta$ decreases further the exponential part decreases due to the negative sign in its power and therefore, the likelihood decreases.</p>
"
"2395747","2395774","<p>You want to use ""implicit differentiation"" to start, here:</p>

<p>$$ y^3 + 3a^2x + x^3 = 0 $$
$$ 3y^2y' + 3a^2 + 3x^2 = 0 \implies y^2y' + a^2 + x^2 = 0 $$
$$ 6yy'^2 + 3y^2y'' + 6x = 0 \implies 2yy'^2 + y^2y'' + 2x = 0 $$</p>

<p>Then a bit of algebra gives:</p>

<p>$$ y'' + \frac{2yy'^2 + 2x}{y^2} = 0 \quad \text{or} \quad y = 0 $$
$$ y'' + \frac{2y\left(\frac{-a^2 - x^2}{y^2}\right)^2 + 2x}{y^2} = 0 \quad \text{or} \quad y = 0 $$
$$ y'' + \frac{2\left(-a^2 - x^2\right)^2 + 2\left(-3ax^2 - x^3\right)x}{y^5} = 0 \quad \text{or} \quad y = 0 $$</p>

<p>If you then simplify further, you should find you get the desired result.</p>
"
"2395751","2395811","<p>You can't remove a position just because it leaves a win for the opponent.  The main idea behind combinatorial game theory (not emphasized in most expositions on the subject, but it is implicit in how the definitions are set up) is that the value of a game $G$ tells us not only how to play $G$, but also how to play $G + X$ for <em>any</em> game $X$. We can only remove a move from $G$ without changing its value if that move would not be a good move in $G + X$ for <em>any</em> game $X$.  </p>

<p>So how do we simplify a game?  In general, there are two ways of simplifying games: Deleting dominated positions, reversing reversible positions.  This is discussed in ONAG, Chapter 10, Theorem 68.</p>

<p>In your example for $\uparrow + \mbox{ }\ast$, everything is correct up to this form:</p>

<p>$$G = \{\ast,\uparrow|\:0,\uparrow\}$$</p>

<p>On the right side of $G$, we can indeed remove $\uparrow$, but the reason isn't because $\uparrow$ is a win for Left.  The reason is because $0 &lt; \,\,\uparrow$, so 0 dominates $\uparrow$ for Right (we say that $\uparrow$ is a dominated option).  In <em>any</em> context, $G + X$, Right should never move to $\uparrow$ if 0 is available as an alternative, because $0 + X$ will always be better than $\uparrow + \,X$ for Right (in the sense that if Right wins the latter then Right also wins the former).</p>

<p>On the left side of $G$, the option $\uparrow$ is a <em>reversible option</em>.  I'll let you look up the details in ONAG (or Winning Ways), but the idea is as follows.  If Left moves to $\uparrow$, Right can follow with a move to $\ast$.  But $\ast &lt; \, G$ (because $G =\,\uparrow +\,\ast$ and $\uparrow\, &gt; 0$), which is saying that if Left moves to $\uparrow$, Right can immediately follow with a move to $\ast$ that leaves Left in a worse position than the original $G$.  This means that there is no reason for Left to move to $\uparrow$ unless he plans to continue following through after Right's response to $\ast$ (as opposed to playing in some other component $X$; remember that we are studying $G+X$ for any game $X$). But after Right's response to $\ast$, Left's only move is to 0.  </p>

<p>The upshot of the previous paragraph is that we can replace Left's option $\uparrow$ with a Left option directly to 0.</p>

<p>In summary, in $G$, we have deleted $\uparrow$ from the right side, and replaced $\uparrow$ with 0 on the left side, giving the simplified form $$\{\ast, 0 | 0 \}$$ as claimed by Conway.</p>
"
"2395771","2395860","<p>The choice
$$x=64m^6+8m^2$$
works.</p>

<p>Then
$$x^3+x+m^2=(512m^9+96m^5+3m)^2.$$</p>
"
"2395775","2395856","<p>The quartic subfield is the fixed field of a subgroup of $S_4$ of order 6.  If the quartic field had a quadratic subfield, then the quadratic subfield would be fixed by a subgroup of $S_4$ of order $12$ containing the subgroup of order $6$.  $S_4$ has a unique subgroup of order $12$, namely $A_4$, and since $A_4$ contains no subgroup of order $6$ this is impossible.</p>
"
"2395782","2395788","<p>Hint. Note that for $x\not=a$,
$$\frac{x^2 + ax - 2a^2}{\sqrt{2x^2 - ax} -a}=\frac{(x+2a)(x-a)(\sqrt{2x^2 - ax} +a)}{(2x+a)(x-a)}.$$</p>
"
"2395789","2395795","<p>There are $4!$ ways to arrange the queue without the order restriction.  Let us say that two queues are equivalent if one is obtained from the other by switching $a,b$ or $c,d$ (or both or neither).  Then each equivalence class has $4$ queues in it, so there are $6$ equivalence classes.  Noting that each class contains exactly one member that passes your tests finishes the claim.</p>
"
"2395798","2395808","<p>You only need the fact that $f$ is continuous at $z=0$, which is a way weaker assumption than the analycity. Write the epsilon-delta definition of continuity at $0$ with $\epsilon = \lvert f(0) \rvert /2$.</p>

<p>In your post, you give two proofs that are moslty equivalent and use the <a href=""https://en.wikipedia.org/wiki/Principle_of_permanence"" rel=""nofollow noreferrer"">principle of permanence</a>. Both are correct but it is way too complicated to show such a simple fact.</p>

<p>Let me rewrite the second proof :</p>

<blockquote>
  <p>By contradiction, assume that every neighbourhood of $0$ contains a zero of $f$. Then there is a sequence $(z_n)$ of zeros of $f$ that converges to $0$. Indeed, consider the open ball centered at $0$ with radius $1/n$. It is a neighbourhood of $0$, hence it contains a zero $z_n$. By the principle of permanence, the set of zeros of $f$ has a limit point, which is a contradiction.</p>
</blockquote>
"
"2395799","2395850","<p>Hint: let $\Delta r = r / n\,$, then the sum runs from $0$ to $n\,$, and the terms are in arithmetic progression:</p>

<p>$$\require{cancel}
\frac{r}{n} \sum_{k=0}^{n}(r - k \frac{r}{n}) = \frac{r}{n} \left(r \sum_{k=0}^{n} 1 - \frac{r}{n} \sum_{k=0}^{n} k \right) = \frac{r^2}{n}\left(n - \frac{1}{\cancel{n}}\cdot \frac{\cancel{n}(n+1)}{2}\right) = \frac{r^2}{2} \cdot \frac{n-1}{n}
$$</p>

<p>For large enough $n\,$, the second factor $\,(n-1)/n = 1 - 1/n \,\simeq\, 1\,$.</p>
"
"2395800","2396001","<p>$C$ is the union of the graphs of two functions $f,g$ on $[-a,a]$, with  $f=\{(x,y)\in C: y\geq 0\}$  and $g=\{(x,y)\in C:y\leq 0\}.$ Since $g(x)=-f(x)$ for all $x\in [-a,a],$ it suffices to show that the content of the graph of $f$ is $0.$ </p>

<p>(i). For $a&gt;r&gt;0$ let $g_r$ be the part of the graph of $f$ restricted to the domain $[-a,-a+r]\cup [a-r,a].$ Now $g_r$ is covered by two rectangles: $[-a, -a+r]\times [0,f(-a+r)], $ and $[a-r,a]\times [0,f(a-r)]$.</p>

<p>So the content of $g_r$ is at most $D(r)=r(f(a-r)+f(-a+r)).$ Since $f$ is continuous at $\pm a$ with $f(\pm a)=0,$ the value $D(r)$ can be as small as desired, by taking sufficiently small  $r$.</p>

<p>We can conclude that the content of the graph of $f$ is $0$ if we can show that the content of $f$ resticted to $[-a+r,a-r]$ is $0, $ ....</p>

<p>.... which we do by putting $p=-a+r$ and $q=a-r$ in the following:</p>

<p>(ii).  Lemma. For $p&lt;q$ and for differentiable $f:[p,q]\to \mathbb R$  such that  $\sup \{|f'(x)|:x\in [p,q]\}=K&lt;\infty , $  the content of the graph of $f $ on $[p,q]$ is $0.$ </p>

<p>Proof: For $n\in \mathbb N$ let $d_n=\frac {q-p}{n}$ and let $a_j=a+jd_n$ for $0\leq j\leq n.$ The graph of $f$ on the interval $[a_j,a_{j+1}]$ is covered by the rectangle  $[a_j,a_{j+1}]\times [f(a_j)-Kd_n,\;f(a_j)+Kd_n].$  This rectangle has area $2K(d_n)^2.$ There are $n$ such rectangles, so the content of the graph of $f$ on $[p,q]$ cannot exceed $$n\cdot 2K(d_n)^2 =2K(q-p)^2/n.$$ Since $n$ can be any natural number, the content of the graph of $f$ on $[p,q]$ is zero.</p>

<p>This is not as general as  the comments and answer of Paramanand Singh but suffices for the Q.</p>

<p>Remarks: I hope I got ""the graph of..."" in all the right places. To a set-theorist  a function $is$ its graph.... In the Lemma it would also suffice that $f$ is Lipschitz-continuous on $[p,q]$ with Lipschitz constant $K$.</p>
"
"2395825","2395899","<p>Let
\begin{align}
x^x &amp;= z\\
x\ln x &amp;= \ln z\\
e^{\ln x}\ln x &amp;= \ln z.
\end{align}
Since for all $t\geq0$ the <a href=""https://en.wikipedia.org/wiki/Lambert_W_function"" rel=""nofollow noreferrer""><em>Lambert W function</em></a> has the <a href=""https://en.wikipedia.org/wiki/Lambert_W_function#Identities"" rel=""nofollow noreferrer"">property</a>
$$
W\left(t\cdot e^t\right) = t,
$$
for $t=\ln x$, we have $W\left(\ln x \cdot e^{\ln x}\right) = \ln x$, so by applying $W$ on both sides, we have
$$
\ln x = W\left(\ln z\right).
$$
From here <a href=""https://en.wikipedia.org/wiki/Lambert_W_function#Example_2"" rel=""nofollow noreferrer"">we have</a>
$$
x = e^{W\left(\ln z\right)},
$$
or since by the definiction of the $W$ function $\ln z = W\left(\ln z\right)e^{W\left(\ln z\right)}$, an equivalent solution is
$$
x = \frac{\ln z}{W\left(\ln z\right)}.
$$
For $z=2$ this argument answers your question.</p>
"
"2395827","2395924","<p>I'm a little rusty, but here goes.  Given a rotation angle $\theta$, we have rotations by $\theta$ radians
$$
R_z(\theta) = \begin{pmatrix} \cos{\theta} &amp; -\sin{\theta} &amp; 0 \\
                                                            \sin{\theta} &amp; \cos{\theta} &amp; 0 \\
                                                            0 &amp; 0 &amp; 1 \\\end{pmatrix}
$$ </p>

<p>$$
R_y(\theta) = \begin{pmatrix} \cos{\theta} &amp; 0 &amp; -\sin{\theta} \\
                                                                     0 &amp; 1 &amp; 0 \\
                                                                     \sin{\theta} &amp; 0 &amp; \cos{\theta} \\\end{pmatrix}
$$</p>

<p>$$
R_x(\theta) = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\
                                                                     0 &amp; \cos{\theta} &amp; -\sin{\theta} \\
                                                                     0 &amp; \sin{\theta} &amp; \cos{\theta} \\\end{pmatrix}
$$
in the $xy$-, $xz$-, and $yz$-planes respectively (subscript denotes the axis of rotation).  You can always use these to generate any rotation in $SO(3)$ by moving the axis first (then moving it back).</p>

<p>Let $A$ be a non-identity element of $SO(3)$.  Then $A$ has a 1-diml eigenspace (the axis of rotation), about which we rotate by an angle $\theta$.  This eigenspace intersects the unit sphere somewhere, so write this point of intersection as
$$
\mathbf{n} = \langle \cos{\alpha}\sin{\beta},\sin{\alpha}\sin{\beta},\cos{\beta}\rangle^T
$$
with spherical coordinates (and as a column vector). </p>

<p>Note that $R_y(\beta)R_z(\alpha)^{T}\mathbf{n} = \langle 0,0,1\rangle^T$ so we have moved the axis so that this rotation occurs in the $xy$-plane.  We now rotate by our original angle $\theta$ and then rotate our axis back to its original position.  You can do the math and check that
$$
A = R_z(\alpha)R_y(\beta)^TR_z(\theta)R_y(\beta)R_z(\alpha)^{T}
$$
accomplishes this.</p>
"
"2395838","2395861","<p>As we know, the number is divisible by $4$ if the number made from its last two digits is divisible by $4$.
So there are pairs of digits we can put in the end: $36, 48, 64, 68, 84$<br>
 ($5$ variants).
As we have two remaining positions and two remaining digits, we can form $2$ variants of <em>first</em> two digits for each variant of <em>last</em> two digits. As a result:
$$ 5 \times 2 = 10 $$</p>
"
"2395845","2395887","<p>You are correct that to use multi-hypergeometric distribution to count. Let $S, H, D, C$ be the event that having KQ in the suits respectively. Then by inclusion-exclusion principle,</p>

<p>$$ \begin{align} 
&amp;~ \Pr\{S \cup H \cup D \cup C\} \\
= &amp;~ \binom {4} {1} \frac {\displaystyle \binom {2} {2} \binom {34} {10}} {\displaystyle \binom {36} {12} } 
- \binom {4} {2} \frac {\displaystyle \binom {2} {2} \binom {2} {2} \binom {32} {8}} {\displaystyle \binom {36} {12} }
+\binom {4} {3} \frac {\displaystyle \binom {2} {2} \binom {2} {2}\binom {2} {2}\binom {30} {6}} {\displaystyle \binom {36} {12} } \\
&amp;~- \binom {4} {4} \frac {\displaystyle \binom {2} {2} \binom {2} {2}\binom {2} {2}\binom {2} {2}\binom {28} {4}} {\displaystyle \binom {36} {12} } \\
= &amp;~ \frac {339749} {916980} \\
 \approx &amp;~ 0.3705
\end{align}$$</p>

<p>Each term corresponding to the generic case with $1, 2, 3, 4$ pairs of KQ in particular suit, where the leading combinatoric coefficient is counting the number of particular suit combinations for each generic case.</p>
"
"2395846","2396051","<p>Without orientations, we cannot do the calculations, but you can move paths around by adding and subtracting triangles.  Each triangle gives a relation such as $ag=\pm gi\pm ai$ (depending on the orientations).  This is a replacement rule.</p>

<p>Your approach, while it does identify the fact that the 18 triangles are independent, fails to find the extra 1-cycle $ab+bc+ca$.  This is because the 18 triangles only can get $2(ab+bc+ca)$.  Over a field this would not be an issue, but $\mathbb{Z}$ does not have division by 2.  This might be why you could not find a way to represent $bc+cf+fi+ib$, since homologically it is a sum of $ab+bc+ca$ and $ag+gd+da$.</p>

<p>I am not sure you can actually do this calculation without something equivalent to a big matrix, unfortunately, since independence can be tricky to infer (being a statement about non-existence of a solution to a system of equations), and over $\mathbb{Z}$ even spanning can be tricky.</p>

<p>If what you want is an explicit description of a basis of 1-cycles, here is a computational trick from graph theory: (1) take any spanning tree of the graph (2) count the number of edges outside the spanning tree. Each edge corresponds to an independent cycle by attaching the unique shortest path through the tree between the two endpoints. Spanning implies there is such a path (otherwise you can add that edge to make the tree bigger). In your example, the complement of a spanning tree has 19 edges. These span the 1-cycles because an arbitrary 1-cycle will be cut up by the spanning tree, and anything completely inside the tree is not a cycle or 0.</p>
"
"2395848","2395859","<p>Just compute one of the derivatives in the definition of $V_n$ to get the recursion.</p>

<p>$$V_n = \frac{d^n}{dx^n} (x^n \log x) = \frac{d^{n-1}}{dx^{n-1}} (n x^{n-1} \log x + x^{n-1}) = n V_{n-1} + (n-1)!$$</p>

<p>Then, applying this recursion repeatedly yields
\begin{align}
V_n
&amp;= nV_{n-1} + (n-1)!\\
&amp;= n[(n-1) V_{n-2} + (n-2)!] + (n-1)!
\\
&amp;= n(n-1) V_{n-2} +  n! \left(\frac{1}{n-1} + \frac{1}{n}\right)
\\
&amp;\;\vdots\\
&amp;= n! V_0 + n!\left(1+\frac{1}{2} + \frac{1}{3} + \cdots + \frac{1}{n}\right),
\end{align}
where $V_0= \log x$.</p>
"
"2395851","2395954","<p><strong>Lemma(I)</strong>: Let $3 \leq y$, then we have: 
$$2y &lt; y^2-2.$$</p>

<p><strong>Proof</strong>: $3 \leq y$, so we must have: </p>

<p>$$2 \leq (y-1) 
\Longrightarrow 
3 &lt; 4 \leq (y-1)^2 
\Longrightarrow 
3 &lt; y^2-2y+1 
\Longrightarrow 
2y &lt; y^2-2 . 
$$</p>

<hr>

<p><strong>Lemma(II)</strong>: 
Let $3 \leq y$ to be any fixed arbitrary integer 
and let $2 \leq z$, then we have: 
$$  \color{Red}{yz &lt; y^z-2}  \ \ \ \ \ \ \ \ \ \  \text{(II)}   .   $$</p>

<p><strong>Proof</strong>: 
Let $y$ be an  arbitrary (but fixed) integer 
greater or equal than $3$. 
We will prove the lemma by induction on $z$.</p>

<p>$z=2$; which is done by <strong>Lemma(I)</strong>. </p>

<p>Now suppose that the <strong>Lemma(II)</strong> is true for $z=k$; 
then we will show it for  $z=k+1$. 
Only notice that: </p>

<p>$$ \color{Blue}{y} &lt; zy+2 &lt; y^z &lt; (y-1)y^z = 
\color{Blue}{y^{z+1}-y^z}
\ 
; $$</p>

<p>adding this last inequality by inequlaity (II) we get: </p>

<p>$$ y(z+1)= \color{red}{yz}+ \color{Blue}y &lt;  
\color{Red}{y^z-2} +   \color{Blue}{y^{z+1}-y^z} = 
y^{z+1}-2  .$$</p>

<hr>

<p><strong>Lemma(III)</strong>: 
Let $y=2$ 
and let $4 \leq z$, then we have: 
$$  \color{Red}{2z &lt; 2^z-2}  \ \ \ \ \ \ \ \ \ \  \text{(III)}   .   $$</p>

<p><strong>Proof</strong>:<br>
We will prove the lemma by induction on $z$.</p>

<p>$z=4$; is trivial. </p>

<p>Now suppose that the <strong>Lemma(III)</strong> is true for $z=k$; 
then we will show it for  $z=k+1$. 
Only notice that: </p>

<p>$$ \color{Blue}{2} &lt; 2z+2 &lt; 2^z = 
\color{Blue}{2^{z+1}-2^z}
\ 
; $$</p>

<p>adding this last inequality by inequlaity (III) we get: </p>

<p>$$ 2(z+1)= \color{red}{2z}+ \color{Blue}2 &lt;  
\color{Red}{2^z-2} +   \color{Blue}{2^{z+1}-2^z} = 
2^{z+1}-2  .$$</p>

<hr>

<hr>

<hr>

<hr>

<p><strong>First Case</strong>: Let $3 \leq y$: </p>

<ul>
<li>$2 \leq z$; which is impossible by <strong>Lemma(II)</strong>.</li>
<li>If $z=1$; then we have $y.1=y^1-2$, 
which is obviously impossible again!</li>
</ul>

<p><strong>Second Case</strong>: Let $y=2$: </p>

<ul>
<li><p>$4 \leq z$; which is impossible by <strong>Lemma(III)</strong>.</p></li>
<li><p>$z=3$; which gives the solution $\color{Green}{(y,z)=(2,3)}.$</p></li>
<li><p>$z=2$; which does not give any solution!  </p></li>
<li><p>$z=1$; which does not give any solution again! </p></li>
</ul>

<p><strong>Third case</strong>: Let $y=1$: </p>

<ul>
<li>in this case we have: $1.z=1^z-2$; 
which does not have a solution in $\mathbb{N}$.</li>
</ul>
"
"2395879","2395884","<p>Think of the ""objects"" as little coin holders.  Each time you flip a coin, you put that coin into one of your ten little coin holders, with either a heads or tails showing.  If you want to get exactly 3 heads, then you are picking three of your little coin holders to hold coins with heads showing (which forces the remaining 7 coin holders to hold coins with tails showing).</p>
"
"2395881","2396091","<p>They should have had a Kronecker $ \delta $; a Dirac delta makes no sense in that context since it is a sum over $ n$ and not an integral. Based on my skim on the next page, this does not seem to affect those results much.</p>

<p>The correct derivation is as follows. The first equation is
$$
{\cal N}_L=\frac{L!}{2 L} \sum_{(n_q)_{q \in \mathbb{N}}} \delta(\Sigma_q n_q, L)  \prod_q \frac{N_q!}{n_q! (N_q-n_q)!}\left(\frac{q^2}{Nc}\right)^{n_q}$$
where $ \delta(m,n) $ is the Kronecker delta:
$$
\delta(m,n) = \frac{1}{2 \pi } \int_{-\pi}^{\pi} e^{ i x ( m - n) } dx
$$
Putting this into the formula gives
$$
{\cal N}_L=\frac{L!}{2 L} \frac{1}{2 \pi }  \sum_{(n_q)_{q \in \mathbb{N}}} \int_{-\pi}^{\pi} e^{ i x L - \sum_q i x n_q}  \prod_q \frac{N_q!}{n_q! (N_q-n_q)!}\left(\frac{q^2}{Nc}\right)^{n_q} dx \\ =\frac{L!}{2 L} \frac{1}{2 \pi }  \sum_{(n_q)_{q \in \mathbb{N}}} \int_{-\pi}^{\pi} e^{ i x L}  \prod_q \frac{N_q!}{n_q! (N_q-n_q)!}\left(\frac{q^2 e^{-i x}}{Nc}\right)^{n_q} dx \\ =\frac{L!}{2 L} \frac{1}{2 \pi } \int_{-\pi}^{\pi} e^{ i x L}  \prod_q \left(1 - \left(\frac{q^2 e^{-i x}}{Nc}\right)\right)^{N_q} dx. $$
In the last line, I used the binomial theorem to do the sums over $ (n_q)_{q \in \mathbb{N} } $. Moving on, rewriting the product over $ q $ as a sum in the exponent,
$$
\frac{L!}{2 L} \frac{1}{2 \pi } \int_{-\pi}^{\pi} e^{ i x L}  \exp \left( \sum_q \log \left(1 - \left(\frac{q^2 e^{-i x}}{Nc}\right)\right) N_q \right) dx
$$
and recognizing that $$ \frac{1}{N} \sum_q \log \left(1 - \left(\frac{q^2 e^{-i x}}{Nc}\right)\right) N_q = \left\langle \log \left(1 - \left(\frac{q^2 e^{-i x}}{Nc}\right)\right) \right\rangle$$
gives the final answer. </p>

<p>In their analysis, it looks like the saddle points are in the interval $ (- \pi , \pi ) $, so that the asymptotics are not affected by the incorrect bounds of the integral.</p>
"
"2395885","2395947","<p>I think this problem is easier than it seems. We have to remember that the integral is just  a sum and the double integral is just a double sum. Also, when defining the integral we observed that the length (similarly area) for the partitions are not fixed.</p>

<p>Hence, we get the same definition if $\Delta x = (b-a)/n$ and $\Delta y = (c-d)/m$ where $n,m$ denote the number of intervals in the partitions of the $x,y$-axis respectively. Hence it follows immediately that,</p>

<p>\begin{align*} \iint_{[-1,1]^2} f \ dA =  \lim_{n,m \to \infty} \sum_{i,j}^{m,n}f(p_{ij}) \ \Delta x \Delta y &amp;= \lim_{n,m \to \infty} \sum_{\textrm{$i,j$ s.t $f \not = 0$}} f(p_{ij}) \Delta x \Delta y  \\ \\ &amp; \leq \lim_{n,m \to \infty} \Delta x \Delta y = 0\end{align*}</p>
"
"2395895","2396291","<p>There's a missing assumption $g\ge 0$; without it the statement is false since $u(x,t)\le g(x)$ for all $t\ge 0$. </p>

<p>An intuitive way to think of the minimum of $\frac{|x-y|^2}{4t} + g(y)$ is that it's the smallest number $C$ for which the equation $g(y) = C- \frac{|x-y|^2}{4t}$ has a solution. Well this is not quite intuitive yet, but think of the family $C- \frac{|x-y|^2}{4t}$ parameterized by $C$ as an upside-down paraboloid rising from $C=0$ until it meets the graph of $g$. Then the claim is: when  the paraboloid is very flat ($t$ is large),  it can't rise far before hitting the graph of $g$. Then the statement becomes obvious because $g$ decays at infinity: that far-away, low part of the graph of $g$ will get in the way of the rising paraboloid.</p>

<p>But ""it's obvious"" still isn't a proof, so let's fix $x$ and suppose $u(x,t) \ge \epsilon$ for some large $t$. Then $g(y)  + |x-y|^2/(4t) \ge \epsilon  $ for all $y$. Pick $y$ such that $g(y) \le \epsilon/2$ and conclude that 
$$
t\le \frac{|x-y|^2}{2\epsilon} \tag{1}
$$
So, for every $\epsilon&gt;0$ there exist $t_0$ such that $u(x,t)&lt;\epsilon$ when $t&gt;t_0$. The very definition of $\lim_{t\to\infty}u(x,t) = 0$.</p>

<p>And the above can be made uniform with respect to $x$, using the assumption
$g(x)\le \frac{M}{|x|}$. Indeed, any $y$ with $|y|\ge 2M/\epsilon$ will do now, so  in (1) we can be assured to have $|x-y|\le 2M/\epsilon$ no matter what $x$ is, leading to<br>
$$
t\le \frac{2M^2 }{\epsilon^3} \tag{2}
$$</p>
"
"2395900","2395907","<p>""Does the fact that the collection is indexed imply that we can only have countably many open sets ?""</p>

<p>Not at all ! We can define a family indexed by <em>any</em> set, including uncountable ones. Set-theoretically, a family $(G_{\alpha})_{\alpha\in A}$ of open subsets of $K$ is just a map $A\to \{\text{open subsets of }K\}$ and it does not depend on the cardinality of $A$.</p>
"
"2395905","2395916","<p>Hint. Note that
$$\sum_{k = 0}^{n}k\binom{n}{k}=D_x\left(\sum_{k = 0}^{n}\binom{n}{k}x^k\right)_{x=1}=D_x\left((1+x)^n\right)_{x=1}=n(1+1)^{n-1}=n2^{n-1}.$$
Therefore
$$\frac{n2^{n-1}}{n(2^n+1)}=\frac{1}{n(2^n+1)}\sum_{k = 0}^{n}k\binom{n}{k}\leq \sum_{k = 0}^{n}\frac{k\binom{n}{k} }{n2^n+k}\leq
\frac{1}{n2^n}\sum_{k = 0}^{n}k\binom{n}{k}=\frac{1}{2}$$
Now use the <a href=""https://en.wikipedia.org/wiki/Squeeze_theorem"" rel=""nofollow noreferrer"">Squeeze Theorem</a>.</p>
"
"2395906","2395910","<p>In the (using the binomial theorem) expansion of </p>

<p>$$\left(1 + \sqrt{\frac{2}{n}} \right)^n$$</p>

<p>there is $1$, and there is $${n \choose 2} \left( \sqrt{\frac{2}{n}}\right)^2 = n-1$$</p>

<p>and other $&gt;0$ terms. So $\left(1 + \sqrt{\frac{2}{n}} \right)^n &gt; n-1 + 1 = n$.</p>
"
"2395909","2395965","<p>Each admissible arrangement can be encoded as a binary word of length $20$ containing exactly $6$ ones. The $14$ not chosen people (zeros) create $15$ slots where groups of chosen people (ones) may be squeezed in. These groups may have sizes $(6)$, $(4,2)$, $(3,3)$, and $(2,2,2)$. It follows that the total number of admissible arrangements is given by
$${15\choose 1}+2\cdot{15\choose 2}+{15\choose 2}+{15\choose3}=785\ .$$</p>
"
"2395941","2395991","<p>If you think of a probability as the fraction of the time something happens, it is conceptually clear that the probability that two things happen must be no greater than the probability that either one of them happens individually.  So $\Pr[A\cap B]\le \Pr[A]$ and $\Pr[A\cap B]\le\Pr[B]$.</p>

<p>The question is, in what way does $\Pr[A\cap B]$ get reduced relative to $\Pr[A]$ or $\Pr[B]$?  Let's say $\Pr[A]=\frac{3}{5}$ and $\Pr[B]=\frac{2}{3}$, so that $A$ happens $\frac{3}{5}$ of the time and $B$ happens $\frac{2}{3}$ of the time.  If one didn't think about it carefully, one might think that $B$ happens on $\frac{2}{3}$ of those occasions when $A$ happens and therefore that $A\cap B$ happens $\frac{3}{5}\cdot\frac{2}{3}=\frac{2}{5}$ of the time, in just the same way as $\frac{2}{3}$ of $\frac{3}{5}$ of a cake is $\frac{2}{5}$ of a cake.   But that is, in fact, only true when $A$ and $B$ are independent.  It could well be the $B$ happens whenever $A$ happens, so that $\Pr[A\cap B]=\Pr[A]\cdot1=\Pr[A]$.  Or it could be that $B$ never happens when $A$ happens so that $\Pr[A\cap B]=\Pr[A]\cdot0=0$.  In fact, $\Pr[A\cap B]$ can take any value between $0$ and $\Pr[A]$ depending on whether the occurrence of $A$ makes $B$ less likely, leaves the probability of $B$ unchanged, or makes $B$ more likely.</p>

<p>Of course this discussion really hasn't got away from the conditional definition.  The general formula for $\Pr[A\cap B]$ is
$$
\Pr[A\cap B]=\Pr[A]\cdot\Pr[B\mid A].
$$
Independence is precisely the condition that the occurrence of $A$ makes $B$ neither more nor less likely, $\Pr[B\mid A]=\Pr[B]$, so that the formula becomes $\Pr[A\cap B]=\Pr[A]\cdot\Pr[B]$.  Nevertheless, I hope that this way of thinking about it makes the multiplicative definition of independence just as intuitive as the conditional definition.</p>
"
"2395946","2396208","<p>The formula is:
$$f_{[X\,\mid\, 3Y&lt;X]}(x) ~=~ \dfrac{\int\limits_{-\infty}^{x/3}f_{X,Y}(x,t)\;\mathrm d t}{\int\limits_{-\infty}^\infty\int\limits_{-\infty}^{s/3}f_{X,Y}(s,t)\;\mathrm d t\;\mathrm d s}$$</p>

<hr>

<p>It is <em>analogous</em> to the discrete case.</p>

<p>$$\mathsf P(U=u\mid 3V&lt;U) ~{~=~ \dfrac{\mathsf P(U=u, 3V&lt;u)}{\mathsf P(3V&lt;U)} \\[4ex] ~= ~ \dfrac{\sum\limits_{t&lt;u/3}\mathsf P(U=u, V=t)}{\sum\limits_s\sum\limits_{t&lt;s/3}\mathsf P(U=s, V=t)} }$$ </p>
"
"2395949","2396571","<p>I am assuming that you are measuring the diameter using the Euclidean metric (although the answer for the arclength metric is very similar, just with a different critical value).</p>

<p>I claim that the answer is no, at least for finite partitions. More generally, I will prove that such a finite partition exists on a sphere of radius $r$ if and only if $r&lt; \frac{1}{2}$. </p>

<p>Indeed, if $S_r$ is a sphere of radius $r&lt;1/2$, then diam$ (S_r)=2r&lt;1=\mu(S_r)$ and therefore we can take our partition to be the singleton $\{S_r\}$.</p>

<p>Conversely, suppose that $S_r$ is a sphere of radius $r\ge 1/2$. Then for any finite partition $\{A_k\}_{k=1}^N$ of $S_r$, I claim that $$\sum_k \text{diam}(A_k) \geq \text{diam}(S_r)=2r \geq 1 = \mu(S_r) = \sum_k \mu(A_k)$$
and therefore there exists some $k$ such that diam$(A_k) \geq \mu(A_k)$. The only nontrivial part of this expression is the first inequality: $\sum_k \text{diam}(A_k) \geq \text{diam}(S_r)$, so we will prove this. First, we note that if $A,B$ are any sets such that $A \cap B \neq \emptyset$, then $$\text{diam}(A)+\text{diam}(B) \geq \text{diam}(A \cup B)$$ Indeed, for any $x \in A$ and $y \in B$, we let $z \in A \cap B$ and we have that $$|x-y| \leq |x-z|+|z-y| \leq \text{diam}(A)+\text{diam}(B)$$ Now given our partition $\{A_k\}_{k=1}^N$, we assume wlog that $\overline A_k \cap \overline A_{k+1} \neq \emptyset$ for all $k&lt; N$ (this must hold after some reordering of the $A_i$, by the connectedness of $S_r$). Then by repeatedly applying the above observation (and using the fact that any set and its closure have the same diameter) we find \begin{align*}\text{diam}(S_r) &amp;\le \text{diam}(A_1)+ \text{diam} \bigg( \bigcup_{k=2}^N A_k \bigg) \\ &amp; \leq \text{diam}(A_1)+\text{diam}(A_2)+\text{diam}\bigg( \bigcup_{k=3}^N A_k \bigg) \\ &amp;\leq \text{diam}(A_1)+\text{diam}(A_2)+\text{diam}(A_3)+ \text{diam}\bigg( \bigcup_{k=4}^N A_k \bigg) \\ &amp;\dots \dots \dots \\ &amp;\leq \sum_{k=1}^N \text{diam}(A_k)
\end{align*}
which is the desired result.</p>
"
"2395955","2395961","<p>Hint. Note that for $x\not=1$, 
$$\frac{1}{1-x}-\frac{3}{1-x^3}=\frac{x^2+x+1-3}{(1-x)(x^2+x+1)}
=\frac{(x+2)(x-1)}{(1-x)(x^2+x+1)}=-\frac{x+2}{x^2+x+1}.
$$</p>
"
"2395959","2396387","<p>This just means that for any vertices $v$ and $w$, there exists a cycle $C$ which contains both $v$ and $w$.  The word ""common"" is informal and merely emphasizes that this cycle $C$ depends on $v$ and $w$ and is shared by both of them (you could say that being in $C$ is a property that is common to $v$ and $w$).</p>
"
"2395964","2395970","<p>If $x\geqslant0$, then $\min_{t&lt;x}t^2=0$. Otherwise, $\min_{t&lt;x}t^2=x^2$. Therefore, the graph of your function is the union of:</p>

<ul>
<li>the half-parabola $y=x^2$ ($x\leqslant0$);</li>
<li>the ray formed by the positive elements of the $x$-axis.</li>
</ul>
"
"2395967","2395972","<p>Hint: Work through the proof of Egoroff's theorem. In order to apply continuity from above, one needs that one of the sets has finite measure. In the usual setting, this is given by the fact that the space has finite measure. Here, we instead use the fact that the sequence is dominated.</p>

<p>After you finish proving this, you can redo the proof a couple more times to prove the following results.</p>

<blockquote>
  <p>1) If $\sum_{n=1}^\infty \|f_n-f\|_1 &lt; \infty$, then $f_n\to f$ almost uniformly.</p>
  
  <p>2) If $\sum_{n=1}^\infty \mu[|f_n-f|&gt;1/n] &lt; \infty$, then $f_n\to f$ almost uniformly.</p>
</blockquote>

<p>They show that, in particular, if $f_n\to f$ in $L^1$ or in measure, then there is a subsequence such that $f_{n_k}\to f$ almost uniformly.</p>
"
"2395971","2396032","<p>Suppose first that $A=\operatorname{Id}$. Consider the matrix $\lambda^3\operatorname{Id}-\lambda B-C$. Its determinant is a polynomial whose degree is $3n$; not just $\leqslant3n$, but $3n$. That's because each entry of the main diagonal of $\lambda^3\operatorname{Id}-\lambda B-C$ is a polynomial of degree $3$ and all other entries of the matrix are polynomials whose degree is $\leqslant1$. Since the degree is odd, there is a $\lambda\in\mathbb R$ for which the determinant is $0$. Therefore, there is a non-zero vector $x$ such that $\lambda^3x+\lambda Bx-Cx=0$.</p>

<p>In the general case, just multiply everything by $A^{-1}$ and you'll get this case.</p>
"
"2395973","2395977","<p>In general, it is not the case that $|A|^2 = A^2$ (for example, if $A$ is a complex number, or a vector, or some other kind of object where some notions of absolute values and squaring make sense).  However, if $A$ is a real number, then you are fine.</p>
"
"2395985","2395992","<p>HINT: it is$$2x^2+2xy+y^2-2x+2y+2\geq -3$$ and the equal sign holds for $x=2,y=-3$
let $$f(x,y)=2x^2+2xy+y^2-2x+2y+2$$ and compute the partial derivatives
and solve the System
$$4x+2y-2=0$$
and
$$2x+2y+2=0$$</p>
"
"2395994","2396019","<p>Note that</p>

<p>$$\frac{d^2}{dx^2}\log_2(x+1)=-\frac1{(x+1)^2\ln(2)}&lt;0$$</p>

<p>Thus, $\log_2(x+1)$ is concave and it is bounded below by its secant lines and bounded above by its tangent lines, mainly, on the interval $[0,15]$, we have the bounds</p>

<p>$$\frac x4=\frac{\log_2(15+1)-\log_2(0+1)}{15}x\le\log_2(x+1)\le x\lim_{t\to0}\frac d{dt}\log_2(t+1)=\frac x{\ln(2)}$$</p>
"
"2395997","2396004","<p>By comparing coefficient of $x^2$,
$$A+B+C=1$$</p>

<p>rather than </p>

<p>$$A+B+C=0$$</p>
"
"2396003","2396614","<p>Your solutions for questions 17, 19, and 20 are correct.  As for question 18:
\begin{align*}
\sin(3x) + \cos(3x) &amp; = 0\\
\sin(3x) &amp; = -\cos(3x)\\
\tan(3x) &amp; = -1\\
3x &amp; = -\frac{\pi}{4} + k\pi, k \in \mathbb{Z}\\
x &amp; = -\frac{\pi}{12} + \frac{k\pi}{3}
\end{align*}
Since $-\pi &lt; x &lt; \pi$, $-2 \leq k \leq 3$.  Thus, the solution set is 
$$\left\{-\frac{3\pi}{4}, -\frac{5\pi}{12}, -\frac{\pi}{12}, \frac{\pi}{4}, \frac{7\pi}{12}, \frac{11\pi}{12}\right\}$$</p>
"
"2396018","2396021","<p>The Euler's characteristic is a topological invariant, namely two homeomorphic spaces have the same Euler's characteristic but is not a <strong>total</strong> topological invariant. It does not suffice that two topological spaces have the same Euler's characteristic to ensure they are homeomorphic.</p>
"
"2396022","2396060","<p>One can see the limit results in the improper integral</p>

<p>$$L=\int_0^{10}\sqrt{\frac{400-3x^2}{400-4x^2}}~\mathrm dx$$</p>

<p>Now consider the substitution $x=10t$ to get</p>

<p>$$L=10\int_0^1\sqrt{\frac{400-300t^2}{400-400t^2}}~\mathrm dt=10\int_0^1\frac{\sqrt{1-\frac34t^2}}{\sqrt{1-t^2}}~\mathrm dt=10E(\sqrt{3}/2)$$</p>

<p>Where $E(k)$ is a <a href=""https://en.wikipedia.org/wiki/Elliptic_integral#Complete_elliptic_integral_of_the_second_kind"" rel=""nofollow noreferrer"">complete elliptic integral of the second kind</a>.</p>
"
"2396035","2396044","<p>When an experiment is performed, there is always exactly one outcome.  If that outcome is in the event in question, we say the event occurred.</p>
"
"2396046","2396055","<p>Underneath ""<strong>Claim</strong>"" in my post <a href=""https://math.stackexchange.com/questions/2017810/galois-group-of-irreducible-cubic-equation/2017815#2017815""><strong>here</strong></a>, I show that the Galois group of any cubic polynomial in $\mathbb{Q}[x]$ is determined by its discriminant.  However, we can attack this with a more naive approach:</p>

<p>First, we know that the Galois group of a polynomial $f \in \mathbb{Q}[x]$ is a subgroup of the symmetric group $S_{\deg(f)}$, and when $f$ is irreducible, this subgroup is <a href=""http://mathworld.wolfram.com/TransitiveGroup.html"" rel=""nofollow noreferrer"">transitive</a>$^\dagger$.  Therefore, the Galois group of any irreducible cubic polynomial must be either $S_3$ or $A_3 \cong \mathbb{Z}_3$.  If the cubic polynomial in question has complex roots, complex conjugation will be a nontrivial automorphism of the splitting field.  Since complex conjugation is an element of the Galois group of order $2$, we'll have $\text{Gal}(f) \cong S_3$ per <a href=""https://en.wikipedia.org/wiki/Lagrange%27s_theorem_(group_theory)"" rel=""nofollow noreferrer"">Lagrange's theorem</a>. </p>

<p>So for irreducible cubic polynomials, we know $\text{Complex roots} \implies S_3$ Galois group.  But what of cubics with all real roots?  Will we necessarily have $\text{Gal}(f) \cong \mathbb{Z}_3$?  Is $\text{Gal}(f) \cong S_3$ still possible?  At this point, I think the discriminant argument is the easiest way to proceed.  Some hints are below:</p>

<blockquote class=""spoiler"">
  <p> It is always the case that $D \in \mathbb{Q}$, and anything in $\mathbb{Q}$ is fixed by elements of $\text{Gal}(f)$.  Next, consider the tower of fields $\mathbb{Q} \subset \mathbb{Q}(\sqrt{D}) \subset \mathbb{Q}(\sqrt{D}, \alpha)$, where $\alpha$ is one of the roots of $f$.  Note that the latter field is the splitting field of $f$ (why?).  </p>
</blockquote>

<hr>

<p>$\dagger$ For a proof of this fact, see Theorem 2.9(b) <a href=""http://www.math.uconn.edu/~kconrad/blurbs/galoistheory/galoisaspermgp.pdf"" rel=""nofollow noreferrer""><strong>here</strong></a>.</p>

<p>$\ddagger$ Alternatively, note that complex conjugation, in general, is an <em>odd</em> permutation and thus not an element of an alternating group.  From this, we can conclude $\text{Gal}(f) \cong S_3$.</p>
"
"2396057","2396074","<p>The Chinese remainder theorem does not apply here.  To apply the Chinese remainder theorem to the ideals $(a)$ and $(b)$, you would need to know that $(a)+(b)$ is the whole ring $R$, which may not be the case (consider $R=k[x,y]$, $a=x$, $b=y$).</p>

<p>In general, multiplication by $a$ will be injective on $R/(b)$ (since $a$ and $b$ are relatively prime) but not surjective.   So for even $i$, you will get that $Tor_i^{R/(ab)}(R/(a),R/(b))$ is the cokernel of multiplication by $a$ on $R/(b)$, which is $R/(a,b)$.</p>
"
"2396068","2396088","<p>You can use the Chebychev's inequality, which is 
$$P(X_{n}\geq n)\leq \frac{Var(X_{n})}{n^2}=\frac{1}{n^2}.$$
Because $Var(X_{n})=E(X_{n}^2)-(E(X_{n}))^2=1$, and the proof of this inequality is easy.</p>
"
"2396080","2396150","<p>So $v$ is a unit vector which is ""optimal"" in the sense that $\| Av \|_2$ is as large as possible.  Let $x$ be a vector that is orthogonal to $v$.  We want to show that $Ax$ is orthogonal to $Av$.</p>

<p><strong>If this were not the case, then $v$ would not be optimal, because we could <em>improve</em> $v$ by perturbing it a bit in the direction $x$!</strong></p>

<p>I'll explain the intuitive idea first. When we perturb $v$ in the direction $x$, the norm of $v$ does not change (at least to a very good approximation).  Imagine standing on the surface of the earth, and $v$ is the vector from the center of the earth to your current location.  If you take a step in a direction orthogonal to $v$, your distance from the center of the earth does not change. You are walking on the surface of the earth.</p>

<p>However, when $v$ is perturbed in the direction of $x$, the vector $Av$ is perturbed in the direction of $Ax$. And if $Av$ is not orthogonal to $Ax$, then the change in the value of $Av$ is <em>non-negligible</em>. So, by perturbing $v$ in the direction of $x$ (or perhaps opposite the direction of $x$), we obtain a unit vector $\tilde v$ for which $\| A \tilde v\|_2$ is larger than $\| Av \|_2$. This shows that $v$ is not optimal after all, which is a contradiction.</p>

<hr>

<p>That is the intuition, and it is simple and clear. It remains only to convert this intuition into a rigorous proof.</p>

<p>To get a rigorous proof, we will have to deal with the fact that $\tilde v = v + \epsilon x$ is not actually a unit vector, even though its norm is very close to $1$ when $\epsilon$ is tiny. So, we introduce the normalized vector
$$
\hat v(\epsilon) = \frac{v + \epsilon x}{\sqrt{1 + \epsilon^2}},
$$
which is a true unit vector. Let
$$
f(\epsilon) = \| A \hat v(\epsilon) \|_2^2.
$$
Because $v$ is optimal, we see that $f'(0) = 0$.  And if we compute $f'(0)$ explicitly (which is a straightforward calculus exercise), we will find that $\langle Av, Ax \rangle = 0$.
Here are the details:
\begin{align}  
f(\epsilon) &amp;= \frac{1}{(1 + \epsilon^2)} \langle Av + \epsilon Ax, Av + \epsilon Ax \rangle \\
&amp;= \left(\frac{1}{1 + \epsilon^2}\right) \| Av \|_2^2 + \left(\frac{\epsilon}{1 + \epsilon^2}\right) \langle Av, Ax \rangle + \left(\frac{\epsilon^2}{1 + \epsilon^2}\right) \| Ax \|_2^2.
\end{align}
Using basic calculus we see that
$$
f'(0) = \langle Av, Ax \rangle = 0.
$$</p>
"
"2396087","2396164","<p>Sure, here's a demonstration of that first inequality. We have
$$
\lambda_1 = \min \{x^*Ax : x = (x_1,x_2,x_3)^T \in \Bbb C^3, \|x\| = 1\}\\
\leq \min\left\{ x^*Ax : x = (x_1,x_2,x_3)^T \in \Bbb C^3, \|x\| = 1, x_3 = 0\right\}\\
= \min\left\{ \pmatrix{\bar x_1 &amp; \bar x_2 &amp; 0}A\pmatrix{x_1\\x_2\\0} : x \in \Bbb C^3,|x_1|^2 + |x_2|^2 = 1 \right\}\\ 
= \min\left\{ x^*Bx : x \in \Bbb C^2,\|x\| = 1 \right\}
= \mu_1
$$
The last eigenvalue has the same trick but with a max.  For the middle eigenvalue, we have to do this with an actual min-max, as in the theorem.</p>
"
"2396093","2396097","<p>Of course
$$1.01^n&gt;1+\frac{n}{100}&gt;2$$
for $n&gt;100$, and obviously $1+0.99^n&lt;2$.</p>
"
"2396103","2396263","<p>A different approach.</p>

<p>By integrating by parts, one has
$$
\begin{align}
\int_0^{\Large \frac{\pi}2}  \cos (2nx)  \log \sin (x)\; dx&amp;=\left[ \frac{\sin (2nx)}{2n}\cdot  \log \sin (x)\right]_0^{\Large \frac{\pi}2} -\frac{1}{2n}\int_0^{\Large \frac{\pi}2}  \sin (2nx)\: \frac{\cos (x)}{\sin (x)}\; dx
\\&amp;=\color{red}{0}-\frac{1}{2n}\int_0^{\Large \frac{\pi}2}  \sin (2nx)\: \frac{\cos (x)}{\sin (x)}\; dx. \tag1
\end{align}
$$Let
$$
u_n:=\int_0^{\Large \frac{\pi}2}  \sin (2nx)\: \frac{\cos (x)}{\sin (x)}\; dx,\quad n\ge1.
$$ One may observe that, for $n\ge1$,
$$
\begin{align}
u_{n+1}-u_n&amp;=\int_0^{\Large \frac{\pi}2}  \left[\frac{}{}\sin (2nx+2x)-\sin(2nx)\right]\cdot \frac{\cos (x)}{\sin (x)}\; dx
\\&amp;=\int_0^{\Large \frac{\pi}2}  \left[2\frac{}{}\sin (x)\cdot \cos(2nx+x)\right]\cdot \frac{\cos (x)}{\sin (x)}\; dx\quad \left({\small{\color{blue}{\sin p-\sin q=2 \sin \frac{p-q}{2}\cdot \cos \frac{p+q}{2}}}}\right)
\\&amp;=\int_0^{\Large \frac{\pi}2} 2\cdot\cos(2nx+x)\cdot \cos (x)\; dx
\\&amp;=\int_0^{\Large \frac{\pi}2} \left[\frac{}{}\cos(2nx+2x)+\cos(2nx)\right] dx\qquad \quad \left({\small{\color{blue}{2\cos a \cos b= \cos (a+b)+ \cos (a-b)}}}\right)
\\\\&amp;=\color{red}{0}  \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \left({\small{\color{blue}{\sin(m\cdot \pi)=0,\, m=0,1,2,\cdots }}}\right)
\end{align}
$$ giving
$$
u_{n+1}=u_n=\cdots=u_1=2\int_0^{\Large \frac{\pi}2}  \cos^2 (x)\; dx=\frac \pi2, \tag2
$$ then inserting $(2)$ in $(1)$ yields</p>

<blockquote>
  <p>$$
\int_0^{\Large \frac{\pi}2}  \cos (2nx)  \log \sin (x)\; dx=-\frac  {\pi}{4n},\qquad n\ge1
$$ </p>
</blockquote>

<p>as wanted.</p>
"
"2396106","2396121","<p><strong>hint</strong></p>

<p>$$\lim_{n\to +\infty}\frac {\ln (n)}{n}=0 \implies $$</p>

<p>$$(n^\frac 1n-1)^p=(e^{\frac{1}{n}\ln (n)}-1)^p $$
$$\sim (\frac {\ln (n)}{n})^p $$</p>

<p>thus the general term is equivalent to</p>

<p>$$u_n=\frac {1}{n^{2-p}(\ln (n))^{p+1}} $$</p>

<p>It is a Bertrand series.</p>

<p>If $2-p&gt;1$ , it converges since
$$\lim_{+\infty}n^\alpha u_n=0$$ where $$2-p&gt;\alpha&gt;1$$</p>

<p>If $2-p&lt;1$, it diverges since
$$\lim_{+\infty}n^\beta u_n=+\infty $$
where $$2-p &lt;\beta &lt;1$$</p>

<p>If $2-p=1$, it converges by comparison 
with the integral.</p>

<blockquote>
  <p>Finally, it converges $\iff p \le 1$.</p>
</blockquote>
"
"2396110","2396505","<p>It depends how you define ""impossible"". </p>

<p>Colloquially speaking, you could define it to mean if the chance is less than some tiny $\epsilon$, then it is ""impossible"". 
Then you could solve for the number of picks that would satisfy that.
Most people would probably be unsatisfied with that though. (Even if $\epsilon=10^{-1000}$ the event <em>could</em> still technically happen)</p>

<p>In stricter sense of the word, one might ask for the probability to be $0$. 
This is not miniscule; it means literally zero.
In this case, the number of picks would have to go to infinity; in the situation described, this will <em>never</em> happen, because there will always be a non-zero chance of picking a string of one color, no matter how long that string is. No matter how small $0.99^n$ is (or how large $n$ is), the probability is positive.</p>

<p>In fact, for me personally, even zero probability is not strong enough for me to call an event ""impossible"". For instance, consider choosing a real number $c\in\mathbb{R}$ uniformly in the interval $[7,13]$. What is the probability of choosing <em>exactly</em> $c=9.713$? It is <strong>zero</strong>. But is it <em>possible</em>? Yes. It is a number inside the interval. To me, therefore, it is not <em>impossible</em>. 
Impossible is a bit of a high bar in that sense :D</p>
"
"2396116","2396149","<p>$1$.  Take any $x\in B \cap (A \cup C)$, then $x\in B$ and ($x\in A$ or $x\in C$) so $x \in A\cap B$ or $x\in A\cap C$. </p>

<p>So $x \in A\cap B$ or $x\in C$ (since $A\cap C\subseteq C$) and thus $x\in (A \cap B) \cup C$. Since $x$ is arbitrary we have $  B \cap (A \cup C)\subseteq \ (A \cap B) \cup C$.</p>

<p>$2.$ Take any $x\in (A \cap B) \cup C$. Then $x\in A\cap B$ or $x\in C$. From here we can't say $x\in A\cap C$ so the other way doesn't hold. </p>

<p>So, we have only: $  B \cap (A \cup C)\subseteq \ (A \cap B) \cup C$.</p>

<hr>
"
"2396118","2396210","<p>I think you meant $* \to I$ instead of $\emptyset \to I$ in your argument, but the rest of it looks fine.  </p>

<hr>

<p>Anyway, I don't think finding a retract (which also lets you write down an explicit extension in the definition of a cofibration) is ""simpler"" than your method, but here's how.  </p>

<p>We need to show that $$(X \times I \times \{0\}) \cup_{X \times \{0\} \times \{0\}} (X \times \{0\} \times I) \subset X \times I \times I$$ is a retract.  We can build such a retract from $(I \times \{0\}) \cup_{(0,0)} (\{0\} \times I) \subset I \times I$.  Consider the homotopy $$H_t(u,v) = \begin{cases} (t(u-v) + (1-t)u, (1-t)v) &amp; u \geq v \\ ((1-t)u, t(v-u) + (1-t)v) &amp; u \leq v. \end{cases}$$  This is a deformation retraction of $I \times I$ onto $(I \times \{0\}) \cup_{(0,0)} (\{0\} \times I)$.  This is essentially a proof that $\{0\} \to I$ is a cofibration.  </p>

<p>Then $K_t(x,u,v) = (x, H_t(u,v))$ defines a deformation retract of $X \times I \times I$ onto $(X \times I \times \{0\}) \cup_{X \times \{0\} \times \{0\}} (X \times \{0\} \times I)$.</p>
"
"2396124","2396196","<p>As it was already pointed out in the comments, the answer to your question is no. Here is a specific counterexample:
Let's take as irreducible affine variety in $\mathbb{C}^2$, the circle given by equation $x^2+y^2-1=0$ and then take simply the singleton $\{(0,1)\}$. It is a quasi-affine variety, since it is an affine variety.
It is closed inside of the circle, therefore its closure is still the singleton.</p>

<p>If on the other hand your quasi-variety $Y$ is open in $X$, then it is true that $\overline Y=X$, since every non-empty open subset of an irreducible set is dense.</p>
"
"2396129","2396132","<p>$$e^{-z} = \sum_{n=0}^{\infty} \frac{(-z)^n}{n!} \neq \sum_{n=-\infty}^{0} \frac{z^n}{n!}.$$ Also, $n!$ is not defined for $n &lt;0$.</p>
"
"2396137","2396202","<p>the line passing through $(1,8)$ has equation</p>

<p>$y-8=m(x-1)\;\;y=mx-m+8$</p>

<p>which intersect $x-$axis at $\left(\dfrac{m-8}{m};\;0\right)$ and $y-$axis at $(0;\;8-m)$</p>

<p>Hypotenuse $h(m)=\sqrt{\left(\dfrac{m-8}{m}\right)^2+(8-m)^2}$</p>

<p>as square root is an increasing function, $h(m)$ will be minimum when </p>

<p>$r(m)=\left(\dfrac{m-8}{m}\right)^2+(8-m)^2=m^2+\dfrac{64}{m^2}-16 m-\dfrac{16}{m}+65$</p>

<p>will be minimum</p>

<p>$r'(m)=-\dfrac{128}{m^3}+\dfrac{16}{m^2}+2 m-16=\dfrac{2 \left(m^4-8 m^3+8 m-64\right)}{m^3}$</p>

<p>$m^4-8 m^3+8 m-64=0\to (m-8) (m^3-8) =0$</p>

<p>$m=8$ gives hypotenuse $h(8)=0$ which makes no sense</p>

<p>$m^3=8\to m=2$ gives $h(2)=3 \sqrt{5}$ which is the minimum we were looking for</p>

<p>indeed second derivative is $r''(m)=\dfrac{2 \left(m^4-16 m+192\right)}{m^4}$ and $r''(2)=22&gt;0$</p>

<p>it is positive at $m=2$  so it is a minimum</p>

<p>hope this helps</p>
"
"2396142","2396172","<p>$f_x = 4x^3 - 4x +4y = 0, f_y = 4y^3 - 4y + 4x = 0\implies 4(x^3+y^3) = 0 \implies x = -y$ or $x^2 - xy + y^2 = 0$ which yields $x = y = 0$. If $x = -y \implies 4x^3 - 8x = 0 \implies 4x(x^2 - 2) = 0\implies x = 0, \pm \sqrt{2}\implies y = 0,\mp \sqrt{2}$. Next $f_{xx}= 12x^2 - 4$. Thus $f_{xx}(\pm \sqrt{2}) = 20 &gt; 0$. So by the second derivative test ( see a calculus book for this test ) the min are at these points. Thus $f_{\text{min}} = f(\pm \sqrt{2}, \mp \sqrt{2})= -8 $ since $f(0,0) = 0 &gt; -8$.</p>
"
"2396148","2396152","<p>Yes. It has to be the splitting field of an irreducible cubic.</p>

<p>By the primitive element theorem $K=\Bbb{Q}(\alpha)$ for some $\alpha\in K$. The minimal polynomial $m(x)$ of $\alpha$ is then necessarily a cubic. But, as $K/\Bbb{Q}$ is normal, all the zeros of $m(x)$ are in $K$.</p>

<hr>

<p>Extras for the question about an arbitrary polynomial $f(x)$ with splitting field $K$. Consider a non-linear irreducible factor of $f(x)$. It cannot be a quadratic for then $[K:\Bbb{Q}]$ would be even. It cannot be $\ge4$ for then $[K:\Bbb{Q}]$ would also be $\ge4$. Ergo, that irreducible factor must be cubic.</p>
"
"2396160","2396479","<p>(1) Yes, you could think of it as knowing $X_0$ gives no new information about $X_{n+m}$ if $X_n$ is known. By the Markov property, one could say that, <em>conditioned on the present</em>, the future does not depend on the past.</p>

<p>(2) I think this is because you are assuming that the <a href=""https://en.wikipedia.org/wiki/Markov_chain#Variations"" rel=""nofollow noreferrer"">Markov process is stationary</a> (i.e. it is time-homogeneous), which means the state-to-state transition probabilities do not change over time. It does also relate to the Markov property as well of course. In this case:
$$ P(X_{n+m}=j\,|\,X_n=k) = P(X_{n+m+s}=j\,|\,X_{n+s}=k) $$
In words, going from state $k$ to state $j$ in $m$ steps is always the same. Hence, set $s=-n$ to get:
$$ P(X_{n+m}=j\,|\,X_n=k) = P(X_{m}=j\,|\,X_{0}=k) $$
In essence, only the time <em>difference</em> matters. Hence, translation of <em>both</em> indices in time does not change the value.</p>

<p>With regards to your question, I would not say that it is irregardless of the current state; rather, it is because the process acts the same once it reaches a given state, no matter how it got there. Hence, going from $s_1$ to $s_2$ in $t$ steps is always the same; it doesn't matter if you arrived at $s_1$ at time $0$ or at time $n$ (due to stationarity), as the chance of reaching $s_2$ in $t$ steps remains the same.</p>
"
"2396180","2396198","<p>First of all I applaud you for the username.</p>

<p>I'll do an induction proof for you, and show you why I do everything. Hopefully this will answer all your questions.</p>

<p>Suppose we have the following:</p>

<p>$$
f(n) = \left\{\begin{array}{lr}
        n, &amp; \text{for } 0\leq n\leq 2\\
        3f(n-2)+2f(n-3), &amp; \text{for } n&gt;2\\
        \end{array}\right\}
$$</p>

<p>Let's prove $f(n)&lt;2^n$ for all $n\in\mathbb{N}$</p>

<p>Let's define a predicate.</p>

<p>For all $n\in\mathbb{N}$, let $P(n):f(n)&lt;2^n$</p>

<p>Pf of $\forall n, P(n)$.</p>

<hr>

<p>Base case(s)</p>

<p>First of all, notice the when we recurse ($n&gt;2$), we have to ""go back"" at most $3$ numbers, hence the $n-3$. This should suggest that we have $3$ base cases.</p>

<p>As for which ones, well definitely we must prove $P(0)$, and after that, we prove 2 more, so we prove $P(1),P(2)$. So in the inductive step, when we ""go back"", we have these base cases to work with, so we are okay. If you only proved $P(0)$, then in the inductive step you would get stuck, because you don't know anything regarding $P(n-2)$.</p>

<p>Let $n=0$</p>

<p>Then $f(n)=0$, by definition</p>

<p>$&lt; 2^0 = 1$</p>

<p>$\therefore P(0)$ holds.</p>

<p>Now do $P(1)$ and $P(2)$</p>

<hr>

<p>Induction Step:
For of all, since we proved $P(0-2)$, now we let $n&gt;2$, or equally $n\geq 3$, so we enter the induction step.</p>

<p>Let $n&gt;2$</p>

<p>Suppose $P(j)$ holds where $0\leq j\leq n-1, j\in\mathbb{N}$. [IH]</p>

<p>What does this say? This says let $j$ be an arbitrary natural number between $0$ and $n-1$, both inclusive. This says that suppose $P(0)\land P(1)\land\dots\land P(n-1)$ hold. This is how I write my IH, it's very clear. Moreover, you can also write this:</p>

<p>Suppose $P(j)$ holds where $0\leq j\lt n, j\in\mathbb{N}$. [IH]</p>

<p>We want to prove that $P(n)$ holds. It's much easier to prove $P(n)$ holds, rather then assuming $P(n)$ and proving $P(n+1)$.</p>

<p>$f(n)=3f(n-2)+2f(n-3)$, by definition of recursive step</p>

<p>$&lt;3\cdot 2^{n-2}+2\cdot 2^{n-3}$, by IH, since $0\leq n-3&lt;n-2&lt;n$</p>

<p>$=6\cdot 2^{n-3}+2\cdot 2^{n-3}$</p>

<p>$=8\cdot 2^{n-3}$</p>

<p>$=2^n$</p>

<p>$\therefore P(n)$ holds.</p>

<hr>

<p>We have showed it holds for all $\mathbb{N}$, and so our proof is done. For proving how many base cases, think about how many steps the recursion ""goes back"", and proceed from there. For writing your induction hypothesis, it becomes much easier with a predicate and proving $P(n)$. If you can understand the way I wrote mine, just use that. Assume the predicate holds within that range, where $0$ is your first base case, and $n-1$ is what you assume it holds to. Then prove $P(n)$. Hope you found this answer useful.</p>
"
"2396184","2396213","<p>The notation you are using is a bit off. For example $\in$ is the symbol indicating that an element belongs to a set, so saying things like</p>

<p>$$\{v_1,v_2,...,v_r\}\in \{cw_1 + cw_2 + ...cw_q\}$$<br>
and<br>
$$\{w_1,w_2,...,w_q\}\in \{kv_1 + kv_2 + ...kv_r\}$$ </p>

<p>is not correct.</p>

<p>For the proof itself, the ""only if"" direction (that is, if $\mathrm{span}\{v_1,\ldots,v_r\}=\mathrm{span}\{w_1,\ldots,w_q\}$, then you have the statement about linear combinations) follows from the definition of span. Can you see why?</p>

<p>As for the other direction, what you need to show is that if each $v_i$ is a linear combination of the $w_j$, then $\mathrm{span}\{v_1,\ldots,v_r\}\subset \mathrm{span}\{w_1,\ldots,w_q\}$. Likewise, if each $w_j$ is a linear combination of the $v_i$, then $\mathrm{span}\{v_1,\ldots,v_r\}\supset \mathrm{span}\{w_1,\ldots,w_q\}$. I'll prove one and leave the rest to you.</p>

<p>Since $v_i$ is a linear combination of the $w_j$, by definition we have $v_i\in \mathrm{span}\{w_1,\ldots,w_q\}$, for each $i=1,\ldots,r$. Since $\mathrm{span}\{w_1,\ldots,w_q\}$ is a vector space, it is closed under taking sums of vectors and scalar multiplication, so we are free to take linear combinations of elements inside $\mathrm{span}\{w_1,\ldots,w_q\}$.</p>

<p>In particular, we can take ANY linear combinations of $S=\{v_1,\ldots, v_r\}$, and it will still lie in $\mathrm{span}\{w_1,\ldots,w_q\}$. Thus, by defintion 
$$\mathrm{span}\{v_1,\ldots,v_r\}\subset \mathrm{span}\{w_1,\ldots,w_q\}.$$</p>

<p>This proves one direction. I leave showing $\mathrm{span}\{v_1,\ldots,v_r\}\supset \mathrm{span}\{w_1,\ldots,w_q\}$, and the ""only if"" direction to you. I hope this helps.</p>
"
"2396192","2396209","<p>Since
$$\sum_{n=0}^{\infty}(1-b_n)$$
converges you have
$$\lim_{n\rightarrow \infty}b_n=1.$$
This means that there exists an index $N$ such that  $1/2&lt; b_n&lt;2$ for all $n\geq N$. Therefore
$$a_n=\sum_{n=0}^{N-1}b_nx^n+\frac{1}{2}\sum_{n=N}^{\infty}x^n&lt;\sum_{n=0}^{\infty}b_nx^n&lt;\sum_{n=0}^{N-1}b_nx^n+2\sum_{n=N}^{\infty}x^n=c_n.$$</p>

<p>$a_n,c_n$ have radii of convergence $1$, by comparison $\sum_{n=0}^{\infty}b_nx^n$ must have the same radius of convergence.</p>
"
"2396199","2396233","<p>It shouldn't be too bad. I will focus on the case where the elements (vectors) in the last column have a different dimension than the elements in the first $m-1$ columns.... So, Assume you are given an $n\times m$ matrix where the (vectors) of the first $m-1$ columns have $k_1$ elements, and the vectors in the last column each have $k_2$ elements. If we count the elements in each vector for each vector in some particular row (say row $i$), we get</p>

<p>$\sum _{j=1} ^{m} |A_{ij}|$<br>
 $=$  $\sum _{j=1} ^{m-1} |A_{ij}|+ \sum _{j=m-1} ^{m} |A_{ij}|$</p>

<p>Each term in the sum on left has value $k_1$ and the single term in the sum on the right (the size of the vector in the last column) is $k_2$</p>

<p>Thus we have $(m-1 \cdot k_1 )$$+k_2$ elements in each row. Call this quantity $Q$</p>

<p>Since there are $n$ rows, you have exactly $n\cdot Q$ elements total. </p>

<p>Now, given $l$ (some id of an element as you described) you can first determine the row by computing $\lfloor \frac{l}{Q} \rfloor$ (floor function). Call this value $V$</p>

<p>The same idea applies to finding the column. Once you know your row, you are essentially ""starting from 0"" when counting over columns, so to figure out how many elements into the given row $l$ is, compute
$l - V\cdot Q$ (or use the mod function and take $l$ mod $Q$).</p>

<p>The quantity $l - V\cdot Q$ tells you the number of elements you need to count over in the given row in order to find your element. I would break this into two cases, </p>

<p>case 1) If  $l - V\cdot Q \geq (m-1)\cdot k$ then you know you are in the last column and it should be smooth sailing from there.</p>

<p>case 2) o.w. $l - V\cdot Q &lt; (m-1)\cdot k$
to determine the column just compute:
$col$=$\lfloor (l - V\cdot Q )/ k \rfloor$</p>

<p>Last, the element number in the vector is given by the remainder of (l - V\cdot Q )/ k, i.e.$elt_in_vector=(l - V\cdot Q )/ \mod k$</p>

<p>Let me know if you have any questions. It's tough to get this stuff perfect first try, i probably made a mistake somewhere.</p>
"
"2396201","2396203","<p>So that we could be able to factorize it and use the $AB = 0$ Principle.</p>

<p>The $AB = 0$ is like this, if you have two numbers with products equal to zero, then either both numbers are zero, or one of then must be zero.</p>

<p>Consider your equation, $x^2 - 4x + 9 = 5$ now if we can set the the $LHS$ equal to zero, then we can factorize and use the $AB = 0$ principle.</p>

<p>As $x^2 - 4x + 4 = 0$ then $x(x - 2) -2(x - 2) = 0$ which yields $( x - 2)(x - 2) = 0$</p>

<p>For equality to hold, $(x - 2)$ must be equal to $0$, which is a clever way to find the roots of a quadratic equation.</p>
"
"2396204","2396235","<p>As the arithmetic mean is greater than or equal to the geometric mean, we know that
$$
\frac{2^{1/n}+2^{-1/n}}{2} \geq \sqrt{2^{1/n}\cdot 2^{-1/n}} = 1
$$
Therefore $2-2^{1/n} \leq 2^{-1/n}$ and
$$
(2-2^{1/2})(2-2^{1/3})\cdot \ldots \cdot (2-2^{1/n})
\leq 
2^{-\left( \frac{1}{2} +\frac{1}{3} +\ldots +\frac{1}{n} \right) }
$$
As the harmonic series diverges, the limit of the original product is $0$.</p>
"
"2396222","2396251","<p>The gradient of a (""nice"") scalar function is related to its total differential
$$
\DeclareMathOperator{grad}{grad}
A \cdot dr = \grad \Phi \cdot dr = \sum_i \frac{\partial \Phi}{\partial x_i}  \, dx_i = d\Phi 
$$
so this $A$ (with $A = \grad \Phi$) can be integrated independent of path:
$$
\int\limits_{r_1}^{r_2}A \cdot dr =\int\limits_{r_1}^{r_2}\grad\Phi \cdot dr = \int\limits_{r_1}^{r_2} d\Phi = \phi(r_2)-\phi(r_1)
$$
There are differentials
$$
A \cdot dr = \sum_i A_i dx_i
$$
which can not be expressed as total differential of a potential function, where the integration value depends on the path and not just on the endpoints, where integration along closed paths does not vanish, where $A=(A_i)$ is non-conservative. 
These form inexact differentials.</p>

<p>Theoretical thermodynamics features such differentials prominently.</p>

<p>For (""nice"") vector fields in 3D one has the Helmholtz decomposition,
where a vector field can be written as sum of a gradient field and a curl of a vector potential.</p>
"
"2396226","2396240","<p>In general $$\int_{0}^{2\pi} f(\theta)\,d\theta$$ is not the area of the curve, even when $f(\theta)&gt;0$ for all $\theta$. This is because for $\Delta\theta$, the area is estimated by a triangle, not a rectangle. In particular, you have a triangle with lengths $f(\theta),f(\theta+\Delta \theta)$ and angle $\Delta\theta$. The triangle area is thus 
$$\frac{1}{2}f(\theta)f(\theta+\Delta\theta)\sin \Delta\theta.$$</p>

<p>Turns out, $\sin\Delta x\sim \Delta x$ is good enough. Thus, to get the area bounded by $f(\theta)$, you want:</p>

<p>$$\int_{0}^{2\pi} \frac{1}2f(\theta)^2\,d\theta$$</p>

<p>So you get a factor of $\frac{1}{2}$, plus you want to square your function.</p>

<p>It's a little trickier of $f(\theta)$ is negative for some values $\theta$.</p>

<p>See: <a href=""http://tutorial.math.lamar.edu/Classes/CalcII/PolarArea.aspx"" rel=""nofollow noreferrer"">http://tutorial.math.lamar.edu/Classes/CalcII/PolarArea.aspx</a></p>
"
"2396246","2396256","<p>Put $b,s$ be the max capacity of the larger, and smaller tanks. Then:
$b = s+500, \dfrac{2s}{3} = \dfrac{b}{2}$. Substitute $s+500$ for $b$ in the second equation to get: $\dfrac{2s}{3} = \dfrac{s+500}{2}\implies s = 1,500, b = 2,000$ liters respectively.</p>
"
"2396260","2397507","<p>So, we have a radical tower $F=F_0\subset F_1\subset\cdots\subset F_r$ with $F_{i+1}=F_i(\alpha_i)$, $\alpha_i^{d_i}=a_i$, $a_i$ in $F_i$, and $E\subset F_r$. Let $n$ be the least common multiple of $d_1,\dots,d_r$. Now consider the tower $F\subset E_0\subset E_1\subset\cdots\subset E_m$ given by $E_0=F(\zeta)$ where $\zeta$ is a primitive $n$th root of unity, and $E_{i+1}=E_i(\alpha_i)$. </p>

<p>This is clearly a radical tower. </p>

<p>Condition b) is clearly met. </p>

<p>Condition c) is met since $n$ is the least common multiple of the $d_i$. </p>

<p>$E$ is contained in $F_r$, and $F_r$ is contained in $E_m$, so $E\subset E_m$. </p>

<p>So all that remains is to show that $E_m/F$ is Galois. </p>

<p>Now $E_m=F(\zeta,\alpha_1,\dots,\alpha_r)$, so $E_m/F$ is finite. It is also normal, since it contains all the conjugates of all of its generators. So, it's Galois. </p>
"
"2396268","2396270","<p>Let $N$ be a subset of $[0,1]$ that is not Lebesgue measurable, and consider the set
$A:=N\times\{0\}$. Now $A$ is Lebesgue measurable with measure zero since $A\subseteq [0,1]\times\{0\}$ and $[0,1]\times\{0\}$ has Lebesgue measure zero.
However $f^{-1}(A)=N$ is Lebesgue nonmeasurable.</p>
"
"2396281","2396306","<p>I'll use '>' instead of using 'â¥', and also won't worry about sub-scripting, and use capitalization instead.</p>

<p>An induction step assumes that if a proposition predicated of n holds, then the same proposition predicated of the successor member S(n) holds also.  Or in other words, P(n) implies that P(S(n)), or ""if P(n), then P(S(n))"".  Here, the successor of n is just (n + 1).  </p>

<p>Now, what is the proposition predicated of n here?</p>

<p>The proposition is the entire if-then statement that you posted.</p>

<p>Thus, changing from using the variable 'n' to 'k' to avoid confusion between the proposition itself and if-then the induction step, the induction step can get written as:</p>

<p>If ""If A1 > A2, ... , A(k - 1) > Ak, then A1 > Ak"" [this entire if-then part I call the first hypothesis], then ""if A1 > A2 ... Ak > A(k + 1) [only the if  part of this I call the second hypothesis], then A1 > A(k + 1)"".</p>

<p>Now, by the second hypothesis A1 > A2, ..., A(k - 1) > Ak.  Thus, by detachment and the first hypothesis, A1 > Ak.  Ak > A(k + 1) also holds by the second hypothesis.  Thus, (A1 - Ak) > 0 and Ak > A(k + 1).  </p>

<p>Lastly, adding the inequalities eventually yields A1 > A(k + 1).</p>

<p>So, your technique had potential here.</p>
"
"2396292","2396320","<p>Using the fact that $a^na^m=a^{n+m}$ or $ b^n/b^m=b^nb^{-m}=b^{n-m} $:</p>

<p>$$ \frac{2}{\sqrt{2}}=\frac{2^1}{2^{1/2}} = 2^1\,2^{-1/2} = 2^{1-1/2}=2^{1/2} = \sqrt{2} $$</p>

<p>i.e. the algebra of exponents.</p>
"
"2396295","2396311","<p>we can rewrite it a bit and possibly make things clearer:$$(B-A)=B\setminus A= \{b:b\in B, b\notin A\}$$ then:$$A\cap (B-A)$$ becomes$$\{b:b\in A \land b\in B \land b\notin A\}$$ but the first of these, and the last, logically contradict. b can't both be in A, and not in A ( at least in finite sets). this is set builder notation, and $\land$ is the logical AND. </p>
"
"2396302","2396356","<p>For part (a) I'd skip using induction and manipulate the series instead.</p>

<p>Define $\phi (x,t)$ as the generating function:</p>

<p>$$ \phi (x,t) = \dfrac{1}{\sqrt{1 - 2xt + x^2}}$$</p>

<p>Differentiate this with respect to $x$ to get</p>

<p>$$ \dfrac{\partial \phi}{\partial x} = \dfrac{t - x}{(1 - 2xt + x^2)^{3/2}}$$</p>

<p>which can be rearranged to </p>

<p>$$ (1 - 2xt + x^2) \dfrac{\partial \phi}{\partial x} + (x - t) \phi = 0$$</p>

<p>Insert the series representation for $\phi$ into the above:</p>

<p>$$ (1 - 2xt + x^2) \sum_{n=0}^{\infty}n P_n(t) x^{n-1} + (x - t) \sum_{n=0}^{\infty} P_n(t)x^n = 0 $$</p>

<p>After this expand to five separate series and collect powers of $x$; there's some index shifting involved. The end result is:</p>

<p>$$P_1(t) - t P_0(t) + \sum_{n=1}^{\infty}[ (n+1) P_{n+1}(t) - (2n+1)tP_n(t) + n P_{n-1}(t)] x^n = 0$$</p>

<p>From this you can conclude that </p>

<p>$$ P_1(t) - t P_0(t) = 0$$</p>

<p>and </p>

<p>$$ (n+1) P_{n+1}(t) - (2n+1)tP_n(t) + n P_{n-1}(t) = 0$$</p>

<p>which you can rearrange.</p>

<p>For reference a lot of texts on mathematical physics cover special functions/orthogonal polynomials in detail. <em>Special Functions: An Introduction to the Classical Functions of Mathematical Physics</em> by Nico Temme is particularly thorough.</p>
"
"2396307","2397027","<p>The question is about the sum of a $5$-subset of $\{1,2,\dots,80\}$, taken modulo $10$. </p>

<p>Yes, there are some tricky ways to enumerate those. </p>

<p>Denote $\varepsilon_k = e^{\pi k i/5}$, $k=1,\dots,9$, $10$th roots of unity. Then 
$$\tag{1}
\sum_{k=0}^9 \varepsilon_k^n = \begin{cases}
10, &amp; 10\mid n,\\
0, &amp; 10\nmid n.
\end{cases}
$$</p>

<p>Note that 
$$
F(u,z) = \prod_{j = 1}^{80} (1+ u z^j) = \sum_{j,n} A(j,n) u^j z^n,
$$
where $A(j,n)$ is the number of $j$-subsets with sum $n$. 
Therefore, the number of $5$-subsets with sum equal to $0$ modulo $10$ is equal, thanks to $(1)$, to 
$$
[u^5]\frac{1}{10}\sum_{k=0}^9 F(u,\varepsilon_k),
$$
(the coefficient before $u^5$). </p>

<p>If $\varepsilon_k$ is a primitive root (i.e. $k=1,3,7,9$), then (noting that $\varepsilon^{10}_k = 1$)
$$
F(u,\varepsilon_k) = \Big(\prod_{n=0}^{9}(1+u \varepsilon_k^{n})\Big)^{8} = \big(1 - (-u)^{10}\big)^{8} = (1-u^{10})^8,
$$
since $\varepsilon_k^n$, $n=0,\dots,9$, are different $10$th roots of unity.</p>

<p>If $k=2,4,6,8$, then $\varepsilon_k$ is a primitive $5$th root of unity, so, similarly,
$$
F(u,\varepsilon_k) = \big(1 - (-u)^{5}\big)^{16} = (1+u^5)^{16}.
$$
Further,
$$
F(u,\varepsilon_5) = \big(1-(-u)^2\big)^{40} = (1-u^2)^{40}.
$$
Finally,
$$
F(u,\varepsilon_0) = F(u,1) (1+u)^{80}.
$$</p>

<p>Therefore, the number of $5$-subsets with sum divisible by $10$ is
$$
[u^5]\frac{1}{10}\big( 4(1-u^{10})^8 + 4(1+u^5)^{16} + (1-u^2)^{40} + (1+u)^{80} \big)\\
 = \frac25 {16 \choose 1} + \frac1{10}{80\choose 5} = 2\,404\,008.
$$</p>

<p>The number of $4$-subsets:
$$
[u^4]\frac{1}{10}\big( 4(1-u^{10})^8 + 4(1+u^5)^{16} + (1-u^2)^{40} + (1+u)^{80} \big)\\
 = \frac1{10} {40 \choose 2} + \frac1{10}{80\choose 4} = 158\,236.
$$</p>

<p>I leave you other last digits as an exercise. </p>

<hr>

<p>If we were interested in the sum modulo some prime number, the computation would be much easier.</p>
"
"2396308","2396337","<p>Let $X$ be a discrete random variable and let $p$ be the probability mass function (pmf) of $X$. Write $P(X=x)$ to mean the probability of observing $x$.
Then 
$$
p(x)= P(X=x) = 
\begin{cases}
\dfrac{\mbox{the number of occurrences of }x}{\mbox{total number of occurrences}} &amp;\mbox{ if } x = 1,2,3, \\ 
\hspace{2cm} 0 &amp;\mbox{ otherwise}. 
\end{cases}
$$</p>

<p>So for your occurrences: $1,1,1,2,1,3,1,3,1,1,3,3,3,2$, the pmf is
$$
p(x) =
\begin{cases} 
\frac{1}{2} &amp;\mbox{ if } x=1, \\ 
\frac{1}{7} &amp;\mbox{ if } x=2, \\ 
\frac{5}{14} &amp;\mbox{ if } x=3, \\ 
0            &amp;\mbox{ otherwise}. \\ 
\end{cases} 
$$ </p>
"
"2396310","2396333","<p>Assuming - since you mention number theory - that $n\in \Bbb N$, and taking $c$ as a real (ratio) as hinted, we have $n&gt;3$, so $n\ge 4$. You will already have proven that ratio $\frac{3^n}{n^3}$ increases with increasing $n$, so the tightest constraint on $c$ is for the lowest value of $n$.</p>

<p>Thus for the smallest possible $n=4,$ we require $81&gt;64c\ $ and so $c&lt;\frac{81}{64}=1.265625$</p>
"
"2396316","2396328","<p>Count the number of sequences directly.  The number of sequences possible is $k^N$.  The number of sequences with no $1$'s and no $2$'s is $(k-2)^N$.  The number of sequences with no $1$'s and no requirement on $2$'s is $(k-1)^N$.  So... the number of sequences with no $1$'s and at least one $2$ is... </p>

<blockquote class=""spoiler"">
  <p> $(k-1)^N-(k-2)^N$</p>
</blockquote>

<p>And so the probability is...</p>

<blockquote class=""spoiler"">
  <p> $$\dfrac{(k-1)^N-(k-2)^N}{k^N}$$</p>
</blockquote>
"
"2396324","2397010","<p>Let $X \sim \text{Gamma}(a,b)$ with pdf $f(x)$:</p>

<p><a href=""https://i.stack.imgur.com/woBp2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/woBp2.png"" alt=""enter image description here""></a></p>

<p>and $Y \sim \text{Gamma}(\alpha,\beta)$ be independent with pdf $g(y)$:</p>

<p><a href=""https://i.stack.imgur.com/n74Aw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n74Aw.png"" alt=""enter image description here""></a></p>

<p>Then, the pdf of the product $Z = X Y$ can be obtained as $h(z)$:</p>

<p><a href=""https://i.stack.imgur.com/Pdj1d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Pdj1d.png"" alt=""enter image description here""></a></p>

<p>where I am using the <code>TransformProduct</code> function from <em>mathStatica/Mathematica</em> to do the nitty-gritties, and <code>BesselK[n,z]</code> denotes the modified Bessel function of the second kind. This is much simpler than requiring MeijerG functions. I should note that I am one of the authors of the software function used. </p>

<p><strong>Quick Monte Carlo check</strong> </p>

<ul>
<li>against theoretical solution derived above when  $a =2$, $b = 3$, $\alpha = 4$, and $\beta = 1.1$</li>
</ul>

<p><a href=""https://i.stack.imgur.com/P1aXG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P1aXG.png"" alt=""enter image description here""></a></p>

<p>Looks fine :)</p>
"
"2396334","2396351","<p>Yes, $4 = A(x^2+4)~+~(Bx+C)x \implies A=1, B=-1, C=0$</p>

<p>So</p>

<p>$$\require{enclose}\int \dfrac{4}{x(x^2+4)}~\mathrm d x ~{= \int \dfrac{1}{x}+\dfrac{\enclose{circle}{~-x~}}{(x^2+4)}~\mathrm d x \\ = \ln\lvert x\rvert  -\int\dfrac{\tfrac 12\mathrm d (x^2+4)}{(x^2+4)} \\ = \ln\lvert x\rvert -\tfrac 12\ln\lvert x^2+4\rvert+D}$$</p>
"
"2396340","2396398","<p>The transformation you are trying is </p>

<p>$$\iint_D x^2+y^2\mathrm d(x,y) ~{= \iint_D (x(u,v)^2+y(u,v)^2){\mathsf J_{(u,v)}^{(x,y)}}\mathrm d (u,v) }$$</p>

<p>Where $\mathsf J_{(u,v)}^{(x,y)} = \begin{Vmatrix}\dfrac{\partial \big(x(u,v),y(u,v)\big)}{\partial (u,v)}\end{Vmatrix}$</p>

<p>And that is okay.... but complicated. &nbsp; Try another angle of attack.</p>

<hr>

<p>Recall that $\mathsf J_{(u,v)}^{(x,y)} ={\mathsf J^{(u,v)}_{(x,y)}}^{-1}$</p>

<p>And since we have: $\mathsf J_{(x,y)}^{(u,v)}~{=\begin{Vmatrix}\dfrac{\partial (u,v)}{\partial (x,y)}\end{Vmatrix} \\= \begin{Vmatrix}\dfrac{\partial \big(x^2-y^2,xy\big)}{\partial (x,y)}\end{Vmatrix} \\= 2(x^2+y^2)} \\  $</p>

<p>Therefore we have:</p>

<p>$$\iint_D x^2+y^2\mathrm d(x,y) ~{= \iint_D (x^2+y^2){\mathsf J_{(u,v)}^{(x,y)}}\mathrm d (u,v) \\ = \iint_D \tfrac 12\mathrm d (u,v) \\ \vdots}$$</p>
"
"2396348","2396353","<p>You can only do that if the series converges uniformly in $x$. Which, luckily for you, it does. On any compact subset of $\mathbb{R}$, say $[-M,M]$, you have that 
$$\sum_{n=1}^\infty \frac{n^2x^2}{n^4+x^4} \le \sum_{n=1}^\infty \frac{M^2}{n^2}$$.</p>

<p>All you need, in particular, is uniform convergence on a neighborhood of $1$. Can you conclude?</p>
"
"2396349","2396370","<p>For $G_1$ and $G_2$, they are not isomorphic as every vertex in $G_1$ has degree $3$ but vertex $10$ in $G_2$ has degree $4$. </p>

<p>If they are isomorphic, then they should share the same degree. </p>

<p>To prove that they are not isomorphic, look for graph invariant that are not obeyed, for example</p>

<p>(a)  degree,</p>

<p>(b)   number of edges,</p>

<p>(c)  number of components,</p>

<p>(d) cycle length.</p>

<p>Since graph isomorphism means a relabeling of vertices to match a graph, the above properties are preserved.</p>

<p>To prove that two graphes are isomorphic, construct a bijection between the nodes and check that they are connected in the same way.</p>

<p>Complexity wise:</p>

<p>According to <a href=""https://en.wikipedia.org/wiki/Graph_isomorphism"" rel=""nofollow noreferrer"">wikipedia page of graph isomorphism</a>, in November 2015, LÃ¡szlÃ³ Babai, a mathematician and computer scientist at the University of Chicago, claimed to have proven that the graph isomorphism problem is solvable in quasi-polynomial time. This work has not yet been vetted. In January $2017$, Babai shortly retracted the quasi-polynomiality claim and stated a sub-exponential time time complexity bound instead. He restored the original claim five days later.</p>
"
"2396362","2397373","<p>The number of oriented loops is the same as the number of permutations of letters that begin with any one of the letters, So the total number of loops is equal to the number of permutations of the remaining 4 letters $$N_{tot}=4!=24$$</p>

<p>To count the number of legal loops it is best to consider permutations starting with the letter C since C can be followed only by D or E and can be preceded by anything.</p>

<p>This is a good problem for using a tree diagram - something like ...</p>

<pre><code>CD -&gt; CDA -&gt; CDAE -&gt; CDAEB
   -&gt; CDB -&gt; CDBA -&gt; CDBAE
          -&gt; CDBE -&gt; CDBEA
CE -&gt; CEA -&gt; CEAD -&gt; CEADB
   -&gt; CEB -&gt; CEBA -&gt; CEBAD
          -&gt; CEBD -&gt; CEBDA
   -&gt; CED -&gt; CEDA 
          -&gt; CEDB -&gt; CEDBA
</code></pre>

<p>In this way you can generate the 7 legal loops.</p>
"
"2396363","2396374","<p>Choose one point, arbitrarily (since the graph is symmetric). You can choose the Hamiltonian path through that vertex in $3$ ways, effectively choosing which of its incident edges will not be on the path. The adjacent vertex that will not now be adjacent on the path has also had its local path chosen, since we have removed the option of one edge. </p>

<p>To complete the path from these two short initial sections, one end is linked directly and the other end must be connected through the vertices not yet included on the path - $2$ choices. You can demonstrate this by choosing the non-connecting option at one end of a short section, then noticing that the other end of the same section must now directly connect to the other initial section of path to avoid a short cycle.</p>

<p>So there are a total of $3\times 2 = 6$ undirected Hamiltonian cycles for the cube, as you found.</p>
"
"2396368","2396376","<p>Let $\ r = X/(AB).\,$ Then $\,r^2 = \overbrace{(X/A^2)}^{\large \in\,\Bbb Z}\overbrace{(X/B^2)}^{\large \in\,\Bbb Z} =: n\in \Bbb Z\ $ so $\ r\in\Bbb Z\ $ by </p>

<p><strong>Theorem</strong> $\ \ \ \rm r = \sqrt{n}\;\;$ is integral if rational, $ $ for $\:\rm n\in\mathbb{N}$</p>

<p><strong>Proof</strong> $\ \ $ Writing $\rm\,\ r = a/b,\ \ \gcd(a,b) = 1\ \Rightarrow\ ad\!-\!bc \,=\, \color{#C00}{\bf 1}\;$  for some $\:\rm c,d \in \mathbb{Z}\ $ by Bezout.   </p>

<p>$\rm\color{#C00}{That\,}$ and $\rm\: r^2\! = \color{#0a0}{ n}\:\Rightarrow\ 0 \,=\, (a\!-\!br)\, (c\!+\!dr) \, =\, ac\!-\!bd\color{#0a0}{ n} \:+\: \color{#c00}{\bf 1}\cdot r,\ $ so $\rm\ r = bdn\!-\!ac \in \mathbb{Z}$</p>

<hr>

<p><strong>Remark</strong> $ $ There are many ways to prove this Theorem going back to the ancient proofs of  irrationality of $\,\sqrt 2.\,$ See <a href=""https://math.stackexchange.com/q/4467/242"">this thread</a> for many proofs. One can also use the Rational Root Test and variants, or descent on denominators, or principality of denominator ideals, e.g. <a href=""https://math.stackexchange.com/a/16544/242"">here</a> or <a href=""https://math.stackexchange.com/a/4491/242"">here.</a> There are over a handful of variants in my older posts.</p>
"
"2396373","2396411","<p>The series is not uniformly convergent. Consider the partial sum: $$S_n(x) = \sum_{k=0}^n \frac{x^2}{(1+x^2)^k} = 1+x^2-\frac{1}{(1+x^2)^n}$$ for all $x\in [-1, 1]$. Let $$S(x) = \begin{cases} 1+x^2,&amp; x\neq 0 \\ 0,&amp; x = 0 \end{cases}$$ be the pointwise limit of $S_n$. Then, $$\|S-S_n\|_{\infty} = \left\|\frac{1}{(1+x^2)^n}\right\|_{\infty} = 1$$ for all $n$, as $\lim_{x\to 0} \frac{1}{(1+x^2)^n} = 1$ for all $n$. As $\|S-S_n\|_{\infty}\not\to 0$, $S_n$ does not converge uniformly to $S$.</p>
"
"2396389","2396469","<p>Note that $$\frac{1}{2k+1}=\int_{0}^{1}x^{2k}\,dx$$ and hence $$\int_{0}^{1}\frac{dx}{1+x^{2}}-\sum_{k=0}^{n}(-1)^{k}\frac{1}{2k+1}=\int_{0}^{1}\left(\frac{1}{1+x^{2}}-\sum_{k=0}^{n}(-1)^{k}x^{2k}\right)\,dx$$ The integral on the right can be expressed (using sum of a geometric progression) as $$(-1)^{n+1}\int_{0}^{1}\frac{x^{2n+2}}{1+x^{2}}\,dx=(-1)^{n+1}I_{n}\text{(say)}$$ and clearly $$0\leq I_{n} \leq \int_{0}^{1}x^{2n+2}\,dx=\frac{1}{2n+3}$$ By Squeeze Theorem $I_{n}\to 0$ and hence $(-1)^{n+1}I_{n}\to 0$ as $n\to\infty$. It follows now that $$\int_{0}^{1}\frac{dx}{1+x^{2}}=\sum_{k=0}^{\infty}(-1)^{k}\frac{1}{2k+1}$$ Note that the analysis of $I_{n} $ is easy but still necessary. </p>
"
"2396408","2396417","<p>Using arithmetic-quadratic mean inequality, $$|c| = \frac{|z-a|+|z+a|}{2} \le \sqrt{\frac{|z-a|^2+|z+a|^2}{2}}=\sqrt{|z|^2+|a|^2}.$$
Consequently, $|z| \ge \sqrt{|c|^2-|a|^2}$. The minimum value is achieved when $z = \pm i a \sqrt{\frac{|c|^2}{|a|^2}-1}$.</p>

<p>For the maximum value of $|z|$, observe that $$2|c|=|z-a|+|z+a| \ge |z+a+z-a|=2|z|.$$
Consequently, $|z|\le|c|$, and the maximum value is achieved when $z = \pm a \frac{|c|}{|a|}$.</p>
"
"2396410","2396427","<p>Your answers are correct, but your proofs need some improvement. You jump too quickly to unjustified conclusions, which immediately invalidates the whole ""proof"".</p>

<p><strong>Injectivity.</strong> Up to $\color{blue}{(aâc)+\sqrt{11}(bâd)=0}$ everything was fine, but how do you know that $\color{red}{aâc=0}$ and $\color{red}{\sqrt{11}(bâd)=0}$ from there? Two numbers that add up to zero don't have to be zeroes themselves. Yes, it has a lot to do with $a,b,c,d$ being integers and $\sqrt{11}$ being, hmmm&hellip; a different kind of number. But you still have to justify each conclusion. Hint: go back to the blue equation and solve it for $\sqrt{11}$.</p>

<p><strong>Surjectivity.</strong> Same thing: how do you know that $\color{red}{f(a,b)\neq1,\;\forall a,b\in\mathbb{N}}$, i.e. that $\color{red}{a+b\sqrt{11}\neq1,\;\forall a,b\in\mathbb{N}}$? I'm not saying that it's false, because it's actually true. The question is: how do you know that? You have to prove this claim. You can use the same idea: solve for $\sqrt{11}$.</p>

<p>By the way, you can show that this map isn't surjective by a completely different line of reasoning: counting. How many elements are there in the domain $\mathbb{N}\times\mathbb{N}$?</p>
"
"2396414","2396423","<p>Yes, your answer is correct. But it can be written in a shorter form. Notice that this set consists precisely of all integer multiples of $5$, so
$$[0]=\{0,5,-5,10,-10,\ldots\}=\{5k\mid k\in\mathbb{Z}\}=5\mathbb{Z}.$$</p>
"
"2396430","2396438","<p>Even for varieties over a field, $Cl(G)$ may not be a group.  The problem is that $Cl(G\times G)$ is not the same as $Cl(G)\times Cl(G)$.  The group operation gives a map $Cl(G\times G)\to Cl(G)$, but not a map $Cl(G)\times Cl(G)\to Cl(G)$.</p>

<p>For instance, consider the additive group $\mathbb{G}_a=\mathbb{A}^1$ over $\mathbb{R}$.  The closed points $Cl(\mathbb{G}_a)$ are equivalence classes of complex numbers under conjugation, which do not have any natural group structure under addition.  In particular, if you take two points $p$ and $q$ with residue field $\mathbb{C}$, there is no canonical closed point of $Cl(\mathbb{G}_a\times\mathbb{G}_a)$ which corresponds to the ordered pair $(p,q)$, and so you cannot define a sum of $p$ and $q$.</p>

<p>(In fact, there are two different closed points of $\mathbb{G}_a\times\mathbb{G}_a$ whose projections are $p$ and $q$, coming from the two different maximal ideals of $\mathbb{C}\otimes_\mathbb{R}\mathbb{C}$.  The images of these closed points under the addition map $\mathbb{G}_a\times\mathbb{G}_a\to\mathbb{G}_a$ correspond to the two different ways to ""add"" the equivalence classes of complex numbers that $p$ and $q$ represent.  That is, if $p$ corresponds to the complex numbers $a\pm ib$ and $q$ corresponds to the complex numbers $c\pm id$, the two different possible sums are $(a+b)\pm i(c+d)$ and $(a+b)\pm i(c-d)$.)</p>
"
"2396432","2396442","<p>Assuming your logs are base $2$ we have 
$$\lg(2x-24)=2+ \frac 13 \lg8 - \frac 14 x \lg16\\
\lg(x-12)+1=2+1-x\\
lg(x-12)=2-x\\x-12=2^{2-x}\\
2^x(x-12)=4$$
Clearly $x$ has to be just barely greater than $12$, so let $x=12+y$  Then we have 
$$y=\frac 1{1024\cdot 2^y}$$
This needs a numeric solution and is in a good form as the right side will change slowly with $y$.  Let $y_0=0$ and iterate.  After two iterations we have converged to $0.000975902$.  <a href=""http://www.wolframalpha.com/input/?i=y%3D1%2F1024%2F2%5Ey"" rel=""nofollow noreferrer"">Alpha</a> will give you a solution of $y=\frac {W\left(\frac {\log 2}{1024}\right)}{\log 2}$in terms of the Lambert W function where these logs are natural logs.  </p>

<p>Added:  for base $10$ logs we can do the same.  Again $x$ has to be a little greater than $12$ so write $x=12+y$  The equation becomes $$y=\frac {25}{1024\cdot 2^y}$$ which converges to $y\approx 0.0240$</p>
"
"2396440","2396443","<p>We do not have to pick the last element. We can pick other element as well and the proof will still work.</p>

<p>If $j_n = n$, then it is not a derangement anymore. Hence $j_n = k$ where $k \neq n$.</p>

<p>We can change the proof.</p>

<p>You can pick a particular index $p \in \{ 1, \ldots, n\}$</p>

<blockquote>
  <p>For any derangement $(j_1, j_2, \ldots, j_n)$, we have $j_p \neq p$.  Let $j_p = k$, where $k \in \{1, 2, \ldots, n\}\setminus \{p\}$.  We now break the derangements on $n$ elements into two cases.</p>
</blockquote>

<p>Case $1$: $j_k = p$</p>

<p>Case $2$: $j_k \neq p$.</p>
"
"2396444","2396446","<p>Yes, it is Riemann integrable and 
$$\int_0^1 f(x) dx=\sum_{n=1}^{\infty}\frac{1}{n}\left(\frac{1}{n}-\frac{1}{n+1}\right)=\sum_{n=1}^{\infty}\frac{1}{n^2}-\sum_{n=1}^{\infty}\frac{1}{n(n+1)}=\frac{\pi^2}{6}-1.$$
where we used the <a href=""https://en.wikipedia.org/wiki/Basel_problem"" rel=""nofollow noreferrer"">Basel problem</a> and  <a href=""https://en.wikipedia.org/wiki/Telescoping_series"" rel=""nofollow noreferrer"">Mengoli's telescopic series</a>.</p>
"
"2396455","2396480","<p>We have for all $x,y$ and $\lambda\geq 0$, by the triangle (Minkowski) inequality:
$$|\!|\lambda x+(1-\lambda) y)|\!|_p^p\leq\left(\lambda|\!|x|\!|_p+(1-\lambda)|\!|y|\!|_p\right)^p$$
and the result follows from the strict convexity of the function $t\to t^p$ for $p&gt;1$ and $t\geq 0$.</p>
"
"2396457","2396512","<p>You can also solve this by using Lagrangian multipliers. You have the problem
$$
\underset{x,y}{\text{minimize}}\ \quad \quad x^2+3y^2-y\\
\text{subject to}\ \ x^2+2y^2-2\leq 0
$$
To put it in Lagrangian form, we simply restate the problem as
$$
L(x,y,\lambda) = x^2+3y^2-y + \lambda(x^2+2y^2-2)
$$
What we need to do now is to find all the <a href=""https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions"" rel=""nofollow noreferrer"">KKT points</a>. To do so, we solve the following problem
$$
\frac{\partial L}{\partial x}=0 \iff 2x+2\lambda x=0\quad (1)\\
\frac{\partial L}{\partial y}=0 \iff 6y-1+4\lambda y=0\quad (2)\\
\lambda(x^2+2y^2-2)=0 \quad (3)
$$
First, assume that $\lambda=0$, then we must have (from $(1)$ and $(2)$) $x=0,y=1/6$. This is our first KKT-point. To find the others, assume that $\lambda\neq 0$. Then from $(1)$ we get either $x=0$ or $\lambda=-1$ from $(2)$. This yields the points $(x,y)=(0,1)$, $(x,y)=(0,-1)$, and $(x,y) = (\frac{\pm\sqrt(3)}{2},\frac{1}{2})$. Now lets check which point is minimium/maximum:
$$
f(0,1/6) = -\frac{1}{12}\\
f(0,1) = 2\\
f(0,-1) = 4\\
f(\frac{\sqrt(3)}{2},\frac{1}{2})=1\\
f(\frac{-\sqrt(3)}{2},\frac{1}{2})=1
$$
Thus the minimum is $-\frac{1}{12}$ attained at $(x,y) = (0,\frac{1}{6})$ and the maximum is $4$ attained at $(x,y) = (0,-1)$.</p>

<p>Note: If you had more constraints (equality or inequality) you could use the same approach, however then you would have more Lagrange multipliers. (3) is called complementary slackness. </p>
"
"2396463","2396468","<p>To prove directly that $A$ is bounded, you have to show that if $x_n\to x$, then $Ax_n\to Ax$.  On the other hand, to prove that the graph of $A$ is closed, you only have to show that if $x_n\to x$ and $Ax_n\to y$, then $y=Ax$.  The difference is that in the latter case, you get to assume that $(Ax_n)$ does converge to <em>something</em> (namely $y$).  The proof you mentioned would not work without this assumption, because you cannot refer to $y$ at all if you don't yet know that the sequence $(Ax_n)$ converges.</p>
"
"2396464","2396533","<p>Consider the triangle $T\subset S^2$ formed by the three points $e^{i\alpha}$, $e^{i\beta}$, $e^{i\gamma}$. The assumptions say that the centroid of $T$ coincides with its circumcenter $0$. From this we may conclude that $T$ is in fact equilateral. This implies $\beta=\alpha+120^\circ$, $\gamma=\alpha+240^\circ$. It follows that $2\beta=2\alpha+240^\circ$, $2\gamma=2\alpha+480^\circ\sim2\alpha+120^\circ$. The three points $e^{2i\alpha}$, $e^{2i\beta}$, $e^{2i\gamma}$ therefore again form an equilateral triangle on $S^1$, so that the stated claim holds true.</p>

<p>Now the proof using complex numbers: It is assumed that
$z:=e^{i\alpha}+e^{i\beta}+e^{i\gamma}=0$. It follows that
$$z^2=e^{2i\alpha}+e^{2i\beta}+e^{2i\gamma}+2\bigl(e^{i(\alpha+\beta)}+e^{i(\beta+\gamma)}+e^{i(\gamma+\alpha)}\bigr)=0\tag{1}$$
as well. But
$$e^{i(\alpha+\beta)}+e^{i(\beta+\gamma)}+e^{i(\gamma+\alpha)}=e^{i(\alpha+\beta+\gamma)}\bigl(e^{-i\alpha}+e^{-i\beta}+e^{-i\gamma}\bigr)=e^{i(\alpha+\beta+\gamma)}\bar z=0\ ,$$
so that $(1)$ implies $e^{2i\alpha}+e^{2i\beta}+e^{2i\gamma}=0$.</p>
"
"2396466","2396470","<p>Your solution is not correct.  You have not ruled out the possibility that $g(0)=0$ but $g(x)\neq x$ for values of $x$ different from $0$.  So it is not correct to conclude that $g(x)=x$ in the case $g(0)=0$.</p>
"
"2396476","2396535","<p>Since we are in the case $x&gt;0$, $\underline{nx&gt;0}$ and there exists $m\in \Bbb{N}$ such that $$mâ1 \leq \underline{nx} &lt;m$$(The proof that such an $m$ exists uses the well-ordering property of $\Bbb{N}$) </p>

<p>Then,$$ny&gt; \underline{1+nx \geq m}$$Thus $$nx&lt;m&lt;ny$$ It then follows that the rational number $r=\frac{m}{n}$
satisfies $x&lt;r&lt;y$
.</p>
"
"2396481","2396520","<blockquote>
  <p><strong>Definition.</strong> Let $\{x_n\}_{n=1}^\infty$ be a sequence of real numbers. We say that the sequence $\{x_n\}_{n=1}^\infty$ converges to $x\in\mathbb{R}$ and write $x_n\to x$ as $n\to\infty$ if for every $\epsilon&gt;0$ there exists a corresponding $N\in\mathbb{N}$ such that for all $n$
  $$
n\geq N\Rightarrow |x_n-x|&lt;\epsilon
$$</p>
  
  <p><strong>Problem.</strong> You are asked to show that
  $$
\lim_{n\to\infty}x_n=x\Longleftrightarrow \lim_{n\to\infty}|x_n-x|=0
$$</p>
</blockquote>

<p>First suppose that $\lim_{n\to\infty}x_n=x$. Then for every $\epsilon&gt;0$ there exists a corresponding $N\in\mathbb{N}$ such that for all $n$
$$
n\geq N\Rightarrow |x_n-x|&lt;\epsilon
$$
The result follows by noting that
$$
|x_n-x|=||x_n-x|-0|
$$</p>

<p>Now suppose that $\lim_{n\to\infty}|x_n-x|=0$. Then for every $\epsilon&gt;0$ there exists a corresponding $N\in\mathbb{N}$ such that for all $n$
$$
n\geq N\Rightarrow ||x_n-x|-0|&lt;\epsilon
$$
Again the result follows by noting that $||x_n-x|-0|=|x_n-x|$.</p>
"
"2396482","2396493","<p>You have  $N $ choices each of $N $ different times.  The choices are <em>independent</em>, so you multiply the number of possibilities, to get</p>

<blockquote class=""spoiler"">
  <p>  $N^N $.</p>
</blockquote>
"
"2396483","2396489","<p>It $3p+2q=50$, then $q=25-\frac32 p$ meaning that $$10pq=10p(25-\frac32 p) = 250p - 15p^2$$</p>

<p>Now all you need to do is to find the maximum of a single-variable quadratic function, something a high school student should be capable of.</p>

<hr>

<p>Remember, if you know the zeroes of a quadratic function, then the maximum or minimum lies directly between the two. The zeroes of $$10p(25-\frac32 p) = 5p(50-3p)$$</p>

<p>are $p=0$ and $p=\frac{50}{3}$.</p>
"
"2396486","2396492","<p>Your logic looks fine to me. There are a few shortcuts you could take, though. Note, for instance, that $x \in A^c \cup B^c$ and $x \notin A\implies x\notin A$, which allows you to skip a few steps.</p>
"
"2396501","2396554","<p>It's just a clever definition where the key appears to be taking a <em>small</em> number of points as being dense. For one thing a larger number of points has an easier time being dense...  I guess one might try replacing countable with finite,  and look at a smaller class of spaces.  But to make things more interesting,  countable was used in the definition.  Of course countable is the smallest level of infinity.   A countable set can be put in a list.  In the case of a separable topological space, I guess you could almost think of the points being partitioned, if you will, or <em>separated</em> by the subset. </p>

<p>If you went in the other direction and said, instead of countable,  $\aleph_3 $, or $\aleph_{1000}$, or higher cardinality yet, you would have something a little more obscure and hard to get a feel for...  though still interesting perhaps. ..</p>

<p>Of course,  it's a <em>definition</em>...  When you define something,  you can be creative and define <em>whatever you want</em>, quite apart from whether such things even exist,  or proving theorems about them...</p>
"
"2396507","2396572","<p>a) </p>

<ul>
<li>with $a_n=1$, $\dfrac{a_n}{n+a_n}=\dfrac{1}{n+1}$ and the series diverges</li>
<li>with $a_n=\frac 1n$, $\dfrac{a_n}{n+a_n}=\dfrac{1}{n^2+1}$ and the series converges</li>
</ul>

<p>b) The series $\displaystyle \sum_{n} \dfrac{a_n}{\dfrac{1}{n} + a_n}$ always diverges. Indeed, if it were convergent, for $n$ larger than some $N$ we would have $ \displaystyle \frac{a_n}{\frac{1}{n} + a_n} \leq \frac 12 \iff a_n \leq \frac 1n$, and then $\displaystyle \frac{a_n}{\frac{1}{n} + a_n}\geq \frac{na_n}2 \geq \frac{a_n}2$.</p>

<p>But since $\sum_n a_n$ diverges, then $\displaystyle \sum_n\frac{a_n}{\frac{1}{n} + a_n}$ would diverge too, a contradiction.</p>
"
"2396531","2396534","<p>Let's say the shorter side of the house has length $x$ (on the <strong>outside</strong>). This means the longer side has length $1.6\cdot x$.</p>

<p>The inner rectangle, therefore, has sides of length $x-0.8$ and $1.6x - 0.8$, so the equation you want to solve is</p>

<p>$$(x-0.8)\cdot (1.6\cdot x - 0.8) = 100$$</p>

<p>This is a regular quadratic equation that can easily be solved using the standard technicque:</p>

<ul>
<li>Multiply out the left</li>
<li>Bring everything to the left side</li>
<li>Change the equation to the form $ax^2+bx+c=0$</li>
<li>Remember that if $ax^2+bx+c=0$, then $x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}$</li>
</ul>
"
"2396541","2396546","<p>Since $\eta(x) = f(x)-f(a) + L(x-a)$, you have, if $f$ is continuous,</p>

<p>$$\lim_{x\to a} \eta(x) = \lim_{x\to a} (f(x)-f(a) + L(x-a)) = f(a)-f(a)+L(a-a)=0$$</p>

<p>which means that just demanding that $\eta$ vanishes would mean that <strong>any</strong> continuous function is differential at all points, and all real numbers $L$ are its derivative.</p>

<hr>

<p>Adding the additional demand that $\frac{\eta}{x-a}\to 0$ insures that</p>

<ol>
<li>$L$ might not always exist, so that differentiable functions are ""more"" than just continuous</li>
<li>$L$ is always unique, and it can thus be called the <em>derivative</em> or <em>differential</em> of the function.</li>
</ol>
"
"2396550","2396638","<p>Let's expand as you did:</p>

<p>$$(1-2xt+x^2)\sum_{n=0}^{+\infty} nP_n(t)x^{n-1} + (x-t)\sum_{n=0}^{N} P_n(t)x^n =0 $$</p>

<p>$$\Leftrightarrow \sum_{n=0}^{+\infty} nP_n(t)x^{n-1} -2 \sum_{n=0}^{+\infty} ntP_n(t)x^n+\sum_{n=0}^{+\infty} nP_n(t)x^{n+1}+\sum_{n=0}^{+\infty} P_n(t)x^{n+1}-\sum_{n=0}^{+\infty} \color{red}{t}P_n(t)x^n =0$$ </p>

<p>Working with each of these sums gives us :</p>

<p>First sum: $$\sum_{n=0}^{+\infty} nP_n(t)x^{n-1} = \sum_{n=1}^{+\infty} nP_n(t)x^{n-1}= \sum_{n=0}^{+\infty} (n+1)P_{n+1}(t)x^n= \sum_{n=1}^{+\infty} (n+1)P_{n+1}(t)x^n + P_1(t)$$</p>

<p>Third sum: $$\sum_{n=0}^{+\infty} nP_n(t)x^{n+1} = \sum_{n=1}^{+\infty} nP_n(t)x^{n+1} = \sum_{n=2}^{+\infty} (n-1)P_{n-1}(t)x^n = \sum_{n=1}^{+\infty} (n-1)P_{n-1}(t)x^n $$</p>

<p>Fourth sum: $$\sum_{n=0}^{+\infty} P_n(t)x^{n+1} = \sum_{n=1}^{+\infty} P_{n-1}(t)x^n$$</p>

<p>Fifth sum: $$ \sum_{n=0}^{+\infty} \color{red}{t}P_n(t)x^n = \sum_{n=1}^{+\infty} \color{red}{t}P_n(t)x^n + tP_0(t)$$</p>

<p>Now, if you rearrange the 5 sums, you would get</p>

<p>$$ \sum_{n=0}^{+\infty} nP_n(t)x^{n-1} -2 \sum_{n=0}^{+\infty} ntP_n(t)x^n+\sum_{n=0}^{+\infty} nP_n(t)x^{n+1}+\sum_{n=0}^{+\infty} P_n(t)x^{n+1}-\sum_{n=0}^{+\infty} \color{red}{t}P_n(t)x^n =0$$</p>

<p>$$\Leftrightarrow \sum_{n=1}^{+\infty} (n+1)P_{n+1}(t)x^n + P_1(t) -2 \sum_{n=0}^{+\infty} ntP_n(t)x^n+\sum_{n=1}^{+\infty} (n-1)P_{n-1}(t)x^n+\sum_{n=1}^{+\infty} P_{n-1}(t)x^n-\sum_{n=1}^{+\infty} \color{red}{t}P_n(t)x^n + tP_0(t)=0$$</p>

<p>$$\Leftrightarrow P_1(t)-tP_0(t)+\sum_{n=1}^{+\infty}((n+1)P_{n+1}(t)-2ntP_n(t)+(n-1)P_{n-1}(t)+P_{n-1}-tP_n(t))x^n =0$$</p>

<p>$$ \Leftrightarrow P_1(t)-tP_0(t)+\sum_{n=1}^{+\infty}((n+1)P_{n+1}(t)-(2n+1)tP_n(t)+nP_{n-1}(t))x^n =0$$</p>
"
"2396558","2396578","<p>Your reasoning is correct.</p>

<hr>

<p>Alternative route: </p>

<p>$f$ is continuous and $\overline{f(A)}$ is closed, so we conclude that $f^{-1}\left(\overline{f(A)}\right)$ is closed. </p>

<p>This with: $$A\subseteq f^{-1}\left(f(A)\right)\subseteq f^{-1}\left(\overline{f(A)}\right)$$allowing us to conclude:$$\overline{A}\subseteq f^{-1}\left(\overline{f(A)}\right)$$
or equivalently:
$$f(\overline{A})\subseteq\overline{f(A)}$$</p>
"
"2396560","2396579","<p><strong>Hint</strong>. One may integrate by parts,
$$
\begin{align}
I_{n,a}&amp;=\int_{-\infty}^\infty\frac{dx}{(1+x^2/a)^{n}}\qquad (n\ge1)
\\\\&amp;=\int_{-\infty}^\infty 1 \cdot\frac{1}{(1+x^2/a)^{n}}\:dx
\\\\&amp;=\left[ x\cdot  \frac{1}{(1+x^2/a)^{n}}\right]_{-\infty}^\infty -\int_{-\infty}^\infty  x\cdot  \frac{-n\cdot\frac{2x}{a}}{(1+x^2/a)^{n+1}}\; dx
\\\\&amp;=\color{red}{0}+2n\int_{-\infty}^\infty  \frac{\frac{x^2}{a}}{(1+x^2/a)^{n+1}}\; dx 
\\\\&amp;=2n\int_{-\infty}^\infty  \frac{1+\frac{x^2}{a}-1}{(1+x^2/a)^{n+1}}\; dx 
\\\\&amp;=2n\int_{-\infty}^\infty  \frac{1}{(1+x^2/a)^{n}}\; dx-2n\int_{-\infty}^\infty  \frac{1}{(1+x^2/a)^{n+1}}\; dx 
\\\\&amp;=2nI_{n,a}-2n I_{n+1,a}
\end{align}
$$ giving $(a)$.</p>

<p>One may apply the <a href=""https://en.wikipedia.org/wiki/Dominated_convergence_theorem"" rel=""nofollow noreferrer"">dominated convergence theorem</a> to obtain $(b)$.</p>
"
"2396561","2396604","<p>Your complementary solution is correct. A more efficient method to do this is to recognize the left hand size as a homogeneous <a href=""https://en.wikipedia.org/wiki/Cauchy%E2%80%93Euler_equation"" rel=""nofollow noreferrer"">Cauchy-Euler equation</a> and use the ansatz $y=x^{\lambda}$. This will yield a quadratic:
$$x^2y''+xy'-y=0$$
$$\lambda (\lambda-1)+\lambda-1=0$$
$$\lambda^2-1=0$$
Therefore, $\lambda_1=1$ and $\lambda_2=-1$, thus the fundamental set of solutions are $\{x,\frac{1}{x}\}$, which is as you have shown.</p>

<hr>

<p>We can find a particular solution via <a href=""http://tutorial.math.lamar.edu/Classes/DE/VariationofParameters.aspx"" rel=""nofollow noreferrer"">Variation of Parameters</a>. Note that for a differential equation $y''+p(x)y'+q(x)y=f(x)$, we must solve the system:
$$\begin{cases} v_1'y_1+v_2'y_2=0 \\ v_1' y_1'+v_2' y_2'=f(x) \end{cases}$$
Where $y_1$ and $y_2$ are the two fundamental solutions to $y''+p(x)y'+q(x)y=0$. Hence, in your case, we have:
$$\begin{cases} v_1'\cdot x+v_2'\cdot \frac{1}{x}=0 \\ v_1'-v_2'\cdot \frac{1}{x^2}=\frac{1}{2+x} \end{cases}$$
Solving the above system by elimination, we see that the problem boils down to evaluating the following 2 integrals:
$$v_1=\int \frac{1}{2(x+2)}~dx\tag{1.1}$$
$$v_2=-\int \frac{x^2}{2(x+2)}~dx \tag{1.2}$$
Recall that Variation of Parameters assumes that the particular solution will be given in the form of $(2)$, therefore all you need to do is substitute the results from $(1.1)$ and $(1.2)$ you obtain.
$$y_p=v_1 y_1+v_2 y_2 \tag{2}$$</p>
"
"2396570","2396584","<p>Your counterexample is not correct, as it has maximum degree $3$. Hint: think about disconnected graphs.</p>

<p>The problem with the proof is not the base case, that is fine (you are doing induction on $n$, not $k$, so $k$ is fixed and could be much more than the maximum degree for small cases). The issue is that when they remove a vertex $v$, they say each of its neighbours will now have degree less than $k$. What if it didn't have any neighbours?</p>
"
"2396581","2396585","<p>For $r = 1$, we have $r^j = 1$, and thus we just add $a$ $(n+1)$ times (from $j=0$ to $j=n$).</p>
"
"2396582","2396658","<p>For a rigorous proof lets recast our problem a bit. Assume $f:[-1,1]\to \Bbb R$ is differentiable, $f(0)=0$ but for $x&gt;0$ you have $f(x)&gt;0$. We want to show that there is a contradiction if there exists a continuous $g:\Bbb R\to \Bbb R$ so that $f'(x)=f(x)g(f(x)+x)$. If you want I can explain how solving this problem is the same as solving the above one.</p>

<p>Since $f(0)=0$ we can find a differentiable $h$ so that $f(x)=xh(x)$ (this is sometimes called the Morse Lemma, specifically $h(x)=\int_0^1 f'(tx)\,dt$). It follows that
$$f'(x)=h(x)+xh'(x)\overset!= f(x)g(f(x)+x)=xh(x)g(x(h(x)+1))$$
for $x&gt;0$ divide by $xh(x)$ (because $f$ is non-zero here, so too must $h$) to get:
$$g(x h(x)+x)=\frac1x+\frac{h'(x)}{h(x)}\qquad\text{whenever }x&gt;0$$
Now taking the limit $x\to0^+$ gives $g(0)$ on the left, so it should also do so on the right. First note that $\frac{h'(x)}{h(x)}=\partial_x\ln(h(x))$ and if the term on the right is to have a limit you must have that
$$\partial_x \ln(h(x)) = -\frac1x + c(x)$$
where $C(x)$ is some continuous function. So
$$\ln(h(x)) = \ln(1/x) + C(x)$$
where $C(x)$ is a continuous anti-derivative of $c$. It follows by taking exponentials (remember $h(x)$ is positive) that
$$h(x)=\frac{e^{C(x)}}x$$
for $x&gt;0$. This is a contradiction to $h(x)$ being <em>continuous</em> at $0$, let alone differentiable.</p>
"
"2396587","2397455","<p>As is often the case with ""at least one"" events, it seems easier to work with the events' complements.  We use a generating function approach, with a computer algebra system to ease the computations.</p>

<p>$A^c$ is the event that all the rows contain at least one zero.  The generating function for the number of ways to place $r$ zeroes in a 16 by 3 matrix with at least one zero in each row is
$$f(x) = \left( \binom{3}{1} x + \binom{3}{2} x^2 + \binom{3}{3}x^3 \right)^{16}$$
The coefficient of $x^{24}$, which is the number of ways to place $24$ zeroes in the matrix with at leaast one zero in each row, is
$[x^{24}]f = 2,348,209,259,370$,
so 
$$\Pr(B^c) =\Pr(A^c) = [x^{24}]f \; / \; \binom{48}{24} \approx 0.0728181$$
and
$$\Pr(B) = \Pr(A) \approx \boxed{0.927182}$$</p>

<p>Continuing, $A^c \cap B^c$ is the event that each row contains at least one zero and at least one one, or equivalently, each row contains one or two zeroes.  This time the generating function is
$$g(x) = \left( \binom{3}{1} x + \binom{3}{2} x^2 \right)^{16}$$
and the coefficient of $x^{24}$, which is the number of ways to place $24$ zeroes in the matrix with one or two zeroes in each row, is $[x^{24}]g = 554,011,299,270$, so
$$\Pr(A^c \cap B^c) = [x^{24}]g \; / \;\binom{48}{24} \approx 0.0171799$$
Therefore
$$\begin{align}
\Pr(A \cap B) &amp;= 1-\Pr((A \cap B)^c) \\
&amp;= 1 - \Pr(A^c \cup B^c) \\
&amp;= 1-[\Pr(A^c) + \Pr(B^c) - \Pr(A^c \cap B^c)] \\
&amp;\approx \boxed{0.871544}
\end{align}$$</p>
"
"2396589","2397558","<p>An interesting question in game theory would be the ever-elusive definition of what it means for an agent/person/player to be 'rational.'  </p>

<p>To motivate, imagine a world with one person in it, trying to do his or her best in the face of outside forces.  Within such a context, it is not unreasonable to suppose that the person either knows the true probabilities with which events may happen, or at the least has some subjective beliefs about this.  In such a world then, given these beliefs about what events they are facing, it makes sense to deem a person rational if they act in such a way as to maximize their expected utility (where the expectation is taken with regard to their beliefs/the true probabilities over events).  </p>

<p>Consider now game theory, where multiple (even just two) equally clever people are matched up against each other.  Even in the simplest possible context when the payoffs/structure of the game and the rationality of both players is common knowledge (either formally or informally construed), what it means to be rational is much more difficult and elusive thing, and indeed arguments over its definition have at least implicitly been the driving force behind the small cottage industry of refinements to the basic Nash equilibrium.</p>

<p>If you already have a background in basic game theory, depending upon your mathematical level you might find it interesting to discuss the various motivations for Nash equilibrium versus sub-game perfect Nash equilibrium, and perhaps, depending upon your mathematical background, things such as trembling-hand perfect equilibrium or sequential equilibrium.</p>
"
"2396590","2396601","<p>The following <a href=""https://mathoverflow.net/questions/259732/when-the-automorphism-group-of-an-object-determines-the-object/259742#259742"">MO thread</a> deals with the question: </p>

<blockquote>
  <p>when does the automorphism group of an object determine the object?</p>
</blockquote>
"
"2396596","2396607","<p>Yes. The maximum of a convex function on a convex set, if exists, is always attained at an extreme point of the feasible set. In your case, your feasible set is the triangle whose vertices (extreme points) are $A = (0, 0)$, $B = (\tfrac{B}{\beta_1}, 0)$, and  $C = (0, \tfrac{B}{\beta_2})$. It is compact, so the maximum indeed exists.</p>

<p>The point $A$ clearly does not achieve the maximum objective value, and therefore either $B$ or $C$ must be a maximizer. You chose among the points by comparing the objective function value these two points.</p>
"
"2396598","2396683","<p>Fix $(a,b,c,d)\in SL(2,\mathbb{R})$. w.l.o.g. say $a\not=0$. Then since $\partial_d(ad-bc)=a\not=0$ we have by the implicit function theorem a $C^\infty$ map $\mathbb{R}^3\supset U\ni(x,y,z)\stackrel{f}{\to} \mathbb{R}$ centered at $(a,b,c)$, such that $(x,y,z,f(x,y,z))\in SL(2,\mathbb{R})$. In other words, $f$ is a chart for $SL(2,\mathbb{R})$ around $(a,b,c,d)$. Taking some open $V$ around $q\in \mathbb{H}$ we see that the action takes on the form:
$$(x,y,z)\times s\mapsto \frac{xs+y}{zs+f(x,y,z)}$$
and this is clearly a smooth map (is a composition of smooth maps: complex multiplication, addition, division away from zero, $f$).</p>
"
"2396605","2396634","<p>$$I=\int_0^1 \sqrt[3]{x\log{\frac{1}x}} \, \mathrm dx$$</p>

<p>set $ u =-\ln x =\ln\frac{1}{x}$ then $dx = -e^{-u} du$ and 
$$I=\int_0^1 \sqrt[3]{x\log{\frac{1}x}} \, \mathrm dx =\int_0^\infty e^{-u/3}e^{-u} u^{1/3}  du = \int_0^\infty e^{-4/3u} u^{1/3}du.$$ then set $v= 4/3u $
 so that 
$$I=3/4\int_0^\infty e^{-v} (3/4v)^{1/3}dv  = {\left(\frac{3}{4}\right)}^{4/3}\int_0^\infty e^{-v} v^{1/3}dv ={\left(\frac{3}{4}\right)}^{4/3}\Gamma(\frac{4}{3}) = {\left(\frac{3}{4}\right)}^{4/3}\frac{1}{3}\Gamma(\frac{1}{3}).$$</p>
"
"2396609","2396622","<p>A partial answer would be </p>

<p>$$P(x)=(x-1)(x+2)Q(x),$$ for some polynomial $Q$ that satisfies $$Q(x)=Q(x-3)+3.$$  First, note that $P(1)=P(-2)=0,$ by evaluating in $x=3,-3.$ So there must exists a polynomial $Q(x)$ such that   $$P(x)=(x-1)(x+2)Q(x).$$ Putting this into the equality, we find that $Q$ must satisfy</p>

<p>$$Q(x+1)=Q(x-2)+3.$$ After changing $x+1 $ for $x$ we find </p>

<p>$$Q(x)=Q(x-3)+3.$$  It is easy to verify that $Q(x)=x-a$ is valid for each real $a.$ Not sure if those are the only ones.</p>

<p>$\textbf{EDIT:}$ LeGrandDODOM(see below) already completed this answer where I left it.</p>
"
"2396618","2396627","<p>No, your expansion is not a power series centered at $-2$. 
Let $w=z+2$ then for $|3w/5|&lt;1$, or $|z+2|&lt;5/3$,
\begin{align*}\frac{1}{3z+1}&amp;=\frac{1}{3(w-2)+1}=\frac{1}{3w-5}=\frac{-1/5}{1-3w/5}=
-\frac{1}{5}\sum_{n=0}^{\infty}\left(\frac{3w}{5}\right)^n\\
&amp;=-\frac{1}{5}\sum_{n=0}^{\infty}\left(\frac{3(z+2)}{5}\right)^n.
\end{align*}</p>
"
"2396619","2396731","<p>The need of introducing branch cuts usually arises from points which causes some trouble. That's the case with the point $0$ when you try to define $\log(z)$ or $\sqrt z$. By eliminating a ray whose origin is that point, we get a simply connected set. And in a simply connected set, all sorts of good things happen. For instance, every holomorphic functions has a primitive and every holomorphic functions without zeros has a holomorphic logarithm.</p>

<p>Therefore, the choice of a ray is just because that's the simplest option. You could define a holomorphic function in $\mathbb{C}\setminus\bigl\{x+i\sin x\,|\,x\in(-\infty,0]\bigr\}$.</p>
"
"2396640","2396689","<p>For every $\color{Red}{m \notin \{ \pm 1, \pm 3 \}} $ , 
we will construct a 
$\color{Red}{\text{counter-example}}$! </p>

<p><strong>Remark(I)</strong>: 
Let  $\color{Red}{m \neq \pm 3}$, 
and  $\color{Red}{m \neq \pm 1}$; 
in this case note that $m^2 \color{Red}{\nmid} 3m$. </p>

<hr>

<p>Let $a:=m+1$ and $b:=1$; 
then one can see easily that:</p>

<p>$$a^3-b^3=(m+1)^3-1^3=\color{Blue}{m^3+3m^2}+\color{Red}{3m}.$$</p>

<hr>

<p>Notice that the above $a$ and $b$ are 
$\color{Red}{\text{counter-example}}$. </p>

<p>[ 
<strong>Suppose on contrary</strong> that $a^3-b^3$ is divisible by $m^2$; 
on the otherhadn note that 
$\color{Blue}{\text{the first two terms}}$ are divisible by $m^2$, 
so $\color{Red}{\text{the third term}}$ must divisible by $m^2$, 
which has an obvious contradiction with remark(I). 
] </p>

<hr>

<hr>

<hr>

<hr>

<p>Note that your assertion is $\color{Green}{\text{true}}$ when 
$\color{Green}{m \in \{ \pm 1, \pm 3 \}} $. </p>

<p><strong>Remark</strong>: 
Let $m \in \{ \pm 1, \pm 3 \} $; 
in this case 
for every $b,k \in \mathbb{Z}$ 
we have: 
$m^2 \color{Green}{\mid} 3mkb^2.$ </p>

<hr>

<p>Notice that $a \overset{m}{\equiv} b$ 
implies that there is an integer $k$, 
such that $a=b+mk$, 
as you noted, therefor we have: </p>

<p>$$a^3-b^3=\color{Blue}{m^3k^3}+\color{Blue}{3m^2k^2b}+\color{Green}{3mkb^2} ,$$ </p>

<p>$\color{Blue}{\text{the the first two terms}}$ are divisible by $m^2$, </p>

<p>and by the remark(II) 
$\color{Green}{\text{the third term}}$ 
is again divisible by $m^2$, 
so we are done! </p>
"
"2396641","2396703","<p><strong>CIT= Cauchy Integral's Theorem</strong> stand for holomorphic function (without polar or essential singularity) on a domain $\Omega \subset \mathbb C$. Therefore  for any curve $\gamma \subset\subset \Omega $ such that a function $f$ is holomorphic on $\Omega$. we have 
$$\oint_{\gamma} f(z)dz = 0$$. 
Which is the case 
 for the function $$ f : D(0,1+\epsilon)\to \mathbb C, z\mapsto \frac{e^{z^2+\sin(z)}}{4(z-2)^2e^{\cos(z)}} $$. 
$0&lt;\epsilon &lt;&lt;1$ is small enough. and $\gamma = \mathcal{C}(0,1) =\partial D(0,1)$.
 $f$  has no singularities in $\Omega = D(0,1+\epsilon)$</p>

<p>But <strong>CIF = Cauchy Integral's Formula</strong> applies for meromorphic functions  on $\Omega $ (means holomorphic except at some locally finite singularities.) Hence one needs to use Residues theorem. </p>

<p>let $a \in \overset{\circ}{\gamma}$ where $\gamma \subset \subset \Omega $ is curve in the domain omega. Let $f: \Omega \to \mathbb C$ be holomorphic then the function $$ g: z\mapsto \frac{f(z)}{(z-a)^{n+1}}$$ has only one singularity (pole) at $z= a$ The CIF infer that, </p>

<p>$$\oint_{\gamma} g(z)dz = 2\mathrm{i}\pi Res(g,a) = 2\mathrm{i}\pi f^{n}(a)/n! $$</p>

<p>that is 
$$ f^{n}(a)= \frac{n!}{2\mathrm{i}\pi}\oint_{\gamma}\frac{f(z)}{(z-a)^{n+1}}dz. $$
 Which is applicable to the second case where 
$$ \frac{\cos(z)}{z^2+4z+3}=\frac{\cos(z)}{(z-1)(z-3)} = \frac{f(z)}{z-1}$$ </p>

<p>with $f(z) = \frac{\cos(z)}{z-3}$ is holomorphic on $D(0,2 +\frac{1}{2})$ without any singularities therein. And $\gamma = \mathcal{C}(0,2)= \partial D(0,2)\subset \subset D(0,3/2).$ So in this case we have </p>

<p>$$ \oint_{\gamma} g\frac{\cos(z)}{z^2+4z+3}dz  = 2i\pi f(1) = -i\pi \cos(1)$$</p>
"
"2396643","2396667","<p>As far as I see It is enough that $T$ is normal. Because if $T$ is normal we know 
\begin{align}
\sup |\sigma(T)| = \|T\|
.
\end{align}
Therefore you get the inequality you need. Since from $$\|T\| \in \sigma(T)\subseteq \overline{N(T)}$$
we conclude
$$\|T\| \leq \sup_{\lVert w\rVert=1}|\langle Tw,w\rangle|$$</p>

<hr>

<h2>Details</h2>

<p>For you further question in the comments:
There is a theorem which says</p>

<blockquote>
  <p><strong>Theorem</strong> If $\mathcal{A}$ is a unital Banach-algebra and $A\in\mathcal{A}$ then $\sigma(A) \neq \emptyset$ and \begin{align}
 \sup_{\lambda \in \sigma(A)} |\lambda| = \max_{\lambda \in \sigma(A)}
 |\lambda| = \lim_{n\in\mathbb{N}} \|A^n\|^{\frac{1}{n}} =
 \inf_{n\in\mathbb{N}} \|A^n\|^{\frac{1}{n}}. \end{align}</p>
</blockquote>

<p>I found only a <a href=""https://en.wikipedia.org/wiki/Spectral_radius#Bounded_Linear_Operators"" rel=""nofollow noreferrer"">wikipedia reference</a> so far. I will also use $\|TT^*\| = \|T\|^2$.
However using these two facts and the fact that $T^2$ is also normal
$$
\|T^2\|^2 = \|(T^2)^*(T^2)\| = \|(T^*T)^2\| = \|(T^*T)^* (T^* T)\| = \|T^*T\|^2 = (\|T\|^2)^2
$$
By induction $\|T^{2^k}\|= \|T\|^{2^k}$ therefore
$$
\sup_{\lambda \in \sigma(T)} |\lambda|= \lim_{n\in\mathbb{N}} \|T^n\|^{\frac{1}{n}}
= \lim_{k\in\mathbb{N}} \|T^{2^k}\|^{\frac{1}{2^k}} = \|T\|
$$</p>
"
"2396644","2396721","<p>If $z$ is the triangle's side on the rotation axis and $h$ is the height of the triangle onto that side, then the center of mass of the triangle is at a distance $$r=\frac 13h$$ from the axis of rotation.</p>

<p>Then the length (cricumference) of the circle drawn by the center of mass during the rotation is $$L=2\pi r = \frac 23\pi h$$
and the triangle's area is $$A=\frac 12 zh$$
so, by the Pappus theorem, the solid's volume is
$$V = A\cdot L = \frac 12\cdot \frac 23 zh\pi h = \frac 13 \pi h^2 z$$</p>
"
"2396649","2396946","<p>$\bullet$ Just use the chain rule for smooth functions $f: \mathbb{R}^n \to \mathbb{R}^1$ i.e,</p>

<p>\begin{align*} \frac{d \phi_t}{d t}\Bigr|_{t = 0} &amp;= \nabla \phi_0 \cdot \gamma'(0) \\ \\ &amp;= \begin{pmatrix} \frac{\partial \phi_t}{\partial r}\Bigr|_{t=0} &amp; \frac{\partial \phi_t}{\partial \theta}\Bigr|_{t = 0}  \end{pmatrix} \begin{pmatrix} \frac{\partial x}{\partial t}\Bigr|_{t=0} \\ \frac{\partial y}{\partial t}\Bigr|_{t=0} \end{pmatrix}\end{align*}</p>

<p>where,</p>

<p>\begin{cases}
 \gamma(t) = (r \cos ( \theta + t), r \sin (\theta + t))\\
 \\
\phi_t(r, \theta) = (x(r, \theta,t), y(r, \theta,t)) = (x,y)\end{cases}</p>

<p>$\bullet$ For the relationship between the derivatives , first recall that $\phi_{m+n} = \phi_{m} \circ \phi_n = \phi_n \circ \phi_m$.  Next we define the vector field,</p>

<p>$$\textbf{v}(x_0)=\frac{d}{dt}\Bigr|_{t = 0} \phi_t(x_0), x_0 \in \mathbb{R}^2$$</p>

<p>i.e  $\textbf{v}$ gives the initial rate of change of $x_0$ under the $1$-parameter subgroup. Now we just manipulate the leibniz notation,</p>

<p>$$ \frac{d \phi_t}{dt} \Bigr|_{t = s} = \frac{d (\phi_{\epsilon + s})}{d\epsilon} \Bigr|_{\epsilon = 0} = \frac{d}{d \epsilon} \Bigr|_{\epsilon = 0} (\phi_{\epsilon} \circ \phi_s) = \textbf{v}(\phi_s)$$</p>

<p>Hence, if you want to know how $\phi_s$ moves $p \in\mathbb{R}^2$, you just take the velocity field $\textbf{v}$, and translate it to the point $p$.</p>

<p>$\bullet$ Geometrically what is going on is that you are just taking a particle on a circle of radius $r$ centered at the origin and moving it along this circle by $t$ adding to the angle. </p>
"
"2396679","2396778","<p>They key is for $m \ge 0$, $f_{m+2}$ can be expressed as a single integral over $f_1$. More precisely,</p>

<p>$$f_{m+2}(x) = \int_0^x \frac{(x-y)^m}{m!} f_1(y) dy\tag{*1}$$</p>

<p>One can show this by induction. </p>

<p>The case $m = 0$ is trivial, it is essentially the definition of $f_2$. </p>

<p>Assume $(*1)$ is true for some $m$.  By definition of $f_{m+3}$ and induction assumption, we have</p>

<p>$$\begin{align}f_{m+3}(x) 
&amp;= \int_0^x f_{m+2}(y) dy
= \int_0^x \int_0^y \frac{(y-z)^m}{m!} f_1(z) dzdy\\
&amp;= \int_0^x \int_0^x \frac{(y-z)^m}{m!}\theta(y-z)f_1(z) dzdy
\end{align}
$$
where $\theta(t)$ is the <a href=""https://en.wikipedia.org/wiki/Heaviside_step_function"" rel=""nofollow noreferrer"">Heaviside step function</a>.</p>

<p>Notice $\displaystyle\;\frac{(y-z)^m}{m!}\theta(y-z)\;$ is $L^\infty$ on $[0,x]^2$. Since the product of a $L^\infty$ function with a $L^1$ function is $L^1$. 
Using <a href=""https://en.wikipedia.org/wiki/Fubini&#39;s_theorem"" rel=""nofollow noreferrer"">Fubini's theorem</a>, we can
exchange order of integration and get</p>

<p>$$f_{m+3}(x) = \int_0^x \int_0^x \frac{(y-z)^m}{m!}\theta(y-z)f_1(z) dydz
= \int_0^x \frac{(x-z)^{m+1}}{(m+1)!}f_1(z)dz
$$
So $(*1)$ is also true for $m+1$. By principle of induction, $(*1)$ is true for all $m \ge 0$.</p>

<p>As a result, we find</p>

<p>$$\sum_{n=1}^\infty f_n(x) = f_1(x) + \sum_{m=0}^\infty f_{m+2}(x)
= f_1(x) + \sum_{m=0}^\infty \int_0^x \frac{(x-z)^m}{m!}f_1(z) dz$$</p>

<p>Using <a href=""https://en.wikipedia.org/wiki/Dominated_convergence_theorem"" rel=""nofollow noreferrer"">DCT</a>
(which is essentially using Fubini in disguise ), one can exchange the order of summation and integration. At the end, we obtain:
$$\sum_{n=1}^\infty f_n(x)= f_1(x) + \int_0^x \left(\sum_{m=0}^\infty \frac{(x-z)^m}{m!}\right) f_1(z) dz
= f_1(x) + \int_0^x e^{x-z} f_1(z) dz
$$</p>
"
"2396680","2396766","<p>We know that $(q;q)_n=(1-q)^n[n]_q!\;$ and $\lim_{q\to1} [n]_q!=n!.\quad$ Therefore $$e_q((1-q)(1+q)x) = \sum_{n=0}^\infty \frac{(1-q)^n(1+q)^nx^n}{(q;q)_n}=\sum_{n=0}^\infty \frac{(1+q)^n x^n}{[n]_q!}.$$
The limit as $q\to1$ gives $e^{2x}$ which is justified because of absolute convergence.</p>
"
"2396690","2396706","<p>Let </p>

<p>$$A_k = \left\{ x \in \mathbb{R} \setminus \mathbb{Q} : f(x) \leqslant \frac{1}{k} \right\}.$$</p>

<p>Let $a \in \mathbb{Q}, k \in \mathbb{N}$. For each $b \in \mathbb{R} \setminus \mathbb{Q}$ we have $\displaystyle f(b) \leqslant \frac{|b-a|}{f(a)}$, so there is an open neighborhood $U_a^{(k)} \subseteq \mathbb{R}$ of $a$ such that $U_a^{(k)} \setminus \mathbb{Q} \subseteq A_k$, namely $U_a^{(k)} = (a-r, a+r)$ where $r = \frac{f(a)}{k}$.</p>

<p>The union $\displaystyle G_k = \bigcup_{a \in \mathbb{Q}} U_a^{(k)} \subseteq \mathbb{R}$ is a dense open set such that $G_k \setminus \mathbb{Q} \subseteq A_k$. Hence </p>

<p>$$\bigcap_{k=1}^{\infty} G_k \setminus \mathbb{Q} \subseteq \bigcap_{k=1}^{\infty} A_k,$$</p>

<p>therefore the set on the right is comeager in $\mathbb{R}$, so it must be non-empty. That is a contradiction. </p>

<p>P.S. Did you come up with this question studying the (lack of) $\mathrm{T}4$ property of the <a href=""https://en.wikipedia.org/wiki/Moore_plane"" rel=""nofollow noreferrer"">Moore plane</a>? </p>
"
"2396694","2396715","<p>If I got the problem right:
$$ V_{drilled \; sphere} = \int_{-\sqrt{r_2^2 - r_1^2}}^{\sqrt{r_2^2 - r_1^2}} \int_{0}^{2\pi} \int_{r_1}^{\sqrt{r_2^2 - z^2}} r \;\; dr \; d\theta \; dz = 2\pi \int_{-\sqrt{r_2^2 - r_1^2}}^{\sqrt{r_2^2 - r_1^2}} dz \int_{r_1}^{\sqrt{r_2^2 - z^2}} r \; dr = 2\pi \int_{-\sqrt{r_2^2 - r_1^2}}^{\sqrt{r_2^2 - r_1^2}} dz \; \frac{r^2}{2} \bigg\vert _{r = r_1}^{\sqrt{r_2^2 - z^2}} = \pi \int_{-\sqrt{r_2^2 - r_1^2}}^{\sqrt{r_2^2 - r_1^2}} dz \left( r_2^2 - r_1^2 - z^2 \right) = \pi \left[ \left( r_2^2 - r_1^2 \right) z \big\vert_{z=-\sqrt{r_2^2 - r_1^2}}^{\sqrt{r_2^2 - r_1^2}} - \frac{z^3}{3} \bigg\vert_{z=-\sqrt{r_2^2 - r_1^2}}^{\sqrt{r_2^2 - r_1^2}} \right] = \pi \frac{4 \left(r_2^2 - r_1^2 \right)^{3/2}}{3} $$</p>

<p>By the way, what is $h$ in your answer? And where are $r_1$ and $r_2$?</p>
"
"2396701","2396708","<p>The sets $\{-180+25n\,|\,n\in\mathbb{Z}\}$ and $\{-180-25n\,|\,n\in\mathbb{Z}\}$ are the same.</p>
"
"2396702","2396714","<p>Clearly, because $X$ is an open set, there exists some $\delta$ such that $B(\xi, \delta)\subset X$.</p>

<p>Now, take some point $t\in\mathbb R$ such that $|t-\xi_j|&lt;\delta$, and let's define $$\xi_t = (\xi_1,\dots, \xi_{j-1},t,\xi_{j+1}\dots, \xi_n).$$</p>

<p>Then, $$\|\xi - \xi_t\|=\sqrt{(\xi_1-\xi_1)^2 + (\xi_2-\xi_2^2)+\cdots+(\xi_{j-1} - \xi_{j-1})^2 + (\xi_j-t)^2 + (\xi_{j+1}-\xi_{j+1})^2+\cdots + (\xi_n-\xi_n)^2} \\=\sqrt{0+\cdots+0+(\xi_j-t)^2+0+\cdots+0} = |\xi_j-t| &lt; \delta$$</p>

<p>which means that, for every $t$ such that $|t-\xi_j|&lt;\delta$, you have $\|\xi-\xi_t\|&lt;\delta$ which means $\xi_t\in B(\xi, \delta)\subset X$.</p>

<hr>

<p>Now, this also means that since $\xi_t\in X$, that $t\in I_j(\xi)$. So, you know now that</p>

<blockquote>
  <p>For every $t$ such that $|t-\xi_j|&lt;\delta$, we have $t\in I_j(\xi)$</p>
</blockquote>

<p>or in other words,</p>

<blockquote>
  <p>$$(\xi_j-\delta, \xi_j+\delta)\subset I_j(\xi)$$</p>
</blockquote>

<p>or in other words,</p>

<blockquote>
  <p>$I_j(\xi)$ is a neighborhood of $\xi_j$</p>
</blockquote>
"
"2396705","2396719","<p>$x^6+3x^3+1-y^4=0$ is a quadratic equation.</p>

<p>Thus, there is an integer $n$ for which $$3^2-4(1-y^4)=n^2$$ or
$$n^2-4y^4=5$$ or
$$(n-2y^2)(n+2y^2)=5$$
and we have four cases only. </p>
"
"2396720","2396724","<p>He shows why it is important to be exact in math.<br>
It is $$\{n+3:nâN,0\leqslant n\leqslant 5\} = \{8-n:nâN,0\leqslant n\leqslant 5\},$$
but that does not imply 
$$n+3 = 8 - n,\qquad \text{ for } nââ, 0\leqslant n \leqslant 5.$$</p>

<p>The reason why this example works is that sets ignore the order of their elements.</p>

<p>So while $$\tilde{n}+3 \neq 8-\tilde{n},$$
for a specific $\tilde{n}ââ$, $0\leqslant\tilde{n}\leqslant5$, </p>

<p>the following is true:<br>
For every $nââ, 0\leqslant n\leqslant 5$ there exists a $mââ, 0\leqslant m\leqslant 5$, such that 
$$n+3 = 8-m.$$</p>

<hr>

<p>Edit: I really like this Example, because it illustrates the problem of abstraction that everyone faces in math - all the time. </p>

<p>In the first grade you abstract from ""1 apple + 2 apple = 3 apple"" to ""1+2=3"". 
In the first term you abstract e.g. from ""functions like $f(x)$"" to general mappings. </p>

<p>A very common question you hear in the first term, is: Lecturer writes  $$M=\{mââ | ...\} \\ \text{Let } nâM \quad ...$$
""Why is $nâM$, shouldn't it be $mâM$?""</p>

<p>This is one moment of uncountable moments one faces during their studies. Some might only take fragments of a second to overcome, some might take years until it makes click and you suddenly understand. </p>
"
"2396727","2396739","<p>Take $f_n=\frac1{n^{1/p}} \chi_{[n,2n]}$. Then in any compact set $\omega$ you have that $f_n\to 0$ in $L^p$ or $L^1$ but the sequence does not converge to zero in $L^p(\mathbb{R})$.</p>
"
"2396736","2396738","<p>I didn't check the computations but, yes, you are doing it right.</p>

<p>As for the other question, note that the expression $\frac{xe^x}{\sin x}$ is undefined if $x=0$. In order to make $f$ continuous at $0$; you must define $f(0)$ as $\lim_{x\to0}\frac{xe^x}{\sin x}$, which is equal to $1$.</p>
"
"2396741","2396745","<p>Let $L$ be set of odd numbers up to $n$ and $S$ set of even numbers up to $n$.</p>

<p>So $|L| = \lceil {n/2} \rceil$ and $|S| = \lfloor {n/2} \rfloor$.</p>

<p>Then you must take always $2j$ odd numbers from the set $L$, (thus you can do this on $\displaystyle {\lceil {n/2} \rceil \choose 2j}$ ways)</p>

<p>and the rest of them , that is $k-2j$ form the set $S$ (and you can do this on $\displaystyle {\lfloor {n/2} \rfloor \choose k-2j}$ ways). </p>

<p>So for fixed $j$ you can do this on $\displaystyle {\binom{ \lceil {n/2} \rceil}{2j}\binom{ \lfloor {n/2} \rfloor
}{k-2j}}$ ways.</p>

<p>You can do this for all $j\leq \displaystyle \lceil {n/4} \rceil$ thus the sum. </p>
"
"2396757","2396827","<p>Consider the two sets:</p>

<ol>
<li>the set $A(C)$ of cones with apex $C$ over the diagram of <em>continuous</em> finite quotients of $G$</li>
<li>the set $B(C)$ of cones with apex $C$ over the diagram of quotients of the form $G/G^n$</li>
</ol>

<p>Elements in $A(C)$ are cones factoring through $G$, and hence extend to map to each quotient $G/G^n$. And in the other direction, we can extend an element of $B(C)$ to a finite quotient $G/H$ using $C\to G/G^{|G/H|}\to G/H$. It is easy to see these maps are mutually inverse and natural in $C$.</p>

<p>But $A$ and $B$ are the hom functors for the limit objects, so by Yoneda's lemma the limits are isomorphic.</p>
"
"2396758","2396861","<p>Case $k=2$ is a so-called ""partial theta function"".  </p>

<p>Consider this Jacobi theta function:
$$
\vartheta_3(z,q) := \sum_{k=-\infty}^{\infty} q^{k^2} e^{i 2 k z} \;.
$$
Then
$$
p\,\vartheta_3\left(\frac{\log(1-p)}{2i}\, , \, e^{-1/a}\right) =
\sum_{k=-\infty}^\infty  p(1-p)^k e^{-k^2/a}\;.
$$
The corresponding ""partial theta function"" is then what you want:
$$
p\,\tilde\vartheta_3\left(\frac{\log(1-p)}{2i}\, , \, e^{-1/a}\right) =
\sum_{k=1}^\infty  p(1-p)^k e^{-k^2/a}\;.
$$
Unfortunately, partial theta functions do not have the nice properties of theta functions, and are almost never seen in the literature.</p>
"
"2396759","2396767","<p>The eigenvector for $-1$ you already have, call it $v_1$. For $0$ what you want is a vector $v_4$ such that $A^3v_4 = 0$ but $A^2v_4 \ne 0$. I'll let you think of how to do this. (Incidentally all three vectors you have for a basis of $N(A^3)$ work.)</p>

<p>Once you have $v_4$, let $v_3 = Av_4$ and $v_2 = A^2v_4 = Av_3$. Then take</p>

<p>$$ S = (v_1, v_2, v_3, v_4). $$</p>
"
"2396772","2396787","<p>Expanding along the final column results in</p>

<p>$$a_n\cdot(-1)^{n+2}
\begin{vmatrix}
-y_1 &amp; * &amp; *&amp;*&amp;* \\
0&amp; -y_2&amp; *&amp;*&amp;*\\
0&amp; 0 &amp; -y_3&amp;*&amp;*\\
\vdots&amp;\vdots&amp;\vdots &amp; \ddots&amp;*\\
0&amp;0&amp;0&amp;\cdots &amp;-y_n
\end{vmatrix} + x_n\cdot \begin{vmatrix}
a_0 &amp; a_1 &amp; a_2 &amp; \dots &amp; a_{n-1} \\
-y_1 &amp; x_1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; -y_2 &amp; x_2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \dots &amp; x_{n-1}
\end{vmatrix}$$</p>

<p>The first determinant is easy to calculate (it is $(-1)^n\cdot y_1\cdot y_2\cdots y_n$), while the second one is similar to the first, only smaller. So, if we define the determinant you wanted to calculate as $D_n$, you have</p>

<p>$$D_{n} = a_n\cdot y_1\cdot y_2\cdots y_n + x_nD_{n-1}$$</p>

<p>Now expanding that $D_{n-1}$ part further can yield some sort of solution (I don't see it being particularly pretty, however...)</p>

<hr>

<p><strong>Edit</strong>:</p>

<p>If I am not mistaken, the final result is</p>

<p>$$a_0x_1x_2\cdots x_n + a_1y_1x_2x_3\cdots x_n + \cdots + a_iy_1y_2\cdots y_i x_{i+1}\cdots x_n + \cdots + a_ny_1y_2\cdots y_n$$</p>

<p>or, written without all the dots:</p>

<p>$$\sum_{k=0}^n \left(a_k\prod_{i=1}^{k} y_i\prod_{i=k+1}^n x_i\right).$$</p>

<p>I don't see any obvious way to simplify this, however.</p>
"
"2396779","2396803","<p>You already have
$$\cos^2(a)\cos^2(b)-\cos^2(a)\sin^2(b)+\sin^2(a)\cos^2(b)+\sin^2(a)\sin(b)\cos(b),$$
which is pretty close, but not entirely correct. So first of all, go back to the $3\times3$ determinant formula and recheck and correct your calculation.</p>

<p>In the expression above, the first and third terms are correct. And you can simplify them by factoring out the common factor and using a trig identity:
$$\cos^2(a)\cos^2(b)+\sin^2(a)\cos^2(b)=[\cos^2(a)+\sin^2(a)]\cos^2(b)=1\cdot\cos^2(b)=\cos^2(b).$$
After you correct the other two terms, you'll be able to simplify further in a similar fashion.</p>
"
"2396780","2396798","<p>Singularities of </p>

<p>$\frac{z^2-\pi^2}{\sin(z)}$  are given by $z=k\pi $ with $ k\in \mathbb Z$ which are poles except $z=\pm \pi. $ where the singularity is apparent(fake singularity or removable singularity). Indeed $\sin z =0 
$ iff $z=\pi k$. But 
$$\lim_{z\to \pm \pi} \frac{z^2-\pi^2}{\sin z } =-\lim_{z\to \pm \pi} \frac{(z-\pi)(z+\pi)}{\sin( z\mp\pi )} = \mp 2\pi.$$</p>

<p>and the same for</p>

<p>$\frac{z}{e^z-1}$ are given by $z=2i\pi k $ with $ k\in \mathbb Z$ which are poles except $z=0. $ where the singularity is apparent(fake singularity or removable singularity). $e^z=1
$ iff $z=2i\pi k$. but 
$$\lim_{z\to 0} \frac{z}{e^z -1 } =1$$</p>
"
"2396790","2396862","<p>${e^{iz}-e^{-iz}}=-1.5$</p>

<p>Set $e^{iz}=t$ so the equation becomes</p>

<p>$t-\dfrac{1}{t}+1.5==0$</p>

<p>$t_1=-2;\;t=\dfrac{1}{2}$</p>

<p>so $e^{iz}=-2$ which leads to $z=  (2 c_1+1)\pi-i \log 2,\forall c_1\in\mathbb{Z}$</p>

<p>and $e^{iz}=\dfrac{1}{2}$ which gives $z=2 \pi  c_2+i \log 2,\forall c_2\in\mathbb{Z}$</p>

<p>Hope this is useful</p>

<p><strong><em>Edit</em></strong></p>

<p>$e^{i z}=\dfrac{1}{2}$</p>

<p>$e^{i z}=e^{\log \frac{1}{2}}$</p>

<p>$iz=\log \frac{1}{2}+2k\pi i,\forall k\in\mathbb{Z}$</p>

<p>$z=\dfrac{1}{i}\,(-\log 2+2k\pi i)$</p>

<p>$z=-i(-\log 2+2k\pi i)$</p>

<p>$z=-2k \pi+i\log 2,\;\forall k\in\mathbb{Z}$</p>

<p>can be written also as $z=2k \pi+i\log 2,\;\forall k\in\mathbb{Z}$</p>
"
"2396815","2396879","<p>Maybe we can look at it like this ...  Consider the equation
$$
d(x^2y) = 2xy\;dx + x^2\;dy
\tag{1}$$</p>

<p><strong>Problem 1.</strong>  Compute the total differential of $x^2 y$.  Solution: write down (1).</p>

<p><strong>Problem 2.</strong>  Is $2xy\;dx + x^2\;dy$ an exact differential?  Solution: Yes.  Proof: (1).</p>

<p>So, as you say, in a certain sense they are the same.  But the point of view is different.  In  Problem 1, we start with the function and compute its differential.  In Problem 2, we start with the differential, and find the function.  </p>

<p>In fact, in Problem 2 there are ways to answer ""Yes"" without finding the function.  Instead we can compute
$$
\frac{\partial(2xy)}{\partial y} = 2x
\\
\frac{\partial(x^2)}{\partial x} = 2x
$$
and these are equal, so $2xy\;dx + x^2\;dy$ is an exact differential.</p>
"
"2396817","2396909","<p>First of all, $z^z$ is not analytic at $z=0$. It is defined as $z^z=e^{z\log z}$, where $\log$ is an analytic branch of the logarithm. Take the usual branch, defined on $\Bbb C\setminus(-\infty,0]$ with $\log1=0$. If $z=r\,e^{i\theta}$, $-\pi&lt;\theta&lt;\pi$, then
$$
|z\log z|=r\,|\log r+i\,\theta|\le r(|\log r|+\pi),
$$
which goes to $0$ as $r\to0$. It follows that $z^z\to1$ as $z\to0$.</p>
"
"2396826","2396829","<p>Take 
$$ f(x) = 100x+ x^2\sin\frac{1}{x} \qquad\text{with $f(0)=0$}$$
which is differentiable and 
$$ f'(x) =  100+ 2x\sin\frac{1}{x}-\cos\frac{1}{x}  \qquad\text{with $f'(0)=100$}$$
$f'$ is not continuous a $x=0$ and you might choose the interval $[a,b]$ around $x=0$ as it suite to you. Indeed </p>

<p>$$ f'(x)=100+2x\sin\frac{1}{x}-\cos\frac{1}{x} \ge 100+ 2x\sin\frac{1}{x}-1$$ </p>

<p>Since $|\sin a| \le |a|$ and $-1\le-\cos a\le 1$ then $|2x\sin\frac{1}{x}|\le 2$ i.e $$2x\sin\frac{1}{x}\ge -2 $$</p>

<p>therefore </p>

<p>$$ f'(x)=100+2x\sin\frac{1}{x}-\cos\frac{1}{x} \ge 100-2-1= 97&gt;0$$ for every $x\in \mathbb R$. so $f $ is increasing and  $f'$ is not continuous at $x=0$.</p>
"
"2396833","2397288","<p>I think this is a can of worms if you want an elliptic curve, unless you mean that the curve is half of an ellipse, but it's not drawn that way. I worked out a solution for the piece of fabric as segment of a circle with chord $c$ and arc $s$. The geometry is shown in the figure below. The relevant equations are</p>

<p>$$
h=R-d\\
s=R\theta\\
d=R\cos\frac{\theta}{2}\\
c=2R\sin\frac{\theta}{2}\\
$$</p>

<p>First we need to solve for $\theta$ as follows</p>

<p>$$c=2R\sin\frac{\theta}{2}=s\frac{2}{\theta}\sin\frac{\theta}{2}\\
\frac{2}{\theta}\sin\frac{\theta}{2}=\frac{c}{s}
$$</p>

<p>This has to be done numerically, I found that</p>

<p>$$\frac{\theta}{2}\approx 1.2756981$$</p>

<p>We can then calculate (in this order), $R,~d$ and $h$ from the equations above. I found that $h\approx 2.2236$ for $c=6$ and $s=8$. This is consistent with the comment that suggested that $h\lt\sqrt{7}$.</p>

<p><a href=""https://i.stack.imgur.com/Pt4CJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Pt4CJ.png"" alt=""segment of a circle""></a></p>
"
"2396836","2396850","<p>Yuriy Drozd says that this is the Jordan algebra $J_n(K)$, a subalgebra of $M_n(K)$. Of course, there are other Jordan algebras, but he refers to it by $J_n(K)$, and it is just listed as an Example in the text (Example 3.).
By the way, we may also start from any associative algebra $A$, where we obtain another Jordan algebra $A^+$ by setting
$$
a\circ b:=\frac{ab+ba}{2}.
$$
In particular, we could take $A=M_n(K)$. Again, $A^+$ is called Jordan algebra $A^+$.</p>
"
"2396839","2397145","<p>All you need is the following: </p>

<blockquote>
  <p>Let $h\colon (a,b)\to\mathbb R$ be a function such that for any $a\lt x_0\lt x_1\lt b$, $\int_{(x_0,x_1)} h(x)\mathrm dx=0$. Then $h=0$ almost everywhere.</p>
</blockquote>

<p>As you pointed out, if we manage to show that 
$$\tag{*}   \int_{B} h(x)\mathrm dx=0$$
for each Borel subset of $(a,b)$, then we reach the wanted conclusion. The assumption give only (*) for interval. </p>

<ol>
<li>This can be trivially extended to finite union of disjoint intervals.</li>
<li>Using the dominated convergence theorem, we can see that (*) holds for countable unions of disjoint intervals. </li>
<li>An open subset of $(a,b)$ can be written as a disjoint union of intervals, hence (*) holds for any open subset of $(a,b)$. </li>
<li>Now, in order to extend (*) to each Borel subset of $(a,b)$, we need the following facts: 

<ul>
<li>if $A\subset (a,b)$ is a Borel set, then for each positive $\delta$,   there exists an open set $O\supset A $ such that the Lebesgue measure of $O\setminus A$ is smaller than $\delta$;</li>
<li>for each positive $\varepsilon$, there exists $\delta$ such that if $S$ is a set of measure smaller than $\delta$ then $\int_S\left\lvert h(x)\right\rvert\mathrm dx\lt \varepsilon$.            </li>
</ul></li>
</ol>
"
"2396840","2396847","<p>As suggested, suppose the induction formula holds, then we can show $P_n(t)$ is a degree $n$ polynomial of $t$ directly by an induction process, i.e., suppose $P_{k}(t)$ is of degree $k$ and $P_{k-1}(t)$ is of degree $k-1$, show that $P_{k+1}(t)$ is of degree $k+1$.</p>

<p>We only need to check $P_0(t)=1$ and $P_1(t)=t$, which can be derived from the series representation.</p>

<p>For $P_0$, set $x=0$ on the series representation.</p>

<p>For $P_1$, taking derivative of both sides of the series, and set $x=0$.</p>
"
"2396845","2396898","<p><strong>Hint</strong>:</p>

<ol>
<li>If $Ax = \lambda x$, then what is $(A+I)x$ equal to?</li>
<li>If $Bx=\lambda x$, then what is $B^2x$ equal to? (remember, $B^2x=B(Bx)$!)</li>
<li>From (2), if $Bx=\lambda x$, then what is $B^{100}x$ equal to?</li>
</ol>

<p>Now, join (1) and (3) together and you should find a simple connection between eigenvectors of $A$ and eigenvectors of $(A+I)^{100}$.</p>
"
"2396849","2396928","<p>Assume for now that both $r$ and $I_1$ are given.</p>

<p>Consider the real value $r*I_1$. Let $I_2=[r*I_1]$ be the nearest integer, i.e. the integer you get when rounding $r*I_1$.
Then $\epsilon=I_2/I_1 - r$ is the smallest change that makes $(r+\epsilon)*I_1$ an integer.</p>

<p>You have several choices for the value of $I_1$, so simply calculate $\epsilon = [r*I_1]/I_1 - r$ in each of those cases and choose the best one.</p>
"
"2396854","2396870","<p>Write $y$ and $z$ in terms of $x=t$. You will find $x(t)=t$, $y(t)=2t+1$ and $z(t)=4t$. Then write $$t\mapsto(t,2t+1,4t)$$ that is the parametric form of a line, namely the line passing through the point $P=(0,1,0)$ with direction given by the vector $\vec{v}=\langle1,2,4\rangle$.</p>
"
"2396875","2396880","<p>The set $M$ is dense in $[0,2\pi]$ since $r$ is rational. Otherwise suppose it has a smallest element $a$, you have $a= rn +2p\pi$, $r(n+l)=2q_l\pi+m_la$, $n,p,q_l,m_l,l$ are integers and $m_la\in [0,2\pi]$. this implies $m_la=m_lnr+2pm_l\pi=r(n+l)-2q_l\pi$. You deduce that</p>

<p>$(2pm_l+2q_l)\pi=r(n+l-m_ln)$. This implies that $n+l=m_ln$ and $n(m_l-1)=l$, thus $n$ divides every integer, we deduce that $|n|=1$, remark that there exists $B&gt;0$ such that $m_l&lt;B$ since $m_la\in [0,2\pi]$. This implies that $|n(1-m_l)|=|l|$ is bounded for every integer $l$. Contradicition. </p>

<p>So $inf M=0$, this implies that $M$ is infinite (what you need) but it implies also that $M$ is dense, since for every $m\in M, l\in \mathbb{N}$ such that $lm&lt;2\pi, lm\in M$.</p>
"
"2396884","2397424","<p>The variational PDE normally does not involve the boundary conditions... setting $r=w-g$ makes little sense if $g$ is only defined on $\partial U$. As far as calculus of variations is concerned, the role of boundary condition is to force us to use compactly supported perturbations only, as is done in the derivation of the Euler-Lagrange equation. </p>

<p>There is an <a href=""http://www.mpri.lsu.edu/textbook/chapter8-b.htm#integral"" rel=""nofollow noreferrer"">integral constraint</a> here: handle it with a Lagrange multiplier, letting 
$$
L[w] = I[w]+\lambda J[w] = \int_U ((1+|Dw|^2)^{1/2} + \lambda w)
$$
The Euler-Lagrange equation is 
$$
- \operatorname{div}\left( \frac{Dw}{(1+|Dw|^2)^{1/2}} \right) + \lambda = 0
$$
So, the vector field $Dw/\sqrt{1+|Dw|^2}$ must be of constant divergence. We won't know $\lambda$ until the boundary value problem is solved, which isn't going to happen with abstract boundary data $g$.</p>
"
"2396891","2396901","<p>Take $x\in(-1,1)$
$$ f(x) =100000+ 100x+ x^2\sin\frac{1}{x} \qquad\text{with $f(0)=100000$}$$
which is differentiable,  and 
$$ f'(x) =  100+ 2x\sin\frac{1}{x}-\cos\frac{1}{x}  \qquad\text{with $f'(0)=100$}$$
$f'$ is not continuous a $x=0$ and you might choose the interval $[a,b]$ around $x=0$ as it suite to you. Indeed </p>

<p>$$ f'(x)=100+2x\sin\frac{1}{x}-\cos\frac{1}{x} \ge 100+ 2x\sin\frac{1}{x}-1$$ </p>

<p>Since $|\sin a| \le |a|$ and $-1\le-\cos a\le 1$ then $|2x\sin\frac{1}{x}|\le 2$ i.e $$2x\sin\frac{1}{x}\ge -2 $$</p>

<p>therefore </p>

<p>$$ f'(x)=100+2x\sin\frac{1}{x}-\cos\frac{1}{x} \ge 100-2-1= 97&gt;0$$ for every $x\in \mathbb R$. so $f $ is increasing and  $f'$ is not continuous at $x=0$.</p>

<p>Also $|x^2\sin\frac{1}{x}|\le 1$ for all $x\in (-1,1)$ so that 
$$f(x) =100000+ 100x+ x^2\sin\frac{1}{x} &gt; 100000-100 -1 &gt;0$$ for every $x\in (-1,1).$</p>
"
"2396892","2396904","<p>By C-S
$$u^2=a^2+b^2+2\sqrt{(a^2\cos^2\alpha+b^2\sin^2\alpha)(b^2\cos^2\alpha+a^2\sin^2\alpha)}\geq$$
$$\geq a^2+b^2+2\sqrt{(ab\cos^2\alpha+ab\sin^2\alpha)^2}=a^2+b^2+2|ab|=(|a|+|b|)^2.$$
The equality occurs when $$(a\cos\alpha,b\sin\alpha)||(b\cos\alpha,a\sin\alpha),$$
which says that $(|a|+|b|)^2$ is a minimal value. </p>
"
"2396893","2396930","<p>The huge negative exponents will make the last two terms on the right very small, so you can get a preliminary value by ignoring them.  We get $\ln \frac {0.2}{0.819}\approx -11.5x, x \approx 0.122588$  Now you can incorporate the other terms in a fixed point iteration 
$$0.2 = 0.819 \exp(-11.5x)+0.0975 \exp(-70.1x)+0.0325 \exp(-179x)\\
\frac {-1}{11.5}\log \left(\frac 1{0.819}\left(0.2-0.0975 \exp(-70.1x_i)0.0325 \exp(-179x_i)\right)\right)=x_{i+1}$$
The iteration will converge very rapidly</p>
"
"2396907","2396915","<p>Any vector in the reflection plane will be mapped into itself, so the eigenvalue will be 1.  Such vectors span a space of dimension 2, so, yes, there are your two eigenvalues of 1.  Any vector lying on the line perpendicular to the plane and passing through the origin will be mapped into its own negative, so there is your eigenvalue of -1.  And, modulo an overall scaling factor, there is only one such vector, so there will be only one eigenvalue of -1.  So I think you are right.</p>

<p>You can find the type of base you are looking for by rotating your coordinates so that $z$ is normal to the reflection plane.  You can rotate your coordinates around that axis any way you like and you will get the diagonal matrix you are looking for.</p>

<p>One procedure for getting that rotation would be to first take the gradient of your expression</p>

<p>$\nabla(ax+by+cz) = a\hat{x}+b\hat{y}+c\hat{z}$</p>

<p>The gradient is always normal to the function, so you know that you have here the normal to the plane.  So take the vector $(a,b,c)$ and normalize it, and you have the vector normal to the plane.  Now, at this point, you just need any two other vectors that are normal to each other and also normal to this vector.  One way to get one would be to take this new vector, call it $\hat{w}$, and cross it with $\hat{x}$, $\hat{w}\times\hat{x}$ and normalize that to get a vector $\hat{v}$.  Then take $\hat{v}\times\hat{w}$ to get $\hat{u}$.  Your new basis is $\hat{u}, \hat{v}, \hat{w}$.  But you have to be prepared for the exception when $\hat{w}\times\hat{x} = 0$.</p>

<p>After you have your new vectors, you have to calculate 9 dot products to get the transformation matrix you need.  If you have a vector in $\hat{x}, \hat{y}, \hat{z}$ coordinates, say $(q,r,s)$, you transform it to the $\hat{u}, \hat{v}, \hat{w}$ basis by multiplying by</p>

<p>$\displaystyle
\left(\begin{array}&amp; u\cdot x &amp; u \cdot y &amp; u \cdot z \\
v \cdot x  &amp; v \cdot y &amp; v \cdot z  \\
w \cdot x &amp; w \cdot y &amp; w \cdot z\end{array}\right)
\left(\begin{array} &amp;q \\ r \\ s\end{array} \right)$</p>
"
"2396923","2397665","<p>Write $f(\gamma) = \left( \sum_{n=1}^{N} n^{-\gamma/2} \right)\left( \sum_{n=1}^{N} n^{-\gamma} \right)^{-1/2}$. Then
$$
h(\gamma_) \equiv \log f(\gamma) = \log\left(\sum_{n=1}^{N} n^{-\gamma/2} \right) - \tfrac{1}{2} \log\left( \sum_{n=1}^{N} n^{-\gamma} \right)
$$
Now $\log(\cdot)$ is everywhere increasing so it is sufficient to show that $h(\gamma)$ is everywhere monotonic decreasing. We have
$$\begin{align*}
h'(\gamma)
&amp;=
\frac{\sum_{n=1}^{N} -\tfrac{1}{2} n^{-\gamma/2} \log(n)}{\sum_{n=1}^{N} n^{-\gamma/2}}
-
\frac{\sum_{n=1}^{N} -n^{-\gamma} \log(n)}{2 \sum_{n=1}^{N} n^{-\gamma}}
\\ &amp;=
-\frac{1}{2}\left(
\frac{\sum_{n=1}^{N} n^{-\gamma/2} \log(n)}{\sum_{n=1}^{N} n^{-\gamma/2}}
-
\frac{\sum_{n=1}^{N} n^{-\gamma} \log(n)}{\sum_{n=1}^{N} n^{-\gamma}}
\right)
\\ &amp;=
-\frac{1}{2}
\cdot
\frac{
\left(
\sum_{n=1}^{N} n^{-\gamma/2} \log(n)
\right)
\left(
\sum_{n=1}^{N} n^{-\gamma}
\right)
-
\left(
\sum_{n=1}^{N} n^{-\gamma/2}
\right)
\left(
\sum_{n=1}^{N} n^{-\gamma} \log(n)
\right)
}{
\left(
\sum_{n=1}^{N} n^{-\gamma/2}
\right)
\left(
\sum_{n=1}^{N} n^{-\gamma} \log(n)
\right)
}
\end{align*}$$
It is thus sufficient to show that the following proposition $\mathcal{P}(N)$ 
$$
\mathcal{P}(N) : 
\left(
\sum_{n=1}^{N} n^{-\gamma/2} \log(n)
\right)
\left(
\sum_{n=1}^{N} n^{-\gamma}
\right)
-
\left(
\sum_{n=1}^{N} n^{-\gamma/2}
\right)
\left(
\sum_{n=1}^{N} n^{-\gamma} \log(n)
\right)
\geq 
0
\quad
\text{for all}\
\gamma \geq 0
$$
is true for all positive integers $N$. We proceed by induction on $N$. </p>

<p>We see that $\mathcal{P}(1)$ is true as
$$
(n^{-\gamma/2} \log(n))(n^{-\gamma}) - (n^{-\gamma/2})(n^{-\gamma} \log(n))
=
0
$$
for all $\gamma \geq 0 $.
Now 
$$\begin{align*}
\, &amp;
\left(
\sum_{n=1}^{N} n^{-\gamma/2} \log(n) + (N+1)^{-\gamma/2}\log(N+1)
\right)
\left(
\sum_{n=1}^{N} n^{-\gamma} + (N+1)^{-\gamma}
\right)
-
\left(
\sum_{n=1}^{N} n^{-\gamma/2} + (N+1)^{-\gamma/2} 
\right)
\left(
\sum_{n=1}^{N} n^{-\gamma} \log(n) + (N+1)^{-\gamma}\log(N+1)
\right)
\\ = &amp;\,
\left(
\sum_{n=1}^{N} n^{-\gamma/2} \log(n) 
\right)
\left(
\sum_{n=1}^{N} n^{-\gamma} 
\right)
+
\left(
\sum_{n=1}^{N} n^{-\gamma/2} \log(n) 
\right)
(N+1)^{-\gamma}
\\ &amp;+
(N+1)^{-\gamma/2}\log(N+1)
\left(
\sum_{n=1}^{N} n^{-\gamma} 
\right)
+
(N+1)^{-\gamma/2}\log(N+1)
(N+1)^{-\gamma}
\\
&amp;-
\left(
\sum_{n=1}^{N} n^{-\gamma/2}  
\right)
\left(
\sum_{n=1}^{N} n^{-\gamma} \log(n)
\right)
-
\left(
\sum_{n=1}^{N} n^{-\gamma/2}  
\right)
(N+1)^{-\gamma}\log(N+1)
\\ &amp;-
(N+1)^{-\gamma/2}
\left(
\sum_{n=1}^{N} n^{-\gamma} \log(n)
\right)
-
(N+1)^{-\gamma/2}
(N+1)^{-\gamma}\log(N+1)
\\ = &amp;\,
\left(
\sum_{n=1}^{N} n^{-\gamma/2} \log(n) 
\right)
\left(
\sum_{n=1}^{N} n^{-\gamma} 
\right)
-
\left(
\sum_{n=1}^{N} n^{-\gamma/2}  
\right)
\left(
\sum_{n=1}^{N} n^{-\gamma} \log(n)
\right)
\\ &amp;+
\sum_{n=1}^{N}
\log(N+1)\left(
(N+1)^{-\gamma/2}
n^{-\gamma}
-
(N+1)^{-\gamma}
n^{-\gamma/2}  
\right)
\\ &amp;-
\sum_{n=1}^{N}
\log(n)\left(
(N+1)^{-\gamma/2}
n^{-\gamma}
-
(N+1)^{-\gamma}
n^{-\gamma/2}  
\right)
\\ = &amp;\,
\left(
\sum_{n=1}^{N} n^{-\gamma/2} \log(n) 
\right)
\left(
\sum_{n=1}^{N} n^{-\gamma} 
\right)
-
\left(
\sum_{n=1}^{N} n^{-\gamma/2}  
\right)
\left(
\sum_{n=1}^{N} n^{-\gamma} \log(n)
\right)
\\ &amp;+
\sum_{n=1}^{N}
\left(
  \log(N+1) - \log(n)
\right)
(N+1)^{-\gamma}n^{-\gamma}
\left(
  (N+1)^{\gamma/2} - n^{\gamma/2}
\right)
\end{align*}$$
Hence if $\mathcal{P}(N)$ is true then $\mathcal{P}(N+1)$ is true, as required.</p>
"
"2396933","2397037","<p>Note:
$$\frac{\sqrt[n+1]{n!}}{\sqrt[n]{n!}} &lt;\frac{\sqrt[n+1]{(n+1)!}}{\sqrt[n]{n!}}&lt;\frac{\sqrt[n+1]{(n+1)!}}{\sqrt[n+1]{n!}}.$$
Taking limit:
$$\lim_{n\to\infty} (n!)^{\frac{1}{n(n+1)}}\le L \le \lim_{n\to\infty} (n+1)^{\frac{1}{n+1}} \Rightarrow$$
$$1\le L \le 1 \Rightarrow $$
$$L=1.$$</p>
"
"2396937","2396960","<p>In terms of a Cayley graph, yes. </p>

<p>Let $C_n$ be any cyclic group of order $n.$ Then there is a corresponding Cayley graph who's vertices are labeled by the elements of $C_n,$ and whose directed edges are labeled by a generating set, where if the pair of vertices $(a,b)$ are connected, with edge label $x$, where $x$ is a generator of $C_n,$ then we would say $xa=b.$ </p>

<p>This will always produce a cycle graph for cyclic groups. In the case of products, you get a ""prism"" like structure of the graph, and for semi-direct products, you can see a natural twist in some of the vertices indicating the non-commutativity. </p>
"
"2396943","2396969","<p><strong>Hint</strong></p>

<p>Let $\Gamma_R=\{Re^{i\theta }\mid \theta \in [0,\pi]\}\cup[-R,R]$. Then, $$\int_{\Gamma_R} \frac{1}{(z^2+4)^5}=Res_{z=2i}(f)=\lim_{z\to 2i}\frac{1}{4!}\frac{d^4}{dz^4}\frac{1}{(z+2i)^5}=...$$</p>
"
"2396956","2396966","<p>writing $$\int r^2\cdot r e^{-r^2}dr$$ and let $$-r^2=x$$ then we get $$-2rdr=dx$$</p>
"
"2396968","2396985","<p>If the automorphism group were $S_5$, there would be an automorphism that swaps two of the vertices and fixes the other three. But any symmetry of the star that swaps two vertices is a reflection, hence has only one fixed point.</p>

<p>In fact, the automorphism group of the star is just the automorphism group of the pentagon, i.e. the dihedral group of order 10. That's proved in <a href=""https://math.stackexchange.com/questions/2393640/the-group-of-symmetries-of-the-regular-pentagram-is-isomorphic-to-what-math-gr?rq=1"">this</a> answer.</p>
"
"2396970","2396990","<p>Hint. Note that the vector $w=(2,1,-2)$ is orthogonal to the plane $2x+y-2z=-2$. Therefore a vector is parallel to the plane iff it is orthogonal to $w$.</p>
"
"2396978","2397128","<p>Guide:</p>

<p>After you construct the graph by treating each region as a vertex and construct an edge between them for each line they share as boundary, the question is asking if there is an eulerian cycle.</p>

<p>An undirected graph has an Eulerian cycle if and only if every vertex has even degree, and all of its vertices with nonzero degree belong to a single connected component.</p>
"
"2396981","2398774","<p>The $i$th nonlinear equation with $j=1...n$ little squares wildcards each one with $2k, k=1...n$ choices for $v_k|1-v_k$ on each little white, can be rewritten in a more canonical way as:
$$
\mathscr{E}_i: \prod_{j=1}^n \prod_{k=1}^n v_k^{a_{ijk}} (1-v_k)^{a_{ij(k+n-1)}} = 0\\
a_{ijk} \in \{0,1\}, i=1...n
$$</p>

<p>As properly pointed in the comments, for every $k$th variable solution, this system is </p>

<ul>
<li>$v_k=0$ if and only if $a_{ijk}=1 \forall i,j,k$, or </li>
<li>$v_k=1$ if and only if $a_{ij(k+n-1)}=1 \forall i,j,k$, or </li>
<li>undefined either case.</li>
</ul>

<p>That is, the system is solved if only if every of the $v|1-v$ choices is made consistently for every variable and for every wildcard.</p>
"
"2396982","2396996","<p>The arrows $g'$ and $h'$ have some common codomain $Z$ in $\mathcal{D}$. If $Z$ is in the image of $F$, your argument works fine. But full functors need not be surjective on objects. And if $Z$ is not in the image of $F$, you certainly can't find $g$ and $h$ which map to $g'$ and $h'$!</p>
"
"2396998","2397058","<p>The solution is incorrect as it stands - the two $[-2,2]$ distributions should add to a $[-4,4]$ triangle.</p>

<p>However if $U1$ and $U2$  were uniform on $[-1,1]$, the density equation would be correct, because the two sides of the triangular distribution fold together with Theo taking the bigger piece, so the slope is $-\frac 12,$ not $-\frac 14$. The shortcut is to remember that the area under the density graph must be $1$, so the right triangle sitting on a base of $2$ units will have maximum density value $1$ at measure $10$.</p>
"
"2397001","2397004","<p>The answer is no.  Take $A = \Bbb R$ and $f(x) = |x|$.</p>

<hr>

<p>In general, we can say that if $f(f(x)) = f(x)$, then $f$ is equal to the identity over the image of $f$.  In particular: consider any $y$ in the image of $f$.  Then $y = f(x)$ for some $x \in A$, and we have
$$
f(y) = f(f(x)) = f(x) = y
$$
If $f$ is surjective, then the image of $f$ is $A$, and so $f$ must be the identity map over $A$.</p>

<p>Note that if $f$ is injective, then there exists a function $g(x)$ such that $g(f(x)) = x$, and we have
$$
f(f(x)) = f(x)  \quad \text{for all }x \in A \implies \\
g(f(f(x)) = g(f(x)) \quad \text{for all }x \in A\implies \\
f(x) = x \quad \text{for all }x \in A
$$</p>
"
"2397006","2397009","<p>The standard trick here is to write $X=x^2$ and solve $2X^2-3X-9=0$.</p>
"
"2397024","2397038","<p>Notice that if we have:</p>

<p>$$(abc)^i(abd)^j(bcd)^k(acd)^l=a^5b^5c^5d^6 ;$$ </p>

<p>then this implies that </p>

<p>$$ 
             \left\{ \begin{array}{lcc}
             i+j+l=5 ,  
             &amp; 
             \text{by comparison of the power of} \ \ a , \\
             i+j+k=5 ,  
             &amp; 
             \text{by comparison of the power of} \ \ b , \\
             i+k+l=5 ,  
             &amp; 
             \text{by comparison of the power of} \ \ c , \\ 
             j+k+l=6 ,  
             &amp; 
             \text{by comparison of the power of} \ \ d . \\
             \end{array}
   \right.$$ </p>

<hr>

<p>The above system of equations has the solution: </p>

<p>$$i=1, \ \ j=2, \ \ k=2, \ \ l=2.$$</p>

<hr>

<p>So we have the following: </p>

<p>$$(abc)^1(abd)^2(bcd)^2(acd)^2=a^5b^5c^5d^6.$$ </p>

<hr>

<p>So the coefficeint is equal to $\dfrac{7!}{1!2!2!2!}$. </p>
"
"2397029","2397452","<p>$n \star A\,$ is called the <strong><em>n-fold iterated sumset</em></strong> on the wikipedia <a href=""https://en.wikipedia.org/wiki/Sumset"" rel=""noreferrer"">sumset page</a> (which also includes more references):</p>

<blockquote>
  <p>In additive combinatorics, the sumset (also called the <a href=""https://en.wikipedia.org/wiki/Minkowski_addition"" rel=""noreferrer"">Minkowski sum</a>) of two subsets $A$ and $B$ of an abelian group $G$ (written additively) is defined to be the set of all sums of an element from $A$ with an element from $B$. That is,
  $$\displaystyle A+B=\{a+b:a\in A,b\in B\}$$
  The <strong><em>n-fold iterated sumset</em></strong> of $A$ is
  $$\displaystyle nA=A+\cdots +A$$
  where there are $n$ summands.</p>
</blockquote>
"
"2397036","2397119","<p>Since
$(\log \zeta_f(z) )'
=\dfrac{\zeta_f'(z) }{\zeta_f(z) }
=\dfrac{g_f(z)}{z}
$,
if
$\zeta_f(z)$
is rational then
$\zeta_f'(z)$
is rational so
$g_f(z)
=\dfrac{z\zeta_f'(z) }{\zeta_f(z) }
$
is rational.</p>
"
"2397039","2397066","<p>It is not the formal definition, but I think it like this. Assume you have a square area, and the size of one of the edge is 5. That is, the area is 25. And there are uniformly distributed 5 balls in that area. What is the probability that you grab a ball from that square area when your eyes are covered ? <br><br> 5 / 25 right ? This is the relation of density and probability. <br><br> Lets turn back to continuous distribution. For example Gauss. You want to know the probability of 3 from a gauss distribution $g(\mu,\sigma)$. It is 0. because probability is 1\infinity. Why infinity ? Because under the Gaussian curve there are infinite number of possible real number ! <br><br> So lets pick an interval. 3 - 3.1 then the probability is selected area / whole area (As in the first example) <br> <br> This is not the formal definition. As you can find the formal one every where ... If I am mistaken please some one fix me as well :)</p>
"
"2397041","2397050","<p>$$\int_{|z|=1}\frac{cosz}{z^5}dz  = 2i\pi f^{(4)}(0)/4! =2i\pi/4! $$
where $f(z) =  \cos (z)$
See here 
<a href=""https://math.stackexchange.com/questions/2396641/calculation-of-oint-vert-z-vert-1-fracez2-sinz4z-22e-cosz/2396703#2396703"">Calculation of $\oint_{\vert z\vert=1}\frac{e^{z^2+\sin(z)}}{4(z-2)^2e^{\cos(z)}}dz$</a>.</p>
"
"2397046","2397057","<p>Let $\theta = 2\pi/5$. We have $z^5 = 1$ and $z \neq 1$, so $z^4 + z^3 + z^2 + z + 1 = 0$.</p>

<p>Since $z^4 = \frac1{z} = z^{-1}$, $z^3 = z^{-2}$, we can rewrite:</p>

<p>$$z+ z^{-1} + z^2 + z^{-2} + 1 =0$$</p>

<p>Now $z+z^{-1} = 2\cos\theta$ and $z^2 + z^{-2} = 2 \cos 2\theta = 2(2 \cos^2 \theta -1)$. Putting $r = \cos \theta$, we have:</p>

<p>$$2 r + 2(2r^2 -1) + 1 =0$$</p>

<p>Then $4r^2 +2r - 1 =0$ and you can find $r$ by the quadratic formula. Of course $\sin \theta = \sqrt{1 - r^2}$. So you are done (write $i = \sqrt{-1}$).</p>
"
"2397051","2397427","<p>I will try to answer this question as I understand it, based on a quick scan
of parts of the article linked in your Comment.</p>

<p>'Discretization' or 'discrete approximation' of a continuous distribution
means (a) to divide the possible continuous values into non-overlapping intervals
( or 'bins') that cover the entire span of the observed data and then (b) to assign increasing numerical values (often integers) to bins of increasing continuous values. </p>

<p>'Equal areas' means that the bins contain (about) the same numbers of
subjects so that corresponding population bins are of (about) equal probability.
Then the integers that correspond to the bins are (about) equally likely.
It seems the rationale for wanting equal probabilities is to simplify simulations, because the integers representing the bins can be sampled
strictly at random.</p>

<p>The answer to your question is that means and standard deviations can be
approximately preserved if the discretization is done with care. Some information is lost when continuous values are
reduced to bins (and then integer values), so the preservation cannot be
exact. In general, the greater the number of bins, the greater the accuracy.</p>

<p>For example, suppose the population of some test scores is distributed $\mathsf{Norm}(\mu = 100,\,\sigma = 15), $ 
 and you have $n = 10\,000$ observations from that distribution.
Then you could pick cutpoints between ten bins roughly to match the deciles
of the sample. Here is a specific example with data simulated using
R statistical software:</p>

<pre><code>set.seed(1234); n = 10000  
x = rnorm(n, 100, 15)
mean(x);  sd(x)
## 100.0917       # exact sample mean
## 14.81294       # exact sample SD

q = quantile(x, (0:10)/10); q
       0%       10%       20%       30%       40%       50% 
 49.05905  81.07926  87.44960  92.54443  96.54166 100.06952 
      60%       70%       80%       90%      100% 
103.80729 107.79283 112.57182 118.96523 154.27160 
</code></pre>

<p>So that the bins are roughly $[49.06, 81.08), [81.08, 87.49), \dots
[112,57, 118.97), (118.97, 154,28].$ </p>

<p>For comparison, here is a plot of the population density of $\mathsf{Norm}(\mu = 100,\,\sigma = 15),$
with vertical lines at <em>population</em> deciles. The lines divide the area under the
curve into ten equal parts (although, due to an optical illusion the outer
areas may look larger at first, or even second, glance).</p>

<p><a href=""https://i.stack.imgur.com/uQr7k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uQr7k.png"" alt=""enter image description here""></a></p>

<p>Notice that the sample mean and SD of the 10,000 (""continuous"") observed values are not far from the population mean
and SD, respectively.</p>

<p>We can use the <code>hist</code> function in R to get the corresponding interval
midpoints (rounded to integers) and frequencies.</p>

<pre><code>hist.stats = hist(x, br=q, plot=F)
f = hist.stats$counts
m = round(hist.stats$mids)
f; m
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000  # frequencies
## 65   84   90   95   98  102  106  110  116  137    # midpoints
</code></pre>

<p>Thus we have completed the 'equal areas discretization', where the integers
that represent the bins are their rounded midpoints.</p>

<p>Now the sample mean $A = \bar X$ and standard deviation $S$ can be
approximated as $100.3$ and $18.4,$ respectively, using the formulas</p>

<p>$$\bar X \approx \frac{\sum_{j=1}^{10} f_jm_j}{n}\;\;\text{and}\;\;\;
S \approx \sqrt{\frac{\sum_j f_j(m_j - \bar X)^2}{n-1}}.$$</p>

<pre><code>a = sum(f*m)/n;  s = sqrt(sum(f*(m-a)^2)/(n-1))
a;  s
## 100.3         # aprx 'discretized' sample mean
## 18.36964      # aprx 'discretized' sample SD
</code></pre>

<p>For many purposes these would be useful approximations of $\bar X$ and $S$
and thus useful estimates of the population $\mu$ and $\sigma.$</p>

<p><em>Note:</em> Without the 'equal area' restriction, this method was used before the
computer age to simplify computations of sample means and SDs. 
Traditionally, it is the bin <em>widths</em> that are made equal, not the
bin <em>frequencies</em>. Nowadays, the histogram procedures in most statistical
software make histogram bins of equal widths by default. </p>

<p>The same formulas as above
(involving bin frequencies and midpoints) can be used to approximate
$\bar X$ and $S.$  In general, I suppose the method of equal widths
gives somewhat better approximations of $\bar X$ and $S.$ Below is a
histogram of the simulated $X_i$s above, with frequency labels atop
histogram bars.</p>

<p><a href=""https://i.stack.imgur.com/b8BTv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b8BTv.png"" alt=""enter image description here""></a></p>
"
"2397069","2397957","<p>Using that $Tr(A^k)=\sum_i \lambda_i^k = 0$ for all odd $k$, the sets of complex numbers having this property cancel each other in pairs as proven <a href=""https://math.stackexchange.com/questions/2397468/the-sum-of-odd-powered-complex-numbers-equals-zero-implies-they-cancel-each-othe"">here</a>.</p>
"
"2397072","2397100","<p>You could use the Maclauren series for arcsin:</p>

<p>$$\arcsin x = x+\frac{x^3}{6} + \frac{3x^5}{40}+\frac{5x^7}{112} +\cdots.$$</p>

<p>So $\arcsin 0.42 \approx 0.43343.$  This is in radians, so convert to degrees:
$0.43343\cdot 180/\pi \approx 24.833$ degrees.</p>
"
"2397081","2397086","<p>Let $x$ be a real number and assume that $x\in \bigcap_{n \in \mathbb{N}} K_{n}$, that is $x\in K_{n}$ for any $n\geq 1$. Now let $N$ be an integer such that $N&gt;x$ and $N\geq 1$ (it exists by the <a href=""https://proofwiki.org/wiki/Archimedean_Principle"" rel=""nofollow noreferrer"">Archimedean Principle</a>), then $x\in K_{N}=[N,+\infty)$ which means the $x\geq N$. Contradiction!
Therefore $\bigcap_{n \in \mathbb{N}} K_{n}=\emptyset$.</p>
"
"2397083","2397098","<p>Suppose the girls are $A,B,C,D$ and the boys are $T,U,V,W,X,Y,Z$.</p>

<p>And you chose $A$ and $B$ and then choose $T,U,C,V$.  You count that once.</p>

<p>And then suppose you chose $A$ and $C$ and then chose $T,U,B, V$.  You count that once.</p>

<p>So counted the team $A,B,C,T,U,V$ twice.</p>

<p>Even worse, the team, $A,B,C,D,T,U$ was counted when you picked $A,B$ and then $C,D,T,U$ and when you picked $A,C$ and then $B,D,T,U$ and when you picked $A,D$ then $B,C,T,U$ and when you picked $B,C$ and then $A,D, T,U$, when you pick $B,D$ and then $A,C, T,U$, and when you picked $C,D$ and then $A,B,T,U$.  So you counted the team $A,B,C,D,T,U$ <em>six</em> times!</p>
"
"2397085","2397113","<p>It is only sufficeint 
to take the same number of stones 
as your friend chooses; but take it from the opposite pile. </p>

<hr>

<p>In other words at each turn: </p>

<ul>
<li><p>if your friend take $x$ stones from the first pile, you should take $x$ stones from the second pile. </p></li>
<li><p>if your friend take $x$ stones from the second pile, you should take $x$ stones from the first pile. </p></li>
</ul>

<p>This is awinning strategy. </p>
"
"2397097","2397105","<p>Let's first calculate the expected winnings of each round.</p>

<p>He will win \$$20$ with probability $21/(21+21+4)$ and win -\$$20$ (that is, lose \$$20$) with proability $(21+4)/(21+21+4)$. Therefore his expected winnings per round is:
$$
E:=20\left(\frac{21}{21+21+4}\right) - 20\left(\frac{21+4}{21+21+4}\right)
= -\frac{80}{46} = -\frac{40}{23}.
$$
Given that he starts with \$$40$, this means that he can expect to play $40/(-E)=23$ rounds.</p>
"
"2397099","2397101","<p>Hint. Note that
$$f_n(x)=f_n(x_0)+\int_{x_0}^xf'_n(t)dt.$$</p>
"
"2397116","2397123","<p>To see <em>how</em> to get the answer, know the factorization of a finite geometric series:</p>

<p>$$ \begin{array}{ll} 1+x+\cdots+x^8 &amp; \displaystyle=\frac{x^9-1}{x-1} \\[5pt] &amp; \displaystyle =\frac{(x^3)^3-1}{x-1} \\[5pt] &amp; \displaystyle =\frac{(x^3-1)\big((x^3)^2+x^3+1\big)}{x-1} \\[5pt] &amp; = (x^2+x+1)(x^6+x^3+1) \end{array} $$</p>

<p>As Cauchy mentions in the comments, this generalizes with cyclotomic polynommials.</p>
"
"2397129","2397192","<blockquote>
  <p>Can anyone explain what went wrong?</p>
</blockquote>

<p>The positions of the husbands and wives do not need to alternate since the wives may sit in adjacent seats.  For instance, the seating arrangement $H_1W_1H_2W_2W_3H_3$ is permissible since no two of the husbands sit in adjacent seats.</p>

<blockquote>
  <p>A group of $6$ people consisting of $3$ married couples are to be seated together in a straight row. How many different ways are there of seating the $6$ people if no husband is to sit next to another husband?</p>
</blockquote>

<p><strong>Method 1:</strong>  We arrange the wives, then insert the husbands among the wives.</p>

<p>Arrange the husbands in some order, say alphabetically, then hand each of them a chair.</p>

<p>Lay out three chairs for the wives.  The wives can arrange themselves in those chairs in $3!$ ways.  For each such arrangement arrangement, there are four spaces in which we can place the husbands, two between successive wives and two at the ends of the row.
$$\square W_1 \square W_2 \square W_3 \square$$
To ensure that the husbands do not sit in adjacent positions, they must choose three of these four spaces.  The first husband has four choices where to place his chair, the second has three, and the third has two.  Hence, the number of permissible seating arrangements is 
$$3! \cdot 4 \cdot 3 \cdot 2 = 3!4!$$ </p>

<p><strong>Method 2:</strong>  We use the <a href=""https://en.wikipedia.org/w/index.php?search=Inclusion-Exclusion+Principle&amp;title=Special%3ASearch&amp;fulltext=Search"" rel=""nofollow noreferrer"">Inclusion-Exclusion Principle</a>.</p>

<p>The six people can be arranged in $6!$ ways.  From these, we must exclude those arrangements in which two husbands sit in adjacent seats.</p>

<p>A pair of husbands sit in adjacent seats:  There are $\binom{3}{2}$ ways to choose a pair of husbands who sit in adjacent seats.  This gives us five objects to arrange, the block consisting of the pair of husbands and the other four people.  The objects can be arranged in $5!$ orders.  Within the block, the two husbands can be arranged in $2!$ orders.  Hence, there are 
$$\binom{3}{2}5!2!$$
arrangements in which two husbands sit in adjacent seats.</p>

<p>However, 
$$6! - \binom{3}{2}5!2! = 0$$
Clearly, we have subtracted too much since the seating arrangement stated above shows it is possible to seat the three couples so that no two husbands sit in adjacent seats.</p>

<p>The reason we have subtracted too much is that we have subtracted those cases in which all three husbands sit together twice, once when we counted the leftmost and middle husband as our designated pair and once when we counted the rightmost and middle husband as our designated pair.  We only want to subtract them once, so we need to add those cases to the total.</p>

<p>Two pairs of adjacent husbands:  This can only occur if all three husbands sit in a block.  This gives us four objects to arrange, the block of three husbands and the three wives.  The four objects can be arranged in $4!$ ways.  Within the block, the husbands can be arranged in $3!$ ways.  Hence, there are 
$$4!3!$$
seating arrangements with two pairs of adjacent husbands.</p>

<p>Total:  By the Inclusion-Exclusion Principle, the number of permissible seating arrangements is 
$$6! - \binom{3}{2}5!2! + 4!3!$$</p>
"
"2397133","2397587","<p>Hint: Suppose $f$ is continuously differentiable on $[-1,2].$ Then for any $x\in [0,1]$ and $h\in (0,1],$</p>

<p>$$\tag 1 \frac{f(x+h)-f(x-h)}{2h} = f'(c(x,h)),$$</p>

<p>by the MVT. Here $c(x,h)$ lies between $x-h$ and $x+h.$ Thus a dominating function for the left side of $(1)$ is just the constant $\sup_{[-1,2]}|f'|.$</p>
"
"2397143","2397178","<p>Given a triangle $ABC$ with sides $a$ opposite angle $A$ etc. Let the altitude from $B$ to side $b$ intersect at $D$.  Then via right triangle $ABD$ we see the altitude, $BD$ is equal to $c\sin A$ and right triangle $BDC$ that $BD$ is equal $a\sin C$.</p>

<p>So if $A,B,C = 40, 60, 80$ respectively then the three altitudes will be $d = c \sin 40 = a\sin 80$ and $e = b \sin 40 = a\sin 60$ and $f = c \sin 60 = b \sin 80$.</p>

<p>We know $\sin 80 &gt; \sin 60 &gt; \sin 40$ so $c &gt; a; b&gt;a; c &gt; b$ so $c &gt;b&gt;a$.  So $e = b\sin 40 &lt; d = c\sin 40 &lt; f= c\sin 60$.  So $K = \frac {shortest}{longest} = \frac ef = \frac {b\sin 40 = a\sin 60}{c\sin 60 = b\sin 80} = \frac {\sin 40}{\sin 80}$.</p>
"
"2397182","2397393","<p>When triangle $\triangle ABC$ is acute, the circumcenter $O$ is the point that maximize the minimal distance.</p>

<p><a href=""https://i.stack.imgur.com/MpSyH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MpSyH.png"" alt=""An acute triangle""></a></p>

<p>Let $R = |OA| = |OB| = |OC|$ be the circumradius.</p>

<p>Let $A', B', C""$ be the mid-points of sides $BC$, $CA$ and $AB$ respectively.  </p>

<p>Draw six lines segments from $O$ to $A,B,C, A',B',C'$. They will divide triangle $ABC$ into six right angled triangles $\triangle OAC'$, $\triangle OC'B$, $\triangle OBA'$, $\triangle OA'C$, $\triangle OCB'$ and $\triangle OB'A$.</p>

<p>Consider the two triangle $\triangle OAC'$ and $\triangle OB'A$ which are sharing the segment $OA$.<br>
Since $\angle AC'O = \angle OB'A = 90^\circ$, both triangles lie completely inside the circle with $OA$ as diameter (the green circle in above diagram). Since this circle is lying within the circle with $A$ as center passing through $O$ (the blue circle), every point $P$ belongs to triangle $\triangle OAC'$ and $\triangle OB'A$ satisfies
$$|PA| \le |OA| = R
\quad\implies\quad \min(|PA|, |PB|, |PC|) \le R$$</p>

<p>Similar thing happen to the two pairs of triangles $\triangle OC'B, \triangle OBA'$ and $\triangle OA'C, \triangle OCB'$.<br>
From this, we can conclude</p>

<p>$$\min(|PA|,|PB|,|PC|) \le R\quad\text{ for all points } P \text{ in triangle }ABC$$</p>

<p>When $P = O$, above inequality becomes an equality. This implies </p>

<p>$$\max_{P \in \triangle ABC}\left(\min(|PA|,|PB|,|PC|)\right) = R$$</p>

<p>and $O$ is the point that maximize the minimal distance.</p>

<p><strong>Update</strong></p>

<p>Let us switch to case where triangle $ABC$ is obtuse.</p>

<p><a href=""https://i.stack.imgur.com/HXwgV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HXwgV.png"" alt=""An obtuse triangle""></a></p>

<p>WOLOG, we will assume $a = |BC| &gt; b = |CA| \ge c = |AB|$. </p>

<p>Construct the perpendicular bisector of side $CA$, let it intersect $BC$ at $B''$.<br>
Construct the perpendicular bisector of side $AB$, let it intersect $BC$ at $C''$.</p>

<p>These two perpendicular bisectors split $\triangle ABC$ into 3 regions, two triangles
$\triangle BC''B$ (the red one), $\triangle CB'B''$ (the green one) and a pentagon $AC'C''B''B'$ (the blue one). </p>

<p>For any $P$ inside $\triangle ABC$, it is easy to see:</p>

<p>$$
\begin{array}{rcl}
P \in \triangle BC''B &amp; \implies   &amp; |PB| \le |PA|,|PC|\\
P \in AC'C''B''B' &amp; \implies &amp; |PA| \le |PB|, |PC|\\
P \in \triangle CB'B'' &amp; \implies &amp; |PC| \le |PA|, |PB
\end{array}
$$</p>

<p>Since the distance functions $|PA|$, $|PB|$ and $|PC|$, their maximum will be achieved at the vertices.</p>

<p>Since both  $\triangle BC''C'$ and $\triangle CB'B''$ are right angled triangles,
we have $|BC''| &gt; |BC'|$ and $|CB''| &gt; |CB'|$. Together with the obvious equalities, $|AC'| = |BC'|$, $|AC''| = |BC''|$, $|AB'| = |CB'|$, $|AB''| = |AB''|$, there are only two possiblities. The maximum minimum distance 
is either</p>

<ul>
<li>achieved at $B''$ with value $|AB''| = |CB''| = \frac{b}{2\sin\gamma}$</li>
<li>or achieved at $C''$ with value $|AC''| = |BC''| = \frac{c}{2\sin\beta}$.</li>
</ul>

<p>where $\beta = \angle C'BC''$ and $\gamma = B''CB'$. </p>

<p>Using sine rule, we have</p>

<p>$$|CB''| : |BC''| = \frac{b}{2\sin\gamma} : \frac{c}{2\sin\beta} = \frac{b}{c} : \frac{c}{b}$$</p>

<p>Since $b \ge c$, we have $|CB''| \ge |BC''|$. This confirms Joe Knapp's conjecture. The maxmium minimum distance is achieved at $B''$, the intersection of the perpendicular bisector of the second longest side with the longest side.</p>

<p>Using the relation $c = 2R\sin\gamma$, one find the maximum minimal distance
equals to $\displaystyle\;\frac{bR}{c}\;$ in this case.</p>
"
"2397187","2397330","<p>To give a simple example, consider the vector space $(\Bbb R\times \Bbb R, +, \cdot)$ with the usual entry wise sum and scalar multiplication. </p>

<p>We can write any vector $\psi$, say $(45,68)$ as a sum of basis vectors $\phi_1=(1,0), \phi_2=(3,4)$, namely, $\psi=-6 \cdot\phi_1 +17\cdot \phi_2$. </p>

<p>The same applies to any kind of object in a vector space, including functions.</p>
"
"2397190","2397208","<p>By symmetry, we assume $x\leq y$ throughout this discussion.</p>

<p>Cases:</p>

<p>If $x=1$, then the equation simplifies to $\frac{1}{2}+\frac{1}{y}=\frac{1}{z}$.  Then $z=1$ since otherwise the LHS is larger than the RHS.  When $z=1$, we have $y=2$.</p>

<p>If $x=2$, then the equation simplifies to $\frac{1}{y}=\frac{1}{z}$, which can be solved for any $y$ and $z$.</p>

<p>If $x=3$, then we have $3\leq y&lt;5$ as possible left-hand-sides (each of which can be checked).</p>

<p>If $x=4$, then $y\geq 4$ and the LHS is less than the RHS and there are no solutions.</p>
"
"2397194","2397201","<p>It's impossible to have $a\geq \varepsilon$ for every $\varepsilon&gt;0$. Indeed, take $\varepsilon:=\max\{1,a+1\}$. Then $\varepsilon&gt;0$ and $\varepsilon\geq a+1&gt;a$.</p>

<p>I think you mean to ask if we have $a\geq \varepsilon$ <strong>for some</strong> $\varepsilon&gt;0$, then can we conclude $a&gt;0$? The answer to this question is ""yes"" by the transitivity of ""$&lt;$"":
$$
0 &lt; \varepsilon \leq a.
$$</p>
"
"2397198","2397245","<p>It is also called the <a href=""https://en.wikipedia.org/wiki/Residue_at_infinity"" rel=""nofollow noreferrer"">residue at infinity</a> which is (up to $1/c_d$) polynomial in the coefficients of the denominator.</p>

<p>More precisely :
Let $ P(z) = \sum_{n=0}^d c_n z^n, Q(z) =\sum_{n=0}^{d-1}  \frac{c_n}{c_d} z^{d-n} = P(1/z) \frac{z^d}{c_d}-1$. Then $$\sum_{P(\alpha) = 0} Res(\frac{H(z)}{P(z)},\alpha) = \lim_{R \to \infty}\frac{1}{2i\pi}\int_{|z| = R} \frac{H(z)}{P(z)}dz=\lim_{\epsilon \to 0}\frac{1}{2i\pi}\int_{|z| = \epsilon} \frac{H(1/z)}{z^2 P(1/z)}dz$$ 
$$=\lim_{\epsilon \to 0}\frac{1}{2i\pi}\int_{|z| = \epsilon} \frac{c_d z^{d-2} H(1/z)}{1+Q(z)}dz=
\sum_{m=0}^\infty \frac{1}{2i\pi} \int_{|z| = \epsilon} c_d z^{d-2} H(1/z)(-Q(z))^mdz
\\=\sum_{m=0}^{deg(H)+1-d} Res(c_d z^{d-2} H(1/z)(-Q(z))^m,0)
$$ 
for $H$ and $d$ fixed this last expression is polynomial in the coefficients of $Q$.</p>
"
"2397200","2397209","<p>Note that
$$ f^{-1}(b) = \left\{ x\in X : f(x) \in b \right\}, $$
where $b$ is some measurable set from the $\sigma$-algebra $B$.  In this case, $f^{-1}$ is <em>not</em> the inverse function (indeed, an inverse function may not even exist, which might be the cause of your confusion).  In English, we are saying that $f$ is an $(A,B)$-measurable function if the preimage of each measurable set is measurable.</p>

<p>The preimage <em>always</em> makes sense, though it can be empty.  For example, if $f : \mathbb{R} \to \mathbb{R}$ is defined by $f(x) = x^2$, then
$$ f^{-1}([-2,-1]) = \emptyset. $$</p>
"
"2397204","2397397","<p>Without seeing the OP's calculation for Descartes' circle theorem it's not possible to say where the OP went wrong. Here's my calculation:</p>

<p>Descartes' theorem gives</p>

<p>\begin{align}
\frac{1}{R} &amp; =\frac{1}{2}+\frac{1}{2}+\frac{1}{3}+2\sqrt{\frac{1}{2}\cdot\frac{1}{2}+\frac{1}{2}\cdot\frac{1}{3}+\frac{1}{3}\cdot\frac{1}{2}}\\
&amp; =\frac{4}{3}+2\sqrt{\frac{7}{12}}\\
&amp; =\frac{4}{3}+\frac{1}{3}\sqrt{21}\\
&amp; =\frac{\sqrt{21}+4}{3}
\end{align}</p>

<p>and from here it's easy to show that this is the same as the OP's answer.</p>

<p>Note that we use the positive value from Descartes' theorem to get the radius of the small inscribed circle. Taking the negative value would give the radius of the large circumscribed circle.</p>
"
"2397212","2397219","<p>This is the same as solving $x^2\equiv1\pmod{15}$.</p>

<p>You tried $x=1$, you found that worked.
I'll try $x=2$, then $x^2=4\not\equiv1\pmod{15}$.
I'll now try $x=3$, then $x^2=9\not\equiv1\pmod{15}$. 
I'm losing patience.</p>

<p>Maybe
you can go further than me, and perhaps find something interesting.</p>
"
"2397220","2397406","<p>It is unbounded because if you solve the system of inequalities which means that ALL of them must be satisfied you get the region $\mathcal{R}$ of the plane where</p>

<p>$(y\geq 8-2x) \land (y\geq 5-\dfrac{x}{2})\land (x\geq 0 )\land (y\geq 0)$</p>

<p>As you can see all the results contain $\geq$  which means that, even if we want to be cautious, we can take the largest values for $x$ and for $y$ which satisfy ALL the previous inequalities for instance $\mathcal{S}=(x\geq 10;\;y\geq 10)$  and we have a subregion of the plane $\mathcal{S}\subset \mathcal{R}$ which is unbounded, so to a greater extent is unbounded $\mathcal{R}$</p>

<p>Hope this helps</p>
"
"2397221","2397244","<p>As I said in the comments, your proof works but it can be shortened. There are many other possible proofs, here is one:</p>

<p>Let $(w_n)_{n\ge 1}$ be a sequence of independent vectors in $W$, let $W_n = \text{span}(w_1,\ldots,w_n)$. Then ${\cal{L}}(V, W_n) \subset {\cal{L}}(V, W)$, but $\dim({\cal{L}}(V, W_n)) = n \dim(V)$.</p>
"
"2397227","2397233","<p>You correctly note that a finite geometric series can be computed as
$$ \sum_{i=0}^{n} k^i = \frac{1-k^{n+1}}{1-k}.$$
Note that the exponent in this series is <em>positive</em> $i$. (As an aside, do you know how to show that this formula is correct, or is it something that you have memorized?  If the latter, it would be a good idea to try to figure it out.)</p>

<p>Now, to evaluate
$$ 1 + 2^{-1} - 2^{-2} + \dotsb + 2^{-10}, $$
we can write
$$ 1 + 2^{-1} - 2^{-2} + \dotsb + 2^{-10} = \sum_{i=0}^{10} (-2)^{-i}. $$
In this sum, the exponent is <em>negative</em>, so we cannot directly use the formula.  But, using the fact that
$$ (-2)^{-i} = \left(-\frac{1}{2}\right)^{i}, $$
we can rewrite the series.  Then the computation looks like this:
$$ \sum_{i=1}^{10} (-2)^{-i}
= \sum_{i=1}^{10} \left( -\frac{1}{2} \right)^i
= \frac{1-\left( -\frac{1}{2} \right)^{11}}{1 - \left(-\frac{1}{2}\right)}
= \frac{2^{11}+1}{2^{11}}\cdot\frac{2}{3}
= \frac{2^{11}+1}{3\cdot 2^{10}}
= \frac{2049}{3072}
= \frac{683}{1024}. $$</p>
"
"2397228","2397291","<p>Note that we can write</p>

<p>$$\begin{align}
\left|\int_0^1 \frac{f(hx)}{x^2+1}\,dx-f(0)\frac\pi4\right|&amp;=\left|\int_0^1 \frac{f(hx)-f(0)}{x^2+1}\,dx\right|\\\\
&amp;\le \int_0^1 \frac{|f(hx)-f(0)|}{x^2+1}\,dx\\\\
&amp;
\le\frac\pi4 \sup_{x\in [0,1]}|f(hx)-f(0)|
\end{align}$$</p>

<p>For any $\epsilon&gt;0$, there exists a number $\eta&gt;0$ such that $|f(hx)-f(0)|&lt;4\epsilon/\pi$ whenever $|hx|&lt;\eta$.  </p>

<p>Since $x\in [0,1]$, then whenever $|hx|&lt;|h|&lt;\eta$, we have $|f(hx)-f(0)|&lt;4\epsilon/\pi$.  </p>

<p>Hence, $\sup_{x\in [0,1]}|f(hx)-f(0)|&lt;4\epsilon/\pi$ whenever $|h|&lt;\eta$.</p>

<p>Putting it all together, we have for $|h|&lt;\eta$,</p>

<p>$$\left|\int_0^1 \frac{f(hx)}{x^2+1}\,dx-f(0)\frac\pi4\right|&lt;\epsilon$$</p>
"
"2397240","2397257","<blockquote>
  <p>How many ways can six people out of the group of $10$ be arranged for the photograph if Ken and Karen must be in the photograph.</p>
</blockquote>

<p>Since Ken and Karen are in the photograph, we must select four of the other eight people.  The six selected people can then be arranged in $6!$ ways.  Hence, the number of permissible arrangements is 
$$\binom{8}{4}6!$$</p>
"
"2397249","2397477","<p>A lot of algebraic geometry is developed by analogy with differential geometry.</p>

<p>In differential geometry, we speak of <em>manifolds</em> rather than varieties. The main feature of the definition of a manifold is that it is covered by <em>coordinate charts</em>: open sets $U$ together with a (smooth) homeomorphism $x : U \to \mathbb{R}^n$ (or alternatively, to an open subset of $\mathbb{R}^n$).  The individual components of $x$ are called coordinates.</p>

<p>As an example, consider the pair of functions $(x,y)$ defined on the Euclidean plane after choosing coordinate axes.</p>

<hr>

<p>The situation here is not too different. On $\mathbb{A}^n_K$, affine $n$-space over a field $K$, the standard 'coordinate chart' has coordinates given by the $n$ variables. Since we're doing algebra, the 'functions' we can make out of these are polynomials.</p>

<p>The term ""coordinate function"" here refers to a function we can build out of coordinates &mdash; much like the ring of smooth continuous real-valued functions on a manifold. The coordinate ring is the ring of all such functions. (although, I don't think the phrase ""coordinate function"" tends to be used this way in differential geometry)</p>

<p>For subvarieties $V \subseteq \mathbb{A}^n_K$, we understand coordinate functions to be restrictions of coordinate functions on $\mathbb{A}^n_K$, thus we use the quotient ring of the coordinate ring of $\mathbb{A}^n_K$.</p>

<p>Furthermore, since every element of $K[V]$ can be viewed as the image of $t$ under a unique $K$-homomorphism $K[t] \to K[V]$, we can actually identify coordinate functions on $V$ with algebraic maps $V \to \mathbb{A}^1_K$.</p>

<p>(if we're doing absolute geometry, the target would instead be $\mathbb{A}^1_\mathbb{Z} = \operatorname{Spec} \mathbb{Z}[t]$)</p>
"
"2397252","2397286","<p>The phrases</p>

<blockquote>
  <p>real Banach algebra</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>complex Banach algebra</p>
</blockquote>

<p>are <strong>very important</strong>. In general, we define a Banach algebra <em>over a certain field</em>, and the choice of field matters. You can't ignore the specific field, here: $\mathbb{H}$ is a Banach algebra over $\mathbb{R}$, but not over $\mathbb{C}$.</p>
"
"2397255","2397272","<p>Green's Theorem allows us to compute line integrals on simple closed curves by computing a double integral on the plane region which is enclosed by the aforementioned curve, and vice-versa. 
$$\iint_D\left(\frac{\partial N}{\partial x}-\frac{\partial M}{\partial y}\right)dA$$
represents a double integral over the surface $D$ which is enclosed by the curve $C$ in the $xy$-plane. This gives you an alternative way to compute the line integral
$$\oint_C\left(M~dx+N~dy\right)$$
by using a double integral. A double integral is analogous to an integral over the real line (e.g. $\int_a^bf(x)dx$). Where an integral $\int_a^bf(x)dx$ is the area under $f$ from $a$ to $b$, the integral $\iint_Df(x,y)dA$ is the volume under $f$ on the 2D-region $D$.</p>
"
"2397263","2397357","<p>Your middle morphism ($k_\star$) is not quite right. The correct morphisms are:
$$ 0 \to H_1(S^1) \to \mathbb Z \oplus \mathbb Z \overset{\begin{bmatrix} a \\ b\end{bmatrix} \mapsto \begin{bmatrix} a + b \\ - a - b\end{bmatrix}}\longrightarrow \mathbb Z \oplus \mathbb Z \overset{\begin{bmatrix} c \\ d\end{bmatrix} \mapsto c + d}{\longrightarrow}\mathbb Z \to 0$$
So the kernel of $k_\star$ has a single generator, namely, $(1,-1)$, hence $H_1(S^1) \cong \mathbb Z$.</p>

<p>But how can we be sure that $k_\star$ is as I've written it? I'll explain...</p>

<ul>
<li><p>Suppose $U$ and $V$ are your two open arcs covering your $S^1$, i.e. $U$ and $V$ are your two $B^1$'s. Then $U \cap V$ has two path-connected components, so $$H_0(U \cap V) \cong \mathbb Z \oplus \mathbb Z,$$ as you correctly identified. (You're also right to observe that $U \cap V$ is homotopy-equivalent to $S^0$.) Anyway, the generator $(1,0) \in H_0(U \cap V) $ is the homology class of any point in the first  connected component of $U \cap V$, and the generator $(0,1) \in H_0(U \cap V) $ is the homology class of any point in the second connected component.</p></li>
<li><p>$U$ has a single path-component, so we simply have $$H_0(U) \cong \mathbb Z.$$ We'll denote the generator of $H_0(U)$ as $1 \in \mathbb Z$. This generator is the homology class of any point in $U$.</p></li>
<li><p>Under the inclusion map $U \cap V \hookrightarrow U$, a point in the first path-component of $U \cap V$ simply becomes a point in $U$. A point in the second path-component of $U \cap V$ also becomes a point in $U$. Therefore, the inclusion map $U \cap V \hookrightarrow U$ includes the map
$$ H_0(U \cap V) \to H_0(U)$$
given by
$$ \begin{bmatrix} 1 \\ 0\end{bmatrix} \mapsto 1, \ \ \ \ \ \ \ \ \ \ \ \begin{bmatrix} 0 \\ 1\end{bmatrix} \mapsto 1..$$</p></li>
<li><p>The same remarks apply to $V$: the inclusion map $U \cap V \hookrightarrow V$, includes the map
$$ H_0(U \cap V) \to H_0(V)$$
given by
$$ \begin{bmatrix} 1 \\ 0\end{bmatrix} \mapsto 1, \ \ \ \ \  \ \ \ \ \ \ \begin{bmatrix} 0 \\ 1\end{bmatrix} \mapsto 1.$$</p></li>
<li><p>By the definition in your textbook, the map $$k_\star : H_0(U \cap V) \to H_0(U) \oplus H_0(V)$$ is obtained by taking the direct sum of the two inclusion-induced maps that I described above in the previous two bullet points. There is one caveat, which is that we must insert a minus sign into the action of the second map (the one involving $H_0(V)$); this extra minus sign is a part of how $k_\star$ is defined in the textbook. So the action of $k_\star$ on our generators is:
$$ \begin{bmatrix} 1 \\ 0\end{bmatrix} \mapsto \begin{bmatrix} 1\\ - 1\end{bmatrix} , \ \ \ \ \ \ \  \ \ \ \ 
 \begin{bmatrix} 0\\ 1\end{bmatrix} \mapsto \begin{bmatrix} 1\\ - 1\end{bmatrix} , \\
$$
which matches what I wrote originally.</p></li>
</ul>

<hr>

<p>For completeness, I'll also explain how to work out the action of $j_\star$.</p>

<ul>
<li><p>The full space $S^1 = U \cup V$ has a single path-component, so
$$ H_0(S^1) \cong \mathbb Z$$
The generator $1 \in H_0(S^1)$ is the homology class of any point in $S^1$.</p></li>
<li><p>Under the inclusion $U \hookrightarrow S^1$, a point in $U$ simply becomes a point in $S^1$. So the map
$$ H_0(U) \to H_0(S^1) $$
induced by this inclusion map is given by
$$ 1 \mapsto 1.$$</p></li>
<li><p>Similarly, the inclusion map $V \hookrightarrow S^1$ induces the map
$$ H_0(V) \to H_0(S^1) $$
defined by
$$ 1 \mapsto 1.$$</p></li>
<li><p>By definition, the map
$$ j_\star : H_0(U) \oplus H_0(V) \to H_0(S^1)$$
is given by applying the two inclusion-induced maps described in my previous two bullet-points to $H_0(U)$ and $H_0(V)$ separately, then adding up the two answers. In other words, it is given by
$$ \begin{bmatrix} 1 \\ 0 \end{bmatrix} \mapsto 1, \ \ \ \ \ \ \ \ \ \begin{bmatrix} 0 \\ 1 \end{bmatrix} \mapsto 1.$$</p></li>
</ul>

<p>At this point, it is perhaps worth checking that ${\rm Im}(k_\star) = {\rm Ker}(j_\star)$ - and this is true, because both ${\rm Im}(k_\star)$ and ${\rm Ker}(j_\star)$ are generated by $(1, -1) \in H_0(U) \oplus H_0(V)$. We should also check that ${\rm Im}(j_\star) = H_0(S^1) $, and indeed this is true as well.</p>
"
"2397271","2397323","<p>Since the Sellmeier coefficients are usually quoted in square microns (not square nanometers) I think you have a units error.  </p>

<p>Try using the wavelength as $0.5876$ microns, which gives an answer of $1.517$ by my slide-rule calculation.</p>
"
"2397276","2397293","<p>AP only: 
$$(1^2-0^2) + (3^2-2^2)+\ldots + (2003^2-2002^2)$$
$$=1\cdot 1 + 1\cdot 5 + \ldots + 1\cdot 4005$$
$$=1+5+9+\ldots + 4005 $$
$$= \ldots$$</p>
"
"2397277","2397301","<p>Observe that
$$
\int_{-\infty}^\infty \frac{dx}{1+x^2} = \pi
$$
and suppose that for every measurable $A\subseteq\mathbb R$
$$
\Pr(Y\in A) = \int_A \frac{dx/\pi}{1+x^2}. \tag 1
$$
This is the standard Cauchy distribution. Let $Z= 1/Y.$ Then
\begin{align}
&amp; f_Z(z) \, dz = d \Pr(Z\le z) = d \left. \begin{cases} \Pr( Y \le 1/z ) &amp; \text{if } z&lt;0, \\  \frac 1 2 + \Pr( Y\ge 1/z ) &amp; \text{if } z&gt;0, \end{cases} \right\} \\[10pt]
= {} &amp; d \left. \begin{cases} \displaystyle \int_{-\infty}^{1/z} \frac {dx/\pi}{1+x^2} &amp; \text{if } z&lt;0, \\[10pt] \displaystyle \frac 1 2 + \int_{1/z}^\infty \frac{dx/\pi}{1+x^2} &amp; \text{if } z&gt;0,   \end{cases} \right\} = \frac{dz/\pi}{1+z^2}.
\end{align}
Thus the distribution of $Z=1/Y$ is the same as that of $Y,$ and the density at $0$ is not $0.$</p>

<p><b>Here is a geometric way of looking at it:</b></p>

<p>Beams of light shine north, south, east, and west from the point $(0,1).$ Then we make the object on which they are mounted rotate at a uniform rate. The proportion of the time that the initially north-south beams shine on the set $A$ on the $x$-axis is then given by the integral $(1)$ above. By symmetry, the same is true of the initially east-west beams. But the $x$-coordinate of the point on the $x$-axis illuminated by the initially east-west beam is  $-1/Y$ when the point illuminated by the initially north-south beam is $Y.$</p>
"
"2397294","2397304","<p>Square root of a number $a$ is defined as ""number, which when multiplied by itself, gives $a$"", also $\left(\sqrt{a}\right)^2 = a$.</p>

<p>We can clearly see that if $\sqrt{a}\times\sqrt{a} = a$ and $\sqrt{b}\times\sqrt{b} = b$, then by multiplying both sides we get $\sqrt{a}\times\sqrt{a}\times\sqrt{b}\times\sqrt{b} = ab$.</p>

<p>But we already know, that $\sqrt{ab}\times\sqrt{ab}=ab$, from the definition.</p>

<p>And now it is obvious - $\sqrt{a}\times\sqrt{b}=\sqrt{ab}$.
Similarly for other roots.</p>
"
"2397303","2397508","<p>I understand why you're confused!  Perhaps Bob explains this in the lectures (I haven't watched the videos), but from the PDF notes and homework alone I don't see how you could guess what's going on here; and it's actually a really subtle thing that it took me a long time to even vaguely understand.</p>

<p>There are a lot of different ways of presenting logic as a formal system.  Three of the most common are (1) Hilbert systems, (2) natural deduction, and (3) sequent calculus.  Bob disparages Hilbert systems a bit on p9 of the week 1 notes; they do have a conceptual explanation in terms of combinatory logic and closed categories, but they're not relevant to the point here.  Natural deduction and sequent calculus both work with ""entailments"" $\Gamma\vdash A$ separate from implication, making explicit a context of ""hypotheses"", and both divide the logical rules into ways to ""use"" things and ways to ""prove"" things; the differences are in the details.</p>

<p>The ""prove"" rules are called ""introduction"" rules in natural deduction and ""right"" rules in sequent calculus, but in both cases they look basically the same: they introduce a new connective in the <em>consequent</em> of the <em>conclusion</em>.  (In $\Gamma\vdash A$ the $\Gamma$ is the <em>antecedents</em> or <em>context</em> or <em>hypotheses</em> while $A$ is the <em>consequent</em>, whereas in a rule $\frac{J_1 \quad J_2}{J_3}$ the judgments $J_1,J_2$ above the line are <em>premises</em> and the $J_3$ below the line is the <em>conclusion</em>.)  By contrast, the ""use"" rules of natural deduction are called ""elimination"" rules and ""get rid of"" a connective in the <em>consequent</em> of a <em>premise</em>, whereas the ""use"" rules of sequent calculus are called ""left"" rules and introduce a new connective as an <em>antecedent</em> of the <em>conclusion</em>.  Thus for instance the elimination rule for $\vee$ in natural deduction is</p>

<p>$$\frac{ \Gamma,A\vdash C \qquad \Gamma,B\vdash C \qquad \Gamma\vdash A\vee B }{\Gamma\vdash C} $$</p>

<p>whereas the left rule for $\vee$ in sequent calculus is</p>

<p>$$\frac{\Gamma,A\vdash C \qquad \Gamma,B\vdash C}{\Gamma,A\vee B \vdash C}$$</p>

<p>Thus, in natural deduction, to prove $C$ using a ""case split"" on $A\vee B$, you do three things: (1) prove $A\vee B$, (2) prove $C$ assuming $A$, and (3) prove $C$ assuming $B$.  Whereas in sequent calculus, to prove $C$ using a case split on $A\vee B$, you have to <em>already have $A\vee B$ as a hypothesis</em>, and then you do only (2) and (3).</p>

<p>It follows that the ""transitivity"" or ""using lemmas"" rule (which is also called ""composition"", ""substitution"", and ""cut"") has a different status in the two cases.  In natural deduction, transitivity is <em>built into</em> most of the rules: e.g. if you have a lemma that proves $A\vee B$, there's hardly anything to prove when claiming that such a lemma can be used in the middle of another proof to justify a case split, since the primitive rule for ""using a $\vee$"" says that you get to give a proof of $A\vee B$, so you can just slot in the proof of the lemma.  In sequent calculus this is not the case: to justify using a lemma for $A\vee B$, you have to inspect the proofs of the lemma and of the theorem that wants to use the lemma and match up the corresponding parts inductively; this is called <em>cut elimination</em> or <em>cut admissibility</em>.</p>

<p>Overall, this difference means that sequent calculus is more ""spartan"": there are fewer proofs and less redundancy.  In natural deduction, you can do all sorts of detours in a proof, introducing a new statement and then immediately eliminating it.  Sequent calculus forbids that, mandating that new connectives be introduced (never eliminated) as you pass <em>downwards</em> through the proof; this has many nice metatheoretic consequences like the ""subformula property"".  There is a formal way of ""eliminating the redundancy"" in natural deduction, called <em>normalization</em>: basically whenever you eliminate something that was introduced, you can excise that part of the proof.  Such normalization corresponds (roughly) to the more involved process of cut-elimination in sequent calculus, and the ""normal"" natural deduction proofs correspond (again, roughly) to the proofs of sequent calculus.  Under the Curry-Howard correspondence to $\lambda$-calculus, a natural deduction proof can contain ""redexes"" such as $(\lambda x.M)(N)$, which can be $\beta$-reduced to $M[N/x]$ (i.e. $M$ with $N$ substituted for $x$), whereas a sequent calculus produces the normal forms directly without allowing any redexes.</p>

<p>The solution given by Bob to the exercise you're wondering about, then, is the solution that you would have to give in a <em>sequent calculus</em>, whereas the simple argument you suggest works only in a <em>natural deduction</em>, because it involves an elimination rule acting on a connective in a premise.  So it's no wonder you were confused, because the notes (at least) don't say anything about sequent calculus, presenting only the rules of natural deduction!  I don't know what Bob intended here; perhaps it was made clearer in the lectures or by personal communication to the students.</p>

<p>Finally, the kind of induction Bob is doing that you may not be used to is called <a href=""https://en.wikipedia.org/wiki/Structural_induction"" rel=""nofollow noreferrer"">structural induction</a>.  Induction on natural numbers is just one kind of induction; you can do induction on any kind of ""inductively generated"" structure; here he is inducting on proofs, which are generated inductively from inference rules.</p>
"
"2397305","2397317","<p>The entire progression is $\frac{a}{1-r}$, as we know. Now $a(1+r)=12$, so that
$$
\frac{a}{1-r}=\frac{a(1+r)}{1-r^2}=\frac{12}{1-r^2}.
$$
Now use that each term is twice the sum of all the terms that follow it, to conclude that $r=\frac{1}{3}$. Hence the entire progression is
$$
\frac{12}{1-\frac{1}{9}}=\frac{27}{2}.
$$</p>
"
"2397314","2397318","<p>You are so close. Note that
\begin{align*}
\frac{\sin(x)+\sin(2x)+\cdots+\sin(kx)}{x}
&amp;= \frac{\sin(x)}{x}+\frac{\sin(2x)}{x}+\cdots+\frac{\sin(kx)}{x} \\
&amp;= \frac{\sin(x)}{x}+2\frac{\sin(2x)}{2x}+\cdots+k\frac{\sin(kx)}{kx} \\
&amp;\to 1 + 2 + \cdots + k \\
&amp;= \frac{k(k+1)}{2}
\end{align*}
as $x\to0$.</p>

<p>I suspect you may not have been able to finish because you didn't recognize the identity
$$
1 + 2 + \cdots + k = \frac{k(k+1)}{2}.
$$
This identity has a very cute proof.
Set $S:=1+2+\cdots+k$. Adding
\begin{align*}
1 + 2 + \cdots + k &amp;= S \\
k + (k-1) + \cdots + 1 &amp;= S \\
\end{align*}
gives
\begin{align*}
\underbrace{(k+1)+(k+1)+\cdots+(k+1)}_{k\ \text{times}} = 2S. \\
\end{align*}
Therefore $k(k+1)=2S$ and consequently
$$1+2+\cdots+k = S = \frac{k(k+1)}{2}.$$</p>
"
"2397315","2397329","<p>Everything looks good.  </p>

<p>Once one has shown$^\dagger$ that $\mathbb{Q}(\sqrt{3} + \sqrt{2}) = \mathbb{Q}(\sqrt{3}, \sqrt{2})$, it becomes more obvious that $f(x) = x^4 -5x^2 + 6 = (x^2-2)(x^2-3)$ splits in this field.  You might want to discuss why this is the <em>smallest</em> field inside which $f$ splits, but this isn't difficult.</p>

<p>Next, as you've argued, $\mathbb{C}$ cannot be a splitting field for any polynomial: we'll always have $\mathbb{Q} \Big( \{ \alpha_k \}_{k=1}^n \Big) \subsetneq \mathbb{C}$ where the $\alpha_k$'s are the roots of the polynomial, so $\mathbb{C}$ cannot be the smallest field inside which that polynomial splits.</p>

<hr>

<p>$\dagger$ See arguments <a href=""https://math.stackexchange.com/questions/1311598/show-mathbbq-sqrt2-sqrt3-is-a-simple-extension-field-of-mathbbq/1311614#1311614""><strong>here</strong></a> or <a href=""https://math.stackexchange.com/questions/2199743/is-the-extension-mathbbq-sqrt2-sqrt3-simple/2199761#2199761""><strong>here</strong></a> for further discussion.</p>
"
"2397337","2397345","<p>A homomorphism from $R=\Bbb C[x,y]/(xy-1)$ to $\Bbb C$ is a homomorphism
$\Phi$ from $\Bbb C[x,y]$ to $\Bbb C$ with $\Phi(xy-1)=0$. Each homomorphism $\Phi:\Bbb C[x,y]\to \Bbb C$ has has the form
$\Phi_{a,b}:f(x,y)\to f(a,b)$ where $a$, $b\in\Bbb C$. Then
$\Phi_{a,b}(xy-1)=ab-1$. So $\Phi_{a,b}$ defines a homomorphism
on $R$ iff $ab=1$, etc.</p>
"
"2397355","2397399","<p>As Daniel Fischer noted, the inequality $|\hat f(n)|\ge a_n$ cannot hold for all $n$ if $(a_n)$ is not square-summable, since $\sum |\hat f(n)|^2&lt;\infty$ for every function in $L^2([0,2\pi])$, in particular for every continuous function on $[0,2\pi]$. </p>

<p>But, as reuns said, it's possible to find continuous $f$ such that $|\hat f(n)|\ge a_n$ infinitely often. Indeed, there is a subsequence $a_{n_k}$ such that $|a_{n_k}|\le 2^{-k}$ for every $k$. Let 
$$
f(x) = \sum_{k} a_{n_k}\exp(n_k i x)
$$
By the Weierstrass M-test, the series converges uniformly, hence $f$ is continuous. It has the desired Fourier coefficients by construction.</p>
"
"2397358","2397375","<p>A relation is asymmetric if and only if it is anti-symmetric and irreflexive.</p>

<p>So, the second definition can be rewritten as:</p>

<ol>
<li><p>Irreflexive</p></li>
<li><p>Anti-symmetric</p></li>
<li><p>Transitive</p></li>
</ol>

<p>And now the relation between the two definitions is a little more clear:  they are both transitive and anti-symmetric, but the strict one is irreflexive, and the non-strict one is reflexive.</p>

<p>Note that a 'non-strict' partial order is not the same as a relation that is anti-symmetric, transitive, but not strict, for it could be anti-symmetric, transitive, but neither reflexive nor irreflexive ... I also wish there was a term for relations that are anti-symmetric and transitive (and I believe 'order' fits the bill just right) ... but I get the impression that we don't consider such 'in-between orders' because they are mathematically not that interesting ... see also:</p>

<p><a href=""https://math.stackexchange.com/questions/2210560/orders-partial-orders-strict-partial-orders-total-orders-strict-total-orders"">Orders, Partial Orders, Strict Partial Orders, Total Orders, Strict Total Orders, and Strict Orders</a></p>

<p>As far as the connection with trichotomy goes: There I do not understand the textbook. Trichotomy for any relation $R$ is that for any two objects $x$ and $y$: $xRy$ or $yRx$ or $x=y$. Put differently: if $x \not = y$, then either $xRy$ or $yRx$, which is also known as a 'connex' relationship: every two different elements are 'connected'. Thus, for example, $\subseteq$ is not connex, as $\{ 1 \}$ and $\{ 2 \}$ do not stand in the $\subseteq$ relation in any way, but $\le$ is connex ... which is also why we call it a 'total' order (a relation is 'total' when for any $x$ and $y$, either $xRy$ or $yRx$ ... note that connex/trichotomy plus reflexive implies total).</p>

<p>But $\le$ is clearly not strict. That is, $\le$ is a partial order that is connex (i.e for which trichotomy holds), but it is not a strict partial order. </p>

<p>So, frankly, I disagree with the book on that one: the difference between 'non-strict' and 'strict' is not 'connex' or trichotomy (<em>that</em> is the difference between partial and total). Rather, the difference between 'strict' and 'non-strict' is the difference between reflexive and irreflexive.</p>
"
"2397359","2397653","<p>Your proposed map $[G,G] \to [E,E]$ isn't well defined.  It's possible that $[g_1, g_2] = [h_1, h_2]$, but the lifts $[\tilde{g_1}, \tilde{g_2}] \ne  [\tilde{h_1}, \tilde{h_2}]$.</p>

<p>Consider the case $E = Q_8$ (the quaternion group).  The center of $E$ is $\{1, -1\}$ and the quotient $G := E/Z(E) \cong C_2 \times C_2$.  The map $E \to G$ is the quotient map. Since $G$ is abelian, we have $[G,G] = 1$ i.e., no matter what we choose $g_1$ and $g_2$ to be, we get $[g_1, g_2] = 1$.  But when we lift we can get anything in $[E,E] = \{1, -1\}$, so the map $[G,G] \to [E,E]$ isn't well defined.</p>
"
"2397368","2397370","<p>$$
\frac{(x+1)^x}{x^{x+1}}=\underbrace{\frac{1}{x}}_{\le 1/3}\underbrace{\left(1+\frac{1}{x}\right)^x}_{ \le e} \le \frac{e}{3}&lt;1.
$$</p>
"
"2397384","2397818","<p>Consider the following set of elements of $K$: $\{1,u_1,u_1^2,\ldots,u_1^{d_1-1},u_2,u_2u_1,u_2u_1^2,\ldots,u_2u_1^{d_1-1}, \ldots,u_2^{d_2-1},u_1u_2^{d_2-1},u_1^2u_2^{d_2-1},\ldots,u_1^{d_1}u_2^{d_2-1},\ldots\}$, i.e. the set of elements of the form $u_1^{i_1}u_2^{i_2}\cdots u_n^{i_n}$ where $0 \leq i_k &lt; d_k$ (it can be considered the set of monomials that can be formed in $K$). It is not hard to prove that they span $K$ as a vector space and their number is exactly $d_1d_2\cdots d_n$.</p>
"
"2397391","2397407","<p><strong>hint</strong></p>

<p>We look for $f (x) $ such that</p>

<p>$$(1+10)\int_0^xf (t)dt=xf (x) $$</p>

<p>(sum of two areas $=$area of the rectangle).</p>

<p>differentiating  gives</p>

<p>$$11f (x)=f (x)+xf'(x) $$</p>

<p>You can finish and get</p>

<blockquote>
  <p>$$f (x)=\lambda x^{10} $$</p>
</blockquote>

<p>The other solution satisfies
$$(1+\frac {1}{10})\int_0^xf (t)dt=xf (x) $$</p>

<p>gives </p>

<blockquote>
  <p>$$f (x)=\mu x^\frac {1}{10} $$</p>
</blockquote>
"
"2397442","2397478","<p>For convenience, let $|M| = \sqrt{M^*M}$.</p>

<p>It suffices to note that
$$
\langle A,B \rangle = \operatorname{Tr}(AB^*) = \operatorname{Tr}(B^*A)
$$
is an inner product on the space of $m \times n$ matrices.  From there, the properties of an inner product are enough to prove the Cauchy-Schwarz inequality:
$$
|\operatorname{Tr}(AB^*)| = |\langle A,B \rangle| \leq \|A\| \cdot \|B\| \ = \sqrt{\operatorname{Tr}(A^*A)\operatorname{Tr}(B^*B)} = \sqrt{\operatorname{Tr}(|A|^2) \operatorname{Tr}(|B|^2)}
$$
Now, if $M$ has rank $r$, then the polar decomposition tells us that there exists a partial isometry $U$ (i.e. $U^*U$ is an orthogonal projection) with $\operatorname{rank}(U) = M$ such that $M = U|M|$ and $|M| = U^*M$.  From there, 
$$
\operatorname{Tr}(|M|)^2 = \langle M,U \rangle^2 \leq \langle M, M \rangle \langle U,U \rangle = \operatorname{Tr}(M^*M) \cdot \operatorname{rank}(M)
$$</p>
"
"2397448","2398006","<p><a href=""https://en.wikipedia.org/wiki/Tangent_developable"" rel=""nofollow noreferrer"">Tangent developables</a>: union
of the tangents to a space curve.</p>

<p>Also, one can join developable surfaces together:
<hr />
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
<a href=""https://i.stack.imgur.com/K3fXe.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K3fXe.jpg"" alt=""fork""></a>
<br />
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
<sup>
Developable fork: <a href=""http://thegeometryofbending.blogspot.com/2009/11/developable-fork.html"" rel=""nofollow noreferrer"">thegeometryofbending</a>.
</sup></p>

<hr />
"
"2397458","2397475","<p>For every $x&gt;1$, $$\mathrm{agm}(1,x)=x\cdot\mathrm{agm}(1,x^{-1})=\frac{\pi x}{2K(u(x))}$$ where $$u(x)^2=1-x^{-2}$$ and $K$ denotes the <a href=""https://en.wikipedia.org/wiki/Elliptic_integral#Complete_elliptic_integral_of_the_first_kind"" rel=""nofollow noreferrer"">complete elliptic integral of the first kind</a>. </p>

<p>When $x\to\infty$, $u(x)\to1$. The <a href=""https://math.stackexchange.com/q/1111135"">asymptotic expansion of $K(k)$ when $k\to1$</a> reads $$K(k)=-\frac12\log|1-k|+O(1)$$ hence, when $x\to\infty$, $$\mathrm{agm}(1,x)=\frac{\pi x}{-\log|1-u(x)|+O(1)}=\frac{\pi x}{2\log x+O(1)}$$ in particular,</p>

<blockquote>
  <p>$$\lim_{x\to\infty}\frac{\log x}x\cdot\mathrm{agm}(1,x)=\frac\pi2$$</p>
</blockquote>
"
"2397460","2398119","<p>My <a href=""https://stackoverflow.com/questions/27085362/how-to-implement-a-derivative-of-a-symbolic-function-by-a-symfun-in-matlab"">example approach</a> that you cite is for a simpler situation. Your function <code>f</code> depends on not just abstract symbolic functions, but also their derivatives. The problem is that <code>subs(f,th,v)</code> by itself returns zero. This is because the substitution also replaces the <code>th(t)</code> inside <code>diff(th(t),t)</code> with a constant symbolic variable that is not a function of time. The derivative of a constant is zero so your entire function simplifies to zero.</p>

<p>To work with an equation like this you need to replace <code>diff(th(t),t)</code> with something else so that it is not impacted by the substitution. This can be done fairly painlessly by using the <a href=""https://www.mathworks.com/help/symbolic/subs.html#btqh71e-1"" rel=""nofollow noreferrer"">multiple substitution</a> capabilities of <a href=""https://www.mathworks.com/help/symbolic/subs.html"" rel=""nofollow noreferrer""><code>sym/subs</code></a>:</p>

<pre><code>syms t a1 a2 m2 th(t) p(t) 
v = m2*a1*a2*diff(p,t)*diff(th,t)*cos(th-p);
syms x dx
v2 = subs(v,{th,diff(th,t)},{x,dx}); % Order does not matter if done at same time
v3 = diff(v2,x);
dvdth = subs(v3,{x,dx},{th,diff(th,t)})
</code></pre>

<p>Or more compactly (but very hard to read):</p>

<pre><code>syms t a1 a2 m2 th(t) p(t) x dx
v = m2*a1*a2*diff(p,t)*diff(th,t)*cos(th-p);
dvdth = subs(diff(subs(v,{th,diff(th,t)},{x,dx}),x),{x,dx},{th,diff(th,t)})
</code></pre>

<p>These returns <code>a1*a2*m2*sin(p(t) - th(t))*diff(p(t), t)*diff(th(t), t)</code>, which is equivalent to your expected result.</p>

<p>As I noted in my StackOverflow answer, this is, in my opinion, an ugly hack. One might be able turn this approach into a general function with a bit of work, but it might be quite challenging to handle every possible case.</p>
"
"2397468","2397525","<p>Let $\displaystyle P(z) = \sum_{k=0}^n (-1)^ke_k z^{n-k}$ be the $n^{th}$ degree polynomial with roots $z_k \mid k=1,\cdots,n$, where by <a href=""https://en.wikipedia.org/wiki/Vieta%27s_formulas"" rel=""nofollow noreferrer"">Vieta's formulas</a> $e_k$ are the <a href=""https://en.wikipedia.org/wiki/Elementary_symmetric_polynomial"" rel=""nofollow noreferrer"">elementary symmetric polynomials</a>.</p>

<p>Let $\displaystyle p_i=\sum_{k=1}^n z_k^i\,$, where it is given that $p_l=0$ for all odd $l$. </p>

<p>From <a href=""https://en.wikipedia.org/wiki/Newton%27s_identities#Formulation_in_terms_of_symmetric_polynomials"" rel=""nofollow noreferrer"">Newton's identities</a> $\displaystyle k e_k = \sum_{i=1}^k (-1)^{i-1}e_{k-i}p_i$ it follows (by induction, for example) that $e_l=0$ for all odd $l$. Therefore, the polynomial $P(z)$ has every other coefficient $0$, so it contains either only even powers of $z$, or only odd powers of $z$, depending on the parity of $n$. In the first case $P(z)$ is an even function, in the second case an odd one. In both cases $P(z)=0 \iff P(-z)=0$ so the roots of $P(z)$ can be grouped in pairs of mutually opposites.</p>
"
"2397488","2397494","<p>Let $F(x)$ be an antiderivative of $\log(x)$. Then the integral evaluates to $F(x)-F(e)$. Taking the derivative of that, you get $\log(x)-0$, because the derivative of a constant is zero.</p>

<p>Now, applying the FTC even more directly, the formula is this:</p>

<p>$$\frac{d}{dx}\int_a^x f(t) \,\,dt = f(x),$$</p>

<p>without an $f(a)$ subtracted.</p>
"
"2397490","2397557","<p>First of all notice that $\triangle AEF \cong \triangle DFA$ (in the same order of vertices), where $D$ is the midpoint of $BC$. Now let the incenter of $\triangle AEF$ touch $EF$ at $H$. Now by the symmetry of the abovementioned triangles we have that $H$ and $X$ are reflections of each other over the midpoint of $EF$. In otherwords $X$ is the touching point of the excircle of $\triangle AEF$ opposite the vertex $A$. (It's a well-known property that the touching point of the incircle and the corresponding excircle are reflections of each other over the midpoint of the corresponding side). </p>

<p>Now take the excirle of $\triangle AEF$ opposite the vertex $A$ and by homothety send it to the excircle of $\triangle ABC$ opposite the vertex $A$. Now using the fact that $EF \parallel BC$ is not hard to conclude that the touching points are collinear with the center of homothety, i.e. $A,X,T_a$ are collinear. Hence the proof.</p>
"
"2397500","2397503","<p>You were right first time ... the initial velocity for the second part of the motion is $u=2t_1$.</p>

<p>For the first part of the motion $u=0$,$t=t_1$,$a=2$ then use $v=u+at$; you got $v=2t_1$.</p>

<p>For the second part of the motion $u=2t_1$,$t=t_2$,$a=-3$,$v=0$ then use $v=u+at$ &amp; we get $0=2t_1-3t_2$.</p>

<p>I will leave you to solve the simultaneous equations &amp; deduce when the maximum velocity was attained.</p>
"
"2397511","2397520","<p>Whenever you use the quotient rule repeatedly, you end up with powers of the denominator all over the place. In your example, we have $(x^2+3)$ occurring in both terms of the numerator (see how the numerator consists of two large terms?), and also in the denominator. Calling that factor $A$, we have:</p>

<p>$$\frac{-12A^2 + 48x^2A}{A^4}$$</p>

<p>See how you can reduce that, by canceling one power of $A$?</p>

<p>$$\frac{-12A + 48x^2}{A^3} = \frac{-12(x^2+3)+48x^2}{(x^2+3)^3} = \frac{36x^2-36}{(x^2+3)^3}$$</p>

<hr>

<p>Similarly on the second example, there are powers of $(x-5)$ all over the place. Call it $B$:</p>

<p>$$4(3xB^2 + B^3) + 4B^3 = 12xB^2 + 8B^3 = B^2(12x+8B)$$</p>

<p>Now, we can simplify $12x+8B$:</p>

<p>$$12x+8(x-5) = 20x-40 = 20(x-2)$$</p>
"
"2397515","2397565","<p>if you are asking about what the computer does it is like this:
you have the variable $x$</p>

<p>$\lfloor x\rfloor=max\{m\in\mathbb{Z}~|~m\le x\}$</p>

<p>which means that out off all the integers that beneath $x$ take the largest.</p>

<p>now the computer doesn't has the function $\in$ or the group $\mathbb{Z}$ or any of those stuff
 so he do it differently, the computer save memory with $0$'s and $1$'s, bits, integer he saves with 32-bits(usually)</p>

<p>for understanding with 8-bits it looks like this:</p>

<p>$1111~1111$bits$=-127$</p>

<p>$1000~0000$bits$=1$</p>

<p>$0111~1111$bits$=0$</p>

<p>now for float he has a different method, 32-bit format looks like this:</p>

<p>$\underbrace{0}_{0=positive\\1=negative}\underbrace{00000000}_{the~exponent }~~\underbrace{00000000000000000000000}_{the~fraction~part}$</p>

<p>now how exactly this format works is not important now, but you can see from this format that if you have the float, for example, $0~~10000000~11000000000000000000000(=3.5)$ the computer can just ignore the last 22 bits and take only $0~~10000000~1$, the computer can extract all he needs from the first 10 bits
if you do interested in how the float itself works:</p>

<p>the computer look at the first bit and put it in var name AXL(for this example) and do $AX=(-1)^{AXL}$ now he takes the last part and do $DX=1+\text{[the bit]}^\text{[the bit position]}+\text{[the bit]}^\text{[the bit position]}+...$</p>

<p>and the end result is:</p>

<p>$AX\times (DX\times 2^{\text{[the middle part value]}})$</p>

<p>now because that every part after the 10th bit is quarter or less you don't need them when you use floor</p>
"
"2397534","2397545","<blockquote>
  <p>How does the scale balancing twice evenly solve the problem?</p>
</blockquote>

<p>What has happened is that you've had $A$ as one side's total and $B$ as the other side's total, where $A = B + w$. After adding more stones as described, you have once again arrived at $C$ and $D$, where $C = D + w$. In particular, the stones that were added to the former side have weight $C-A$, and the stones added to the latter side have weight $D-B$.</p>

<p>But $C-A = (D+w) - (B+w) = D-B$, which means those two groups of stones -- the ones added to the former side, and the ones added to the latter side -- have the same total weight, which means that they furnish a solution to the problem at hand.</p>
"
"2397544","2397549","<p>Check out Chaos-An Introduction to Dynamical Systems by Kathleen T. Alligood, Tim D. Sauer, and James A.Yorke. In it you'll find resources regarding the quadratic map as a function of a complex variable which generate the Mandelbrot set, also related to the Julia set. </p>
"
"2397548","2397551","<p>$\nabla f$ is a 3-dimensional vector. The book uses $|\nabla f|$ to mean the norm of this vector. They could have written $\nabla f \cdot\nabla f$ instead of $|\nabla f|^2$</p>

<p>One has
$$|\nabla f| = \sqrt{\left(\frac{\partial f}{\partial x}\right)^2+\left(\frac{\partial f}{\partial y}\right)^2+\left(\frac{\partial f}{\partial z}\right)^2}$$</p>
"
"2397560","2397693","<p>2) As Mariano says in the comments, you've got it backwards in your second question.  The ideal $m_p$ consists of the regular functions that vanish at $p$.  By localizing at the set $S := A(X) \setminus m_p$, we obtain the collection of functions
$$
\left\{f/g \in k(X) : g \notin m_p \right\} = \left\{f/g \in k(X) : g(p) \neq 0 \right\}
$$
where $k(X)$ is the field of rational functions of $X$.  This means that $g$ does not vanish at $p$, so the above set is the collection of rational functions that are regular (i.e., defined) at $p$.  Thus we have ""zoomed in"" on $p$ by expanding our view from globally regular functions to those that are only guaranteed to be regular at $p$.</p>

<p>1) By the correspondence theorem for prime ideals in a localization, the only primes $\mathfrak{p}$ that survive in $A(X)_{m_p}$ are those with $\mathfrak{p} \subseteq m_p$.  By contravariance, then $Z(\mathfrak{p}) \supseteq Z(m_p) = \{p\}$, so $Z(\mathfrak{p})$ is an irreducible variety containing the point $p$.</p>

<p>Take for example $X = \mathbb{A}^2 = \operatorname{Spec}(k[x,y])$, $p = (0,0)$, so $m_p = (x,y)$.  The localization $A = k[x,y]_{m_p}$ is local with maximal ideal which (abusing notation slightly) is just $(x,y)$.  We have for example, $(y - x^2) \subseteq (x,y)$, which corresponds to the fact that the point $(0,0)$ lies on the parabola $y - x^2$.</p>
"
"2397567","2397573","<p>What you are noticing is that it is not, in general, true that the codimension of a projective (or affine) variety is equal to the number of equations used to define it (or even the minimum number of generators of it's ideal). When the codimension is equal to  the minimum number of generators of the ideal of the variety, it is called a complete intersection and is quite special. See examples and non-examples <a href=""https://en.wikipedia.org/wiki/Complete_intersection"" rel=""nofollow noreferrer"">here on the wikipedia page.</a></p>
"
"2397569","2397570","<p>In probability we restrict our attention to a special set called the <strong>sample space.</strong> This contains all possible outcomes of an experiment. Now <strong>events</strong> are just subsets of this sample space. Given two events $A$ and $B$, their intersection and union are also events. They are defined in the same way they are for sets; namely
$$
A\cap B := \{x\in S \mid x\in A\ \text{and}\ x\in B\}
$$
and
$$
A\cup B := \{x\in S \mid x\in A\ \text{or}\ x\in B\},
$$
where $S$ denotes the sample space.
Thus if we want to find the probability that both $A$ and $B$ happen, we compute the probability of the intersection. Likewise for union.</p>
"
"2397589","2397597","<p>In general no, a scalar multiple of a Moment Generating Function (MGF) or a square of an MGF will not be a MGF. Consider the MGF of a standrd normal density</p>

<p>$M_X(t)=e^{t^2/2}.$</p>

<p>Then the scalar multiple in the post would be </p>

<p>$5e^{t^2/2}$ which is not an MGF. </p>

<p>If you multiply your random variable by a constant then the resulting MGF of that random variable $Y=5X$ (say) would be </p>

<p>$\int e^{t5x}f(x)dx = \int e^{t^\prime x}f(x)dx = M_X(5t) \neq 5M_X(t)$
unless of course your MGF was a linear function in $t$. </p>
"
"2397602","2397612","<p>We show that $f_n\to 0$ pointwise, but not uniformly.</p>

<p>Suppose $x\in\mathbb{R}$. If $x\leq 0$ then $f_n(x)=0$ for all $n$. If $x&gt;0$, we can choose $N\in\mathbb{N}$ such that $1/N&lt;x$. In this case, we have $f_n(x)=0$ for all $n\geq N$. Therefore $f_n(x)\to0$ for all $x\in\mathbb{R}$, which means that $f_n\to 0$ pointwise.</p>

<p>To see that $f_n\not\to0$ uniformly, note that for every $n\in\mathbb{N}$, we have
$$
\sup_{x\in\mathbb{R}} |f_n(x)| = 1.
$$
Indeed, fix $n\in\mathbb{N}$ and take $x:=\frac{1}{n+1/2}$. Then $x$ satisfies $1/(n+1) \leq x \leq 1/n$, and so
$$
|f_n(x)| = |\sin\left(\frac{\pi}{x}\right)| = |\sin(n\pi+\pi/2)| = 1.
$$</p>
"
"2397605","2397618","<p>You are getting hung up on the notation. The variables $x$, $y$, and $z$ used to define the sets $U$, $W$, and $\{(x,x,y,z)\in F^4 : x,y,z\in F\}$ have nothing to do with one another.</p>

<p>We must show that $U+W = S$, where $S:=\{(x,x,y,z)\in F^4 : x,y,z\in F\}$.</p>

<p>Let's first show ""$\subseteq$"": Fix $u:=(x,x,y,y)$ in $U$ and $w:=(a,a,a,b)$ in $W$. Then
$$
u+w = (x+a,x+a,y+a,y+b)
$$
is in $S$ because $x+a=x+a$, $y+a$, and $y+b$ are all elements of $F$.</p>

<p>Now we prove ""$\supseteq$"": Fix $s=(x,x,y,z)$ in $S$. We need to find $u\in U$ and $w\in W$ such that $s=u+w$. Consider $u:=(x-y,x-y,0,0)$ and $w:=(y,y,y,z)$. Then $u\in U$, $w\in W$, and
$$
s=(x,x,y,z)=(x-y,x-y,0,0)+(y,y,y,z)=u+w\in U+W.
$$
Therefore $U+W=\{(x,x,y,z)\in F^4 : x,y,z\in F\}$.</p>
"
"2397607","2397620","<p>Notice that $$A=\begin{bmatrix}1&amp;-2&amp;3\\2&amp;-4&amp;6\\3&amp;-6&amp;9\end{bmatrix}$$ is not invertible. This is because the second and third rows are multiples of the first, so by adding $-2$ times the first row to the second row, and by adding $-3$ times the first row to the third row we have the matrix $$\begin{bmatrix}1&amp;-2&amp;3\\0&amp;0&amp;0\\0&amp;0&amp;0\end{bmatrix}$$which has determinant $0$ obviously. Hence, $A$ has determinant $0$ and is therefore not invertible. Therefore, we cannot row reduce $A$ to the identity matrix. That is, we cannot reduce the matrix in question to $$\begin{bmatrix}1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1\end{bmatrix}$$
From the above argument we see that $A$ has rank $1$ and nullity $2$. This is why there are two ""arbitrary parameters"", or free variables, in the solution system (because the nullity, the dimension of the kernel, is $2$).</p>
"
"2397627","2397630","<p>An inflection point is a point on a curve at which the sign of the curvature (i.e., the concavity) changes</p>

<p>$$ y = 0.768x - 0.00004x^3 $$</p>

<p>$$ y' = 0.768 - 0.00004(3)x^2 $$</p>

<p>$$ y^{''} = -0.00004(3)(2)x $$</p>

<p>Can you see at which point does $y^{""}$ changes sign?</p>
"
"2397633","2397649","<p>You have $a^2:=n+2112$ and $b^2:=n+528$. Then $(a^2-b^2) = 1584 = 2^4\cdot3^2\cdot 11$.</p>

<p>$1584$ thus has $5\cdot 3\cdot 2=30$ factors, but for $(a+b)(a-b)$type factors we need both even, so only $3\cdot 3\cdot 2=18$ factors $\implies 9 $ factor pairs:<br>
$\{2,792\} \implies a=(2+792)/2 = 397, n=155497$<br>
$\{4,396\}$<br>
$\{8,198\}$<br>
$\{6,264\}$<br>
$\{12,132\}$<br>
$\{24,66\}$<br>
$\{18,88\}$<br>
$\{36,44\}$<br>
$\{22,72\}$  </p>

<p>and so on to find the $9$ possible values of $n$. Your $n=97$ comes from the last of these.</p>
"
"2397648","2397656","<p>For any real-valued angle $\theta$, we find that $e^{i\theta} = \cos(\theta)+i\sin(\theta)$, from the Taylor Series expansions of $e, \cos, \sin$.</p>

<p>Thus for any positive real number $A$, we have $A^i {= e^{i\ln A} \\= \cos(\ln A)+i\sin(\ln A)}$</p>

<p>So likewise $A^{-i} {= e^{-i\ln A} \\=\cos(-\ln A)+i\sin(-\ln A) \\= \cos(\ln A)-i\sin(\ln A)}$</p>

<hr>

<p>Then we are releived to find that $A^i\cdotp A^{-i} {= \cos^2\ln A+\sin^2\ln A \\= 1}$ </p>
"
"2397654","2397658","<p>It's correct. The underlying topological argument is as follows. If you can find a collection of pairwise disjoint nonempty open sets in $X$, then any dense subset of $X$ must have cardinality at least as large as that of the collection.</p>

<p>While the argument used in your edit will work in metric spaces, it fails in general for topological spaces.</p>
"
"2397676","2397686","<p>A natural number less than $64$ with exactly three ones in it's binary representation, can be written as the sum of three distinct powers of $2$, and vice-versa. </p>

<p>So what we are essentially doing, is adding up all numbers, that are a sum of three distinct powers of $2$. We know that there are six powers of $2$ below $64$. We'll put a notation : $\{a,b,c\}$ stands for $2^a + 2^b + 2^c$, where $a,b,c$ are distinct and between $0$ and $5$, both included. Now, we are adding up all such possibilities, whose number you have correctly calculated.</p>

<p>The point is, how many times does $a$ appear in these combinations, where $a$ is between $0$ and $5$? Suppose $a$ appears in a combination, then the other two can be chosen in $\binom 52 =10$ ways. Hence, the number of combinations in which $a$ appears is $10$. </p>

<p>Since this happens for all $a$, the answer should be $10(2^0 + ... + 2^5) = 630$. </p>
"
"2397683","2397688","<p>$S_{ABP}+S_{PCD}=\frac{1}{2}PE\cdot AB+\frac{1}{2}PF\cdot CD=\frac{1}{2}(PE+PF)\cdot AB=\frac{1}{2}EF\cdot AB=\frac{1}{2}S_{ABCD}$</p>
"
"2397684","2397698","<p>\begin{align}
\operatorname{E}(e^{sY}) = \operatorname{E}(X^s) = \int_1^\infty x^s \frac \alpha {x^{\alpha+1}} \, dx = \alpha \int_1^\infty x^{s-\alpha - 1} \, dx = \alpha  \left[ \frac{x^{s-\alpha}}{s-\alpha} \right]_1^\infty = \frac \alpha {\alpha - s}.
\end{align}
This last equality holds if $s&lt; \alpha,$ and that is enough. Ask yourself for which distribution this is a moment-generating function.</p>

<p>(You may need to cite a theorem saying if a distribution has the same moment-generating function as a certain other distribution, then they are the same distribution.)</p>
"
"2397702","2397710","<p>The global question is if the existence and uniqueness theorem can be applied. For that you would need local Lipschitz continuity of $f$. This would be the case if $f$ were differentiable and $f_x$ bounded over some interval (containing $x=0$). This in turn would be automatic if $f_x$ were continuous.</p>

<p>But the point is that $f_x$ is not even defined at $x=0$ and further unbounded in every neighborhood of $x=0$ preventing the existence of any Lipschitz constant. As a different example, consider $f(x)=|x|$ which is also not differentiable in $x=0$ but has bounded derivative around $x=0$ and is indeed Lipschitz continuous (trivially by the triangle inequality).</p>

<p>You can demonstrate non-uniqueness by observing that $x(t)=0$ and $x(t)=(\frac23t)^{3/2}$ are both solutions, and thus have an infinity of further solutions with $x(0)=0$ between them.</p>
"
"2397713","2397739","<p>1) If you want to use closed + bounded $\implies$ convergent subsequence, then you have to start with a sequence.  But this is a weasely way to address your question.  Here's a better way.  </p>

<p>Suppose there were no <em>best</em> approximation in $U$.  Then there is surely a sequence of successively improving approximants in $U$.  (Otherwise: ""Hey, look!  We found the best approximant."")  The rest of the proof goes on to show that the resulting sequence has an eventually constant subsequence.  (It's worth pointing out there can be multiple best approximants and our starting sequence can eventually settle down to just output these ""with some random pattern of repetition"".  But then at least one of them is repeatedly infinitely often, so there is a convergent subsequence.)</p>

<p>If you start by writing down (one of) the best approximants, then you are of course done.  How do you do this generically and simultaneously for all finite dimensional subspaces of all normed spaces?  That's trickier.  Are we even sure that sequences of improving approximants ever settle down in unfamiliar spaces and/or with unfamiliar norms?</p>

<p>2) ""$||\phi - u_n|| \longrightarrow d$"" means for any $\varepsilon &gt; 0$, there exists a positive integer $N$, such that for all $n &gt; N$, $||\phi - u_n|| \in (d- \varepsilon, d + \varepsilon)$.  Consequently, the first term on the right hand side is bounded by, say, $d + 1$ for sufficiently large $n$.  So the right hand side is bounded by $||\varphi|| + d + 1$.  (In fact any number $||\varphi|| + d + \varepsilon$ for $\varepsilon &gt; 0$ is established as an upper bound by this argument.)</p>

<p>3) After the first equation, I have an infimum on the right-hand side.  There's no reason to believe that the infimum is actually attained by any $u \in U$.  (In fact, this is what we are trying to prove: that there is a $u$ for which the infimum is attained.)  Also, I have a limit on the left-hand side.  Again, there's no reason to assume that there is a $u_n$ that makes ""$\rightarrow$"" actually ""$=$"".  So you have a sequence of $u_n$ that can get you as close as you like to $d$ and you know there are elements of $U$ that can get you as close as you like to $d$ from above.  How do you know that you can ever actually attain $d$?  </p>

<p>(It's counterfactual for this problem, but could be a useful example of how the idea your third question suggests doesn't quite work.  Suppose the limit on the left-hand side were only approached from below.  Then the <em>upper bound</em> for the left-hand side would match the <em>lower bound</em> for the right-hand side.  But that wouldn't tell you that any value appearing in the sequence on the left agreed with any value in the set on the right.  Unless both actually attain their bounds.  So there's more to do.)</p>
"
"2397718","2397731","<p><a href=""https://i.stack.imgur.com/XU1Nb.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XU1Nb.jpg"" alt=""enter image description here""></a>it is just a natural transformation.</p>
"
"2397723","2397771","<p><strong>It's been a while since I've had to explain the process of solving second order ODE's, so if anyone finds a mistake in this please do let me know</strong></p>

<p>In order to solve this differential equation you would have to learn how to solve Second-Order Differential equations in general. The equation you have provided is known as a Second-Order Inhomogenous Linear Ordinary Differential Equation with Constant Coefficients.</p>

<p>These are of the form (<em>Note that $q$ in this equation ONLY is not the same as your  charge function q</em>):</p>

<p>$$y''+py'+qy=r(x)$$</p>

<p><em>Second-Order</em>: Involves the second derivative of $y$</p>

<p><em>Linear</em>: It is of the form $p_0(x)\frac{d^ny}{dy^n}+p_1(x)\frac{d^{n-1}y}{dy^{n-1}}+p_2(x)\frac{d^{n-2}y}{dx^{n-2}}+...+p_ny(x)=r(x)$</p>

<p><em>Inhomogenous</em>: $r(x) \neq 0$</p>

<p><em>Constant Coefficients</em>: $p$ and $q$ are constants.</p>

<p>Your equation in this form is:</p>

<p>$$-Lq''+Rq'+\frac{q}{C}=-V\Rightarrow q''-\frac{R}{L}q'-\frac{1}{LC}q=\frac{V}{L}$$</p>

<p>To solve this we must do the proceed with the following steps:</p>

<p><strong>Step 1:</strong> Finding the complementary solution</p>

<p>We start by solving the equivalent homogenous problem $y''+py'+qy=0$</p>

<p>Setting $V=0$</p>

<p>$$ q''-\frac{R}{L}q'-\frac{1}{LC}q=0$$</p>

<p>Now we need to find what is known as an <strong>auxillary equation</strong>. We consider that $q$ is of the form $e^{mt}$.</p>

<p>$$q=e^{mt},\,q'=me^{mt},q''=m^2e^{mt}$$</p>

<p>If we plug these in and rearrange, we get:</p>

<p>$$e^{mt}(m^2-\frac{R}{L}m-\frac{1}{LC})=0$$</p>

<p>Since $e^{mt}\neq0$, then $(m^2-\frac{R}{L}m-\frac{1}{LC})=0$ </p>

<p>This is a quadratic equation with solutions:</p>

<p>$$m_{1,2}=\frac{\frac{R}{L}\pm\sqrt{\frac{R^2}{L^2}-\frac{4}{LC}}}{2}\Rightarrow m_1=\frac{\frac{R}{L}+\sqrt{\frac{R^2}{L^2}-\frac{4}{LC}}}{2},m_2=\frac{\frac{R}{L}-\sqrt{\frac{R^2}{L^2}-\frac{4}{LC}}}{2}$$</p>

<p>The discriminant of the roots $\Delta = \frac{R^2}{L^2}-\frac{4}{LC}$ causes three different cases to arise:</p>

<p><em>Case 1</em>:$\Delta&gt;0$, which tells us we have two <em>distinct</em> real roots, and in this case we get two linearly independent solutions:</p>

<p>$$q_1=e^{m_1t},q_2=e^{m_2t}$$ </p>

<p>which gives us the complementary solution:</p>

<p>$$q_c(t)=C_1e^{m_1t}+C_2e^{m_2t}$$</p>

<p><em>Case 2:</em>$\Delta=0$, which tells us we have a double repeated root ($m_1=m_2=m$), and in this case we get two linearly independent solutions:</p>

<p>$$q_1=e^{mt},q_2=te^{mt}$$ </p>

<p>which gives us the complementary solution:</p>

<p>$$q_c(t)=C_1e^{mt}+C_2te^{mt}$$</p>

<p><em>Case 3:</em>$\Delta&lt;0$, which tells us we have a two <em>distinct</em> complex roots and in this case we get two linearly independent solutions:</p>

<p>$$q_1=e^{(a+ib)t}=e^{at}(\cos bt+i\sin bt), q_2 = e^{(a-ib)t}=e^{at}(\cos bt-i\sin bt)$$</p>

<p>which gives us the complementary solution:</p>

<p>$$q_c(t)=e^{at}[C_1(\cos bt+ i\sin bt)+C_2(\cos bt - i \sin bt)]$$</p>

<p><strong>Step 2:</strong> We find the particular solution (or particular integral), and this usually is a trial-and-error approach where your decision depends on the form that $r(x)$ comes in. In our case $r(x)=V$, which is just a constant function of $t$. So we will make the guess that $q_p=A$, another constant function of $t$.</p>

<p>Plugging this in we get: 
$$-\frac{A}{LC}=\frac{V}{L}\Rightarrow A=-CV$$ 
And if my knowledge of Electrical Physics is correct, I believe that $CV=Q$</p>

<p>Therefore,</p>

<p>$$q_p=-CV=-Q$$</p>

<p><strong>Step 3</strong>: Now that we have our complementary and particular solutions, all we need to do is add them together to get our general solution. Hence for cases $1$ and $3$,</p>

<p>$$q(t)=q_c+q_p\Rightarrow q(t)=C_1e^{m_1t}+C_2e^{m_2t}-Q$$</p>

<p>or for case $2$,</p>

<p>$$q(t)=q_c+q_p\Rightarrow q(t)=C_1e^{mt}+C_2te^{mt}-Q$$</p>

<p>The constants $C_1,C_2$ and $Q$ can only be determined by knowing values of $q(t)$ at different times $t$, which brings two new problems in differential equations known as Initial-Value Problems and Boundary-Value Problems.</p>

<p>I believe in your specific equation the electrical circuit is undamped, which means $\Delta&lt;0$ and that is how the ""exponentially decreasing sine wave function arises"".</p>

<p>Hope this gives you some insight on how to solve Second-Order ODE's.</p>
"
"2397725","2397992","<p>Hypergeometric equation
â$$x(1-x)y^{\prime\prime}+[2-(a+b+1)x]y^\prime-aby=0$$â
as $1-c=1-2\notin{\Bbb Z}-{\Bbb N}$, let the answer is
$$y=x^0(a_0+a_1x+a_2x^2+a_3x^3+\cdots)=a_0+\sum_{n=1}^{\infty }a_nx^n$$â
with substitution in equation
\begin{eqnarray*}â
&amp;&amp; x(1-x)y''+(2-(1+a+b)x)y'-aby= 0\\â
&amp;&amp; x(1-x)\sum_{n=1}^{\infty }a_nn(n-1)x^{n-2}+(2-(1+a+b)x)\sum_{n=1}^{\infty }a_nnx^{n-1}-ab-ab\sum_{n=1}^{\infty }a_nx^n= 0\\â
&amp;&amp; \sum_{n=1}^{\infty }a_nn(n-1)x^{n-1}-\sum_{n=1}^{\infty }a_nn(n-1)x^{n}+2\sum_{n=1}^{\infty }a_nnx^{n-1}-(1+a+b)\sum_{n=1}^{\infty }a_nnx^{n}-ab-ab\sum_{n=1}^{\infty }a_nx^n= 0\\â
&amp;&amp; \sum_{n=2}^{\infty }a_nn(n-1)x^{n-1}-\sum_{n=2}^{\infty }a_nn(n-1)x^{n}+2\sum_{n=1}^{\infty }a_nnx^{n-1}-(1+a+b)\sum_{n=1}^{\infty }a_nnx^{n}-ab-ab\sum_{n=1}^{\infty }a_nx^n=0\\â
&amp;&amp; (2a_1-ab)+x(6a_2+a_1(1+a+b-ab))x+\cdots 
â\end{eqnarray*}â
$$a_1=\frac{ab}{2\times1}~~~,~~~a_2=\frac{a(a+1)b(b+1)}{2(2+1)1\times2}~~~,~~~a_3=\frac{a(a+1)(a+2)b(b+1)(b+2)}{2(2+1)(2+2)1\times2\times3}~~~,~~~\cdots$$â
and in general
â$$a_{n+1}=\frac{(a+n)(b+n)}{(2+n)(1+n)}a_n$$â
âthen the answer is
â$$y=1+\frac{ab}{2\times1}x+\frac{a(a+1)b(b+1)}{2(2+1)\times2!}x^2+\frac{a(a+1)(a+2)b(b+1)(b+2)}{2(2+1)(2+2)\times3!}x^3+\cdots$$â
or
â$$y=1+\sum_{n=1}^\infty\frac{a(a+1)\cdots(a+n-1)b(b+1)\cdots(b+n-1)}{2(2+1)\cdots(2+n-1)n!}x^n$$â</p>
"
"2397728","2397776","<p>Scalar Product: </p>

<p>1)$ \vec r :=(x,y)$ , $||(x,y)|| = 1$.</p>

<p>2) $\vec s := (a,b)$ , $||(a,b)|| = 1$.</p>

<p>$\vec r \cdot \vec s = ax +yb =$</p>

<p>$ ||\vec r||$ $ ||\vec s|| \cos(\phi) $.</p>

<p>$\Rightarrow$</p>

<p>$-1 \le ax +yb \le 1$, since </p>

<p>$ -1 \le cos(\phi) \le 1$.</p>
"
"2397758","2397770","<p>(1) Given $|\ell|&lt;1$, you want to find $\ell'$ so that $|\ell|&lt;\ell'&lt;1$ and </p>

<p>$$\frac{|x_{n+1}|}{|x_n|}&lt;\ell'$$</p>

<p>when $n$ is large. One such $\ell'$ is to take $\ell'$ as the midpoint of $|\ell|$ and $1$. That is $|\ell'| = \frac{|\ell|+1}{2}$ and so </p>

<p>$$ \ell' - |\ell| = \frac{|\ell|+1}{2} - |\ell| = \frac{1-|\ell|}{2}$$</p>

<p>is the epsilon they choose. It doesn't have to be this $\ell'$ (and $\epsilon$). Any $\ell'$ so that $|\ell|&lt;\ell'&lt;1$ would do the job. </p>

<p>(2) So $\frac{|x_{n+1}|}{|x_n|}&lt;\frac{1+|\ell|}{2}$ for all $n\ge N_0$. In particular multiplying the inequalities for $n=N_0$ and $n=N_0+1$ gives</p>

<p>$$ \frac{|x_{N_0+2}|}{|x_{N_0}|}= \frac{|x_{N_0+2}|}{x_{N_0+1}|}\frac{|x_{N_0+1}|}{|x_{N_0}|}&lt; \left( \frac{1+|\ell|}{2}\right)^2.$$</p>

<p>Which proves the inequality for $n = N_0 +1$. In general you can multiply $n$'s inequalities to obtain (**). Of course the rigorous way is to do induction. </p>

<p>(3) This is sandwich's theorem (aka squeeze theorem), where the existence of limit is part of the conclusion. </p>
"
"2397762","2397777","<p>Yes, the proof you're presenting is in fact using $\sf DC$.</p>

<p>It is consistent that there is a linearly ordered set which is infinite and Dedekind finite. Such set cannot be well-ordered, so the linear order itself is not a well-order; however every countable subset is finite, so there is no copy of the negative integers inside that order.</p>

<p>As for your proof, you should limit $&gt;$ to $S$ explicitly. Otherwise it is not necessarily the case that $&gt;$ has the full domain. For example in $[0,1]$, $0$ is not in the domain of $&gt;$ as it is in fact the minimum element.</p>
"
"2397768","2397780","<p>It would not be wrong, but useless.</p>

<p>If a strictly positive bilinear form exists on a Banach space $(V,\|\cdot\|_1)$,
then the Banach space is equivalent to a Hilbert space
in the sense that they have the same topology.</p>

<p><em>Proof</em>:
As you already said, the topology defined by $B$ is the same as the original topology.
If $B$ is symmetric, you can show, that $B$ defines a scalar product on $V$, thus $V$ is a Hilbert space.
If $B$ is not symmetric, then $\langle x,y\rangle := B(x,y)+B(y,x)$ defines a scalar product.</p>

<p>So you cannot apply a Banach space version of Lax-Milgram to spaces such as $L^p(\Omega)$
for $p\neq 2$, because you cannot find a $B$ that satisfies the conditions.</p>
"
"2397769","2397814","<p>Hint. Since $2^3\equiv 1 \pmod {7}$, consider $n=3q+r$ with the remainder $r=0,1,2$. </p>

<p>If $r=0$, then $1\leq q \leq 3333$ and 
$$0\equiv 2^n - n^2=1-9q^2\equiv 1-2q^2\implies q\equiv 2, q\equiv 5\pmod{7}$$
Therefore the number of $n=6q$ is equal to
$$\left\lfloor\frac{3333-2}{7}\right\rfloor+1+\left\lfloor\frac{3333-5}{7}\right\rfloor+1=952.$$</p>

<p>Now consider the remaining similar cases. At the end you should find that for $r=1,2$ the number of $n$s are respectively $953$, $953$. </p>

<p>Therefore the answer should be $952+953+953=2858$.</p>
"
"2397774","2397788","<p>The claim is not true. </p>

<p>Counterexample: compare the sets $A=[0,1)$ and $B=[0,1]$. </p>

<p>There $A$ is a <strong>proper</strong> subset of $B$ with $m(A)=m(B)=1&lt;\infty$.</p>

<hr>

<p>In the sequel we work in space $[0,1]$ so for every $A\subseteq[0,1]$ notation $A^{\complement}$ stands for $[0,1]-A$. </p>

<hr>

<p>Note that $m(E_n^{\complement})+1=m(E_n^{\complement})+m(E_n)=m([0,1])=1$ for every $n$, so that $m(E_n^{\complement})=0$ for every $n$. </p>

<p>Consequently:</p>

<p>$$m\left(\left(\bigcap_{n=1}^{\infty}E_n\right)^{\complement}\right)=m\left(\bigcup_{n=1}^{\infty}E_n^{\complement}\right)\leq\sum_{n=1}^{\infty}m\left(E_n^{\complement}\right)=0\text{  implying that }m\left(\bigcap_{n=1}^{\infty}E_n\right)=1$$</p>
"
"2397775","2397804","<p>The answer to your question is that both methods work.</p>

<hr>

<p><strong>Method 1:</strong></p>

<p>Separating variables, and integrating both sides, we obtain:
$$\int dP=\int 400\left(1+\frac{2t}{\sqrt{25+t^2}}\right)~dt$$
$$P=400(2\sqrt{t^2+25}+t)+C$$
Using the initial condition $P(0)=60000$ to solve for $C$, we obtain:
$$60000=400(2\sqrt{25}+0)+C \implies C=56000$$
Therefore, we obtain the population at $t=5$ as:
$$P(5)=400(2\sqrt{5^2+25}+5)+56000=2000(2\sqrt{2}+1)+56000 \approx 63656.8542$$</p>

<hr>

<p><strong>Method 2:</strong></p>

<p>Setting the appropriate bounds and separating variables:
$$\int_{60000}^{P(5)} dP=\int_0^5 400\left(1+\frac{2t}{\sqrt{25+t^2}}\right)~dt$$
Evaluating the integrals, we obtain:
$$P(5)-60000=2000(2\sqrt{2}-1)$$
Solving for $P(5)$ gives us the population at $t=5$ years:
$$\begin{align}P(5)&amp;=2000(2\sqrt{2}-1)+60000\\&amp;=2000(2\sqrt{2}+1)+56000 \end{align}$$
The answer is the same as in Method 1.</p>
"
"2397781","2397789","<p>Sure, this occurs naturally. Consider $F = \mathbb Q$ and its algebraic closure $K = \bar{\mathbb Q}$. Now consider $\mathbb Q$ as a subfield of $K' = \mathbb C$. </p>

<p>$\mathbb C$ is algebraically closed, its an extension of $\mathbb Q$ but it is not algebraic over $\mathbb Q$.</p>

<hr>

<p><strong>edit:</strong> Here is another example. Let $F$ be a finite field and let $K$ be its algebraic closure. Let $F \subseteq F'$ be an extension of $F$ of sufficiently large cardinality (i.e. uncountable) and let $K'$ be the algebraic closure of $F'$. (*)</p>

<p>$K'$ is a field extension of $F$ that is algebraically closed but it cannot be algebraic over $F$ because its cardinality is too large. I.e. there are only countable many polynomial roots over $F[X]$ but $K'$ contains uncountably many elements - most of which cannot be algebraic over $F$. In this fashion you can get an arbitrarily large 'gap' between the cardinality of the algebraic closure of $F$ and an algebraically closed extension. (The fact that $F$ is finite is irrelevant here - I simply thought it might help to make this example more approachable.)</p>

<p>(*) Note that $F'$ exists by the existence of the infinite field $K$ and the upward LÃ¶wenheim-Skolem Theorem, but you can also just view it as a field extension $F' = F(x_i \mid i \in I)$ with indepenent $x_i$ and $I$ uncountable.</p>
"
"2397782","2397785","<p>Because $0\cdot \infty$ is an <a href=""https://en.wikipedia.org/wiki/Indeterminate_form"" rel=""nofollow noreferrer"">Indeterminate form</a>.
Note that
$${\frac{1}{2^x-5^x+3^x}}=\left(\frac{1}{5}\right)^x\cdot \frac{1}{\left(\frac{2}{5}\right)^x-1+\left(\frac{3}{5}\right)^x}.$$
Since $1/5$, $2/5$, and $3/5$ are positive numbers less than $1$, what may we conclude as $x\to +\infty$?</p>
"
"2397793","2397801","<p>Consider $A=\{(x,y)\,:\, xy\ge 1,\ x\ge 0\}$ and $B=\{(-x,y)\,:\, (x,y)\in A\}$.</p>

<p>Then, $A+B=\Bbb R\times (0,\infty)$: if $B$ slides horizontally along the half-line $[x,\infty)\times\left\{\frac1x\right\}$, the whole upper half plane $\{y&gt; \frac1x\}$ gets covered; doing it for all $x\ge0$ covers $\{y&gt;0\}$ and, of course, $A+B$ cannot contain points $(\alpha,\beta)$ with $\beta\le 0$.</p>
"
"2397794","2397797","<p>Here is a $2\times 2$ counterexample, easily extendable to $n\times n$: Let $A$ orthogonally project onto one axis, and let $B$ rotate the plane by $90^\circ$. The operation of $ABA$ is to collapse everything down to one axis, then turn that axis, then collapse that axis down to the origin. However, $A^2 = A\neq 0$.</p>

<p>Specifically, $A = \left[\begin{smallmatrix}1&amp;0\\0&amp;0\end{smallmatrix}\right]$ and $B = \left[\begin{smallmatrix}0&amp;-1\\1&amp;0\end{smallmatrix}\right]$.</p>
"
"2397808","2397848","<p>Plane:</p>

<p>Generally: $ \vec r = \vec a + t \vec u + s \vec v$;</p>

<p>This plane passes through the point</p>

<p>specified by $\vec a$.</p>

<p>The direction vectors  $ \vec u $ and $\vec v$, </p>

<p>starting from this point, vector addition, span the plane.</p>

<p>$\vec a = (0,0,0), \vec u = (1,2,3)$ , and</p>

<p>$\vec v= (-2,1,1).$</p>

<p>For the problem (solution) </p>

<p>the normal vector $ \vec n $,  perpendicular </p>

<p>to both direction vectors is required.</p>

<p>Cross product of vectors:</p>

<p>$\vec n = \vec u Ã \vec v = (-1,-7,5)$;</p>

<p>Plane:</p>

<p>$\vec n \cdot ( \vec r - \vec a) = 0$,</p>

<p>$(-1,-7,5) \cdot (x,y,z) =0$.</p>

<p>Finally:  $ -x -7y +5z = 0.$</p>

<p>Helps?</p>
"
"2397817","2397845","<p>As I mentioned in my comment, you are confused with what the question being asked is. It wants you to show that the rate of change, $R'$, (not $R$) achieves a maximum at $\frac{k}{2}$.</p>

<p>By setting $R''=0$, you have shown that there is a critical point at $\frac{k}{2}$, now you only have to show that it is a maximum.</p>

<p>I will take the approach of the second derivative test as you mentioned in the title.</p>

<p>We must find the second derivative of $R'$, namely, $R'''$.</p>

<p>$R'''=\frac{d}{dD}(R'')=\frac{d}{dD}(K-2D)=-2.$</p>

<p>Since $R'''$, the second derivative of $R'$, is negative regardless of what $D$ is, then $D= \frac{k}{2}$ which we found as the only critical point is a maximum.</p>
"
"2397824","2397839","<p>You are on the right track. Note that $2^x-x$ is increasing for $x\geq 1$. Therefore $f(x)=\frac{1}{2^{2^x-x}}$ is decreasing and
$$\sum_{n=1}^\infty\frac{2^n}{2^{2^n}}=\sum_{n=1}^\infty f(n)\geq \sum_{n=1}^\infty \int_{n}^{n+1}f(x) dx=\int_1^{\infty}f(x)dx=\left[-\frac{1}{2^{2^x}(\ln 2)^2}\right]_1^{\infty}=\frac{1}{4(\ln 2)^2}.$$
This is a stronger inequality because the sum starts form the index $1$ and $f(0)=1/2&gt;0$.</p>
"
"2397844","2397847","<p>Let $(5+2\sqrt6)^{x^2-3}=t$.
Hence, $t+\frac{1}{t}=10$ and we have $t=5\pm2\sqrt{6}$.</p>

<p>Thus, $x^2-3=1$ or $x^2-3=-1$, which gives the answer:
$$\{2,-2,\sqrt2,-\sqrt2\}$$</p>
"
"2397850","2397960","<p>The generalization you expect is correct and it works for any $k$ and $n$. What you need in order to prove this is just the fact how to differentiate multilinear maps between vector spaces, and apply this to the map $(\mathbb R^n)^k\to\bigwedge^k(\mathbb R^n)$ defined by $(v_1,\dots,v_k)\mapsto v_1\wedge\dots\wedge v_k$. </p>
"
"2397852","2397886","<p>Fix $c \in I$. </p>

<p>We can assume that $f(c) \neq 0$, since the continuity of $|f|$ at those points where $f=0$ implies the continuity of $f$. This is fairly trivial.  </p>

<p>So, since $|f|$ is continuous at $c$, there is a $\delta&gt;0$ such that when $x \in (c-\delta, c+\delta)$, $|f(x)| \in \left(\frac{|f(c)|}{2}, \frac{3|f(c)|}{2}\right)$ which implies $f(x) \in \left(\frac{|f(c)|}{2}, \frac{3|f(c)|}{2}\right) \bigcup \left(-\frac{3|f(c)|}{2}, -\frac{|f(c)|}{2}\right)$</p>

<p>Note that the set $$X = \left(\frac{|f(c)|}{2}, \frac{3|f(c)|}{2}\right) \bigcup \left(-\frac{3|f(c)|}{2}, -\frac{|f(c)|}{2}\right)$$ is nonempty and not connected. Any connected subset of $X$ is necessarily a subset of only one of the two intervals. </p>

<p>Since $f$ is Darboux, the image of $(c-\delta, c+\delta)$ under $f$ must be connected. This means $f[(c-\delta, c+\delta)] \subset \left(\frac{|f(c)|}{2}, \frac{3|f(c)|}{2}\right)$
 <a href=""https://en.wikipedia.org/wiki/Exclusive_or"" rel=""nofollow noreferrer"">XOR</a>  $\ f[(c-\delta, c+\delta)] \subset \left(-\frac{3|f(c)|}{2}, -\frac{|f(c)|}{2}\right)$</p>

<p>which implies $|f| \equiv f$ for all $x \in (c-\delta, c+\delta)$, XOR $|f| \equiv -f$ for all $x \in (c-\delta, c+\delta)$. In either case, the continuity of $f$ at $c$ follows. </p>

<p>Since $c$ is arbitrary, we conclude $f$ is continuous on $I$. </p>
"
"2397855","2397915","<p>Let us take $f\in C_c(\Omega)$. We want to find a sequence $f_n\in C_c(\Omega)\cap H^1(\Omega)$ converging to $f$ uniformly on $\Omega$. We are going to construct a sequence $f_n\in C_c^\infty(\Omega)\subseteq C_c(\Omega)\cap H^1(\Omega)$.</p>

<p>Let us take some open sets $\Omega',\Omega''$ such that $supp(f)\subset\Omega'\subset\subset\Omega''\subset\subset\Omega$ (where $supp$ denotes the support of a function) and let $\chi\in C^\infty_c(\Omega)$ such that $\chi|_{\overline{\Omega'}}=1$ and $\chi|_{\overline{\Omega}\setminus\Omega''}=0$. Let us extend $f$ to zero out of $\Omega$. We know that by convolution with a mollifier there exists a sequence $g_n\in C_c^\infty(\mathbb{R}^n)$ that converges to $f$ con compact sets. Now we consider $f_n=\chi g_n$.</p>

<p>We have $f_n\in C^\infty_c(\Omega)$ with $supp(f_n)\subseteq \overline{\Omega''}$; $f_n$ converges uniformly to $f$ on $\overline{\Omega'}$ and on $\overline{\Omega''}\setminus \Omega'$, in particular $f_n$ converges to zero on $\overline{\Omega''}\setminus \Omega'$. Hence we obtain:</p>

<p>$\sup_{x\in\overline{\Omega''}}|f_n-f|\le\sup_{x\in\overline{\Omega'}}|f_n-f|+\sup_{x\in\overline{\Omega''}\setminus \Omega'}|f_n-f|=
            \sup_{x\in\overline{\Omega'}}|g_n-f|+\sup_{x\in\overline{\Omega''}\setminus \Omega'}|f_n|\longrightarrow 0$</p>
"
"2397858","2397863","<p>Hint. A string of lenght $n+1$ may end with a one letter word of with a two-letter word. Therefore for $n\geq 2$,
$$a_{n+1}\leq 2a_n+124a_{n-1}.$$
Now show the inequality by induction. In the inductive step note that
$$a_{n+1}\leq 2a_n+124a_{n-1}\leq 2\cdot 13^n+124\cdot 13^{n-1}=
(2\cdot 13+124)13^{n-1}\leq 13^{n+1}.$$</p>
"
"2397859","2398325","<p>Let $(R, m, k)$ be the commutative local ring with finitely many ideals and $\#k = \infty$. We need the following claim.</p>

<p><em>Claim</em>: $\dim_k m/m^2 \le 1$.</p>

<p>Let us see first that how the statements follows from $\dim_k m/m^2 = 1$ (if $\dim_k m/m^2 = 0$ then $m = 0$ by the Nakayama's lemma and nothing to prove). </p>

<ul>
<li><p>$R$ is uniserial: Since $R$ is local, $R/m^2$ is uniserial by the claim. Hence $R$ is also uniserial.</p></li>
<li><p>$R$ is PIR: Take an element $\pi \in m \setminus m^2$. By the claim, $m = (\pi) + m^2$. Hence we have $m = (\pi)$ by the Nakayama's lemma. From the above proof, all the ideals are of the form $m^k = (\pi^k)$ for some $k \ge 0$.</p></li>
</ul>

<p><em>Proof of Claim</em>: (The basic idea is already shown in the comment of Mohan.) Assume $\dim_k m/m^2 \ge 2$. Then we have two elements $x, y \in m$ such that $\{x + m^2, y + m^2\}$ is linearly independent in $m/m^2$. Consider ideals 
$$I_\lambda := \{\, rx + \lambda ry + z \mid r \in R,\ z \in m^2 \,\}$$ 
parametrized by $\lambda \in k$. Since $\#k = \infty$, this yields an infinite series of ideals. Contradiction.
 QED.</p>
"
"2397874","2397877","<p>Consider three cases:.</p>

<p>i) $\bf x\leq -1$. Then $x\leq -1$, $2^x&lt; 1$, and the equation becomes
$\frac{1}{2}2^{-x} - 2^x = 1- 2^x + 1$, that is $2^{-x}=4$ which implies that $x=-2\leq -1$.</p>

<p>ii) $\bf -1&lt;x&lt;0$. Then $x&gt;-1$, $2^x&lt; 1$, and the equation becomes $2^x=2\cdot 2^{x} - 2^x = 1- 2^x + 1=2-2^x$, that is $2^x=1$ which is impossible for $-1&lt;x&lt;0$.</p>

<p>iii) $\bf 0\leq x$. Then $x&gt;-1$, $2^x\geq 1$, and the equation becomes $2^x=2^x$ which holds for all $x\geq 0$.</p>

<p>Hence the complete set of solutions is $\{-2\}\cup [0,+\infty)$.</p>
"
"2397876","2397889","<p>Here I will suppose that we have a sequence or random variables $X_i,\ i=1...n$ i.i.d. and that $\Pr[X_i \geq 1] = p$.</p>

<p>We can then define $Y_i = [X_i \geq 1]$, which means $Y_i = 1$ if $X_i \geq 1$, else $Y_i = 0$. We also define $Y=\sum\limits_{i=1}^n Y_i$.</p>

<p>We are therefore interested in $\Pr[Y \geq n_1]$.</p>

<p>Clearly $Y \sim Binomial(n,p)$. Hence $\Pr[Y = k] = \binom{n}{k} p^k (1-p)^{n-k}$. Summing for all $k \geq n_1$ :
$$
\Pr[Y \geq n_1] = \sum_{k=n_1}^n \binom{n}{k} p^k (1-p)^{n-k}
$$</p>
"
"2397883","2397899","<p>Yes, it converges uniformly. The only zero of $f_n'$ is $\displaystyle\sqrt[n]{\sqrt{1+n}-1}$. Furthermore, $f'(x)&lt;0$ when $x$ is greater than this number and $f'(x)&gt;0$ if $x$ is smaller. Therefore, the maximum of $f_n$ is$$f_n\left(\sqrt[n]{\sqrt{1+n}-1}\right)=\frac1{2\left(\sqrt{1+n}-1\right)}.$$Since$$\lim_{n\in\mathbb N}\left(\frac1{2\left(\sqrt{1+n}-1\right)}\right)_{n\in\mathbb N}=0,$$your sequence converges uniformly to the null function.</p>
"
"2397894","2397902","<p>This inequality is true because $\frac{ab}{a+b}\leq\frac{a+b}{4}$.</p>
"
"2397898","2397908","<p>You're right that $x=3$ solves the equation, but because there's an $x$ in the exponent, your book and/or teacher might exclude negative bases as $a^x$ doesn't behave <em>nicely</em> for $a&lt;0$.</p>

<p>More specifically, for an expression of the form $a^x$, or in your case $f(x)^x$, it's possible that they demand $a \ge 0$ or $f(x) \ge 0$. In that case, only $x&gt;4$ would satisfy this demand <em>and</em> the equation.</p>
"
"2397901","2397927","<p>Your argument is not correct, because:</p>

<ol>
<li>you did not assume that $U$ is open, but then you assert that it is (not a serious problem, of course);</li>
<li>you say that $U\neq\{y_1\}\cup\{u\}$ because $U$ is open and $\{y_1\}\cup\{u\}$ is closed. Can't a set be closed and open simultaneously?</li>
</ol>

<p>You can prove it as follows. Suppose that there is a neighborhood $U$ of $x$ with only a finite number of elements of $A$. Let $x_1,x_2,\ldots,x_n$ be these elements. Since the space is $T_1$, each $\{x_k\}$ is closed and therefore the set $C=\bigcup_{k=1}^n\{x_k\}$ is closed. But then $U\setminus C$ is a neighborhood of $x$ which does not intersect $A$, which is impossible, since $x$ is a limit point of $A$.</p>
"
"2397909","2397912","<p>For every $x\in\mathbb R\setminus\{0\}$, the vector</p>

<p>$$\begin{bmatrix}x\\0\end{bmatrix}$$</p>

<p>is an eigenvector of the matrix</p>

<p>$$\begin{bmatrix}1&amp;2\\0&amp;1\end{bmatrix}$$</p>
"
"2397916","2397933","<p>Suppose there were $182$ coins.  That could be $14Ã13$ or $26Ã7$, among other things.  So we could not tell how many bags or how many coins per bag there are.</p>

<p>Suppose there were $187$ coins.  That can only be $11Ã17$, the product of two primes.  But which is $11$, the number of bags or the number of coins per bag?</p>

<p>To avoid these ambiguities, you need a number of coins that is the product of two identical primes, meaning it's the square of this common prime.  There is only one such number between $150$ and $200$.</p>

<p>Apparently the king is not triskaidekaphobic ... .</p>
"
"2397918","2397940","<p>Let $$ S_n(p)=\sum_{k=1}^{n} k^p\qquad n, p\in\mathbb N $$</p>

<p>then, 
We know the following Binomial formula </p>

<p>$$ (k+1)^p = k^p+ \sum_{i=0}^{p-1}\binom{p}{i} k^i$$
where $\binom{p}{i}= \frac{p!}{i!(p-i)!}$.
Which implies that, </p>

<p>$$\sum_{k=1}^{n} (k+1)^p =\sum_{k=1}^{n} k^p+\sum_{i=0}^{p-1}\binom{p}{i} \sum_{k=1}^{n} k^i = S_n(p) +\sum_{i=0}^{p-1}\binom{p}{i} S_n(i) $$</p>

<p>But $$\sum_{k=1}^{n} (k+1)^p = \sum_{k=2}^{n+1} k^p = S_{n+1}(p) -1 = S_n(p) +(n+1)^p -1$$</p>

<p>Hence finally we get the relation:</p>

<p>$$(n+1)^p -1  =\sum_{i=0}^{p-1}\binom{p}{i} S_n(i) $$</p>

<p>From this it is possible to compute the sum for any $p\ge 1 $ in $ \mathbb N $.</p>
"
"2397920","2397958","<p>Spherical coordinates are not useful for this problem. Your description of the ""hole"" is not correct. Use instead cylindrical coordinates. The ""hole"" is then described as
$$
0\le r\le b,\quad 0\le\theta\le2\,\pi,\quad-\sqrt{1-r^2}\le z\le\sqrt{1-r^2}.
$$</p>
"
"2397931","2397968","<p>Another way could be logarithmic differentiation</p>

<p>$$y=\frac{x \cos^{-1}(x)} {\sqrt{1-x^2}}\implies \log(y)=\log(x)+\log(\cos^{-1}(x))-\frac 12\log({1-x^2})$$ Differentiate both sides 
$$\frac{y'}y=\frac 1x +\frac{ \left(\cos^{-1}(x)\right)'} {\cos^{-1}(x) }-\frac 12\frac{\left({1-x^2}\right)' }{{1-x^2} }$$ When done $$y'=y \times \frac{y'}y$$ and simplify.</p>
"
"2397938","2397947","<p>How the fact that $U_{\zeta} (\infty) = \infty$ would imply that $\rho \leqslant \zeta$ is a mystery to me, but, to prove the latter, one can note that, for every nonnegative $x$, $$U_{\zeta} (x) = \int_0^x y^{\zeta} dF(y)\leqslant\int_0^x x^{\zeta} dF(y)=x^\zeta F(x)\leqslant x^\zeta$$ hence the assumption that $$U_{\zeta} (x) \sim x^{\rho} L(x)$$ indeed implies that $$x^{\rho} L(x)\leqslant 2x^\zeta$$ when $x\to\infty$, in particular, $$\rho\leqslant\zeta$$</p>
"
"2397944","2397995","<p>Consider the number of non-negative integral solutions to $X_1 + X_2 + X_3 .. + X_7 = 7$. This should equal $\binom{7+ 7 - 1}{7 - 1} = \binom{13}{6}$.</p>

<p>These solutions can also be counted as follows:</p>

<p>Case 1: Let $X_1 + X_2 + X_3 = 0$ and $X_4 + X_5 + X_6 + X_7 = 7$. The combined number of solutions for these are $\binom{2}{2} \cdot \binom{10}{3}$</p>

<p>Case 2: Let Let $X_1 + X_2 + X_3 = 1$ and $X_4 + X_5 + X_6 + X_7 = 6$. The combined number of solutions for these are $\binom{3}{2} \cdot \binom{9}{3}$.</p>

<p>.
.
.</p>

<p>Case 8: Let Let $X_1 + X_2 + X_3 = 7$ and $X_4 + X_5 + X_6 + X_7 = 0$. The combined number of solutions for these are $\binom{9}{2} \cdot \binom{3}{3}$.</p>

<p>Thus, we get that $$\binom{13}{6} = \binom{2}{2} \cdot \binom{10}{3} + \binom{3}{2} \cdot \binom{9}{3} + \ .. \ + \binom{9}{2} \cdot \binom{3}{3}$$ </p>
"
"2397952","2398052","<p>What you showed is $|f(X) - L| &lt; \epsilon \implies |X -X_0| &lt; \frac{\epsilon}{4}$.</p>

<p>What you need to show is $0 &lt; |X -X_0| &lt;\delta \implies |f(X) - L| &lt; \epsilon$.</p>

<blockquote>
  <p>The part about $0 &lt; |X -X_0|$ is there because the value of the limit of $f(X)$ as $X$ approaches $X_0$ needs to have nothing to do with the actual value of $f(X_0)$. In, fact $f(X_0)$ does not even need to exists. For example, if you define $$f(x) = \begin{cases} 10000 &amp; \text{If $x=7$} \\ x &amp; \text{If $x\ne 7$} \end{cases}$$ then $\displaystyle \lim_{x \to 7} f(x) = 7$. In your case, $f(X)$ is ""well-behaved"" at $X = X_0$ so you don't need to worry about the case $X=X_0$.</p>
</blockquote>

<p>Your proof then would look like this.</p>

<p>Let $\epsilon &gt; 0$ be given. Define $\delta = \frac 19\epsilon$ and let $0 &lt; |X -X_0| &lt;\delta$</p>

<p>$|f(X) - f(1,2,1)|=|3(x-1)+4(y-2)+(z-1)| \le 3|x-1| + 4|y-2|+|z-1|$</p>

<p>Note that 
\begin{array}{l}
   |x-1| \le |X-(1,2,1)| &lt; \delta \\
   |y-2| \le |X-(1,2,1)| &lt; \delta \\
   |z-1| \le |X-(1,2,1)| &lt; \delta \\
\end{array}</p>

<p>So $|f(X) - f(1,2,1)| \le 3\delta + 4\delta + \delta &lt; 9\delta = \epsilon$</p>

<p>It follows that $\lim_{X\to X_0} f(X) = f(X_0)$</p>
"
"2397959","2397974","<p><strong>Hint:</strong>
Rewrite as
$$-\frac1{2(1+i)z^2}\cdot\frac{8+z}{1-\cfrac z{2(1+i)}}=-\frac{1-i}{4z^2}\cdot\frac{8+z}{1-\cfrac{(1-i)z}{4}}.$$</p>
"
"2397962","2397967","<p>If $y \ne 0$, then we get from $2y^3=xy$ that $x=2y^2$. From $x^2=3y^2$ we then derive </p>

<p>$4y^4=3y^2$.</p>

<p>Therefore $4y^2=3$.</p>

<p>Can you proceed ?</p>
"
"2397963","2397970","<p>I suspect the denominateur is $\tan 2t$. If I'm right, just use <em>equivalents</em>: near $0$, we have
$$\sin u\sim_0 u,\quad \tan u\sim_0 u,,$$
so that
$$\frac{\sin3x}{\tan2x}\sim_0\frac{3x}{2x}=\frac32.$$</p>
"
"2397964","2398150","<p>Dufford-Pettis theorem is a promising approach. We are sure it works on probability spaces and we will use tightness assumption to be reduced to that case. </p>

<p>For any integer $R$, the sequence $\left(u_n\mathbf 1_{B_R}\right)_{n\geqslant 1}$ is bounded in $\mathbb L^q$ hence if we consider the probability space $\left(B_R,\mathcal B\left(B_R\right),\lambda/\lambda\left(B_R\right)\right)$, we can extract a subsequence which converges weakly in $\mathbb L^1$ to some $u_R$, that is, such that for any measurable bounded $v$, 
$$\tag{*}    \lim_{j\to +\infty}      \int_{\mathbb R^N}u_{n_j} (x)v(x)\mathbf 1_{B_R}(x)\mathrm dx =\int_{\mathbb R^N}u_R (x)v(x)\mathbf 1_{B_R}(x)\mathrm dx.      $$<br>
By the diagonal process, the subsequence can be chosen independently of $R$, hence (*) holds for a fixed sequence $\left(n_j\right)_{j\geqslant 1}$. 
Also, observe that $u_R=u_{R'}$ on $B_R$ if $R \lt R'$ hence we can safely remove the dependence in $R$ write $u_R=u$. For the moment, we got the following: there exists an increasing sequence of integers $\left(n_j\right)_{j\geqslant 1}$ and a function $u$ such that for any integer $R$ and any $v\in\mathbb L^\infty$, 
 $$     \lim_{j\to +\infty}      \int_{\mathbb R^N}u_{n_j} (x)v(x)\mathbf 1_{B_R}(x)\mathrm dx =\int_{\mathbb R^N}u (x)v(x)\mathbf 1_{B_R}(x)\mathrm dx.      $$<br>
Now, we use the tightness assumption: for any positive $\varepsilon$, there exists $R$ such that for all $j$,
$$\left\lvert \int_{\mathbb R^N}u_{n_j} (x)v(x)\left(1- \mathbf 1_{B_R}(x)\right) \mathrm dx\right\rvert\leqslant \varepsilon\left\lVert v\right\rVert_{\mathbb L^\infty}              $$ 
and the same holds with $u_{n_j}$ replaced by $u$. This proves that $u_{n_j}\to u$ weakly in $\mathbb L^1$.       </p>
"
"2397985","2398354","<p>If $D$ is any divisor with $|D|=r&gt;0$ and $P\in X$ is any point, then $|D-P|\geq r-1$. So, in your situation, we get $|D-P-Q|\geq 0$, which means it is non-empty. Thus, there exists an effective divisor $D'\in |D-P-Q|$, that is to say, $D\sim D'+P+Q$, which is what you want.</p>
"
"2397986","2398080","<p>Let $A$, $B$, $P$ be the endpoints of your line segment and your given point respectively.</p>

<p>Noting from don-joe's answer that you need the angles $\angle PAB$ and $\angle PBA$ to both be less than $90^{\circ}$, the angles don't need to be computed explicitly as the sign of the dot product gives the answer: the dot product $u \cdot v = |u| |v| \cos \theta$ where $\theta$ is the angle between the vectors.</p>

<p>So the perpendicular can be constructed if
$$(P-A) \cdot (B-A) \ge 0 \land (P-B) \cdot (A-B) \ge 0$$</p>
"
"2397994","2398004","<p>A linear operator $P$ on a space $V$ is a projector if it satisfies $P^2=P$. When that is the case, the image of$~P$ (acting from the left on vectors) is a subspace $A\subseteq V$ with (by definition) $P\cdot v\in A$ for all $v\in V$, and moreover $P|_A=\mathrm{id}_A$. Dually there is an image subspace $B\subseteq V^*$ for $P$ acting on the right on the dual space $V^*$, with (again by definition) $\phi\cdot P\in B$ for all linear forms $\phi\in V^*$, and $P$ acts on the right as the identity on$~B$. It is not hard to see that $B$ is the set of linear forms vanishing on $\ker P\subseteq V$.</p>

<p>In other words, <em>all</em> projectors are of the kind you describe, for appropriate $A$ and $B$.</p>
"
"2397999","2398008","<p>You can't really simplify it much, but you can use the fact that the derivative of a difference is the difference of derivatives. For example: $$\dfrac{\partial\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right)}{\partial x}=\dfrac{\partial}{\partial x}\dfrac{\partial Q}{\partial x}-\dfrac{\partial}{\partial x}\dfrac{\partial P}{\partial y}=\dfrac{\partial^2 Q}{\partial x^2}-\dfrac{\partial^2 P}{\partial x\partial y}$$</p>
"
"2398015","2398315","<p>Assume that there is a set $O \in \mathcal{T}_1$ such that $O \cap (0,1]=[\frac{1}{2},1]$</p>

<p>Thus $\frac{1}{2} \in O$ and $O$ is open thus exists $\epsilon&gt;0$ such that $(\frac{1}{2} -\epsilon,\frac{1}{2}+\epsilon) \subseteq O$ and </p>

<p>Note that $(\frac{1}{2}-\epsilon,\frac{1}{2}+\epsilon) \cap (0,1] \subseteq O \cap (0,1].$</p>

<blockquote>
  <p>From this we see that we cannot have $\epsilon&gt;1/2$ because if $\epsilon&gt; \frac{1}{2}$ then $\frac{1}{2}- \epsilon=c&lt;0$</p>
  
  <p>Thus $(c,\frac{1}{2}+\epsilon) \cap(0,1]=(0,1] \subseteq [\frac{1}{2},1]$  which is a contradiction.</p>
</blockquote>

<p>So we have that $\epsilon&lt; \frac{1}{2}$ and $$(\frac{1}{2}- \epsilon,\frac{1}{2}+ \epsilon)=(\frac{1}{2}-\epsilon,\frac{1}{2}+\epsilon) \cap (0,1] \subseteq O \cap (0,1]=[\frac{1}{2},1]$$</p>

<p>Therefore $$\frac{1}{2}- \frac{\epsilon}{2} \in [\frac{1}{2},1]$$ which is a contradiction</p>
"
"2398017","2398081","<p>We have</p>

<p>$$\sum_{k = 1}^n \cos \sqrt{k} = \frac{\cos 1 + \cos \sqrt{n}}{2} + \int_1^n \cos \sqrt{t}\,dt - \int_1^n \bigl(\lbrace t\rbrace - \tfrac{1}{2}\bigr)\frac{\sin \sqrt{t}}{2\sqrt{t}}\,dt.$$</p>

<p>The first term is bounded, and the second integral is $O(\sqrt{n})$ as one sees from the boundedness of $\sin \sqrt{t}$ and $\lbrace t\rbrace - \frac{1}{2}$. It thus remains to estimate</p>

<p>\begin{align}
\int_1^n \cos \sqrt{t}\,dt &amp;= 2\int_1^{\sqrt{n}} u\cos u\,du \\
&amp;= 2u\sin u \biggr\rvert_1^{\sqrt{n}} - 2\int_1^{\sqrt{n}} \sin u\,du \\
&amp;= 2\sqrt{n}\sin \sqrt{n} - 2\sin 1 + 2\cos \sqrt{n} - 2,
\end{align}</p>

<p>which clearly is in $O(\sqrt{n})$.</p>

<p>Generally, for $0 &lt; \alpha &lt; 1$, we obtain</p>

<p>$$\sum_{k = 1}^n \cos k^{\alpha} = \frac{\cos 1 + \cos n^{\alpha}}{2} + \int_1^n \cos t^{\alpha}\,dt - \alpha\int_1^n p_1(t)t^{\alpha-1}\sin t^{\alpha}\,dt,\tag{$\ast$}$$</p>

<p>where $p_1(t) = \lbrace t\rbrace - \frac{1}{2}$. We can estimate the first integral substituting $u = t^{\alpha}$ and integrating by parts:</p>

<p>\begin{align}
\int_1^n \cos t^{\alpha}\,dt &amp;= \frac{1}{\alpha} \int_1^{n^{\alpha}} u^{\frac{1}{\alpha}-1}\cos u\,du \\
&amp;= \frac{u^{1/\alpha-1}\sin u}{\alpha}\biggr\rvert_1^{n^{\alpha}} - \frac{1}{\alpha}\biggl(\frac{1}{\alpha}-1\biggr)\int_1^{n^{\alpha}} u^{\frac{1}{\alpha}-2}\sin u\,du.
\end{align}</p>

<p>The first term is $\frac{1}{\alpha}\bigl(n^{1-\alpha}\sin n^{\alpha} - \sin 1\bigr)$, and using $\lvert \sin u\rvert \leqslant 1$ we see that the remaining integral also belongs to $O(n^{1-\alpha})$.</p>

<p>For the integral</p>

<p>$$\alpha\int_1^n p_1(t)t^{\alpha-1}\sin t^{\alpha}\,dt,$$</p>

<p>we immediately obtain an $O(n^{\alpha})$ bound using the boundedness of $p_1$ and $\sin$. For $\alpha \leqslant \frac{1}{2}$, this is smaller than the bound on the other integral. For $\alpha &gt; \frac{1}{2}$, we still have an $O(n^{\alpha})$ bound for the sum, which suffices to conclude that</p>

<p>$$\sum_{n = 1}^{\infty} \frac{\cos k^{\alpha}}{k}$$</p>

<p>converges. But we can lower the bound using integration by parts:</p>

<p>$$\int_1^n p_1(t) t^{\alpha-1} \sin t^{\alpha} = p_2(t)t^{\alpha-1}\sin t^{\alpha}\biggr\rvert_1^n - (\alpha-1)\int_1^n p_2(t)t^{\alpha-2}\sin t^{\alpha}\,dt - \alpha \int_1^n p_2(t)t^{2(\alpha-1)}\cos t^{\alpha}\,dt$$</p>

<p>where the first two terms on the right are bounded, and the last integral is elementarily bounded by $C\cdot n^{2\alpha - 1}$. Continuing integration by parts, we find that the last integral in $(\ast)$ belongs to $O(n^{1 - m(1-\alpha)})$ for every $0 &lt; m \leqslant \frac{1}{1-\alpha}$, and then eventually we obtain</p>

<p>$$\int_1^n p_1(t)t^{\alpha-1}\sin t^{\alpha}\,dt \in O(1),$$</p>

<p>so the sum belongs to $O(n^{1-\alpha})$ for every $\alpha \in (0,1)$ [this is also true for $\alpha = 0$ and $\alpha = 1$].</p>

<p>Noting that $\cos k^{\alpha} \geqslant \frac{1}{2}$ for</p>

<p>$$\bigl(2m - \tfrac{1}{3}\bigr)\pi \leqslant k^{\alpha} \leqslant \bigl(2m + \tfrac{1}{3}\bigr)\pi,$$</p>

<p>we see that the bound above is sharp. For $n \approx \bigl((2m+\frac{1}{3})\pi\bigr)^{1/\alpha}$, we have $\Theta(n^{1-\alpha})$ successive terms that are $\geqslant \frac{1}{2}$, whence the partial sum must have had at least order $\ell^{1-\alpha}$ for some $\ell \leqslant n$.</p>
"
"2398033","2398035","<p>This is true if and only if $\bar\Omega \setminus \Omega$ has measure zero. One extreme case is the following: $\Omega$ is a countable, dense subset. Then, $$\{0\} = L^2(\Omega) \ne L^2(\bar\Omega) = L^2(\mathbb R^n).$$</p>
"
"2398037","2398043","<p>Note that $$\int_{-\pi}^{\pi} \sin(nx)\sin(mx) dx = \begin{cases} 0 \quad \text{ if } n\neq m\\ \pi \quad \text{ if } n=m\end{cases}$$
Hence $$\int_{-\pi}^\pi\bigg(\sum_{n=1}^\infty\frac{\sin(nx)}{2^n}\bigg)^2dx = \pi \sum_{n=1}^{\infty} \frac{1}{2^{2n}} = \frac{\pi}{3}$$</p>

<hr>

<p>Alternatively, we can use the Parseval's theorem on the $C^\infty$ function $f(x) = \sum_{n=1}^{\infty} \frac{\sin(nx)}{2^n}$.</p>

<blockquote>
  <p>If the Fourier series of $g(x)$ is $$g(x) \sim \frac{a_0}{2} + \sum_{n=1}^{\infty} [a_n \cos(nx) + b_n \sin(nx)]$$
  Then $$\frac{1}{\pi}\int_{-\pi}^{\pi} g^2(x) dx = \frac{a_0^2}{2}+\sum_{n=1}^{\infty} (a_n^2+b_n^2)$$</p>
</blockquote>

<p>Here $a_n=0$ and $b_n=\frac{1}{2^n}$.</p>
"
"2398042","2398049","<p>If $a=30Â°,b=60Â°$</p>

<p>LHS=$\frac{5}{4}$</p>

<p>RHS=$\frac{13}{4}$</p>

<p>LHS$\ne $ RHS </p>

<p>Hence question is wrong</p>
"
"2398051","2398056","<p>By solving the equation: $$\binom{n}2=\frac12n(n-1)=10$$ in $\mathbb N$.</p>
"
"2398054","2398139","<p>Your logic is entirely right, but I believe you've miscounted the ways of having 3+2+2 flavours.</p>

<p>Since each pair of cones must share one flavour that the third doesn't have, and there are ways of choosing a pair of cones, three of your flavours must be assigned in this way - e.g. flavour $A$ to cones $1$ and $2$, $B$ to cones $1$ and $3$, and $C$ to cones $2$ and $3$ - leaving one flavour, $D$.</p>

<p>As you rightly note, this leftover flavour can then either be placed on no cones, 1 cone, or 2 cones, but not all 3. </p>

<ul>
<li>If we have 2+2+2, one flavour is left out, and permuting the other three is the same as permuting cones. There are thus $4!/3! = 4$ ways to assign flavours.</li>
<li>If we have 3+2+2, e.g. $ABD$, $AC$, and $BC$, then one flavour only appears once, and on the 3-scoop cone ($D$) and one flavour is not on that cone ($C$). Swapping the other two flavours ($A$ and $B$) is the same as swapping the 2-scoop cones, so $A$ and $B$ are indistinguishable. So there are $4!/2! = 12$ ways of doing this. <strong>This is where we differ</strong>.</li>
<li>If we have 3+3+2, e.g. $AB$, $ACD$, and $BCD$, then swapping $A$ and $B$ is equivalent to swapping the latter two cones, while $C$ and $D$ are clearly indistinguishable from each other. We have two pairs of indistinguishable flavours, so there are $4!/(2!*2!)=6$ ways of assigning flavours.</li>
</ul>

<p>For each case we can then permute the cones, and there are as you say $6$ ways of doing this. I thus get a total of $(4+12+6)*6 = 22*6 = 132$.</p>
"
"2398066","2398074","<p>$$\log _{(x-2)}\left(\frac{2x+3}{7-x}\right)&lt;1$$</p>

<p><strong>Hint</strong></p>

<p>$1)$ $x-2&gt;0$ and $x-2\ne 1$</p>

<p>$2)$ $\frac{2x+3}{7-x}&gt;0$</p>

<p>$2)$ If $x-2&gt;1$ then</p>

<p>$$\frac{2x+3}{7-x}&lt;x-2$$</p>

<p>$3)$ If $0&lt;x-2&lt;1$ then</p>

<p>$$\frac{2x+3}{7-x}&gt;x-2$$</p>

<p>Now you can solve it.</p>
"
"2398069","2398140","<p>You have
$$(p',q') = (p,q+1)$$
where 
$$p-q &gt;0,\;\;\text{and either}\;\;n &gt; p\;\;\text{or}\;\;q \ge n$$
To apply the inductive hypothesis, you need to have
\begin{align*}
&amp;{\small{\bullet}}\;\;p'-q' \ge 0\\[4pt]
&amp;{\small{\bullet}}\;\;p'-q' &lt; p-q\\[4pt]
&amp;{\small{\bullet}}\;\;\text{either}\;\;n &gt; p'\;\;\text{or}\;\;q' \ge n\\[4pt]
\end{align*}
Check the conditions one at a time . . .
\begin{align*}
&amp;p-q &gt; 0 \implies p-(q+1) \ge 0 \implies p'-q' \ge 0\\[4pt]
&amp;p'-q'=p-(q+1) &lt; p-q\\[4pt]
\end{align*}
To show $n &gt; p'\;\;\text{or}\;\;q' \ge n$, consider two cases . . .
<p>
$\qquad$Case $(1)\,\!\!:\;n &gt; p$.$\;\,$Then $n &gt; p \implies n &gt; p'$.
<p>
$\qquad$Case $(2)\,\!\!:\;q \ge n$.$\;\,$Then $q \ge n \implies q+1 &gt; n \implies q' &gt; n \implies q' \ge n$.
<p>
Thus, in each of the two cases, the requirements on $(p',q')$ are met.
<p>
Therefore the inductive hypothesis can be applied.</p>
"
"2398085","2398090","<p>The second equality is wrong. You're replacing an $(X_1,X_2,\dots,X_{n-1})$-measurable random variable on the left with its conditional expectation with respect to $(X_1,X_2,\dots,X_{n-2})$. </p>

<p>There's no reason for that not to change the random variable. </p>
"
"2398086","2398308","<p>As you say, $\alpha$ pulls back to $x$ and also pulls back to $\beta$ in each fiber.  Explicitly, $P(g)^*(\alpha)=x$ and $h^*(\alpha)=\beta$ if $h$ is the composition of the inclusion of a fiber $j:\mathbb{R}P^{n-1}\to P(E)$ and $P(g):P(E)\to \mathbb{R}P^\infty$.  Since $h=P(g)j$, $$\beta=h^*(\alpha)=j^*(P(g)^*(\alpha))=j^*(x),$$ which is exactly what you want.</p>
"
"2398095","2398101","<p>There are many lecture notes and other material on the web about Fuchsian groups. Just to give an example of another book:</p>

<p>$\bullet \;$ Topics on Riemann Surfaces and Fuchsian Groups, by Emilio Bujalance GarcÃ­a,A. F. Costa,E. MartÃ­nez.</p>

<p>Further references are also given at <a href=""https://mathoverflow.net/questions/260193/why-are-fuchsian-groups-interesting"">Mathoverflow</a>. If you search a bit yourself, you will find much material.
You already may have seen the <a href=""http://www.maths.manchester.ac.uk/~cwalkden/hyperbolic-geometry/limit_sets.pdf"" rel=""nofollow noreferrer"">Reading material list for Fuchsian groups</a>. </p>

<p>The more interesting question is: what is really <em>relevant</em> for your seminar talk.</p>
"
"2398098","2398111","<p>The red parts <em>are</em> included in $D'$ as the angles of $\pm\tfrac{\pi}{3}$ do not correspond to the (straight) lines you drew, but to the points of intersection between the circles $x^2+y^2=1$ and $x^2+y^2=2x$.</p>

<blockquote>
  <p>But then again, <strong>how do we find the $\theta$?</strong></p>
</blockquote>

<p>To find this angle, look for the points of intersection:
$$\left\{\begin{array}{l}x^2+y^2=1\\x^2+y^2=2x\end{array}\right. \implies 2x=1$$
Now $x=r\cos\theta$ in polar coordinates and on the unit circle, $r=1$; so:
$$\cos \theta = \frac{1}{2} \implies \theta = \pm\frac{\pi}{3}$$</p>
"
"2398102","2398115","<p>Notice that you can cancel the common factor $2$ first.</p>

<p>Since $x=0$ is a double root, you need to take:
$$\frac{1}{x^2 (x+1)} = \frac{a}{x^2} + \frac{b}{x} + \frac{c}{x+1}$$
Can you take it from there?</p>

<hr>

<p>Sometimes, you can avoid having to solve a system (in your case of three linear equations in three unknowns) to find the coefficients/numerators with a few handy manipulations.</p>

<p>$$\begin{align}
\frac{1}{x^2 (x+1)} 
&amp; = \frac{1\color{blue}{+x-x}}{x^2 (x+1)} \\[5pt]
&amp;  = \frac{1+x}{x^2 (x+1)} - \frac{x}{x^2 (x+1)} \\[5pt]
&amp;  = \frac{1}{x^2} - \frac{1}{x (x+1)}\\[5pt]
&amp;  = \frac{1}{x^2} - \frac{1\color{blue}{+x-x}}{x (x+1)}\\[5pt]
&amp;  = \frac{1}{x^2} - \frac{1+x}{x (x+1)}+\frac{x}{x (x+1)}\\[5pt]
&amp;  = \frac{1}{x^2} - \frac{1}{x}+\frac{1}{x+1}
\end{align}$$</p>
"
"2398127","2398704","<p>Life will be simpler if we simplify the OGF using trigonometry.</p>

<p>Since $a \in [0,\frac12]$, if we define $s = 1-2a$ and $c = \sqrt{4a(1-a)}$, we will find $s, c \in [0,1]$.  </p>

<p>Notice $c^2 + s^2 = 1$, we can pick a $\theta_0 \in [0,\frac{\pi}{4}]$ such that $c = \cos(2\theta_0)$ and $s = \sin(2\theta_0)$.  </p>

<p>For $0 \le cx \le c \le 1$, pick a $\theta \in [\theta_0,\frac{\pi}{2}]$ such that $cx = \cos(2\theta)$. </p>

<p>We can rewrite the OGF as</p>

<p>$$\begin{align}{\rm OGF} = \sum_{n=0}^\infty f_n x^n
&amp;= \sqrt{1 - c^2 x - s\sqrt{1 - c^2x^2}}\\
&amp;= \sqrt{1 - c\cos(2\theta) - s\sin(2\theta)}
= \sqrt{1 - \cos(2(\theta-\theta_0))}\\
&amp;= \sqrt{2}\sin(\theta - \theta_0)
 = \sqrt{2}\left(\cos\theta_0\sin\theta - \sin\theta_0\cos\theta\right)\\
&amp;= \cos\theta_0\sqrt{1-\cos(2\theta)} - \sin\theta_0\sqrt{1+\cos(2\theta)}\\
&amp;= \cos\theta_0\sqrt{1-cx} - \sin\theta_0\sqrt{1+cx}
\end{align}
$$
Expand RHS by <a href=""https://en.wikipedia.org/wiki/Binomial_theorem#Newton.27s_generalized_binomial_theorem"" rel=""nofollow noreferrer"">generalized binomial theorem</a>, we find</p>

<p>$${\rm OGF} = \sum_{n=0}^\infty ((-1)^n \cos\theta_0 - \sin\theta_0)\binom{\frac12}{n} c^n x^\ell
\quad\text{ where }\quad
\binom{\frac12}{n} 
= \frac{1}{n!}\prod\limits_{\ell=0}^{n-1}\left(\frac12 - \ell\right)$$</p>

<p>Comparing coefficients of $x^n$ on both sides, we obtain a closed-form
expression for the sequence $f_n$:</p>

<p>$$f_n = ((-1)^n\cos\theta_0 - \sin\theta_0) \binom{\frac12}{n} c^n$$</p>

<p>To extract the leading asymptotic behavior of $f_n$, we rewrite  $\binom{\frac12}{n}$ using <a href=""https://en.wikipedia.org/wiki/Gamma_function"" rel=""nofollow noreferrer"">gamma function</a>.<br>
We have</p>

<p>$$\binom{\frac12}{n} 
= (-1)^n \frac{\Gamma(n-\frac12)}{\Gamma(-\frac12)\Gamma(n+1)}
= \frac{(-1)^{n-1}}{2\sqrt{\pi}}\frac{\Gamma(n-\frac12)}{\Gamma(n+1)}
$$</p>

<p>Notice for any $\lambda \in \mathbb{C}$, we have
$\displaystyle\;\lim_{n\to\infty}\frac{\Gamma(n+\lambda)}{\Gamma(n)n^\lambda} = 1\;$. For large $n$, $\displaystyle\;\binom{\frac12}{n}$ behaves like $n^{-3/2}$. </p>

<p>As a result, the leading behavior of $f_n$ is given by following expression</p>

<p>$$f_n \approx -\frac{(4a(1-a))^{n/2}}{2\sqrt{\pi n^3}}\times 
\begin{cases} 
\cos\theta_0 - \sin\theta_0 = \sqrt{2a}, &amp; n \text{ even }\\
\cos\theta_0 + \sin\theta_0 = \sqrt{2(1-a)}, &amp; n \text{ odd }
\end{cases}
$$</p>

<p><strong>Update</strong></p>

<p>Let us switch to the side question how to extract the leading behavior of
$f_n' = \sum\limits_{k=0}^n f_k$.<br>
The OGF of $f_n'$ is given by the formula.</p>

<p>$${\rm OGF}' = \sum_{k=0}^n f_n' x^n = \frac{\rm OFG}{1-x} =
\frac{\cos\theta_0\sqrt{1-cx} - \sin\theta_0\sqrt{1+cx}}{1-x}$$</p>

<p>When $a = 0$, OCF' is just constant $0$ and all $f_n' = 0$.</p>

<p>When $a = \frac12$, $c = 1$ and OCF' reduces to $\sqrt{1-x}$. It is easy to show $f_n'$ falls of like $n^{-1/2}$.</p>

<p>For the remaining cases, what happens depends on whether $a$ is far way or near $\frac12$.  </p>

<p>When $a$ is far away from $\frac12$, everything is relatively simple.</p>

<p>The numerator of OCF' has two branch cuts ends at $\pm \frac1c$ and an zero at $x = 1$. This cancels out a potential pole from denominator. OGF' remains to be regular at $x = 1$ and the leading behavior of $f_n'$ is controlled by the behavior of OGF' at $\pm \frac1c$.</p>

<p>For $x$ sufficiently close to $\frac1c$, we have the expansion</p>

<p>$$\frac{\sqrt{1-cx}}{1-x} = -\frac{c}{1-c}\frac{\sqrt{1-cx}}{1 - \frac{(1-cx)}{1-c}}
= -\sum_{\ell=0}^\infty\frac{c}{(1-c)^{\ell+1}} (1-cx)^{\ell+\frac12}
\tag{*1a}
$$</p>

<p>Notice for any $\alpha &gt; 0$ and $|z| &lt; 1$,</p>

<p>$$(1-z)^\alpha = \sum_{k=0}^\infty (-1)^k \binom{\alpha}{k} z^k
\quad\text{ and }\quad
(-1)^k\binom{\alpha}{k} = \frac{1}{\Gamma(-\alpha)k^{\alpha+1}}(1 + O(k^{-1}))\;$$</p>

<p>As far as leading behavior of $f_n'$ are concerned, we only need to keep the $\ell = 0$ term in $(*1a)$.<br>
This is because contributions from the $\ell &gt; 0$ terms have an extra factor $\frac{1}{((1-c)n)^\ell}$ attached to them. Since $a$ is far way from $\frac12$, $(1-c)$ will not be too small and we can safely all of these.</p>

<p>Similar expansion is available when $x$ is sufficiently close to $-\frac1c$, we have</p>

<p>$$\frac{\sqrt{1+cx}}{1-x} = \frac{c}{1+c}\frac{\sqrt{1+cx}}{1 - \frac{1+cx}{1+c}}
= \sum_{\ell=0}^\infty \frac{c}{(1+c)^{\ell+1}} (1+cx)^{\ell+\frac12}
\tag{*1b}
$$</p>

<p>Once again, contribution from the $\ell &gt; 0$ terms have an extra factor $\frac{1}{((1+c)n)^\ell}$ attached to them. We only need to keep the $\ell = 0$ term in $(*1b)$ in the extraction of leading behavior of $f_n'$. </p>

<p>At the end, we find when $a$ is far from $\frac12$, the leading behavior of $f_n'$ is of the form $O(n^{-3/2})$.</p>

<p>$$f_n' \approx \frac{c^{n+1}}{2\sqrt{\pi n^3}}\times \begin{cases}
\frac{\cos\theta_0}{1-c} + \frac{\sin\theta_0}{1+c}, &amp; n \text{ even}\\
\frac{\cos\theta_0}{1-c} - \frac{\sin\theta_0}{1+c}, &amp; n \text{ odd}\\
\end{cases}
$$</p>

<p>For the final case when $a$ is close to $\frac12$, the situation is more
complicated. I have no idea how to extract the leading behavior in a form depends on $a$ uniformly. </p>

<p>When $a \to \frac12^-$, $\frac{1}{1-c}\to \infty$. One no longer able to blindly ignore all the $\ell &gt; 0$ terms in expansion of $(*1a)$.
One consequence of this is when $a \to \frac12^-$, above approximation fails to reproduce the $O(n^{-\frac12})$ behavior of $f_n'$ at $a = \frac12$.</p>

<p>If one want the leading behavior of $f_n'$ for $a$ near $\frac12$, one
should consider how to vary $a$ as one sends $n$ to infinity.</p>
"
"2398131","2398135","<p>If you want a realistic answer to this, you would of course need to take into account that you can't pack the Boeing 'perfectly'; there will be lots of gaps and spaces depending on how the cargo bay is shaped.  Even if you had a perfect rectangular box you would of course still have gaps between the poker chips, since they are round.</p>

<p>But for a rough approximation, maybe do exactly that: consider the Boeing cargo bay to be a rectangular box, and consider stacking the chips as little rectangular 'boxes' (a hexagonal packing would be more efficient, but someone else can do the math for that ...) of 39mm by 39mm. So then the volume of a single chip is $39 \cdot 39 \cdot 3.3 \ =5019.3 mm^3$</p>

<p>And the Boeing has a volume of $854.4 \ m^3 = 854.4 \cdot 10^9 \ mm^3$</p>

<p>So that would be $\frac{854.4 \cdot 10^9}{5019.3} \approx 0.17 \cdot 10^9$, which is about $170$ million poker chips</p>
"
"2398142","2398424","<p>Quadratic BÃ©zier segment $S$ is defined by two endpoints
$A$ and $C$ and one control point $B$:</p>

<p>\begin{align}
S(t)&amp;=A\,(1-t)^2+2\,B\,(1-t)\,t+C\,t^2
,\quad t\in[0,1],\\
S(0)&amp;=A,\quad S(1)=C
.
\end{align}</p>

<p>Given the coordinates of the two endpoints and any other
point $P=S(\tau)$, $\tau\in(0,1)$,</p>

<p>\begin{align}
	P&amp;=S(\tau)=A\,(1-\tau)^2+2\,B\,(1-\tau)\,\tau+C\,\tau^2,
\end{align}
the control point $B$ is found as</p>

<p>\begin{align}
	B&amp;=\tfrac1{2\,\tau\,(1-\tau)}(P-A\,(1-\tau)^2-C\,\tau^2)
	.
\end{align}</p>

<p>Dealing with the graphs of the form $y(x)=a_2\,x^2+a_1\,x+a_0$,
the required parameter $\tau$ is found as
\begin{align}
\tau&amp;=\frac{P_x-A_x}{C_x-A_x}
.
\end{align}</p>

<p>For example, given two endpoints 
$A=(0, 575)$, $C=(20,2375)$ and
a third point $P=(15, 1025)$, we have</p>

<p>\begin{align}
\tau&amp;=\frac{15-0}{20-0}=0.75,\\
B&amp;=(10, -925)
.
\end{align}</p>

<p><a href=""https://i.stack.imgur.com/1P21g.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1P21g.png"" alt=""enter image description here""></a></p>
"
"2398151","2398205","<p>Posting my comments as an answer:</p>

<p>Proving existence of (2) $\implies$ (1). In order to prove (2), all we have to do is prove that there exists <em>a</em> positive root, since by strict monotonicity, we know it will be unique. As such, since $f$ is differentiable on $(0, \infty)$, and we know that $f(0) = -a &lt; 0$ and we also know $f(x)\to \infty$ whenever $x \to \infty$ then there exists at least one $x\in (0, \infty)$ such that $f(x)=0$ as we know $f$ is continuous (by its differentiability). This immediately proves (2) and therefore (1).</p>
"
"2398152","2398364","<p>Since $2^a = 3^b \iff a\ln 2 = b\ln 3$, under the constraint that $\nu + \mu = N$ we have $2^{\nu} = 3^{\mu}$ for $\nu = \frac{\ln 3}{\ln 6}N$ and $\mu = \frac{\ln 2}{\ln 6}N$. The smallest distance $\lvert 2^n - 3^m\rvert$ for $m,n$ integers with $n + m = N$ therefore occurs when $n = \lfloor \nu\rfloor$ or $n = \lceil \nu\rceil$. Writing $\delta = n - \nu$, we thus have $-1 &lt; \delta &lt; 1$ and</p>

<p>$$D = \lvert 2^n - 3^m\rvert = \lvert 2^{\nu + \delta} - 3^{\mu - \delta}\rvert = \exp \biggl( \frac{(\ln 2)(\ln 3)}{\ln 6}N\biggr)\cdot \lvert 2^{\delta} - 3^{-\delta}\rvert.$$</p>

<p>Thus</p>

<p>$$\ln D = \frac{(\ln 2)(\ln 3)}{\ln 6}N + \ln \lvert 2^{\delta} - 3^{-\delta}\rvert.$$</p>

<p>Since $C := \frac{(\ln 2)(\ln 3)}{\ln 6} \approx 0.4250012479336228$, the first term corresponds to your straight line, and that the exact values are close to that line is equivalent to $\delta$ not being too close to $0$. The inequality $\lvert \delta\rvert &lt; 1$ implies $\lvert 2^{\delta} - 3^{-\delta}\rvert &lt; 3^1 - 2^{-1} = \frac{5}{2}$, so $\ln D$ can never be much above that line. But $\delta$ can sometimes be very close to $0$. If $N$ is the denominator of a convergent of $\frac{\ln 3}{\ln 6}$, then</p>

<p>$$\biggl\lvert \frac{\ln 3}{\ln 6} - \frac{K}{N}\biggr\rvert &lt; \frac{1}{N^2},$$</p>

<p>so $\lvert \nu - K\rvert &lt; \frac{1}{N}$, hence $\lvert\delta\rvert &lt; \frac{1}{N}$, and then</p>

<p>$$2^{\delta} - 3^{-\delta} = \exp(\delta\ln 2) - \exp (-\delta\ln 3) = \delta\ln 6 + O(\delta^2),$$</p>

<p>whence $\ln D \approx C\cdot N + \ln \delta &lt; C\cdot N - \ln N$ in that case.</p>

<p>Of course a difference of $\approx \ln N$ is still small relative to $C\cdot N$. However, depending on the irrationality measure of $\frac{\ln 3}{\ln 6}$, there may be convergents where $\frac{K}{N}$ is much closer than $\frac{1}{N^2}$. If the irrationality measure is finite - and that's overwhelmingly likely - then the distance is bounded below by a power of $N$, and then we have $\lvert CN - \ln D\rvert \in O(\ln N)$. But if the irrationality measure is infinite, then $\lvert CN - \ln D\rvert$ can be of larger order than $\ln N$.</p>
"
"2398162","2398554","<p>No. For example, the class of (strict) linear orders in the language $\{&lt;\}$ is equal to its own reflexive reduction. But this class is not axiomatizable in first-order logic without equality. The problem is with expressing the trichotomy condition: $\forall x\, \forall y\, (x&lt;y \lor y&lt;x \lor x = y$).</p>

<p>As a silly way of seeing that this class is not axiomatizable, note that any two nonempty sets in which $&lt;$ is interpreted as $\emptyset$ are indistinguishable in first-order logic without equality. But such a set is a strict linear order if and only if it has size $1$.</p>

<hr>

<p>More generally, elementary-without-equality classes are closed under the following ""blow-up"" operation. Let $M$ be any first-order structure in a relational language, let $M'$ be any set, and let $f\colon M'\to M$ be any surjective map of sets. We turn $M'$ into a structure by setting $(a_1,\dots,a_n)\in R^{M'}$ if and only if $(f(a_1),\dots,f(a_n))\in R^M$. The idea is that we've replaced every element $b\in M$ with a nonempty set of elements $f^{-1}(\{b\})$. </p>

<p>Now you can prove by induction that for any formula $\varphi$ without equality and any tuple $\overline{a}$ from $M'$, $M'\models \varphi(\overline{a})$ if and only if $M\models \varphi(f(\overline{a}))$. In particular, $M$ and $M'$ satisfy the same sentences. So if you have a class of structures which is not closed under ""blow-up"", there's no hope for it to be axiomatizable in first-order logic without equality.</p>
"
"2398176","2398184","<p>Sylvester's rank inequalities hold over any field $K$. However, they may not hold over rings in general - see <a href=""http://www.mathnet.or.kr/mathnet/kms_tex/982159.pdf"" rel=""nofollow noreferrer"">this article</a>.</p>
"
"2398182","2398189","<p>If $a,b$ are differentiable functions and if $g$ is continuous then, by the <a href=""https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus"" rel=""nofollow noreferrer"">fundamental theorem of calculus</a>,
$$\frac{d}{dx}\left(\int_{a(x)}^{b(x)} g(t) dt\right)=
g(b(x))b'(x)-g(a(x))a'(x).
$$
Can you take it from here?</p>
"
"2398185","2398218","<p>In your derivation, you assume the existence of a particular point on the surface of the smaller sphere satisfying $a^2 + b^2 + c^2 = 1$.  You then use the following facts:
$$
(x-a)^2 + (y-b)^2 + (z-c)^2 \leq 2 \\
ax + by + cz = 1
$$
You are then describing all points that satisfy <em>both</em> of these conditions.  This is the intersection of a ball with a plane with a circle of radius $\sqrt{2}$;  in other words, it's a flat circular disc of radius $\sqrt{2}$  centered at $(a,b,c)$ (and orthogonal to this vector.)</p>

<p>Once you see this, it's not too hard to see that nothing is ""wrong"" with your result $x^2 + y^2 + z^2 \leq 3$, since all points on this disc satisfy this inequality.  It's just not the set you're looking for.</p>

<p><strong>EDIT:</strong>  Here's a rendering of what's going on, with $(a,b,c) = (1,0,0)$.  Your inequality is true for the shaded disc on the right.</p>

<p><a href=""https://i.stack.imgur.com/c7Sh1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c7Sh1.png"" alt=""enter image description here""></a></p>
"
"2398188","2398465","<p>Let me assume (for simplicity) that $E[X(t)]=0$ for each $t$. If $Y:=\int_{\mathcal T} X(t)\,dt$ is to be Gaussian, then it must have finite moments. So, let's also assume that 
$$
\int_{\mathcal T} \sqrt{E[X(t)^2]}\,dt&lt;\infty.
$$
This is more than enough (by Cauchy-Schwarz) to ensure that 
$E[Y^2]&lt;\infty$, and will be needed below to justify the use of Fubini's theorem.  Let $L^2$ denote that Hilbert space of square-integrable random variables on the probability space $(\Omega,\mathcal F, P)$ of the $X(t)$, and let $\Gamma$ be the closure in $L^2$ of the linear span of $\{X(t): t\in\mathcal T\}$. Each random variable in $\Gamma$ is normally distributed (with mean $0$). To show that $Y$ is normally distributed it suffices to show that $Y\in\Gamma$; or, what is the same, that $Y\in(\Gamma^\perp)^\perp$. (Here $\perp$ indicates orthogonal complement.) That is, we must show that if $Z\in\Gamma^\perp$ (namely, if $Z\in L^2$ and $E[ZG]=0$ for every $G\in\Gamma$) then $E[ZY]=0$ as well. So fix $Z\in\Gamma^\perp$. Then for each $t\in\mathcal T$, $X(t)\in\Gamma$, and so $E[ZX(t)]=0$.
Consequently, by Fubini's theorem,
$$
E[ZY]=\left[Z\int_{\mathcal T}X(t)\,dt\right]=\int_{\mathcal T} E[ZX(t)]\,dt =0.
$$</p>
"
"2398190","2398206","<p>The 'standard' way to think about this is to say: the bowler needs an average of $170$ over $90$ games, so the bowler needs $90 \cdot 170 = 15300$ points total. So far, the bowler has managed $21 \cdot 164 = 3444$ points, so the bowler still needs $15300-3444=11856$ points, meaning that for the remaining $90-21=69$ games, the bowler needs $\frac{11856}{69} \approx 171.8$ points on average per game.</p>

<p>But here is another way to think about this:</p>

<p>For those first $21$ games the bowler was $6$ points behind the goal of getting $170$ points. So, the bowler 'lost' $6 \cdot 21 = 126$ points that the bowler needs to make up in the remaining $90-21=69$ games. That is, the bowler needs to make up $\frac{125}{69} \approx 1.8$ points per game by getting that many points over $170$ on average. So that would mean the bowler needs to bowl $\approx 170+\frac{125}{69} \approx 171.8$ points per game for the remaining season on average.</p>

<p>I not only like this approach of 'losing ground' and 'gaining ground' conceptually, but as you can see it sometimes simplifies the math in that you don't work with as big of numbers ... Indeed, I have found that there are many instances where the math is simple enough with this method that you can do this in your head, and fairly quickly too. </p>

<p>For example, if it would have been an $84$-game season, then there would have been $63$ games remaining, which is $3$ times the number of past games, so with an average 'loss' of $6$, the bowler should make an average 'gain' of $2$, so the bowler should average $172$ for the remaining games ... indeed, even though the numbers with $90$ total games do not work out to nice whole numbers, I was immediately able to tell that the bowler should average a little under $172$ just eye-balling the numbers. So this is a powerful method!</p>
"
"2398193","2398211","<p>You're on the right track. </p>

<p>$$f(n)=n^3+1\geq 1+1=2$$ Thus, $1$ doesn't have  a pre-image. Hence, $f$ is not surjective.</p>
"
"2398207","2398217","<p>Let $$a_k:=\frac{k!}{\left(k-3\right)!}\cdot \frac{n!}{k!\left(n-k\right)!}.$$
First, simplify the $k!$ to get 
$$a_k=\frac{n!}{\left(k-3\right)!\cdot \left(n-k\right)!}.$$
Then write $  \left(n-k\right)!= \left((n-3)-(k-3)\right)!$. This shows that 
$$a_k=\frac{n!}{(n-3)!} \binom{n-3}{k-3} .$$
If you plug this into the sum you get:
$$\frac{n!}{(n-3)!} \sum \limits_{k=3}^{n}\binom{n-3}{k-3} = \frac{n!}{(n-3)!} \sum \limits_{k=0}^{n-3}\binom{n-3}{k}  = \frac{n!}{(n-3)!} \cdot 2^{n-3}$$</p>
"
"2398215","2398235","<p>For each of these, you have something of the form $f(x) = f(g(x))$ where $g^n(x) \to 0$ (where $g^n$ is the repeated application of $g$) or $g^n(x) \to 1$ (e.g. $g$ is a contraction to some point, let's call it $y$ in general), you can use this and continuity to show that we have
$$
f(x) = f(g^n(x)) \to f(y), \forall x, n\to \infty.
$$</p>

<p>In this case, consider the following:</p>

<p>(1) Assume $|m| &gt; 1$ then $g(x) = x/m$, now we know $f(x) = f(x/m) = f(x/m^2) = \dots \to f(0), \forall x$ by continuity of $f$. A similar case follows for $|m| &lt; 1$, using $g(x) = mx$.</p>

<p>(2) Assume $g(x) = (x-1)/2$, then note that $g^n(x) \to 0, n\to \infty$ for every $x$, so a similar proof $f(x) = f(g^n(x)) \to f(0)$ holds.</p>

<p>(3) Assume that $g(x) = \sqrt{x}$ if $x&gt;0$ else $-\sqrt{x}$ will work. Note that $g^n(x) \to 1, \forall x$ so here we are done as well by noting that $f(x) = f(g^n(x)) \to f(1)$ again by continuity of $f$.</p>
"
"2398219","2398237","<p>Read up on <a href=""https://en.wikipedia.org/wiki/Matrix_multiplication"" rel=""nofollow noreferrer""><strong>matrix multiplication</strong></a>.</p>

<p>Yes you can have it on the right, if the vector you are transforming is a <a href=""https://en.wikipedia.org/wiki/Row_and_column_vectors"" rel=""nofollow noreferrer""><strong>row vector</strong></a>:</p>

<p>$$\begin{bmatrix}x&amp;y\end{bmatrix}\begin{bmatrix}2&amp;-1\\1&amp;1\end{bmatrix}$$</p>

<p>But as you can see here $-1$ and $1$ switch places as we need to do this <strong>transpose</strong> operator we describe below and which is on wikipedia.</p>

<p>If it is a more complicated transformation, say given by the matrix $\bf M$, then a column vector being transformed (multiplied by $\bf M$) from the left is the same as a row vector being transformed (multiplied by ${\bf M}^T$ from the right:
$$({\bf Mv})^T = {\bf v}^T {\bf M}^T$$
This is due to a famous algebra rule of <a href=""https://en.wikipedia.org/wiki/Transpose"" rel=""nofollow noreferrer""><strong>transpose</strong></a> (the $(\cdot)^T$ operator).</p>
"
"2398227","2398249","<p>Assume $n\equiv 7\pmod {12}$. Then we have
$$\exists k\in\Bbb Z:n-7=12k\implies n=12k+7\implies n=(4)(3k)+(3+4)\implies n=4(3k+1)+3$$
and thus $n=4j+3$ with $j=3k+1$. Hence, $n\equiv 3\pmod{4}$.</p>
"
"2398233","2398252","<p>$\lim_{x\to 1} \dfrac {1+\cos \pi x}{\dfrac {\sin^2 \pi x}{\cos^2 \pi x}}=$</p>

<p>$=\lim_{x\to 1} \dfrac {(1+\cos \pi x)(\cos^2 \pi x)}{\sin^2 \pi x}=$</p>

<p>$=\lim_{x\to 1} \dfrac {(1+\cos \pi x)(\cos^2 \pi x)}{1-\cos^2 \pi x}=$</p>

<p>$=\lim_{x\to 1} \dfrac {(1+\cos \pi x)(\cos^2 \pi x)}{(1-\cos \pi x)(1+\cos \pi x)}=$</p>

<p>$=\lim_{x\to 1} \dfrac {\cos^2 \pi x}{1-\cos \pi x}=\dfrac{1}{2}$</p>
"
"2398255","2398261","<p>Your reasoning is correct.  This example, which I like to call the ""melting ice cube,"" is a good example of a sequence of functions that converge pointwise almost everywhere (indeed, the sequence converges <em>everywhere</em>, and even uniformly so!), but which fails to converge in $L^1$ (in particular, you cannot pass the limit through the integral).</p>

<p>Another nice example is the ""traveling box"" example.  Let $f_n = \chi_{[n,n+1]}$.  Observe that $f_n \to 0$ pointwise, but
$$ \int f_n = \int \chi_{[n,n+1]} = m([n,n+1]) = 1,$$
where $m$ is the Lebesgue measure.  The traveling box is maybe slightly more interesting as a counter-example, as the convergence is <em>not</em> uniform (though it is still pointwise).</p>
"
"2398256","2398264","<p>If the center of the square is at $(0,0)$, and one of the corners is originally at $(x,y)$, then after rotating through an angle of $\theta$ counterclockwise, that angle's position is given by:</p>

<p>$$\left[\begin{matrix}\cos \theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{matrix}\right]\left[\begin{matrix} x \\ y \end{matrix}\right],$$</p>

<p>Unpacking that, since you haven't worked with matrices, you get:</p>

<p>$$(x\cos\theta - y\sin\theta, x\sin\theta+y\cos\theta),$$</p>

<p>as your new coordinates.</p>

<p>If the center of your square is originally at $(h,k)$, and your corner is originally at $(x,y)$, then your new location is:</p>

<p>$$(h+ (x-h)\cos\theta - (y-k)\sin\theta, k+(x-h)\sin\theta + (y-k)\cos\theta).$$</p>
"
"2398265","2398280","<p>Note that your equation may be written in the form $x' = Ax + b$ where $x = [x_1,x_2,x_3]^{tr}$, $b = [v,0,0]^{tr}$ and $A$ is the matrix of the $k_i$ which, when multiplied with $x$ in the form above, are equivalent to your equations. </p>

<p>The solution to the system is
$$
x(t) = e^{At}x_0 + \int_0^t e^{A(t-s)}ds\ b, 
$$
where $e^{At}$ is defined by $\sum_{n=0}^{\infty}A^n/n!$. The steady state of the system is $\lim_{n\rightarrow \infty}x(t)$, which you can find analytically by factoring $A$ into its Jordan canonical form and then exponentiating it (the Jordan blocks have known exponentials which you can look up). If the system happens to have three distinct eigenvalues, this is no different from diagonalizing the matrix and exponentiating the diagonal (resulting in the exponentials of each element). Another method is to take the Laplace transform and use the final value theorem; e.g. </p>

<p>$$
\lim_{n\rightarrow \infty}x(t) = \lim_{s\rightarrow 0} sX(s). 
$$</p>

<p>Note that this requires the you to compute $(sI-A)^{-1}$.</p>

<p>The equilibria of this system occur whenever $Ax = -b$ or $x = -A^{-1}b$ if $A$ is invertible. If this system has no solutions then there can be no equilibria. The system clearly only has nulclines if the linear system $Ax = -b$ possesses infinity many solutions. Otherwise there will be a finite number of equilibria or perhaps none at all. </p>
"
"2398284","2398285","<p>If 
$$B = \{ (x,y) \in \mathbb{R}^2 : y \leq x \leq y^2,  0 \leq y \leq 1 \}, 
$$ 
then you should be integrating: 
$$
 \int_0^1 \int_{y}^{y^2} x^2y \  dx dy 
= - \int_0^1 \int_{y^2}^{y} x^2y \  dx dy   = -\frac{1}{40}. 
$$</p>

<p>But if 
$$B = \{ (x,y) \in \mathbb{R}^2 : y \leq x \leq y^2\color{blue}{+1},  0 \leq y \leq 1 \}, 
$$ 
then you should be integrating: 
$$
 \int_0^1 \int_{y}^{y^2+1} x^2y \   dx dy  
=  \int_0^1 \int_{0}^{x} x^2y \   dy dx   +  \int_1^2 \int_{\sqrt{x-1}}^{1} x^2y \   dy dx  
 = \frac{67}{120}. 
$$</p>
"
"2398286","2398295","<p>$K$ does not imply $K'$; indeed, I can't think of a reasonable system which does include $K'$, which doesn't trivialize the modality.</p>

<p>Think of a Kripke frame with three worlds $a, b, c$ where</p>

<ul>
<li><p>$a$ sees all three worlds,</p></li>
<li><p>neither $b$ nor$c$ see any worlds, and</p></li>
<li><p>$b\models p\wedge\neg q$, $c\models q\wedge\neg p$.</p></li>
</ul>

<p>$K$ is true in this frame, but $K'$ is not: ""$\Box p\rightarrow\Box q$"" is true at $a$ (since $\Box p$ is false at $a$), but ""$\Box(p\rightarrow q)$"" is false at $a$ (since $a$ sees $b$ and $b\models\neg(p\rightarrow q)$).</p>

<hr>

<p>EDIT: It's not hard to pin down exactly what Kripke frames validate $K'$: they are the frames in which each world sees at most one world (possibly itself).</p>

<p>Showing that such frames verify $K'$ is easy. If a world $a$ sees no worlds, then we trivially have $a\models\Box (p\implies q)$; conversely, if $aRb$ and $a$ sees no worlds besides $b$, then ""$a\models \Box t$"" is the same as ""$b\models t$,"" so $K'$ holds at $a$ since $(p\implies q)\implies (p\implies q)$ holds at $b$.</p>

<p>Conversely, set $p=\neg q$ in $K'$, and note that $K'$ tells us in particular that if $\neg\Box p$ then $\Box (p\implies q)$ - which in our case translates to $(\Diamond q)\implies(\Box q)$ (since ""$\neg\Box \neg$""=""$\Diamond$""
 and ""$\neg q\implies q$""=""$q$""). So <strong>every frame validating $K'$ also validates the ""trivialization"" principle</strong> $$(*)\quad(\Diamond q)\implies (\Box q).$$ But this statement clearly implies that in such a frame, no world sees more than one world (if $aRb, aRc,$ and $b\not=c$, set $q$ true at $b$ so that $a\models\Diamond q$ and set $q$ false at $c$ so that $a\models\neg\Box q$).</p>

<p><em>Note that we've in fact shown that in Kripke frames, validating $K'$ is the same as validating $(*)$. I don't know off the top of my head what the weakest modal system $X$ is such that $X+(*)$ is equivalent to $X+K'$, but I suspect it's quite weak.</em></p>
"
"2398297","2398311","<p>Since $$P(A \cap B)+P(A \cap B')=P(A)$$</p>

<p>we have,</p>

<p>$$P(A|B)P(B)+P(A|B')P(B')=P(A)$$</p>
"
"2398300","2398304","<p>How about $2\lceil x/2 \rceil$?</p>

<p>EDIT: Yes, it does work.<br>
<em>Proof</em>: We know that $0\le \lceil x/2 \rceil-x/2&lt;1$. Then $0\le 2\lceil x/2 \rceil-x&lt;2$. Since $2\lceil x/2 \rceil$ is an even integer, it is the least even integer that is not lesser than $x$.</p>
"
"2398310","2398316","<p>No. The dihedral group has two types of <em>elements</em> (not operations), which are rotations and <em>reflections</em> (not transpositions). You are right about one thing, though: no composition of rotations may ever lead to a reflection. And... ? We also have two types of integers: odd and even. Furthermore, no sum of even integers may ever lead to an odd integer. Do you have any problem with this? It seems exactly the same situation.</p>
"
"2398313","2398324","<p>If $H=\{0\}$, then $H=0\mathbb Z$.</p>

<p>Otherwise, there is some $n\in H\setminus\{0\}$. Then $n&gt;0$ or $n&lt;0$. If $n&lt;0$, then $-n\in H$ and $-n&gt;0$. So, you know that there is some $n\in\mathbb N$ such that $n\in H$. Let $N$ be the smallest among such elements. If $h\in H$, then $h=Nq+r$, with $r\in\{0,1,\ldots,N-1\}$. Then $r=h-Nq\in H$. But $r&lt;N$. Therefore, $r=0$. So, $H=N\mathbb Z$.</p>

<p>On the other hand, it is clear than, for each natural $n$, $n\mathbb Z$ is a subgroup of $\mathbb Z$.</p>
"
"2398326","2398791","<p>Let $v_i := f^i(v)$.</p>

<hr>

<p><em>Lemma</em>:</p>

<p>Assume $v_l \in \text{span}(\{v_0, ... v_{l-1}\})$.</p>

<p>Then, for every $\text{n} \in \mathbb{N}$, $v_{n} \in \text{span}(\{v_0, ... v_{l-1}\})$.</p>

<p><em>Proof</em>:</p>

<p>Assume $v_k \in \text{span}(\{v_0, ... v_{l-1}\})$, so $v_k = \alpha_0 v_0 + ... +\alpha_{l-1} v_{l-1}$. </p>

<p>Therefore, $v_{k+1} = f(v_{k}) = \alpha_0 f(v_0) + ... +\alpha_{l-1} f(v_{l-1}) = \alpha_0 v_1 + ... +\alpha_{l-1} v_{l} \in \text{span}(\{v_0, ... v_{l-1}\})$, because $v_{l} \in \text{span}(\{v_0, ... v_l\})$. </p>

<p>Starting from $k = l$, the domino effect (or induction) completes the proof.</p>

<hr>

<p>Let $l:=\text{dim}(\text{span}(f^k(v): k\in \mathbb N))$.</p>

<p><strong>Claim:</strong> $$\text{span}(\{f^k(v): k\in \mathbb N\}) =  \text{span}(\{v_0, ... v_{l-1}\})$$</p>

<p><strong>Proof:</strong></p>

<p>If $\{v_0, ... v_{l-1}\}$ are independent, then the proof is complete (because these are $l$ independent vectors, $l$ is the dimension).</p>

<p>Otherwise, $\{v_0, ... v_{l-1}\}$ is a dependent set. </p>

<p>Recall that using the lemma, it is enough to prove that $v_l \in \text{span}(\{v_0, ... v_{l-1}\})$.</p>

<p>$\{v_0, ... v_{l-1}\}$ is a dependent set, so one of the vectors there is a linear combination of the others. Assume $v_{l-1} \in \text{span}(\{v_0, ... v_{l-2}\})$. But then</p>

<p>$v_{l-1} = \alpha_0 v_0 + ... +\alpha_{l-2} v_{l-2}$</p>

<p>$ \rightarrow f(v_{l-1}) = \alpha_0 f(v_0) + ... +\alpha_{l-2} f(v_{l-2})$</p>

<p>$ \rightarrow v_{l} = \alpha_0 v_1 + ... +\alpha_{l-2} v_{l-1}$</p>

<p>and then $v_l \in \text{span}(\{v_1, ... v_{l-1}\}) \subseteq \text{span}(\{v_0, ... v_{l-1}\})$ and the proof is complete.</p>

<p>Otherwise, $v_{l-1} \notin \text{span}(\{v_0, ... v_{l-2}\})$. Still, $\{v_0, ... v_{l-2}\}$ is a dependent set. Assume $v_{l-2} \in \text{span}(\{v_0, ... v_{l-3}\})$. But this is impossible as </p>

<p>$v_{l-2} \in \text{span}(\{v_0, ... v_{l-3}\}) \rightarrow v_{l-1} \in \text{span}(\{v_0, ... v_{l-2}\})$ in contradiction.</p>

<p>If follows that $v_{l-2} \notin \text{span}(\{v_0, ... v_{l-3}\})$.</p>

<p>The recurring argument gives that the set $\{v_0, ... v_{l-1}\}$ is independent, in contrary to the assumption that is is dependent. Therefore, </p>

<p>$v_{l-1} \in \text{span}(\{v_0, ... v_{l-2}\})$, and so $v_{l} \in \text{span}(\{v_0, ... v_{l-1}\})$. As stated above, this completes the proof.</p>
"
"2398327","2398389","<p>Suppose $f$ is integrable (not necessarily continuous) and $\lim_{x \to a+} f(x)= L$. For any $\epsilon &gt; 0$ there is a $\delta &gt; 0$ such that $|f(x) - L| &lt; \epsilon$ when $0 &lt; x &lt; a + \delta$.  </p>

<p>If $0 &lt; h &lt; \delta$, then</p>

<p>$$\left|\frac{1}{h}\int_a^{a+h} f(x) \, dx - L  \right| = \left|\frac{1}{h}\int_a^{a+h} (f(x) - L) \, dx   \right|  \leqslant\frac{1}{h}\int_a^{a+h} |f(x) - L| \, dx &lt; \epsilon  
$$</p>

<p>Thus, $\displaystyle \lim_{h \to 0+} \frac{1}{h}\int_a^{a+h} f(x) \, dx = L$.</p>

<p>A similar argument applies  to the left-hand limit. The mean value theorem for integrals is not needed (nor does it apply to discontinuous functions.)</p>
"
"2398347","2398384","<p>For your first question: if $aH = kH$ then $ak^{-1} \in H \subseteq K$. This means that $ak^{-1} = k'$ for some $k' \in K$. But then $a = kk' \in K$.</p>

<p>I'm not sure I really understand your second question. By definition, $$\pi(K) = K/H = \{ kH : k \in K\}$$
is a set <em>whose elements are cosets</em>. But we want an expression for the set whose elements are <em>the elements of all these cosets</em>. And it turns out that $\pi^{-1}$ takes any coset $kH$ and yields the set of all its elements: $\pi^{-1}(kH) = \{ kh : h \in H\}$. So taking the preimage of all the cosets yields all their elements: $$\pi^{-1}(\pi(K)) = \pi^{-1}(K/H) = \bigcup_{kH \in K/H} \pi^{-1}(kH) = \bigcup_{kH \in K/H} \{kh : h \in H\} = \bigcup_{k \in K} \{kh : h \in H\} = KH.$$</p>
"
"2398351","2398359","<p>If $W$ is a subspace of $V$ <em>of codimension $1$</em>, there is always a functional on $V$ whose kernel is $W$. </p>

<p>In fact, let $\{w_1, \ldots, w_{n-1}\}$ be a basis of $W$ and extend it to a basis $\{w_1, \ldots, w_n\}$ of $V$. Then define $f \colon V \to \mathbb{F}$ as $f(w_i)=0$ for $i=1, \ldots, n-1$ and $f(w_n)=1$. </p>

<p>This construction also shows that if $f$ and $g$ are two non-zero functionals on $V$ such that $\ker f = \ker g$, then there exists $\lambda \in \mathbb{F}^*$ such that $g = \lambda f$. </p>
"
"2398360","2398562","<p><strong>WRONG ANSWER BELOW</strong></p>

<p>Clearly, there are $4! = 24$ ways in a $2\times 2$ rectangle.</p>

<p>Now add one column to the right, making it a $2 \times 3$ rectangle. We already colored the $2 \times 2$ square in the left of this $2 \times 3$ rectangle (in $24$ ways), but the remaining two boxes are not colored yet. This two new boxes can be colored in $2 \cdot 1 =2$ ways.</p>

<p>Therefore, a $2 \times 3$ rectangle can be colored in $24 \cdot 2 = 48$ ways.</p>

<p>Similarly, adding one more column to the right will give two extra boxes, and there will be $48 \cdot 2=96$ ways to color a $2 \times 4$ rectangle.</p>

<p>Finally, adding one more column to the right, we will have $96\cdot 2 = 192$ ways to color a $2 \times 5$ rectangle.</p>

<p>Now we start adding rows.</p>

<p>Observe that, after adding one row to a $2 \times 5$ rectangle, although we will have $5$ new uncolored boxes, when the uncolored box on the very left is colored, the color of the remaining boxes are determined.</p>

<p>Since the box on the very left can be colored in two colors, there are $192 \cdot 2 = 384$ ways to color a $3\times 5$ rectangle.</p>

<p>Finally, we add one more row, and there are $384 \cdot 2 = 768$ ways.</p>

<p><strong>EDIT:</strong> Apparently, I cannot delete my answer if it is accepted. As the comments below suggested, there are some cases where this method fails to create a $4 \times 5$ rectangle.</p>
"
"2398362","2398372","<p>Let $f\colon\mathbb{R}\longrightarrow\mathbb R$ be the function defined by $\displaystyle f(x)=\sin(x)+x\cos(x^2)$. The function $f$ is continuous and $\left(-\infty,\frac12\right)$ is an open subset of $\mathbb R$. Therefore, since $A=f^{-1}\left(-\infty,\frac12\right)$, $A$ is open.</p>
"
"2398366","2398447","<p>The number of ways of matching exactly $k$ of the five cards selected from the first deck is 
$$\binom{5}{k}\binom{47}{5 - k}$$
since if $k$ cards match, the remaining $5 - k$ cards must be selected from among the other $47$ cards in the deck.  </p>

<p>Therefore, the number of ways of selecting at least two matching cards is 
$$\binom{5}{2}\binom{47}{3} + \binom{5}{3}\binom{47}{2} + \binom{5}{4}\binom{47}{1} + \binom{5}{5}\binom{47}{0}$$
Alternatively, we can subtract the number of hands with fewer than two matching cards from the total number of hands.  Hence, the number of favorable hands is $$\binom{52}{5} - \binom{5}{0}\binom{47}{5} - \binom{5}{1}\binom{47}{4}$$
In your attempt, you counted sequences in the numerator and subsets in the denominator.  There is only one subset in which all five cards match.  If you want to use sequences in the numerator, you also have to use them in the denominator.</p>
"
"2398379","2398388","<p>Let $N$ be the sum of the squares of the inputs, then output should be the input but with each entry divided by $N $. This works because the sum of the products will be a sum of things like $a*(a/N)=\dfrac{a^2}{N}$ so you'll have (sum of squares)$/N$, which is $1$ by choice of $N $.</p>
"
"2398391","2398399","<p>$f $ is an even function
$$\implies f (1)=f (-1)=\frac {1}{2} $$</p>

<p>$\implies f $ is not injective</p>

<p>$\implies f $ is not bijective.</p>
"
"2398394","2398444","<p><strong>Lemma(I)</strong>: 
Let $p$ to be an odd prime, 
then $\big(  \mathbb{Z}_p^* , .  \big)$ is a <strong>cyclic</strong> group.</p>

<hr>

<p><strong>Lemma(II)</strong>: 
For every $i \in \mathbb{Z}_p^*$; 
we have: $i \mathbb{Z}_p^* = \mathbb{Z}_p^*$.</p>

<hr>

<p><strong>Lemma(III)</strong>: 
Let $p$ to be an odd prime, 
and let $k$ to be any arbitrary integer.</p>

<ul>
<li><p>If $p-1 \mid k$; 
then for every $j \in \mathbb{Z}_p^*$; 
we have: 
$j^k \overset{p}{\equiv} 1$. </p></li>
<li><p>If $p-1 \nmid k$; 
then there exists $l \in \mathbb{Z}_p^*$; 
such that: 
$l^k \overset{p}{\ncong} 1$. </p></li>
</ul>

<hr>

<p><strong>Proof</strong>: </p>

<ul>
<li><p>The first statement is the trivial result of fermat's little theorem.</p></li>
<li><p>Second statement: 
Let $\varepsilon$ to be a generator of 
$\big(  \mathbb{Z}_p^* , .  \big)$;<br>
and let $d:=\gcd(k,p-1)$.<br>
Also let $t:=\dfrac{p-1}{d}$;<br>
then it is easy to check that 
for every $r$, with $\gcd(r,t)=1$;<br>
we have: 
$(\varepsilon^r)^k \overset{p}{\ncong} 1$.</p></li>
</ul>

<hr>

<p><strong>Lemma(IV)</strong>: 
Let $G$ be any group 
and $a \in G$ any element of finite order. Then we have: 
$$\text{ord}(a^t)=\dfrac{\text{ord}(a)}{\gcd(\text{ord}(a),t)}.$$ </p>

<hr>

<hr>

<hr>

<hr>

<ul>
<li><p>If $p-1 \mid k$; 
then for every $j \in \mathbb{Z}_p^*$; 
we have: 
$j^k \overset{p}{\equiv} 1$. 
So the sum 
$ 
{\sum}_{j \in \mathbb{Z}_p^*}j^k= 
{\sum}_{j \in \mathbb{Z}_p^*}  1= 
p-1=-1.$ </p></li>
<li><p>If $p-1 \nmid k$; 
then there exists $l \in \mathbb{Z}_p^*$; 
such that: 
$l^k \overset{p}{\ncong} 1$. 
Notice that: 
$$ 
\color{Blue}{ 
     {\sum}_{ j \in \mathbb{Z}_p^*} j^k     }= 
     {\sum}_{lj \in \mathbb{Z}_p^*} (lj)^k  = 
     {\sum}_{ j \in \mathbb{Z}_p^*} l^kj^k  = 
\color{Red}{l^k}  
\color{Blue}{ 
     {\sum}_{ j \in \mathbb{Z}_p^*}    j^k  } 
\\ 
\Longrightarrow 
(1-\color{Red}{l^k}  )
\color{Blue}{ 
     {\sum}_{ j \in \mathbb{Z}_p^*}    j^k  } = 
0 ; 
$$ 
but notice that 
$(1-\color{Red}{l^k}  )$ is not zero;<br>
which implies that: 
$\color{Blue}{ 
     {\sum}_{ j \in \mathbb{Z}_p^*}    j^k  }=0$. </p></li>
</ul>
"
"2398398","2398404","<p>I think you missed $197^2$, and perhaps a different pairing might be helpful. I'll denote your sum with $S$.</p>

<p>\begin{align*}
-S &amp;= (3^2-1^2) + (7^2-5^2) + \dotsb +  (199^2-197^2)\\
&amp;= 2(1+3+\dotsb + 199)\\
&amp;= 20000\\
S &amp;= -20000
\end{align*}</p>

<p>where line 2 to line 3 is from arithmetric progression $\frac{n(a_1+a_n)}{2}=\frac{100(1+199)}{2}$. The value $n=100$ could be found from $2n-1=199$.</p>
"
"2398405","2398419","<p>For a field with $0 = 1$, it can be be proven such a field must be trivial (contains only the single element $0$.)</p>

<p>For any $a \in F$ then $a = a*1 = a*0 = 0$.  So $\{a \in F\} = \{0\}$.</p>

<p>Of course, this assumes $a*0 = 0$ which can be proven for all fields.</p>

<p>[$a*0 = a(0 + 0) = a*0 + a*0$</p>

<p>so $0= a*0 + (-(a*0)) = a*0 + a*0 + (-(a*0)) = a*0 + (a*0 + (-(a*0)) = a*0 + 0 = a*0$.]</p>

<p>(Of course ""field"" by definition can't have $0 = 1$.  But if we ignore that condition but kept all other axioms, such a ""field-like structure"" with $0=1$ would have to be trivial.)</p>

<p>====</p>

<p>Okay, rereading your post.  The third axiom is the <em>only</em> axiom that relates the group $F;+$ to the group $F-\{0\}; \cdot$ in any way.  Without it, we can have the group $\mathbb Z_6, +$ where $e=0=1\approx 0;a\approx1; b\approx2;c\approx3;d\approx4;f\approx5$.  And we can have the unrelated group $\mathbb Z_2\times \mathbb Z_3$ where $e=0=1\approx (0,0); c\approx(0,1); b\approx(1,0);f\approx(1,1);a\approx(2,0);d\approx(2,1)$.  Thus we have $b(c + a) = bd=c$ but $bc + b*a= f + e=f$ </p>

<p>But there is utterly nothing of ""meaning"" to such a construction.</p>
"
"2398407","2398430","<p>This generally won't work if $A \in \Bbb R^{n \times n}$ is a constant matrix, since, writing</p>

<p>$x(t) = \sum_0^m x_i t^i, \tag 1$</p>

<p>where the $x_i \in \Bbb R^n$, we have</p>

<p>$\deg Ax(t) = \deg x(t) = m; \tag 2$</p>

<p>but</p>

<p>$\deg \dot x(t) \le m - 1. \tag 3$</p>

<p>There are however certain classes of matrices for which polynomial solutions may be had, e.g. if $A$ is nilpotent, as pointed out in the comment of Kajelad.  But there will never be a solution when $A$ is non-singular, since it will then not annihilate $x_m$, the coefficient of $t^m$.</p>

<p>I may return to this later and discuss conditions on $A$ which imply the existence of a solution.</p>
"
"2398431","2398454","<p>We can think of a friendship graph.  The vertices are the $100$ people and the edges represent a pair of friends.  I am assuming friendship is mutual, that if C is friends with D, so is D friends with C.  We are told it is a cubic graph, which means every vertex has three edges connected to it.  We can turn the problem around by inviting everybody, then removing people from the room and asking what is the maximum number of people we can remove and guarantee that there are two vertices connected by an edge who are still in the room.  </p>

<p>The total degree of the graph is $300$, so by the handshaking lemma there are $150$ edges.  When we remove a person we delete at most three edges.  It could be less if we have already deleted one or more edges from the last person we remove.  This shows $n$ cannot be larger than $51$ because if we remove less than $50$ people there must be some edges left.  </p>

<p>To show $n=51$ we can describe a graph that supports it.  Split the people into groups $A$ and $B$ and number the people in each group from $1$ to $50$.  Each person is friends with the person one number above them and one number below them in their own group (wrapping around at the end) and with the corresponding numbered person in the other group, so $A25$ is friends with $A24, A26, B25$.  Now we can invite all the odd numbered people from group $A$ and all the even numbered people from group $B$, a total of $50$ and have no pair of friends.</p>
"
"2398433","2398440","<p>$f$ may be extended by continuity to $[0,1]$ with $f(0) = 0$. It's continuous on $[0,1]$ and $[0,1]$ is compact. A continuous function on a compact set is uniformly continuous.</p>
"
"2398435","2398437","<p>The codomain of $g$ is $\mathbb{R}$, while the domain of $f$ is $\mathbb{N}^2$, thus $g$ doesn't output the kinds of things that $f$ requires as input.  Hence the composition is not defined.</p>

<p>Edited to Elaborate:  When we say that $f : \mathbb{N}^2 \to \mathbb{Z}$, we are declaring that $f$ is a function that eats pairs of natural numbers, and spits out integers.  If we try to feed $f$ with something that is not a pair of natural numbers, then $f$ chokes.</p>

<p>Next, note that $g : \mathbb{Z}\to \mathbb{R}$.  This means that $g$ eats integers, and spits out real numbers.  When we compose $g$ with $f$, i.e. when we try to define $g\circ f$, there is no problem.  $f$ outputs integers, and $g$ eats them up.  Groovy.</p>

<p>On the other hand, when we try to compose in the other way, i.e. to define $f\circ g$, $g$ outputs real numbers, and $f$, which is expecting pairs of natural numbers, chokes.  It can't handle what $g$ spits out, so the composition is nonsense.</p>
"
"2398436","2398438","<p>HINT: $$a^2+b^2+c^2\geq ab+bc+ca$$</p>
"
"2398456","2398513","<p>If you change $v = -\lambda y$, then you get the following:</p>

<p>$$[u,v]=[\lambda x,-\lambda y] = -\lambda^2[x,y] = -\lambda^2az = -\frac{\lambda^2 a}{\mu}w$$</p>

<p>$$[w,u]=[\mu z,\lambda x] = \mu\lambda[z,x] = \mu\lambda bx = \mu b u$$</p>

<p>$$[w,v]=[\mu z,-\lambda y] = -\mu\lambda[z,y] = -\mu\lambda (-by) = -\mu b v$$</p>

<p>and thus, let $\mu = \frac 2b$, $\lambda = \sqrt{-\frac 2{ab}}$.</p>
"
"2398474","2398478","<p>Your calculations are right, it does indeed intersect. Try drawing a picture to help your intuition!
In this case, the vector emanating from $p_2$ moves up and down <em>only</em> in the z-direction, so must intersect the plane $z=0$ at some point </p>

<p>This is guaranteed because $v2$ is the <em>normal</em> vector, if you want to guarantee that the line doesn't intersect the plane, try using a parallel vector</p>
"
"2398483","2398505","<p>Note that the scalar product is bilinear and:
$$\langle Av_1,Av_2\rangle =\langle w_1,w_2 \rangle =\langle av_1+bv_2,cv_1+dv_2 \rangle =\langle av_1,cv_1+dv_2\rangle+\langle bv_2,cv_1+dv_2\rangle = ac\langle v_1,v_1\rangle +ad\langle v_1,v_2\rangle+bc\langle v_2,v_1\rangle +bd\langle v_2,v_2\rangle.$$
Since $\langle v_1,v_2\rangle=\langle v_2,v_1\rangle=0$, the result follows.</p>
"
"2398491","2398529","<p>No, this cannot be decidable even for the special case of $\{{\to},{\neg}\}$ and modus ponens.</p>

<p>Specifically, if we have a Turing machine and want to know whether it halts on the empty tape, we can construct a set of axioms that are complete if and only if the machine halts.</p>

<p>First we need to devise <em>some</em> way to encode Turing machine configuration (tape and states) as well-formed formulae. This is easily done, because we can completely ignore the <em>meaning</em> of the formulae -- just having some binary connective is enough to represent tree-shaped data.</p>

<p>Then, we can create axioms that allow us to prove $\neg\neg(\sigma\to(A\to A))$ whenever the machine reaches the configuration represented by $\sigma$. Here the $A\to A$ is there to make sure all of these formulas are tautologies, such the system doesn't become unsound no matter what the machine does, and wrapping everything in a double negation makes the internal structure of the formula invisible to MP until we unlock more axioms later.</p>

<p>These axioms have the form
$$ \neg\neg(\cdots\to(A\to A)) \to \neg\neg(\cdots\to(A\to A)) $$
for each transition of the Turing machine, where the ""$\cdots$"" parts represent configurations that move to each other in one step. If our representation of configurations is halfway reasonable, we can generalize over the left and right ends of the tape so we can do with one axiom per transition (and probably a few helper axioms to extend the tape representation when we reach the end of it).</p>

<p>Then also have the axioms
$$ \neg\neg(\sigma_0\to(A\to A)) $$
where $\sigma_0$ is a description of the initial configuration, as well as
$$ \neg\neg(\cdots\to(A\to A)) \to \mathbf K \\
\neg\neg(\cdots\to(A\to A)) \to \mathbf S \\
\neg\neg(\cdots\to(A\to A)) \to ((\neg q\to \neg p)\to(p \to q)) $$
where in each of these cases the ""$\cdots$"" is a pattern that matches a configuration in a terminating state.</p>

<hr>

<p>(The precise details of the translation would be simplest if we start out with a <em>two-counter machine</em> instead of a Turing machine, but the difference is not really material at the level of abstraction I'm describing here).</p>

<hr>

<p>On the other hand, if you have modus ponens and all of your axioms and rules of inference are closed under substitution, then completeness will be <em>semi-decidable</em> because such a system is complete iff it proves each of $\mathbf K,\mathbf S,(\neg q\to\neg p)\to(p\to q)$, and you can just start searching for proofs of those.</p>
"
"2398501","2398525","<p>Suppose $(f_n)$ is Cauchy in $C^0([0,1])$. Then for each $x\in[0,1]$ we have
$$
|f_n(x)-f_m(x)| \leq \|f_n-f_m\|_\infty.
$$
Thus $(f_n(x))$ is Cauchy in $\mathbb{R}$ for each $x\in[0,1]$. Since $\mathbb{R}$ is complete, there exists $f:[0,1]\to\mathbb{R}$ such that $f(x) = \lim_n f_n(x)$ for each $x\in[0,1]$. It remains to show that $f\in C^0([0,1])$ and $f_n\to f$ in $C^0([0,1])$.</p>

<p>To see that $f$ is continuous, fix $x_0\in[0,1]$ and let $\varepsilon&gt;0$. By the fact that $(f_n)$ is Cauchy, we can choose $N\in\mathbb{N}$ such that $\|f_n-f_m\|_\infty &lt; \varepsilon/3$ for all $n,m\geq N$. Since $f_N$ is continuous at $x_0$, there exists $\delta&gt;0$ such that $|x-x_0|&lt;\delta$ implies
$|f_N(x)-f_N(x_0)|&lt;\varepsilon/3$. Thus if $|x-x_0|&lt;\delta$ we have
\begin{align*}
|f(x)-f(x_0)|
&amp;=\lim_n|f_n(x)-f_n(x_0)| \\
&amp;\leq \lim_n|f_n(x)-f_N(x)|+\lim_n|f_N(x)-f_N(x_0)|+\lim_n|f_N(x_0)-f_n(x_0)| \\
&amp;&lt; \varepsilon.
\end{align*}</p>

<p>Now we just need to show that $f_n\to f$ in $C^0([0,1])$.
Given $\varepsilon&gt;0$, choose $N\in\mathbb{N}$ such that $\|f_n-f_m\|_\infty&lt;\varepsilon$ for all $n,m\geq N$. For every $x\in[0,1]$ and $n\geq N$ we have
$$
|f_n(x)-f(x)| = \lim_m|f_n(x)-f_m(x)|\leq \limsup_m\|f_n-f_m\|_\infty \leq \varepsilon.
$$
Therefore $\|f_n-f\|_\infty\leq \varepsilon$ for every $n\geq N$.</p>
"
"2398506","2398633","<p>$$ \begin{eqnarray*}  
P(neg| D^c)=P(D^c | neg)\frac{P(neg)}{P(D^c)}=(0.95)(2)P(neg)
\\P(neg| D)=P(D | neg)\frac{P(neg)}{P(D)}=(0.05)(2)P(neg)
\end{eqnarray*} $$
the factors of $(2)P(neg)$ will cancel, so you are left with.</p>

<p>$$ P(D^c|n_1\cup n_2)=\frac{(0.95)^2}{(0.95)^2+(0.05)^2} \approx 0.99723 $$</p>

<p>This is also what you would get if you recognized that you are either sick or well </p>

<p>so either the test is right both times - $P_{rr}=(0.95)^2$
or the test is wrong both times - $P{ww}=(.05)^2$</p>

<p>So the total probability of the situation described is  $P_{sit}  = P_{rr} + P_{ww}$</p>

<p>And the conditional probability of the test being right both times is 
$$ P(D^c|n_1\cup n_2)=\frac{P_{rr}}{P_{sit}  } \approx 0.99723 $$</p>

<p>It is also super close to what you would get if you just said that the probability of the test being wrong both times is $(0.05)^2$ so the probability of being right both times is $1-(0.05)^2=0.9975$ </p>
"
"2398512","2398516","<p>I'm not sure what $n$ is supposed to vary over in your notation $\bigcup_{n}$, but let me assume that it varies over the positive integers $n=1,2,3,...$</p>

<p>In that case the first set $\bigcup_{n} S(0,\frac{1}{n})$
is bounded, whereas the second set $\bigcup_{n} S(n,\frac{1}{n})$ is unbounded. </p>

<p>Since dilations and isometries each take bounded sets to bounded sets, the image of a bounded set under any composition of dilations and isometries is also a bounded set. Hence the first set cannot be mapped to the second set.</p>
"
"2398528","2398534","<p>How about
$$f(A)=\exp(A-A^t)?$$
Here $A^t$ is the transpose of $A$.
The image of this is $\text{SO}(n)$.</p>
"
"2398533","2398537","<p>To discuss about limit at $c$, we do not need the function value at that value to exists. </p>

<p>Just answer the following question:</p>

<p>what is $\lim_{a \to 1^-} f(a)$? </p>

<p>what is $\lim_{a \to 1^+} f(a)$? </p>

<p>If they are equal, that is the answer to $\lim_{a \to 1} f(a)$</p>

<p>Optional interesting reading:</p>

<p><a href=""https://math.stackexchange.com/q/495378/306553"">The degree of zero polynomial</a> is usually defined to be $-\infty$.</p>
"
"2398539","2398637","<p>In reading your question I presume $A^*$ indicates a conjugate transpose and by $||\vec v||$ you mean the norm $\sqrt{\vec v\cdot\vec v}$ where $\vec v\cdot\vec v\equiv\vec v^*\vec v$. I'll use the same convention.</p>

<p>Let $\vec a_n$ be the columns of $A$. Then the left side of your first inequality can be written as:</p>

<p>$$\sum ||A\hat e_n||=\sum||\vec a_n||=\sum\sqrt{\vec a_n\cdot\vec a_n}$$</p>

<p>It follows from the properties of matrix multiplication that the entries of $A^*A$ are dot products of its columns. Specifically:</p>

<p>$$(A^*A)_{ij}=\vec a_i\cdot\vec a_j$$</p>

<p>If we take the trace, we sum over indices</p>

<p>$$\text{Tr}(A^*A)=(A^*A)_{1,1}+(A^*A)_{2,2}+...=\sum(A^*A)_{nn}=\sum\vec a_n\cdot\vec a_n$$</p>

<p>Now if we let $\sqrt{\ }$ denote an elementwise square root, we can take the trace of $\sqrt{A^*A}$ by taking the square root of the elements in the sum.</p>

<p>$$\text{Tr}\sqrt{A^*A}=\sum\sqrt{\vec a_n\cdot\vec a_n}$$</p>

<p>From this, we see that the quantities on the left sides of both inequalities are identical. Clearly, the same is true for the right sides. Therefore your inequalities describe exactly the same statement, constructed in slightly different ways.</p>

<p>If, instead, we interpret $\sqrt{}$ as a matrix square root (that is, $\sqrt A$ is a unique matrix s.t. $\sqrt A\sqrt A=A$, then the the inequalities are not equivalent. To see why notice that the statement can be rephrased as follows.</p>

<p>Let $A$ and $B$ be positive semidefinite Hermitian matrices and $\sqrt A$, $\sqrt B$ be their matrix square roots.</p>

<p>$$\sum\sqrt{A_{ii}}\le\sum\sqrt{B_{ii}}\implies\text{Tr}\sqrt A\le\text{Tr}\sqrt{B}$$</p>

<p>A counterexample to this would be</p>

<p>$$A=\begin{bmatrix}
1        &amp; \frac 12 \\
\frac 12 &amp; 1
\end{bmatrix},\ \ \ B=\begin{bmatrix}
1        &amp; \frac 16 \\
\frac 16 &amp; 1
\end{bmatrix}$$</p>
"
"2398543","2398666","<p>I guess you mean to say that $f(x_n)\to f(a)$ whenever $x_n\to a$, all in the Cesaro sense.</p>

<p>If this is the case, let $x\in\mathbb R$ and consider the sequence $x_n=(-1)^nx$. Then $x_n\to 0$ in the Cesaro sense, therefore $$f(x)+f(-x)=\lim_{n\to\infty}\frac{1}{2n}\sum_{k=1}^{2n}f(x_k)=f(0)=0,$$ therefore $f$ is odd.</p>

<p>Note now that, if $x,y\in\mathbb R$ and $(y_n)$ is the sequence $$x+y, -x,-y, x+y, -x, -y,\dots,$$ then $y_n\to 0$ in the Cesaro sense, therefore, as above,  $$f(x+y)+f(-x)+f(-y)=\lim_{n\to\infty}\frac{1}{3n}\sum_{k=1}^{3n}f(y_k)=f(0)=0,$$ therefore $$f(x+y)=-f(-x)-f(-y)=f(x)+f(y),$$ since $f$ is odd. This shows that $f(x)=ax$ for some $a$, at least when $x\in\mathbb Q$.</p>

<p>Finally, for $r$ being irrational, let $(x_n)$ be a sequence of rational numbers that converges to $r$, and consider the sequence $(z_n)$, which is defined by $$x_1,-r,x_2, -r,x_3,-r,\dots .$$ Then $z_n\to 0$ in the Cesaro sense, and as above you can show that $f(r)=ar$.</p>
"
"2398552","2398841","<p>$\def\T{\mathcal T}\def\H{\mathcal H}$</p>

<blockquote>
  <p>It is my understanding that an event is a subset of the set of all possible outcomes (sample space). If however the sample space consists of elements which are sets, can an event be defined as one the elements from these ""inner"" sets?</p>
  
  <p>Ex. A coin is flipped twice, $S=\{(\H,\T),(\T,\H),(\T,\T),(\H,\H)\}$ Is the event $A=(\H)$ valid for the sample space despite not being a subset of S?</p>
</blockquote>

<p>Well, no $(\H)$ is not an element of the sample space, nor is $\{(\H)\}$ a subset of it. &nbsp; (Neither are $\H$ or $\{\H\}$ such, respectively.)</p>

<p><em>However,</em> if by $A=(\H)$ you mean such as ""the event of the first coin toss being heads"", then this <em>actually</em> represents the event $\{(\H,\T), (\H,\H)\}$, which is indeed a subset of the sample space.</p>

<hr>

<p>Let $A$ be a <em>random variable</em> mapping the sample space, $S$, to the result of the first toss of the coin, symbolically $A:S\mapsto \{\H,\T\}~,~\forall (x,y)\in S ~ [A(x,y):=x]$. </p>

<p>When we write $\mathsf P(A=x)$ we are actually referring to the event, $\{\omega\in S: A(\omega)=x\}$ . </p>

<p>It is convenient shorthand used because we <em>really don't want</em> to write the entire set builder notation every time. &nbsp; It is also somewhat easier to read.</p>

<p>$$\begin{align*}\mathsf P(A=\H) &amp;= \mathsf P\{\omega\in S: A(\omega)=\H\} \\ &amp;= \mathsf P\{(\H,y)\in S:\forall y\in\{\H,\T\}\} \\ &amp;= \mathsf P(A^{-1}(\H))  \\ &amp;= \mathsf P_A(\H)\\&amp;= \mathsf P\{(\H,\T), (\H,\H)\} \\&amp;= \tfrac 12\end{align*}$$</p>
"
"2398572","2398574","<p>Unfortunately this is not true: let $f(x)=\frac{1}{\sqrt{x}}$ for $0&lt;x\leq 1$, with $f(0)$ defined however you want. Then $f$ is integrable on $[0,1]$ but not bounded.</p>
"
"2398584","2398620","<p>Suppose $F:C\to D$ is any fully faithful functor and $f:X\to Y$ is such that $F(f)$ is an isomorphism.  Let $g:F(Y)\to F(X)$ be the inverse of $F(f)$.  Since $F$ is full, there is a map $h:Y\to X$ such that $g=F(h)$.  Then $F(fh)=F(f)g=1_{F(Y)}$ and $F(hf)=gF(f)=1_{F(X)}$.  Since $F$ is faithful, this implies $fh=1_Y$ and $hf=1_X$.  Thus $f$ and $h$ are inverses and $f$ is an isomorphism.</p>

<p>(This result can be stated briefly as ""any fully faithful functor <em>reflects</em> isomorphisms"".)</p>
"
"2398590","2398916","<p>[Turning my comment into an answer.]</p>

<p>The problem is that $\vec x\cdot\vec x'=\vec v\cdot\vec w$ is only true when $\vec x$ is coplanar with $\vec v$ and $\vec w$ (and the origin). If $\vec x$ is not in this plane, then the angle between it and its image after rotation will be less than the angle between $\vec p$ and $\vec q$. The extreme example of this is $\vec x=\vec u$ itself. Every rotation about $\vec u$ leaves it fixed, so the angle between $\vec x$ and $\vec x'$ is zero regardless of the rotation angle.  </p>

<p>The reason for this is that a rotation acts only on the component of a vector thatâs orthogonal to the rotation axis, i.e., on its projection onto a plane perpendicular to $\vec u$. Imagine a unit sphere with $\vec u$ as the north pole. $\vec v$ and $\vec w$ are then on its equator and the angle between them is the difference in their longitudes. For any other unit vector $\vec x$, the angle between the projections of $\vec x$ and $\vec x'$ onto the equatorial plane is indeed equal to the rotation angleâthis again the difference in longitudesâbut the angle between $\vec x$ and $\vec x'$ themselves is measured in the plane defined by them and the origin. The greater the latitude of $\vec x$, the smaller this true angle gets.</p>
"
"2398592","2398593","<p>As you suggest, making $f$ the identity on the irrationals will do just fine. You also have to define $f$ on the rationals whose numerator is greater than $1$. Easy question: what do you suggest there?</p>

<p>This is perfectly well defined. It's even continuous at most points.</p>
"
"2398598","2398604","<p>Yes.  If $x_n \in \overline{B_X}$, $y_n = (1 - 1/n) x_n \in B_X$, and $\|T(x_n) - T(y_n)\| \to 0$, so if $z = \lim_{n \to \infty} T(x_n)$ then $z = \lim_{n \to \infty} T(y_n)$.</p>
"
"2398606","2398803","<p><strong>Hints:</strong> Using the product rule with integrating factors, we find that $$\frac{d}{dx}(y(x)e^{-x}) = y'(x)e^{-x}-y(x)e^{-x}=f(x)e^{-x}$$ So integrating both sides from $0$ to $u$ and then multiplying by $e^{u}$ we find $$y(u) = \int_0^uf(x)e^{u-x}dx=Tf(u)$$</p>

<p>To prove that $T$ is compact, I suggest using the fact that $|f(x)|\leq \|f\|_{\infty}$ in order to show that the collection $\{Tf:\|f\|_{\infty}&lt;1\}$ is equicontinuous and pointwise bounded in $C[0,1]$.</p>

<p>To show that $T$ has no eigenvalues, then suppose that $$\lambda f(u) = \int_0^u f(x)e^{u-x}dx$$ Now differentiate both sides of this equation with respect to $u$ and you will see that $f$ satisfies a differential equation which is easily solved, with the initial data $f(0)=0$. Can the resulting function be an eigenvector?</p>
"
"2398617","2398697","<p>That's all there is to it: using that ""lifted"" action of $Q[x]$ on the $Q[x]/(x^2+1)$ module, the $Q[x]/(x^2+1)$ modules are exactly the $Q[x]$ modules which are annihilated by $(x^2+1)$. </p>

<p>The finitely generated ones correspond with each other, and you draw conclusions using the structure theorem for f.g. modules over $Q[x]$.</p>
"
"2398618","2398627","<p>Consider $$x\cdot\frac{d}{dx}\sum_{n=1}^{\infty}x^n=x\cdot\frac{d}{dx}\frac{x}{1-x}\\=\frac{x}{(1-x)^2} \ \mbox{for} \ |x|&lt;1$$
Now, in general, interchange of differentiation and infinite summation is not allowed, but for a power series, this is allowed within the radius of convergence. This means that for $|x|&lt;1$ (which is indeed within the radius of convergence):
$$x\cdot\frac{d}{dx}\sum_{n=1}^{\infty}x^n=x\cdot\sum_{n=1}^{\infty}\frac{d}{dx}x^n\\=x\cdot\sum_{n=1}^{\infty}nx^{n-1}\\
= \sum_{n=1}^{\infty}nx^{n}\\
=\frac{x}{(1-x)^2} \ \mbox{(from earlier)}$$</p>
"
"2398619","2398635","<p>Your proof is fine, if a bit excessively complicated.  I think a better idea is a proof by induction.  We can be begin with </p>

<blockquote>
  <p><strong>Claim:</strong> If $S_1$ and $S_2$ are injective, then so is $S_2 S_1$.</p>
</blockquote>

<p>Proof of claim: if $u,v$ are arbitrary elements of $V$, then
$$
S_2S_1(u) = S_2S_1 (v) \implies S_2(S_1(u)) = S_2(S_1(v)) \implies\\
S_1(u) = S_1(v) \implies u = v
$$
Now, we prove the desired statement by induction:</p>

<p><strong>Base case:</strong> For $n=1$, the statement is trivially true</p>

<p><strong>Inductive step:</strong> Suppose that the statement holds for $n$. Then
$$
\prod_{j=1}^{n+1} S_j =  (S_{n+1})\left(\prod_{j=1}^n S_j\right)
$$
is a product of two injective maps.</p>
"
"2398626","2398814","<p>$\forall \varepsilon&gt;0$</p>

<p>$\exists M&gt;0,\ s.t.\ 100/g(M)&lt;\varepsilon/3$.</p>

<p>Due to Egoroff. $\exists E(measurable)\subset[0, 1]\ \&amp;\ m([0, 1]-E)&lt;\varepsilon/(3M)$. And $f_n\xrightarrow{u.}0$ on E.</p>

<p>Find an enough large N such that $\forall n&gt;N$, $|f_n|&lt;\varepsilon/3$ on E.</p>

<p>Then we have</p>

<p>\begin{eqnarray}
\int_0^1|f_n|dx&amp;=&amp;\int_E|f_n|dx+\int_{[0, 1]-E}|f_n|dx\\
&amp;\leq&amp;\varepsilon/3+\int_{([0, 1]-E)\cap\{|f_n|\leq M\}}|f_n|dx+\int_{([0, 1]-E)\cap\{|f_n|&gt; M\}}|f_n|dx\\
&amp;\leq&amp;\varepsilon/3+\int_{[0, 1]-E}Mdx+\int_{[0, 1]\cap\{|f_n|&gt;M\}}|f_n|dx\\
&amp;\leq&amp;2\varepsilon/3+\int_{[0, 1]\cap\{|f_n|&gt;M\}}|f_n|\frac{g(|f_n|)}{g(|f_n|)}dx\\
&amp;\leq&amp;2\varepsilon/3+\frac{1}{g(M)}\int_0^1|f_n|g(|f_n|)dx\\
&amp;\leq&amp;\varepsilon
\end{eqnarray}</p>
"
"2398634","2398845","<p>One of the most important uses of beta distributions is in Bayesian
statistical analysis of binomial data. Beta distributions are used for
prior and posterior distributions. I will give an elementary example below.</p>

<p>In Bayesian statistics a beta distribution is used as a model for the
binomial success probability, which I will denote as $\theta$ (instead of $p$).
[Bayesian analysis treats $\theta$ as a random variable, rather than as
an unknown fixed parameter.]</p>

<p>Suppose you are wondering whether Proposition A will get a majority of
the vote at the next election. The circumstances are that Prop A had to
have some support in order to get on the ballot. Also a similar proposition
passed with about 65% of the vote at the last election. That may be
good or bad: perhaps good because it may show voters favor such propositions;
perhaps bad because such propositions require a slight increase in property
taxes and it may be too soon to ask again.
Everything considered, you think Prop A may be favored by slightly more
than half of the voters, but that the election is likely to be close. </p>

<blockquote>
  <p>Because
  $0 \le \theta \le 1$ a beta distribution seems appropriate because
  beta distributions have support $[0,1].$ </p>
</blockquote>

<p>There is no precisely correct
prior distribution for $\theta,$ because it should reflect your personal
opinion. 
Suppose you choose the prior distribution $\mathsf{Beta}(\alpha_0 = 330, \beta_0=270).$
The 'kernel' of its PDF (omitting the constant of integration) is 
$f(\theta) \propto \theta^{\alpha - 1}(1-\theta)^{\beta-1},$ where the 
symbol $\propto$ indicates omission of the constant. It has mean 0.55, mode 0.5502, and median 0.55006. Also, 
$P(0.51 &lt; \theta &lt; 0.59) \approx 0.95,$ as computed in R statistical software.</p>

<pre><code>qbeta(.5, 330, 270)                         #  'qbeta' is inverse CDF
## 0.5500556
pbeta(.59, 330, 270) - pbeta(.51, 330, 270) #  'pbeta' is CDF
## 0.9513758
</code></pre>

<p>Now suppose that a well-run poll of $n = 1000$ randomly chosen likely voters
shows $x = 620$ in favor of Prop A and $n - x  = 380$ opposed. The likelihood
function corresponding to these results is 
$f(x|\theta) \propto \theta^x(1-\theta)^{n-x}.$</p>

<p>Bayes Theorem states that the posterior distribution $f(\theta|x)$ is found
by multiplying the prior and likelihood functions:
$$f(\theta|x) \propto \theta^{\alpha_0 - 1}(1-\theta)^{\beta_0-1}
\times \theta^x(1-\theta)^{n-x}.$$</p>

<p>In our example, $$f(\theta|x) = 
\theta^{\alpha_0 + x -1}(1-\theta)^{\beta_0 + n - x -1} = 
\theta^{\alpha_n - 1}(1-\theta)^{\beta_n - 1},$$
where $\alpha_n = 950$ and $\beta_n = 650.$
We recognize $f(\theta|x)$ as the kernel of $\mathsf{Beta}(950, 650).$</p>

<blockquote>
  <p>We say that the beta prior and the binomial likelihood are 'conjugate' 
  (mathematically compatible). [Without conjugacy we would not be able to
  recognize the posterior distribution so easily, and we would have to
  use a different form a Bayes' Theorem in which a denominator might
  need to be integrated by numerical methods.]</p>
</blockquote>

<p>Finally, a 95% posterior probability interval for $\theta$ is $(0.570, 0.618).$</p>

<pre><code>qbeta(c(.025,.975), 950, 650)
## 0.5695848 0.6176932
</code></pre>

<p>A melding of the information in the prior and the likelihood has given a
somewhat more optimistic estimate of the chances Prop A will pass, than
did our prior distribution.</p>

<hr>

<p>Notes: (1) If we had used the noninformative prior distribution $\mathsf{Unif}(0,1) \equiv \mathsf{Beta}(1,1),$ then the posterior distribution would have
been $\mathsf{Beta}(621,381)$ with a 95% posterior probability interval
$(0.589, 0.650).$ This is numerically the same (to three places) as a frequentist Agresti-style 95% confidence interval for $\theta$. However, Bayesian and frequentist interval estimates have somewhat different interpretations.</p>

<p>(2) This example is similar to Example 8.1 in Suess (2010).</p>
"
"2398636","2399101","<p>OP proof problem ...</p>

<blockquote>
  <p>$f'(x)=\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h}$. Given $f'(x)=\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h}\rightarrow b$ as $x\rightarrow\infty$. For all $\epsilon&gt;0$, there exists a $\delta&gt;0$</p>
</blockquote>

<p>For all $x$ and for all $\epsilon&gt;0$, there exists $\delta &gt; 0$ ...<br>
Note that $\delta$ may depend on $x$</p>

<blockquote>
  <p>such that as long as $h$ is within $\delta$ distance away from $0$,  </p>
</blockquote>

<p>so  $h$ also depends on $x$</p>

<blockquote>
  <p>$|\frac{f(x+h)-f(x)}{h}-b|&lt;\epsilon$. As $x$ goes to infinity, we have $|b|&lt;\epsilon$.</p>
</blockquote>

<p>Well, for a fixed $h$, as $x \to \infty$ we would get $|b| \le \epsilon$.  But (as noted) $h$ depends on $x$, so this step does not work.</p>
"
"2398646","2398755","<p>Your idea is correct. The expression<br>
$$\alpha \sum_{n=-\infty}^{\infty} \hat{f}(n\alpha)$$
is like Riemann sum for $\int_{-\infty}^\infty \hat f(\xi)\,d\xi$, except, of course, Riemann sums are normally considered on a finite interval. Dealing within this kind of sum is a bit annoying, so let's focus on the left hand side of $(1)$ instead. Suppose there are constants $C$ and $p&gt;1$ such that<br>
$$
|f(x)|\le C|x|^{-p}
$$
for large $x$. This isn't a super strong assumption; reasonable integrable functions tend to decay like that. 
As $\beta\to\infty$, we get
$$
\left| \sum_{k\ne 0} f(k\beta) \right| \le \beta^{-p}\sum_{k\ne 0}|k|^{-p} \to 0
$$
so the left hand side of $(1)$ indeed converges to $f(0)$. </p>

<p>Of course, so does the right hand side since they are equal. At this point I'm inclined to cop out by saying: suppose also that $\hat f$ is integrable; then the Fourier inversion formula holds.
$$
f(0) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty\hat f(\xi)\,d\xi
$$ 
So that's the proof that 
$$\alpha \sum_{n=-\infty}^{\infty} \hat{f}(n\alpha)\to \int_{-\infty}^\infty \hat f(\xi)\,d\xi\tag2$$
as $\alpha\to 0 $.  </p>

<hr>

<p>Proving $(2)$ directly seems awkward. 
If $f$ is integrable, then $\hat f$ is uniformly continuous on $\mathbb{R}$. 
which tells us that $\alpha \hat f(n\alpha)$ is uniformly close to $\int_{n\alpha}^{(n+1)\alpha}\hat f(\xi)\,d\xi$. Unfortunately, a uniform bound isn't much good with infinitely many terms. We need something like 
$$\left|\alpha \hat f(n\alpha) - \int_{n\alpha}^{(n+1)\alpha}\hat f(\xi)\,d\xi\right| &lt; \frac{C(\alpha)}{n^p} \tag3$$
with $p&gt;1$, and $C(\alpha)\to 0$ as $\alpha\to 0$; this way the errors add up to a small quantity. By the mean value theorem the left hand side of $(3)$ is bounded by $\alpha^2 \sup_{[n\alpha, (n+1)\alpha]}|\hat f'|$ so if we have $|\hat f'|\le C/|x|^p$ for some $1&lt;p&lt;2$, then $(2)$ follows. This is a relatively strong assumption compared to the first part of this post.</p>
"
"2398649","2398746","<p>At least in Matlab, your answer are the <a href=""https://www.mathworks.com/help/simulink/sfg/what-is-an-s-function.html"" rel=""nofollow noreferrer"">S-Functions</a>.</p>

<p>You simply define your dynamic model in a state space function, as any linear, non linear algebraic, or not algebraic function, for each of their stages:</p>

<ul>
<li>initialize, $x_0$,</li>
<li>update state, $x=f(t,x)$,</li>
<li>update output, $y=g(t,x)$,</li>
<li>finalize.</li>
</ul>

<p>Besides it is pure code and let Simulink deal with the numerical integration and generic plotting and postprocessing.</p>

<p>If you only need some simplest not dynamical discontinuities, you can use the <a href=""https://www.mathworks.com/help/simulink/slref/interpretedmatlabfunction.html"" rel=""nofollow noreferrer"">Matlab Function</a> Blocks or similar for modelling discontinuities. </p>

<p>Under Octave, or any other pure code languages, one shall remember Simulink is a set of <a href=""https://www.mathworks.com/help/matlab/ref/ode45.html?requestedDomain=www.mathworks.com"" rel=""nofollow noreferrer"">integrators</a>, most based in Runge Kutta methods, such as <code>ode45</code>, <code>ode23</code>, <code>ode15</code>, which essentially integrates the update function $f$:</p>

<p>$$(t,x)=ode45(f,t,x_0,options)$$</p>

<p>Which solved the main dynamical part of the problem. The output can be easily evaluated if $g$ is properly vectorized:</p>

<p>$$
y=g(t,x)
$$</p>
"
"2398667","2398673","<p>I'll denote by $e_1:=(1,0)$ and $e_2:=(0,1)$ the standard orthonormal basis on $\mathbb{R}^2$ and by $V$ the set $\{v_1,\ldots,v_n\}$.</p>

<p>A. False. Consider the basis $e_1,e_2$ on $\mathbb{R}^2$.</p>

<p>B. True. Any basis must be linearly independent, and this is the definition of linear independence for $V$.</p>

<p>C. True. Any basis must span $\mathbb{R}^n$, and this is the definition that $V$ spans $\mathbb{R}^n$.</p>

<p>D. False. While this is true for the standard basis, the vectors $2e_1,2e_2$ form a basis on $\mathbb{R}^2$.</p>

<p>E. False. If $V\setminus\{v_i\}$ spans $\mathbb{R}^n$, then $V$ is not linearly independent.</p>

<p>F. False. This is false for any set $V$ because $c_1=\cdots=c_n=0$ implies $c_1v_1+\cdots+c_nv_n=0$.</p>

<p>G. True (unless $n=0$ is allowed). If $n\geq 1$ and $0\in V$, then $V$ is not linearly independent.</p>

<p>H. False. Consider the basis $e_1+e_2,e_2$ on $\mathbb{R}^2$.</p>

<p>I. False. Take $c_1=\cdots=c_n=0$.</p>

<p>J. False. While removing a vector from $V$ will yield a basis for an $n-1$ dimensional subspace of $\mathbb{R}^n$, this subspace is only isomorphic to $\mathbb{R}^{n-1}$.</p>

<p>K. True. This is true for any set of vectors $V$.</p>
"
"2398670","2398674","<p>Taking the last two digits of a number is equivalent to taking the number $\bmod 100$.  You can write a large number as $100a+10b+c$ where $b$ and $c$ are the last two digits and $a$ is everything else.  Then $(100a+10b+c)^2=10000a^2+2000ab+200ac+100b^2+20bc+c^2$.  The first four terms all have a factor $100$ and cannot contribute to the last two digits of the square.  The term $20bc$ can only contribute an even number to the tens place, so cannot change the result.  To have the last digit of the square odd we must have $c$ odd.  We then only have to look at the squares of the odd digits to see if we can find one that squares to two odd digits.  If we check the five of them, none do and we are done.</p>
"
"2398681","2398686","<p>A set $C$ in a topological space $X$ is <strong>closed</strong> if for every $x\in X\setminus C$, there exists an open set $U$ such that $x\in U \subseteq X\setminus C$.</p>

<p>A point $x$ in a subspace $S$ of a topological space $X$ is an <strong>isolated point of $S$</strong> if there exists an open set $U$ such that $U\cap S = \{x\}$.</p>

<p>A point $x$ is a <strong>limit point of a subspace $S$</strong> of a topological space $X$ if every open set $U$ which contains $x$ satisfies $U\cap (S\setminus\{x\})\ne\emptyset$.</p>

<p>Let's notice something right away:</p>

<blockquote>
  <p>Let $X$ be a topological space, $S\subseteq X$, and $x\in S$. Then $x$ is an isolated point of $S$ iff $x$ is not a limit point of $S$.</p>
</blockquote>

<p><em>Proof:</em>
Since $x\in S$ by hypothesis, we have that $x$ is an isolated point of $S$ iff there exists $U$ open such that $U\cap S=\{x\}$ iff there exists $U$ open containing $x$ such that $U\cap(S\setminus\{x\})=\emptyset$ iff $x$ is not a limit point of $S$. $\square$</p>

<p>So if we have a closed ball $B$ and a point $x$ in $\mathbb{R}^n\setminus B$, then the union $A:=B\cup\{x\}$ is a closed set, $x$ is an isolated point of $B$, and $x$ is not a limit point of $B$.</p>
"
"2398683","2398688","<p>A cyclic group of order $10$ certainly does have an element of order $2$. Let $G=\langle a\rangle$ be cyclic of order $10$, then $a^5$ has order $2$.</p>

<p>A cyclic group of order $5$ has no element of order $2$ simply because its order is odd, and the order of an element must divide the order of the group (Lagrange).</p>
"
"2398684","2398722","<p>It is impossible to have gotten a duplicate on the first draw.  It is impossible to have not gotten a duplicate by the 1297'th draw by pigeon-hole principle.</p>

<p>To have gotten your first duplicate on the $k$'th draw, you need the first $k-1$ draws to all be distinct and the $k$'th to be a duplicate.</p>

<p>The first draw will always be distinct.  The second will be distinct from the first with probability $\frac{1295}{1296}$.  The third will be distinct from the first two with probability $\frac{1294}{1296}$ and so on... the $(n)$'th will be distinct from the earlier $n-1$ with probability $\frac{1296-n+1}{1296}$.  Multiplying these, we get for $n$ draws to all be distinct, this will occur with probability $\frac{1296\frac{n}{~}}{1296^n}$ where $x\frac{n}{~}$ represents a <a href=""https://en.wikipedia.org/wiki/Falling_and_rising_factorials"" rel=""nofollow noreferrer"">falling factorial</a> $x\frac{n}{~}=\underbrace{x(x-1)(x-2)\cdots (x-n+1)}_{n~\text{terms in the product}}=\frac{x!}{(x-n)!}$.</p>

<p>Next, supposing $k-1$ distinct values have all been taken, for the $k$'th to duplicate one of the earlier results, this will occur with probability $\frac{k-1}{1296}$</p>

<p>We have then the probability distribution function for $X$, the number of draws until the first duplicate:</p>

<p>$$Pr(X=k)=\frac{(k-1)1296\frac{k-1}{~}}{1296^k}$$</p>

<p>Applying the definition of expected value for a pdf: $E[X]=\sum\limits_{k\in\Delta} kPr(X=k)$ we have then the expected value is</p>

<p>$$\sum\limits_{k=2}^{1297}\frac{k(k-1)1296\frac{k-1}{~}}{1296^k}\approx 45.7889$$</p>

<blockquote class=""spoiler"">
  <p> wolfram link: <a href=""http://www.wolframalpha.com/input/?i=sum+from+n%3D2+to+1297+of+n(n-1)(1296!%2F(1296-n%2B1)!)%2F1296%5En"" rel=""nofollow noreferrer"">http://www.wolframalpha.com/input/?i=sum+from+n%3D2+to+1297+of+n(n-1)(1296!%2F(1296-n%2B1)!)%2F1296%5En</a></p>
</blockquote>
"
"2398691","2398699","<p>Take \begin{equation} g(x) = \begin{cases}
\exp\left( \frac{1}{x^2-k} \right), &amp; -k \leq x \leq k, \\
0, &amp; \left| x \right| &gt; k. 
\end{cases}
\end{equation}
This function is in $C^\infty_c(\mathbb{R})$ and so its inverse Fourier decays rapidly. </p>
"
"2398692","2398800","<p>You say the words ""random variables"" so I am assuming that $\mu$ is a probability measure, i.e., $\mu(\Omega)=1$.</p>

<p>In this case, your claim is <strong>false</strong> unless $p=\infty$. When $p=\infty$ it is easily verified to be true. </p>

<p>Let $p&lt;\infty$. We let $\Omega=[0,1]$ with Lebesgue measure.</p>

<p>For $n \in \Bbb N$ and $0 \leq k \leq n-1$ we define $X_{n,k}$ to be $n^{1/p}$ times the indicator function of the interval $[k/n, (k+1)/n]$. Then clearly $\|X_{n,k}\|_p=1$ for all $k$ and $n$. However $\sup_{n,k}|X_{n,k}| = +\infty$ since $n^{1/p} \to \infty$ as $n \to \infty$. Hence $$\big\| \sup_{n,k} |X_{n,k}| \big\|_q = +\infty,\;\;\;\;\;\; \forall q \in [1,\infty]$$</p>

<p>The weakest conditions (of which I know) that would make your claim true is that $p&gt;1$, $\mathscr C = [0,\infty)$ or $\mathscr C = \Bbb N$, and $(X_t)_{t \geq 0}$ is a <strong>submartingale</strong>. In this case, the result follows from <strong>Doob's $L^p$ inequality</strong>: $$\big\|\sup_{t \geq 0}|X_t| \big\|_p \leq \frac{p}{p-1} \sup_{t \geq 0} \|X_t\|_p$$</p>
"
"2398709","2399107","<p>You must assume, that w is non-zero, otherwise the result is false.</p>

<p>By definition:$w=Q(T)(v)$, with $Q$ some polynomial. We may assume without loss of generality, that $deg(Q)&lt;deg(p)$, simply by taking a long division of $Q$ by $p$. Since $p$ is irreducible, either $Q=0$ or $p,Q$ are coprime.</p>

<p>$Q$ cannot be zero, since by assumption $w\neq 0$. Hence $p$ and $Q$ are coprime, therefore there exist polynomials $U,V$ such that $pU+QV=1$, therefore $pU(T)(v)+QV(T)(v)=v$. $pU(T)(v)=0$ by definition of $p$ and $Q(T)(v)=w$, hence $V(T)(w)=v$. </p>

<p>From this it followis trivially that $C_T(w)=C_T(v)$.</p>
"
"2398732","2399136","<p>The special property is that it is a gradient field. Note that for an arbitrary vector field ${\bf F}=(F_1,F_2,F_3)$ you can choose independently  <em>three</em> functions $F_i$ of three variables, whereas to produce a gradient field you can only choose <em>one</em> function $f$ of three variables and then put ${\bf F}:=\nabla f$.</p>
"
"2398743","2398889","<p>There is no need to assume that $V$ is finite dimensional, and we have a more  general Theorem:$\newcommand{\ran}{\operatorname{range}}\newcommand{null}{\operatorname{null}}\newcommand{\e}{\varepsilon}$</p>

<blockquote>
  <p>The original image of a basis for 
  $\ran T$ and a basis for 
  $\null T$
  together form a basis for $V$.</p>
</blockquote>

<p>Proof. Suppose $\{\varepsilon_i\}_{i\in \Gamma_1}$ is a (hamel) basis for $\null T$, $\{\beta_j\}_{j\in \Gamma_2}$ is a basis for $\ran T$, and $\alpha_j=T^{-1}(\beta_{j}), j\in\Gamma_2.$ </p>

<p>$\forall v\in V,$ suppose $Tv=\sum_{j\in \Gamma_2^0}k_j \beta_j$ for some finite subset $\Gamma_2^0$ of $\Gamma_2$, then $v-\sum_{i\in \Gamma_2^0}k_j\alpha_j\in \null T$, so there exists some finite subset $\Gamma_1^0$ of $ \Gamma_1$ such taht $v-\sum_{j\in\Gamma_2^0}k_j \alpha_j=\sum_{i\in\Gamma_1^0}k_i\e_i$. Hence every vector in $V$ is a linear combination of $\{\e_i\}_{i\in \Gamma_1}\cup\{\alpha_j\}_{j\in\Gamma_2}$.</p>

<p>Suppose $\sum_{i\in\Gamma_1^0}k_i\e_i+\sum_{j\in\Gamma_2^0}k_j \alpha_j=0$ for some finite subset $\Gamma_1^0$ and $\Gamma_2^0$ of $\Gamma_1$ and $\Gamma_2$,respectively. Let $T$ act on both side, we have $\sum_{j\in\Gamma_2^0}k_j \beta_j=0$, thus $k_j=0(j\in \Gamma_2)$. Hence $\sum_{i\in\Gamma_1^0}k_i\e_i=0$ and $k_i=0(i\in\Gamma_1)$. Therefore, $\{\e_i\}_{i\in \Gamma_1}\cup\{\alpha_j\}_{j\in\Gamma_2}$ is linear independent.$\blacksquare$</p>

<p>Back to your question, $U=\operatorname{span}\{\alpha_j\}_{j\in \Gamma_2}$ is required.</p>
"
"2398749","2398833","<p>The idea is right, just that one can't assure the existence of  a maximum on a non-compact set, so $c$ need not exist.</p>

<p>However, if we consider just one such $c$, we can  get a contradiction.</p>

<p>To do  this, let $c$ be a value such that $f(c) &gt; 1$, and let $b$ be the largest value of $x$ in $[0,c]$ such that $f(x) = 1$, as the hint above says. Such a $b$ exists because $f(0) = 1$, so  the level set of $1$ is non-empty. Note that $c &gt; b$.</p>

<p>On $[b,c]$, it is clear that by the mean value theorem, $f(c) - f(b) = (c-b) f'(d)$ for some $d \in (b,c)$. This implies,  that since the LHS is positive and $c&gt;b$, hence $f'(d) &gt; 0$, but then $f(d) &gt; 1$, since $d &gt; b$ and $d \in [c,b]$. This contradicts the statement made in the question, hence no such $c$ exists. </p>

<p>Hence, the result follows.</p>
"
"2398751","2398778","<p>When I swap $x$ and $z$ I get
$$\int_0^2\int_0^x ze^{x^3}\,dz\,dx$$
which looks much more promising.</p>

<p>Observe that the double integral is over the region
$$\{(x,z):0\le z\le x\le 2\}.$$</p>
"
"2398777","2398810","<p>Note that by denoting $f(x) = \tan x \sin x -x^2$, you found that $$f'''(x)=-\sin x (1-6\sec^4x+\sec^2x) = \sin x (1+3\sec^2 x)(2\sec^2 x -1 ) \geq 0 $$
Hence $f''(x)$ is increasing, with $f''(0)=0$, we conclude that $f''(x) \geq 0$.</p>

<p>Hence $f'(x)$ is increasing, with $f'(0)=0$, we conclude that $f'(x) \geq 0$.</p>

<p>Hence $f(x)$ is increasing, with $f(0)=0$, we conclude that $f(x) \geq 0$. </p>

<p>This is what we wish to prove.</p>

<hr>

<p>From a more advanced perspective, the inequality follows from the fact that Taylor expansion of $$\tan x \sin x = x^2+\frac{x^4}{6}+\cdots$$ at $x=0$ have all coefficients positive, the radius of convergence of this series is $\pi/2$. </p>

<p>To see why all coefficients are positive, write
$$\tan x \sin x = \frac{1}{\cos x} - \cos x$$</p>

<p>The Taylor expansion of $\sec x$ at $x=0$ is $$\sec x = \sum_{n=0}^{\infty} \frac{(-1)^n E_{2n}}{(2n)!} x^{2n}$$
where $E_{2n}$ are <a href=""https://en.wikipedia.org/wiki/Euler_number"" rel=""nofollow noreferrer"">Euler number</a>. The fact that $(-1)^n E_{2n}$ is positive follows from the series evaluation:
$$\beta(2n+1) = \frac{(-1)^n E_{2n} \pi^{2n+1}}{4^{2n+1} (2n)!}$$
with $\beta(n)$ the <a href=""https://en.wikipedia.org/wiki/Dirichlet_beta_function"" rel=""nofollow noreferrer"">Dirichlet beta function</a>. </p>

<p>Also note that we have $|E_{2n}| &gt; 1 $ when $n&gt;1$, hence the power series of $\frac{1}{\cos x}-\cos x$ has all coefficients positive.</p>

<hr>

<p>From this, you might want to prove the stronger inequality:</p>

<blockquote>
  <p>When $0&lt;x&lt;\frac{\pi}{2}$, 
  $$\tan x \sin x &gt; x^2 + \frac{x^4}{6} $$
  $$\tan x \sin x &gt; x^2 + \frac{x^4}{6} + \frac{31x^6}{360} $$</p>
</blockquote>
"
"2398780","2398790","<p><strong>Hint</strong> Since $A+B$ is symmetric, it is orthogonally diagonalizable.</p>

<p>If $v_1,..,v_k$ is a basis for the eigenspace corresponding to $\lambda=0$, then, since $A+B$ is diagonalizable, you have $\rank(A+B)=n-k$.</p>

<p>Now, use 
$$v_j^T (A+B) v_j=0$$
and the fact that $A,B$ are positive semi-definite, to deduce that $$v_j^TAv_j=v_j^TBv_j=0$$</p>
"
"2398786","2398792","<p>If you follow the derivation of the equation of motion, there is an approximation made that $\sin \theta = \theta$.  Before the approximation the equation is $\ddot \theta + \frac gl \sin \theta =0$.  Once you make the approximation you have a simple harmonic oscillator.  As the Taylor series of $\sin \theta$ around zero is $\sin \theta \approx \theta - \frac 16 \theta^2 $ we are ignoring the cubic (and higher) term(s).  This is valid when the angle is small.</p>
"
"2398794","2398796","<p>Consider the sets
$$
\{(0,y) \mid y\in[0,1]\}
\quad\text{and}\quad
\{(1,y) \mid y\in[0,1]\}
$$
in $\mathbb{R}^2$.</p>
"
"2398795","2398849","<p>Let $o(ab)=n$,$o(a)=m$ and $o(b)=k$.</p>

<p>First we show that $n \;| \;mk$.</p>

<p>Observe that $(ab)^{mk}=a^{mk}b^{mk}=(a^m)^k(b^k)^m=1.$ But $n=o(ab)$. Thus $n\;|\;mk$.</p>

<p>Second we show that $mk \; | \; n$.</p>

<p>For that refer to Jyrki Lahtonen's comment. As $(ab)^n=a^n b^n=1 \implies a^n=b^{-n}.$ Let $x=a^n=b^{-n}$. Then $x \in \langle a \rangle \cap \langle b \rangle$. By Lagrange's theorem, $o(\langle a \rangle \cap \langle b \rangle)\;|\;o(a)=m$ and $o(\langle a \rangle \cap \langle b \rangle)\;|\;o(b)=k$. Thus $o(\langle a \rangle \cap \langle b \rangle)\;|\;mk$. But since $\gcd(m,k)=1$, Thus $o(\langle a \rangle \cap \langle b \rangle)=1 \implies a^n=b^{-n}=1. \implies a^n=1=b^n.$</p>

<p>As $m=o(a)$ and $k=o(b)$, we get $m|n$ and $k|n$. Since $\gcd(m,k)=1$, we have $mk\;|\;n.$</p>

<p>$\therefore n=mk$.</p>
"
"2398797","2398825","<p><strong>Hint:</strong> By the Cauchy-Schwarz inequality,</p>

<p>$$\begin{align*} \int_0^1 |f_n(x)| \, dx &amp;= \int_0^1 1_{\{|f_n(x)| \leq \epsilon\}} |f_n(x)| \, dx + \int_0^1 1_{\{|f_n(x)|&gt;\epsilon\}} |f_n(x)| \, dx \\ &amp;\leq \epsilon + \sqrt{\int_0^1 1_{\{|f_n(x)|&gt;\epsilon\}} \, dx} \cdot \underbrace{\sqrt{\int_0^1 f_n(x)^2 \, dx}}_{\leq 10}. \end{align*}$$</p>

<p>Now let first $n \to \infty$ and then $\epsilon \to 0$.</p>
"
"2398798","2398822","<p>For (2),
let
$x=t^u, y=t^v$
and I will try to choose
$u$ and $v$
to make things interesting.</p>

<p>$\begin{array}\\
g(X) 
&amp;= \dfrac{(x^2+y^4)^3}{1+x^6y^4}\\
&amp;= \dfrac{(t^{2u}+t^{4v})^3}{1+t^{6u}t^{4v}}\\
&amp;\sim \dfrac{(t^{\max(2u, 4v)})^3}{t^{6u+4v}}\\
&amp;= \dfrac{t^{\max(6u, 12v)}}{t^{6u+4v}}\\
&amp;= t^{\max(6u, 12v)-6u-4v}\\
\end{array}
$</p>

<p>If $12 v &lt; 6u+4v$,
then $g(X) \to 0$.
This is
$8v &lt; 6u$.
This works if,
for example,
$u=2, v=1$.</p>

<p>Try
$x = t^2, y=t$.
Then
$g(X)
=\dfrac{(t^4+t^4)^3}{1+t^{12}t^4}
=\dfrac{8t^{12}}{1+t^{16}}
\to 0
$.</p>
"
"2398818","2399024","<p>What we know is that for each positive $\varepsilon$, there exists a set $\Omega_\varepsilon $ of probability $1$ such that for all $\omega\in\Omega_\varepsilon$, 
$$\lvert\mathsf{E}[g(X)\mid\mathcal{F}  ]\left(\omega\right) \rvert\leqslant C\epsilon+\mathsf{E}[\lvert X\lvert\mid\mathcal{F}]\left(\omega\right)\epsilon^{-1}.$$
In order to safely replace $\varepsilon$ be something random, we should have the previous inequality on a set of probability one independent of $\varepsilon$. </p>

<p>To this aim, let $\Omega':=\bigcup_{\substack{    \varepsilon\in \mathbb Q\\ \varepsilon\gt 0}}\Omega_\varepsilon$. Then $\Omega'$ has probability one and for all $\omega\in\Omega'$ and all positive rational number $\varepsilon$,<br>
 $$\tag{*}   \lvert\mathsf{E}[g(X)\mid\mathcal{F}  ]\left(\omega\right) \rvert\leqslant C\epsilon+\mathsf{E}[\lvert X\lvert\mid\mathcal{F}]\left(\omega\right)\epsilon^{-1}.$$
Now, if $\varepsilon$ is a positive real number, there exists a sequence of rational positive numbers $\left(\varepsilon_n\right)_{n\geqslant 1}$ which converges to $\varepsilon$, hence applying (*) to $\varepsilon_n$ and letting $n$ going to infinity  gives what we want.                 </p>
"
"2398823","2398827","<p>So, we have $$\dfrac ca=\alpha\beta=\beta^3$$</p>

<p>$$-\dfrac ba=\alpha+\beta=\beta^2+\beta$$</p>

<p>Cube both sides using $$(p+q)^3=p^3+q^3+3pq(p+q)$$</p>

<p>Replace the values of $\beta^3,\beta^2+\beta$</p>
"
"2398828","2398875","<p>It's more correct to say: suppose $(y_n)_n$ is a sequence in $Y$. Because $f$ is surjective we can pick $x_n \in X$ for all $n$ such that $f(x_n) = y_n$, Then $(x_n)$ contains a Cauchy subsequence $(x_{n_k})_k$ and by uniform continuity of $f$, $(f(x_{n_k}))_k = (y_{n_k})_k$ is a Cauchy subsequence of $(y_n)_n$. QED.</p>

<p>You cannot say that the ""preimage of a sequence is a sequence"":
the type is wrong, as a sequence is not a subset, but a function, but also if $f$ were constant, e.g. the preimage is the whole space, not a sequence.</p>

<p>But it is true that there is a sequence in $X$ that maps onto the sequence in $Y$. Introducing some notation makes it easier to follow.</p>

<p>In essence your proof is fine. You could as an exercise show it from the definitions of totally bounded and uniform continuity: start with an $\varepsilon &gt;0$, we get $\delta&gt;0$ from the uniform continuity definition. Now a finite $\delta$-net of $X$ (which exists) maps to a finite $\varepsilon$-net of $Y$, using surjectivity.</p>
"
"2398839","2398852","<p>There is only one small error in your evaluation. The work integral should be
$$\int_{-2}^{0}(e^{-t(1+t^2)},e^{-t},-e^{1})\cdot(0,-1,2t) dt=
\int_{-2}^{0}(-e^{-t}-2et)dt=[e^{-t}-et^2]_{-2}^0\\=1-e^2+4e.$$</p>

<p>P.S. Yes, the work can be negative (and the parameter too). Note that if $L$ is the work of a force field $F$ along a certain curve $C$ from point $A$ to point $B$ then the work from $B$ to $A$ along the same curve is $-L$.</p>
"
"2398851","2398862","<p>In your answer there should be a minus sign.
\begin{align*}\int \frac{\cos(x)}{\sin(x)(1+\sin(x))}dx&amp;=\int \frac{1}{\sin(x)(1+\sin(x))} d(\sin(x))\\&amp;=\int \left(\frac{1}{\sin(x)}-\frac{1}{1+\sin(x)} \right)d(\sin(x))\\&amp;= \ln(|\sin(x)|) - \ln(1+\sin(x))+C\\&amp;=\ln\left(\frac{|\sin(x)|}{1+\sin(x)}\right)+C.
\end{align*}</p>
"
"2398864","2398886","<p>The numerator $$=\sin^2x+\sin^2y-\cos(x-y)\cdot2\sin x\sin y$$</p>

<p>$$=\sin^2x+\sin^2y-\cos(x-y)[\cos(x-y)-\cos(x+y)]$$</p>

<p>$$=\sin^2x+\sin^2y-\cos^2(x-y)+\cos(x-y)\cos(x+y)$$</p>

<p>Now use <a href=""https://math.stackexchange.com/questions/345703/prove-that-cos-a-b-cos-a-b-cos-2a-sin-2b"">Prove that $\cos (A + B)\cos (A - B) = {\cos ^2}A - {\sin ^2}B$</a></p>
"
"2398876","2398880","<p>You are correct but the argument is simpler if you evaluate the integral of $|z|^2$ along the unit circle centred at $0$,
$$\int_{|z|=1} |z|^2\ dz=\int_{|z|=1} 1\ dz=2\pi.$$
If $|z|^2$ has a primitive then the above integral should be zero.</p>
"
"2398879","2398885","<p>Characteristic for ""straight lines"" is that they correspond with functions that can be prescribed by $x\mapsto ax+b$ where $a,b$ are fixed real numbers.</p>

<p>Based on equation $y=ax+b$ we can find an expression for $x$ in $y$ under the extra condition that $a\neq0$: $$x=\frac1{a}(y-b)$$By interchanging $x$ and $y$ we find the inverse function is:$$y=\frac1{a}(x-b)$$ This tells us that such linear functions have an inverse if $a\neq0$. In case $a=0$ we are dealing with a <strong>constant</strong> function prescribed by $x\mapsto b$. This function is not injective hence has no inverse.</p>
"
"2398900","2398906","<p>working backwars then your equation is equivalent to
$$\sin(x)^2-\sin(y)^2=\sin(x+y)\sin(x-y)$$ and this is
$$(\sin(x)-\sin(y))(\sin(x)+\sin(y))=\sin(x+y)\sin(x-y)$$
further use
$$\sin(x)-\sin(y)=2\cos\left(\frac{x+y}{2}\right)\sin\left(\frac{x-y}{2}\right)$$
$$\sin(x)+\sin(y)=2\cos\left(\frac{x-y}{2}\right)\sin\left(\frac{x+y}{2}\right)$$</p>
"
"2398901","2399124","<p>There are infinitely many configurations satisfying the given conditions, but in all of them the pentagon $P$ in question has the same area.</p>

<p>Let $|BC|=b$, $|DC|=d$, and $\angle(BCD)=\gamma$. Then $|AC|=b\sqrt{3}$, $|CE|=d$, and $\angle (ACE)=\gamma-90^\circ$. From these data one computes
$$\eqalign{{\rm area}(P)&amp;={1\over2}b^2\sin 120^\circ+{1\over2}d^2\sin60^\circ+{1\over2}\ b\sqrt{3}\ d\sin(\gamma-90^\circ)\cr
&amp;={\sqrt{3}\over4}\bigl(b^2+d^2-2bd\cos\gamma\bigr)\ .\cr}$$
Now the cosine theorem gives $$b^2+d^2-2bd\cos\gamma=|BD|^2=4\ ,$$
so that we obtain ${\rm area}(P)=\sqrt{3}$, whatever the values of $b$ and $d$ under the given circumstances.</p>
"
"2398902","2399279","<p>Let $u_k(x) = u(x) e^{-\pi |x|^2/k^2}$, $\widehat{u_k} = \widehat{u} \ast k^n e^{-\pi |x|^2 k^2}$.  Then since $u_k(x) \ge 0$ and is increasing in $k$ $$\|u\|_{L^1}=\lim_{k \to \infty}\|u_k\|_{L^1} = \lim_{k \to \infty} \widehat{u_k}(0) \le \|\widehat{u}\|_{L^\infty}\lim_{k \to \infty} \|1 \ast k^n e^{-\pi |x|^2 k^2}\|_\infty =\|\widehat{u}\|_{L^\infty}$$</p>
"
"2398908","2398945","<p>Let $X,Y$ be random variables with $0&lt;\sigma_X^2&lt;\infty$ and $0&lt;\sigma_Y^2&lt;\infty$.</p>

<p>Then defining $U_X:=\frac{X-\mu_X}{\sigma_X}$ and $U_Y:=\frac{Y-\mu_Y}{\sigma_Y}$ we have:$$\rho_{X,Y}=\mathbb EU_XU_Y$$
If $a\neq0$ and $b$ are fixed then it can be shown (try it yourself) that: $$U_{aX+b}=\frac{a}{|a|}U_X$$ and consequently we find:$$\rho_{X,aX+b}=\frac{a}{|a|}\rho_{X,X}$$
Knowing that $\rho_{X,X}=1$ under these conditions (you have a proof for it), we end up with:$$\rho_{X,aX+b}=\frac{a}{|a|}\in\{-1,1\}$$</p>

<hr>

<p>Hint for converse:</p>

<p>If $\rho_{X,Y}=1$ then $\mathbb E(U_X-U_Y)^2=0$ hence $U_X-U_Y=0$ a.s.</p>

<p>If $\rho_{X,Y}=-1$ then $\mathbb E(U_X+U_Y)^2=0$ hence $U_X+U_Y=0$ a.s.</p>
"
"2398917","2398941","<p>$\color{Green}{\text{Lemma}}$: 
Let $1 &lt; m$ be an odd number;<br>
then the equation $m=A^2-B^2$ 
has a solution in natural numbers. </p>

<p><strong>Proof</strong>: 
Let $A:=\dfrac{m+1}{2}$ and 
let $B:=\dfrac{m-1}{2}$.</p>

<hr>

<ul>
<li><p>If $n$ is odd, then let 
$a_5=2, \ a_1=1, \ a_2=1$.<br>
So the equation changes to $n+2=a_3^2-a_4^2$;<br>
and then 
let $a_3=\dfrac{n+3}{2}$ and 
let $a_4=\dfrac{n+1}{2}$. </p></li>
<li><p>If $4 \leq n$ is even, then let 
$a_5=1, \ a_1=1, \ a_2=1$.<br>
So the equation changes to $n-1=a_3^2-a_4^2$;<br>
and then 
let $a_3=\dfrac{n}{2}$ and 
let $a_4=\dfrac{n-2}{2}$. </p></li>
<li><p>If $n=2$ , then let 
$a_1=1, \ a_2=1, \ a_3=5, \ a_4=4, \ a_5=3$.  </p></li>
</ul>
"
"2398932","2398956","<p>Since the leading digit cannot be zero, we break the problem into two cases, depending on whether the leading digit is even or odd.</p>

<p>Leading digit is even:  Since the leading digit cannot be zero, we can fill the hundred thousands place in four ways, namely with $2$, $4$, $6$, or $8$.  We choose two of the remaining five positions for the even digits.  Each of those positions can be filled in five ways, namely with one of the five even digits.  Each of the three remaining positions can also be filled in five ways, namely with one of the five odd digits. </p>

<blockquote class=""spoiler"">
  <p> $$4\binom{5}{2}5^5$$</p>
</blockquote>

<p>Leading digit is odd:  We choose two of the remaining five positions for the odd digits.  There are five ways to fill each of the six positions.  </p>

<blockquote class=""spoiler"">
  <p> $$\binom{5}{2}5^6$$</p>
</blockquote>

<p>Total:  Since the two cases above are mutually exclusive and exhaustive, the total number of six-digit positive integers with three even and three odd digits is found by adding the above results.</p>
"
"2398937","2398977","<p>Since $5$ is a prime number, every $a\in \Bbb Z_5^*$ is a generator. Thus $\langle a\rangle=\langle 2a\rangle=\Bbb Z_5$ for every $a\neq0$. Then the quotient </p>

<p>$$\langle a \rangle / \langle 2a\rangle \cong \{0\}$$</p>
"
"2398948","2398954","<p>Hint: show that if $n$ is odd and $n \mid 2^n + 1$ then $3n \mid 2^{3n} + 1$.</p>
"
"2398961","2398966","<p>Because $37\cdot27=999$ we have $10^3\equiv1\pmod{37}$ and consequently $10^{3a}\equiv1\pmod{37}$ for all $a$. We also have $37\cdot3=111$, so $10^0+10^1+10^2\equiv0\pmod{37}$. Can you take it from here?</p>
"
"2398978","2399043","<p><em>(This might not be a fine solution but it gives some thoughts here.)</em></p>

<p>If such solutions exist, we know $a,b$ must share the same set of $n$ prime factors $p_1,p_2\ldots p_n$.</p>

<p>$$a=\prod_{k=1}^n p_k^{u_k};b=\prod_{k=1}^n p_k^{w_k}$$</p>

<p>We know if $a^x=b^y$, then for every $k$, $xu_k=yw_k$. This is only possible if 
$$u_k=\frac{c_kl}{x},w_k=\frac{c_kl}{y}$$</p>

<p>where $l=\text{lcm}(x,y)$ and $c_k$ is an arbitary integer.</p>
"
"2398988","2399297","<p>Claim (2) follows from first principles about continuity.</p>

<p>If the ""component functions"" $f:\&gt;X\to{\mathbb R}$ and $g:\&gt;X\to{\mathbb R}$ are continuous then the map
$$\psi:\quad X\to{\mathbb R}^2,\qquad x\mapsto\bigl(f(x),g(x)\bigr)$$
is continuous. Now the function $$P(f,g): \quad X\to{\mathbb R},\qquad x\mapsto P\bigl(f(x),g(x)\bigr)$$  is nothing else but  $P\circ\psi$, hence is continuous by claim (1) and the principle that the composition of continuous maps is again continuous.</p>
"
"2398993","2398995","<p>Hint. Note that
$$x^2-(2+a)x+2a=(x-2)(x-a).$$
Moreover, pay attention to the fact that $|x-a|/(x-a)=1$ if $x&gt;a$ and it is equal to $-1$ for $x&lt;a$.</p>
"
"2398997","2399051","<p>Since $P(A\cap B)=P(B\mid A)P(A)$, the equation $P(A\cap B)&gt;P(A)P(B)$ is equivalent to $$P(B\mid A)&gt;P(B)$$ (assume that $P(A)\neq 0$ to avoid trivial cases). This equation, says that: given that $A$ occured, $B$ has become more probable. For example, let $A$ be the event ""there are clouds in the sky"" and $B$ the event ""it rains"". Assume that you know from past data the probability $P(B)$ (perhaps you are a meteorologist). Now, knowing that $A$ applies (you open your window and you see many clouds outside), then certainly this increases the probability of $B$. </p>

<p>But it can also be the other way: let ""A"" be the event ""there are no clouds in the sky"" (you open your window and it is so sunny that you close it again). Then, certainly $P(B\mid A)&lt;P(B)$.</p>
"
"2399000","2399029","<p>Graphical method. Look at the graphs of the left and right hand side functions $y_1=|x-3a|,y_2=|x-a|$: </p>

<p><a href=""https://i.stack.imgur.com/TGpx8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TGpx8.png"" alt=""enter image description here""></a></p>

<p>We want to find the interval of $x$ for which $y_1&gt;y_2$, that is, the graph of $y_1$ is higher than the graph of $y_2$. It is:
$$(-\infty,2a)$$</p>

<p>Note: To find the intersection point $(2a,a)$:
$$y_1=y_2 \Rightarrow |x-3a|=|x-a| \Rightarrow -(x-3a)=x-a \Rightarrow x=2a.$$</p>
"
"2399005","2399007","<p>You are very near to the full solution. </p>

<p>After setting $j=k-1$, by the <a href=""https://en.wikipedia.org/wiki/Binomial_theorem#Theorem_statement"" rel=""nofollow noreferrer"">Binomial Theorem</a> we obtain
$$\sum _{k=1}^n\:k\left(n-1\right)^{k-1}\binom{n}{k}=
n\sum _{k=1}^n\left(n-1\right)^{k-1}\binom{n-1}{k-1}\\=n\sum _{j=0}^{n-1}\binom{n-1}{j}\left(n-1\right)^{j}1^{n-1-j}=n((n-1)+1)^{n-1}=n^n.$$</p>
"
"2399006","2399030","<p>You are right in thought, it's nice if you want to be rigorous. Any person with even basic exposure to real analysis will understand your unrigorous proof  immediately.</p>

<p>As for rigor, there isn't much to say. I'll write it in steps :</p>

<p>1) To  show that $(0,1]$ is not complete under the absolute value metric, it is enough to find a Cauchy sequence in $(0,1]$ which does not converge in $(0,1]$.</p>

<p>2) Define $a_k = \frac 1k$ for all $k \in \mathbb N$. Clearly, $0 &lt; a_k \leq 1$ for all $k$, hence $\{ a_k\} \subset (0,1]$.</p>

<p>3) $|a_k - a_l|= |\frac 1k - \frac 1l| = |\frac{l-k}{kl}|$. Given $\epsilon &gt; 0$, choose $M$ so large that $\frac 1M &lt; \frac \epsilon 2$. Then, 
$$
l,k &gt; M \implies \left|\frac 1l - \frac 1k\right| \leq \left|\frac 1l\right| + \left|\frac 1k\right| &lt; 2\frac \epsilon 2 &lt; \epsilon
$$<br>
So the  sequence $\{a_k\}$ is Cauchy.</p>

<p>4) Note that  $|a_k - 0| = |a_k| = \frac 1k$ can be made less than $\epsilon &gt; 0$ by simply taking $k &gt; \frac 1 \epsilon$. Hence, $a_k \to 0$. However, $0 \notin (0,1]$, hence $\{a_k\}$ is a Cauchy sequence which is not convergent in $(0,1]$.</p>

<p>A rigorous proof, if you like. You will not need this much detail  in future.</p>
"
"2399022","2399047","<p>Here is one simple way to show that $(9 - 5 \sqrt{3}) ( 2- \sqrt{2})$ is not a square in $\mathbb{Q}(\sqrt{2}, \sqrt{3})$. Assume that we have an equality 
$$(9-5 \sqrt{3})(2-\sqrt{2}) = \alpha ^2$$</p>

<p>Consider the automorphism of that takes $\sqrt{2}$ to $- \sqrt{2}$ and fixes $\sqrt{3}$. We get 
$$(9-5 \sqrt{3})(2 + \sqrt{2}) = \alpha_1^2$$ and therefore
$$2=(2- \sqrt{2})(2+\sqrt{2}) = \left(\frac{\alpha \cdot\alpha_1}{9-5 \sqrt{3}}\right)^2$$ and so $\sqrt{2}$ lies in $\mathbb{Q}(\sqrt{3})$, not possible. </p>
"
"2399038","2399063","<p>In your calculations,</p>

<ul>
<li>$5$ is the number of possible top-ranked women</li>
<li>$\binom{5}{k}$ is the number of ways $k$ of the five men can have a lower rank than the top-ranked woman</li>
<li>$(9 - k)!$ is the number of ways of arranging the $9 - k$ people whose rankings are lower than that of the top-ranked woman</li>
<li>$10!$ is the number of possible sequences of rankings</li>
</ul>

<p>In your numerators, you failed to multiply by the number of ways the men who are selected before the first woman can be ranked.  Observe that 
\begin{align*}
P(X = 1) &amp; = \frac{0! \cdot 5 \cdot \binom{5}{5} \cdot 9!}{10!} = \frac{1}{2}\\
P(X = 2) &amp; = \frac{1! \cdot 5 \cdot \binom{5}{4} \cdot 8!}{10!} = \frac{5}{18}\\
P(X = 3) &amp; = \frac{2! \cdot 5 \cdot \binom{5}{3} \cdot 7!}{10!} = \frac{5}{36}\\
P(X = 4) &amp; = \frac{3! \cdot 5 \cdot \binom{5}{2} \cdot 6!}{10!} = \frac{5}{84}\\
P(X = 5) &amp; = \frac{4! \cdot 5 \cdot \binom{5}{1} \cdot 5!}{10!} = \frac{5}{252}\\
P(X = 1) &amp; = \frac{5! \cdot 5 \cdot \binom{5}{0} \cdot 4!}{10!} = \frac{1}{252}
\end{align*}
The reason you obtained the correct answer for $X = 1$ and $X = 2$ is that $0! = 1! = 1$.  </p>

<p><strong>Note:</strong>  The author(s) of your text are calculating the probability that the top-ranked woman is in the $k$th position.  For that to occur, we must select $k - 1$ of the five men to be ranked ahead of her and one of the five women to fill the $k$th position while choosing $k$ of the $10$ people.  Hence, the answer in the text is equivalent to
\begin{align*}
P(X = 1) &amp; = \frac{\binom{5}{0}\binom{5}{1}}{\binom{10}{1}} = \frac{1}{2}\\
P(X = 2) &amp; = \frac{\binom{5}{1}\binom{5}{1}}{\binom{10}{2}} = \frac{5}{18}\\
P(X = 3) &amp; = \frac{\binom{5}{2}\binom{5}{1}}{\binom{10}{3}} = \frac{5}{36}\\
P(X = 4) &amp; = \frac{\binom{5}{3}\binom{5}{1}}{\binom{10}{4}} = \frac{5}{84}\\
P(X = 5) &amp; = \frac{\binom{5}{4}\binom{5}{1}}{\binom{10}{5}} = \frac{5}{252}\\
P(X = 6) &amp; = \frac{\binom{5}{5}\binom{5}{1}}{\binom{10}{6}} = \frac{1}{252}
\end{align*}  </p>
"
"2399050","2399059","<p>Once you know that $0&lt;1/e-s_{2k-1}&lt;\frac{1}{(2k)!}$,</p>

<p>suppose that $e=\frac{p}{q}$ for integers $p,q$ and $p\neq 0$ ($e\neq 0$ obviously). From above</p>

<p>$$
0&lt;\frac{(2k)!}{e}-m_k&lt;1
$$</p>

<p>where $m_k=(2k)!s_{2k-1}$ is an <strong>integer</strong>. Since this is true for all $k$, we can pick some $k$ such that $2k\ge |p|$ so that $\frac{(2k)!}{e}=\frac{q(2k)!}{p}$ is also an integer. </p>

<p>But this is a contradiction, since then $\frac{(2k)!}{e}-m_k$ is an integer between $0$ and $1$.</p>
"
"2399053","2399074","<p>We have the following definition for $f:\mathbb{R}\mapsto \mathbb{R}$ and $l\in\mathbb{R}$:</p>

<p>$$\lim_{x\to -\infty}f(x)=l \Leftrightarrow \forall \epsilon &gt;0, \exists x_{\epsilon}\in\mathbb{R} : x\le x_\epsilon \Rightarrow|f(x)-l|&lt;\epsilon$$</p>

<p>This means that whatever strictly positive number $\epsilon$ I choose, there will be a real number $x_\epsilon$ so that for every $x\le x_\epsilon$ we are guaranteed that $f(x)\in [l-\epsilon, l+\epsilon]$</p>

<p>Intuitively, this is interesting because in particular you can chose $\epsilon$ to be as little as you want.</p>
"
"2399057","2399067","<p>Yes it is closed because $A=Y \cap A$ where $A$ is closed in $X$.</p>
"
"2399060","2399075","<p>It should be
$$\iint_B 1 \ dxdy=\int_{-6}^2\left( \int_{\frac{x^2}{4}-1}^{2-x}1 \ dy\right)dx =\int_{-6}^2 \left(3-x-\frac{x^2}{4} \right)dx\\=\left[3x-\frac{x^2}{2}-\frac{x^3}{12}\right]_{-6}^2=\frac{64}{3}.$$
P.S. Note that
\begin{align*}
&amp;2-x\geq \frac{x^2}{4}-1\Leftrightarrow 2-x-\frac{x^2}{4}+1\geq 0\Leftrightarrow(â1/4)(x^2+4xâ12)\geq 0\\
&amp;\Leftrightarrow(x^2+4xâ12)\leq 0\Leftrightarrow(x+6)(x-2)\leq 0
\Leftrightarrow x\in[-6,2].
\end{align*}</p>
"
"2399064","2399123","<p>You have to be aware that $\lim_{h\to 0}\frac{f(x_0-h)-f(x_0)}{h} \neq f'(x_0)$ but $-f'(x_0)$. In the following all steps are correctly presented:</p>

<p>\begin{align*}
\lim_{h\to 0}\frac{f(x_0+h)-f(x_0-h)}{2h}
&amp;=\frac{1}{2}\lim_{h\to 0}\frac{f(x_0+h)-f(x_0)-f(x_0-h)+f(x_0)}{h} \\
&amp;=\frac{1}{2}\lim_{h\to 0}\frac{f(x_0+h)-f(x_0)-\big(f(x_0-h)-f(x_0)\big)}{h} \\
&amp;=\frac{1}{2}\Big(\lim_{h\to 0}\frac{f(x_0+h)-f(x_0)}{h}-\lim_{h\to 0}\frac{f(x_0-h)-f(x_0)}{h}\Big) \\
&amp;=\frac{1}{2}\Big(\lim_{h\to 0}\frac{f(x_0+h)-f(x_0)}{h}+\lim_{h\to 0}\frac{f(x_0-h)-f(x_0)}{-h}\Big) \\
&amp;=\frac{1}{2}\big(f'(x_0)+f'(x_0)\big)=f'(x_0)
\end{align*}</p>
"
"2399069","2399129","<p>If You understand $f$ to be a function on $\mathbb{R}$ it is not invertible but the restrictions $f|_{\mathbb{R}^{\leq 2}}$ and $f|_{\mathbb{R}^{\geq 2}}$ are invertible with inverses $g_1:\mathbb{R}^{\geq-4}\rightarrow \mathbb{R}^{\leq 2},x\mapsto 2-\sqrt{x+4}$ and $g_2:\mathbb{R}^{\geq-4}\rightarrow \mathbb{R}^{\geq 2},x\mapsto 2+\sqrt{x+4} $  respectively.</p>
"
"2399076","2399081","<p>Your proof has the right ingredients, but it can be made much simpler: $f(x)=\arctan x$ is monotonically increasing, hence it is of bounded variation. In fact the total variation of $f$ over $[0,1]$ is
$$\arctan(1)-\arctan(0)=\frac{\pi}{4}. $$</p>
"
"2399079","2399161","<p>first we see that g is continuous at $(x,y)$ such that $x\neq y$ now let $x_{0} = y_{0}$ and let $ (x,y)$ converge to $(x_{0},y_{0})$ 
we have $\frac{f(y)-f(x)}{y-x}= {f}'(c) $ such that $ x\leq {c} \leq y $ . and the conclusion follow since $ f$ is $C^{1}$ continuous </p>
"
"2399096","2399106","<p>I suppose that $a_n,b_n$ are arithmetic progressions.</p>

<p>Write $a_n = 1 + (n-1)d_1$, and $b_n = 1 +  (n-1)d_2$, where $d_1,d_2$ are the common differences of the arithmetic sequences $a_n,b_n$ respectively.</p>

<p>Now, note that $(1 + (n-1)d_1)(1 + (n-1)d_2) = 2010$. We want the largest value of $n$ so that there exist $d_1,d_2 \geq  1$ with the above happening. First of all, note that since the terms  are integers,  it follows that we must take all possible factorizations of $2010$, at the least.</p>

<p>The factors, in this order, are $1,2,3,5,6,10,15,30,67,134,201,335,402,670,1005,2010$.</p>

<p>Suppose $1  + (n-1)d_1 = a$ and $1 + (n-1)d_2 = b$. Then, $(n-1)d_1 = a-1$ and $(n-1)d_2 = b-1$. That is, $n-1$ divides the $\gcd$ of $a-1$ and $b-1$. We must therefore choose a pair  $a,b$ such  that the $\gcd$ of $a-1,b-1$ is as large as possible (of course, $a\neq 1, b \neq 1$). Analysing:</p>

<p>$
a=2,b=1005 \implies \gcd = 1
$</p>

<p>$
a=3,b=670 \implies \gcd = 1
$</p>

<p>$
a=5,b=402 \implies \gcd = 1
$</p>

<p>$
a=6,b=335 \implies \gcd = 1
$</p>

<p>$
a=10,b=201 \implies \gcd = 1
$</p>

<p>$
a=15,b=134 \implies \gcd = 7
$</p>

<p>$
a=30,b=67 \implies \gcd = 1
$</p>

<p>Hence, we  conclude that  the maximum value of $n-1$ is $7$, so that $n=8$. Indeed, if $d_1 = 2$ and $d_2 = 19$, we see that:
$$
a_n = 1,3,5,7,9,11,13,\color{blue}{15} \\
b_n = 1,20,39,58,77,96,115,\color{blue}{134}
$$
are the eighth terms of the progressions $a_n,b_n$, whose product is $2010$. Hence, the answer is $8$.</p>
"
"2399108","2399115","<p>By C-S
$$(x^2+y^2+z^2)(a^2+b^2+c^2)\geq(ax+by+cz)^2=4S^2.$$
Thus, $$x^2+y^2+z^2\geq\frac{2(a^2b^2+a^2c^2+b^2c^2)-a^4-b^4-c^4}{4(a^2+b^2+c^2)}.$$
The equality occurs for $(x,y,z)||(a,b,c)$, which says that we got a minimal value.</p>

<p>We'll prove that there exist $P$ inside the triangle for which $x:y:z=a:b:c$.</p>

<p>Let $L\in AB$ such that $\frac{S_{\Delta LBC}}{S_{\Delta LAC}}=\frac{a^2}{b^2}$ and   $M\in AC$ such that $\frac{S_{\Delta MBC}}{S_{\Delta MAC}}=\frac{a^2}{c^2}$.</p>

<p>Now, let $BM\cap CL=\{K\}$ and easy to see that for this $K$ we get an equality case.  </p>
"
"2399109","2399119","<p>Let $A,B$ finite sets, the number of function $f:A \rightarrow B$ is $|B|^{|A|}$ (<a href=""https://math.stackexchange.com/questions/223240/how-many-distinct-functions-can-be-defined-from-set-a-to-b"">why?</a>). </p>

<p>In this case the number of considered function is $(2^m)^{2^n}$</p>

<hr>
"
"2399113","2399116","<p>Interpret a function $f: \mathbb R \to \{ 0, 1 \}$ as picking a subset of $\mathbb R$; if $f(x) = 0$, then $x$ is not an element of your subset, and if $f(x) = 1$, it is. The cardinality therefore is that of the powerset of $\mathbb R$ (which is by definition the set of all subsets), i.e. $|2^{\mathbb R}| = |\mathscr{P}(\mathbb R)| = 2^{\beth_1} = \beth_2$ (see <a href=""https://en.wikipedia.org/wiki/Beth_number"" rel=""nofollow noreferrer"">beth number</a>).</p>
"
"2399130","2399134","<p>You're off to a good start.</p>

<p>Hint 1: Let $n^3$ be a positive integers cube. Suppose $n$ has the prime factorization $n = p_1^{a_1} p_2^{a_2} \cdots p_n^{a_n}$. What is the prime factorization of $n^3$? In general, what can you say about the exponents of the prime factors of a perfect cube?</p>

<p>Hint 2: Let's simplify the problem a bit and count the number of positive integer cubes that divide $3^9 5^4$. If $n^3$ divides $3^9 5^4$, then it can only have $3$ and $5$ as prime factors. By Hint 1, the possible exponents for $3$ are $0$, $3$, $6$ and $9$. Similarly, the possible exponents for $5$ are $0$ and $3$. So there are $4 \cdot 2 = 8$ total positive integer cubes that divide $3^9 \cdot 5^4$. Can you generalize this to your number?</p>
"
"2399158","2399190","<p>In the double sum, if you take all the terms corresponding to $(x-\alpha)^ {-j}$ and $(x-\overline{\alpha})^{-j}$, then this partial sum  can be written
$$S = \frac{A(x)}{(x-\alpha)^n(x-\overline{\alpha})^n}$$
where $A$ is a polynomial of degree $&lt;2n$. You can divide $A(x)$ by $(x-\alpha)(x-\overline{\alpha})$,
$$A(x) = A_1(x)(x-\alpha)(x-\overline{\alpha}) + b_n x + c_n$$
so that
$$S = \frac{A_1(x)}{(x-\alpha)^{n-1}(x-\overline{\alpha})^{n-1}} + \frac{b_n x + c_n}{(x-\alpha)^n(x-\overline{\alpha})^n}$$
and $A_1$ has a degree $&lt; 2(n-1)$. By induction, you get the result.</p>

<p>To prove that $A(x)$ has real coefficients, note that when $f$ and $g$ have real coefficients, then for all $x$ such that $g(x)\not = 0$
$$\overline{\left(\frac{f(\overline{x})}{g(\overline{x})}\right)}
= \frac{f(x)}{g(x)}$$
Applying this to the complex decomposition yields
$$\frac{f(x)}{g(x)} = \sum\sum \frac{\overline{a_{i j}}}{x-\overline{\alpha_i}} + \overline{p(\overline{x})}$$
The uniqueness of the decomposition shows that $p$ has real coefficients and also that the quantity $S = S(x)$ above satisfies $\overline{S(\overline{x})} = S(x)$. It follows that
$$A(x) = S(x)(x-\alpha)^n(x-\overline{\alpha})^n$$
satisfies $\overline{A(\overline{x})} = A(x)$. Hence it is a polynomial with real coefficients.</p>
"
"2399169","2399213","<p>You can apply the dominated convergence theorem to the series of derivatives.</p>

<p>By the premises, $g(x) = \sum_{k \in \mathbb{Z}} \lvert f_k'(x)\rvert$ is a bounded function. As the sum of a series of nonnegative continuous functions, $g$ is lower semicontinuous. Hence $g$ is locally integrable, and dominates $\sum_{k\in \mathbb{Z}} f_k'(x)$. By the dominated convergence theorem,</p>

<p>$$\int_x^y \sum_{k\in \mathbb{Z}} f_k'(t)\,dt = \sum_{k\in \mathbb{Z}} \int_x^y f_k'(t)\,dt = \sum_{k \in \mathbb{Z}} \bigl(f_k(y) - f_k(x)\bigr) = \Biggl(\sum_{k\in\mathbb{Z}} f_k(y)\Biggr) - \Biggl(\sum_{k\in\mathbb{Z}} f_k(x)\Biggr).$$</p>

<p>By the fundamental theorem of calculus, $\sum f_k$ is continuously differentiable with $\bigl(\sum f_k\bigr)' = \sum f_k'$.</p>
"
"2399171","2399181","<p>In that setting, the number $z_{3}$ may be called the (complete) quotient, where ""complete"" may be used to contrast the case where the remainder is $\neq 0$, in which case ""incomplete quotient"" may be used instead.</p>

<p>So ""dividend = divisor $\times$ quotient + remainder"".</p>

<p>For those who are not comfortable with ""complete quotient"", please refer to the appendix made by Bill down there :).</p>
"
"2399174","2399199","<p>For every $m \in \mathbb{N}\setminus \{0\}$, we have $\sin \sqrt{k} \geqslant \frac{1}{2}$ when</p>

<p>$$\bigl(2m+\tfrac{1}{6}\bigr)\pi \leqslant \sqrt{k} \leqslant \bigl(2m+\tfrac{5}{6}\bigr)\pi.$$</p>

<p>Therefore</p>

<p>\begin{align}
\sum_{(2m+1/6)^2\pi^2}^{(2m+5/6)^2\pi^2} \frac{\sin \sqrt{k}}{\sqrt{k}} &amp;\geqslant \frac{1}{2} \sum_{(2m+1/6)^2\pi^2}^{(2m+5/6)^2\pi^2} \frac{1}{\sqrt{k}} \\
&amp;&gt; \sum_{(2m+1/6)^2\pi^2}^{(2m+5/6)^2\pi^2}\bigl(\sqrt{k+1} - \sqrt{k}\bigr) \\
&amp;\approx \frac{2}{3}\pi.
\end{align}</p>

<p>The sequence of partial sums is thus not a Cauchy sequence.</p>
"
"2399191","2399197","<p>Not really.  It could be that every farm has exactly the same value, which would be your average.  As you say, you could have $10\%$ of the farms exactly at the threshold and the rest worth nothing.  We all know it is a broad distribution, but we can't say there are any farms over $11M$.</p>
"
"2399195","2399257","<p>$\newcommand{\dom}{\operatorname{dom}}
\newcommand{\graph}{\operatorname{graph}}$
Suppose $A$ is closable. This mean that $A$ has a closed extension $B:\dom{B}\to Y$. Thus
$$\graph(A)\subseteq \overline{\graph(A)}\subseteq\graph(B).$$
Consider the projection $\pi_X':\graph(B)\to X$. This projection is injective because $B$ a well-defined function. In other words $$\pi_X'(x,Bx)=\pi_X'(x',Bx') \implies
x=x'
\implies (x,Bx)=(x',Bx').$$
Therefore $\pi_X$ is injective as well. Indeed, the fact that that $\overline{\graph(A)}\subseteq \graph(B)$ implies that
$$
\pi_X(x,y)=\pi_X(x',y') \implies \pi_X'(x,y)=\pi_X'(x',y') \implies (x,y)=(x',y').$$</p>
"
"2399196","2399218","<p>We have
$
u(x,t)=f(z) 
$  with $z=x-tu$. Using the chain rule and supposing that $x, t$ are independent variables ( so that $\frac{\partial t}{\partial x}=0$), we have:</p>

<p>$$
\frac{\partial u}{\partial x}=
f'(z)\frac{\partial z}{\partial x}=f'(z)\frac{\partial (x-tu)}{\partial x}=
$$
$$
=f'(z)\left(1-\frac{\partial t}{\partial x}u-t\frac{\partial u}{\partial x}\right)=f'(z)\left(1-t\frac{\partial u}{\partial x}\right)
$$
that gives
$$
\frac{\partial u}{\partial x}\left[1+tf'(z) \right]=f'(z)
$$</p>

<p>so we have:
$$
\frac{\partial u}{\partial x}=\frac{f'(z)}{\left[1+tf'(z) \right]}=\frac{f'(x-tu)}{\left[1+tf'(x-tu) \right]}
$$</p>

<p>You can do the same for the derivative with respect to $t$</p>
"
"2399203","2399207","<p>According to the well stated comment of @Nilknarf, if $f^2=(f)^2$ then for $I=[0, + \infty)$ and $f(x)=x$ we have the counterxample.</p>

<p>For a proof of this counterexample  we can use the sequential property of uniform continuity by taking the sequences: $$x_n=n + \frac{1}{n}$$ $$y_n=n$$</p>

<p>If the O.P wants the case where $f^2=fof$ then we know that the composition of uniformly continuous functions is uniformly continuous.</p>
"
"2399210","2399286","<p>You have calculation errors:</p>

<p>$$E(\theta^{X_n})=E(\theta^{\sum_{i=1}^{n}X_i^{(n)}})=\Big( E(\theta^{X^{(n)}_1}) \Big)^n$$</p>

<p>$E(\theta^{X^{(n)}_1})=\theta\cdot\dfrac{\lambda}{n}+(1-\dfrac{\lambda}{n})=1-\color{red}{(1-\theta)}\dfrac{\lambda}{n}$</p>

<p>Then $$E(\theta^{X_n})=\Big(1-(1-\theta)\dfrac{\lambda}{n}\Big)^n\rightarrow e^{-(1-\theta)\lambda}\space\text{as $n\to\infty$}$$</p>

<p>$e^{-(1-\theta)\lambda}=e^{-\lambda}\cdot e^{\theta\lambda}=e^{-\lambda}\Big(\sum_{k=0}^{\infty}\dfrac{\theta^k\lambda^k}{k!}\Big)$</p>

<p>Hence $kâ¥0,P(X_n=k)â\frac{\lambda^k}{k!}e^{-\lambda}$, as $n\to\infty$</p>

<p>You can do the same thing using MGF, calculation will be almost same.</p>
"
"2399212","2399258","<p>I assume you are looking for the probability of drawing the green marble exactly on the $Z$-th draw, right?</p>

<p>Well, if you think about this just intuitively, the chance of drawing the green ball on the 7th draw should be the same as drawing it on the 13th draw: the problem is equivalent to randomly distributing the marbles to $n$ different people; clearly everyone has a $\frac{1}{n}$ chance of getting the green  marble.</p>

<p>Indeed, note that the sum of all probabilities should add up to $1$, which is <em>not</em> the case with your calculations. In fact, for the last draw you have $Z=n$, and so by your formula you would have a probability of $\frac{1}{n-Z+1}=\frac{1}{n-n+1}=\frac{1}{1}=1$ of drawing the green marble on the last draw.... clearly not right! But if the chance is $\frac{1}{n}$, then clearly the probabilities do add up to $1$.</p>

<p>Ok, but why doesn't the order matter? If I hand out the marbles one by one, why wouldn't the 7th person not have some kind of advantage or disadvantage compared to the 13th person? How does the math work out so that everyone has exactly $\frac{1}{n}$ probability of getting the green marble?</p>

<p>Well, like you said, to get the marble on the first draw is $\frac{1}{n}$. But to get it on the second draw is to not get it on the first draw and then get it on the next draw. So that would $\frac{n-1}{n}\cdot \frac{1}{n-1}=\frac{1}{n}$.  Now you try to get the right formula for drawing the marble on the third draw ... and you'll see the general pattern.</p>

<p>BY the way, if you meant to find the probabilities of drawing the green marble in any of the first $Z$ draws, then that probability will be $\frac{Z}{n}$. Again, intuitively go back to distributing the marbles among $n$ people: what is the chance that among any $Z$ of them, one has the green marble? GIven that they each have a chance of $\frac{1}{n}$, and given that these are mutually exclusive, you can just add them all up, so you get $\frac{Z}{n}$.</p>

<p>OR, doing the 'one by one' math: the probability to get the green marble on either of the first two draws is 1 minus the probability of not getting it on either draw, so that would be $1-\frac{n-1}{n}\cdot \frac{n-2}{n-1}=1-\frac{n-2}{n}=\frac{2}{n}$.  Again, I'll leave it up to you to find the general expression for any $Z$, but note that your formula in your Post does also not match up with these probabiliies.</p>
"
"2399227","2399290","<p>Why not rearrange to give $$ab=x^3+a(a+1)x^2+ax-a^2-1$$</p>

<p>Divide through by $a$ to obtain: $$b=a(x^2-1)+\frac {x^3-1}a + x^2+x$$</p>

<p>Now you can set $x$ as a root of the equation - when does this lead to an expression independent of $a$?</p>
"
"2399238","2399296","<p>Are you sure this is true? If $f(x)=\sin x$ in $(0,\pi)$ and $g(x)=\frac1{x^6}$ then I don't think the product is in $H^1$. Note that $g$ is $C^1$ and so in $W^{1,\infty}_{loc}$.</p>
"
"2399250","2399261","<p>The eigenvector with eigenvalue $0$ is $(1,-3)$, so this eigenspace corresponds to the line $y=-3x$. Since the eigenvalue is $0$, $A$ maps this line to the point 
$(0,0)$. $\{(0,0)\}$ is indeed a subset of $y=-3x$, so it is invariant under $A$. The key is that just because the line $y=-3x$ is invariant under $A$ doesn't mean its image is the whole line; it only has to be a subset.</p>
"
"2399251","2399254","<p>hint: we get $$\frac{-(x^2+y^2)+y\cdot 2y}{(x^2+y^2)^2}$$ by the Quotient rule.</p>
"
"2399256","2399273","<p>\begin{align}
\int_0^{\infty}e^{ax}\mathrm{d}x=\lim_{n\to \infty} \int_0^n e^{ax}\mathrm{d}x=\frac{1}{a}\lim_{n\to \infty}[e^{ax}+C]_0^n=\lim_{n\to \infty}\frac{e^{na}-1}{a}
\end{align}
which is $\infty$ is $a\ge 0$, and $-1/a$ if $a&lt;0$.</p>
"
"2399260","2399310","<p><img src=""https://i.stack.imgur.com/6oGZw.jpg"" alt=""""></p>

<p>Looking at the diagram above, note that $\triangle ZPR$ and $\triangle YPO$ are similar since they are isosceles (each with two radii for shorter sides) and share a common angle. From this and $YZ=2ZP$ we have $OR=2RP=2RS$, so $\angle ROS=30^\circ$ ($\triangle ORS$ being half an equilateral triangle). $\angle ROS=\angle POX=2\angle PYX$ by the central/inscribed angle theorem, so $\angle PYX=15^\circ$.</p>

<p>Nothing beyond basic geometry was needed here.</p>
"
"2399263","2399272","<p>$${ 2 }^{ i }={ e }^{ i\ln { 2 }  }=\cos { \left( \ln { 2 }  \right) +i\sin { \left( \ln { 2 }  \right)  }  } $$</p>
"
"2399268","2399280","<p>Your method is correct, but you made a mistake when finding the inverse of $g$. To see why, note that $g(0)=2$ but
$$
-\frac{(2)+2}{(2)-1} = -4 \ne 0.
$$
It should be
$$
g^{-1}(x) = \frac{2-x}{x-1}.
$$
Let's verify that this is correct. Note that
$$
(g\circ g^{-1})(x) = 1+\frac{1}{\frac{2-x}{x-1}+1}
= 1+\frac{1}{\frac{1}{x-1}} = 1+(x-1)=x
$$
and
$$
(g^{-1}\circ g(x) = \frac{2-(1+\frac{1}{x+1})}{(1+\frac{1}{x+1})-1}
= \frac{1-\frac{1}{x+1}}{\frac{1}{x+1}}
= \frac{1}{\frac{1}{x+1}} -1=(x+1)-1=x.
$$</p>

<hr>

<p>Continuing with the solution, we need to solve $g(x)=g^{-1}(x)$. That is:
\begin{align*}
1+\frac{1}{x+1} &amp;= \frac{2-x}{x-1} \\
(x+1)(x-1) + (x-1) &amp;= (x+1)(2-x) \\
(x-1)(x+2) &amp;= (x+1)(2-x) \\
x^2+x-2 &amp;= -x^2+x+2 \\
2x^2-4&amp;=0 \\
x^2 &amp;= 2 \\
x &amp;= \pm\sqrt{2}
\end{align*}</p>

<p>Since the domain excludes $x=-\sqrt{2}$, we are left with only $\sqrt{2}$.</p>
"
"2399289","2399300","<p>Each coset $xB$ of $B$ in $G$ is the disjoint union of $|B:A|$ cosets
of $A$ in $B$. In detail, is $B=\bigcup_{i\in I} y_i A$ is a disjoint
coset decomposition, so is $xB=\bigcup_{i\in I} xy_i A$. Therefore
$G$ is the disjoint union $\bigcup_{j\in J} x_j B$
of $|G:B|$ $B$-cosets, and so a disjoint union 
$\bigcup_{j\in J}\bigcup_{i\in I} x_j y_i A$
of $|G:B||B:A|$
$A$-cosets.</p>
"
"2399294","2399316","<p>This is the definition of the Dirac delta : $$\int_{-\infty}^\infty g(x) \delta(x)dx = \lim_{\epsilon \to 0^+} \int_{-\infty}^\infty g(x) \frac{1_{|x| &lt; \epsilon}}{2 \epsilon}dx=g(0)$$ whenever $g$ is continuous, which means $\delta(x)$ is the limit <strong>in the sense of distributions</strong> of $\frac{1_{|x| &lt; \epsilon}}{2 \epsilon}$ as $\epsilon \to 0^+$. </p>

<p>Thus it is natural to define $\delta(f(x))$ as the limit <strong>in the sense of distributions</strong> of $\frac{1_{|f(x)| &lt; \epsilon}}{2 \epsilon}$ as $\epsilon \to 0^+$, which means $$\int_{-\infty}^{\infty} g(x)\delta \left( f(x) \right)dx =\lim_{\epsilon \to 0^+}\int_{-\infty}^{\infty} g(x)\frac{1_{|f(x)| &lt; \epsilon}}{2 \epsilon}dx= \sum_{f(\alpha)=0} \dfrac{g(\alpha)}{|f'(\alpha)|}\\=\int_{-\infty}^\infty g(x)\sum_{f(\alpha)=0} \dfrac{\delta(x-\alpha)}{|f'(\alpha)|} dx$$
whenever $g$ is continuous and $f$ is $C^1$ and has finitely many zeros.</p>
"
"2399299","2399305","<p>We need to prove that
$$-AB&lt;AD-BC&lt;AB.$$
Indeed, 
$$AD+AB&gt;BD&gt;BC$$ and we got a left inequality and
$$AB+BC&gt;AC&gt;AD$$ and we got a right inequality.</p>

<p>Done!</p>
"
"2399309","2399403","<p>The definition doesn't make much sense. What the author could have meant is a group $(G,+)$ where $G = \{0^\circ, 120^\circ, 240^\circ\}$ and the operation $+$ takes two elements of $G$ and adds them together so that, for example, $0^\circ + 120^\circ = 120^\circ$ and $240^\circ + 240^\circ = 120^\circ$ (since the angle $480^\circ$ is the same as $120^\circ$).</p>

<p>The identity element of the group would then be $0^\circ$ and the order of $0^\circ$ would thus be one. That of $120^\circ$ would be three since $$3\cdot 120^\circ=360^\circ\equiv 0^\circ,$$ and that of $240^\circ$ would also be three as $$3\cdot240^\circ=720^\circ\equiv 0^\circ.$$</p>
"
"2399335","2399340","<p>HINT: $$1+2+3+4+...+97+98+99+100=(1+100)+(2+99)+(3+98)+...$$
Can you solve this? the reult is $5050$ and now you can try it with variables!</p>
"
"2399346","2399369","<p>The $9$-cycles you are after are of the form $\sigma=(a_1\&gt;b_1\&gt;c_1\&gt;a_2\&gt;b_2\&gt;c_2\&gt;a_3\&gt;b_3\&gt;c_3)$ whereby the three $a_i$s are placeholders for the three digits in one of the cycles of $\sigma^3$, and similarly for the $b_i$s and the $c_i$s. WLOG you can assume $a_1=1$; then necessarily $a_2=5$, $a_3=7$. For $b_1$ you have $6$ choices, namely one of $\{2,3,4,6,8,9\}$. After you have made a choice $b_2$ and $b_3$ are determined. For $c_1$ there are $3$ choices left, and then $c_2$ and $c_3$ are determined. It follows that there are $6\cdot3=18$ permutations $\sigma$ with $\sigma^3=(157)(283)(469)$.</p>
"
"2399348","2399349","<p>Your response to (a) is correct. For (b), it is just asking if there exists some positive integer such that $P(x)$ is false. This is a true statement. Indeed, consider $x=3$.</p>
"
"2399367","2399399","<p>+1 good question. This is a point of vagueness in a lot of heuristic descriptions of groups and symmetries. I think one confusing point is when you talk of rotation - do you mean it dynammicaly (here I'm rotating the square around, taking some time up to do so), or all at once (via the function that does the rotation).</p>

<p>The difference between the two is the difference between, one the one hand, a path in $SO_2(\mathbb{R})$ starting at the identity matrix and ending at a rotation matrix $M$, and, on the other hand, that rotation matrix $M$.</p>

<p>It's true that you can find paths of transformations in $SO(2)$ that bring the letter $R$ back to itself. However, these paths start and stop at the same point, namely the identity matrix.</p>

<p>When we talk about symmetries of subset $S \subset \mathbb{R}^2$, we generally interpret it the second way - as the subgroup $GL_2(\mathbb{R})$ (or the affine group if you are allowed translations) consisting of the  matrices that fix the set $S$ and preserve all distances. (The choice between linear and affine symmetries also reminds me to tell you that you generally need to specify the symmetries of your object as belonging to some subgroup of the group of bijections, depending on the properties you want those symmetries to preserve. When people talk about symmetries in Euclidean space, they typically have in mind isometries.)</p>

<p>(Note that here there are two possible choices - do we want matrices $M$ so that as sets $MS = S$ or do we want to require that for all $s \in S$, $Ms = s$. When we talk about the symmetries of physical shape in Euclidean space we generally mean the former. The latter is <em>much</em> more restrictive. However, there are situations when we mean the latter (pointwise) option. For example, automorphisms of a field extension of $E$ over $F$ are taken to mean (among other requirements) that they fix $F$ pointwise.)</p>

<p>On the other hand, $SO(2)$ is a circle, and the path you describe (besides being set theoretically non-trivial), is also homotpically non-trivial, and indeed generates the fundamental group of $SO(2)$, which is $\mathbb{Z}$.</p>
"
"2399373","2399382","<p>$$\frac{2x-1}{x+2}=\frac{2x+4-5}{x+2}=2-\frac{5}{x+2}.$$
Thus, since $-1\leq x\leq2$, we obtain:
$$2-\frac{5}{-1+2}\leq2-\frac{5}{x+2}\leq2-\frac{5}{2+2}$$ or
$$-3\leq\frac{2x-1}{x+2}\leq\frac{3}{4}$$</p>
"
"2399386","2399393","<p>Given the way the sets are chosen (uniformly are random among sets of a specific size, instead of choosing every element to be in the set independently), this does not follow <em>directly</em> from a Chernoff bound, but due to the properties of sampling without replacement <em>(specifically, negative correlations, which make things even ""better"" in terms of concentration)</em> the bound will be the same as in that of sampling <em>with</em> replacement, where we can use Chernoff bounds.</p>

<p>Suppose each element is put independently at random in $A$ (similarly for $B$) with probability $p\stackrel{\rm def}{=}\frac{\log k}{k}$, so that $$\mathbb{E}[\lvert A\rvert] = \mathbb{E}[\lvert B\rvert] = pk^2 = k\log k$$
and the size are actually tighly concentrated around this expected value.</p>

<p>Let, for element $i\in[k^2]$, $X_i$ denote the indicator random variable equal to $1$ iff $i\in A\cap B$. Then, by independencce of the choices of $A$ and $B$, we have
$$
\mathbb{E}[X_i] = \Pr[i\in A\text{ and } i\in B] = p^2
$$
and we can apply a Chernoff bound to the random variable $\lvert A\cap B\rvert=\sum_{i=1}^{k^2} X_i$, which is a sum of independent random variables in $\{0,1\}$. Note that $\mathbb{E}[\lvert A\cap B\rvert] = k^2p^2 = \log^2 k = 2l(k)$.</p>

<p>$$
\Pr[\lvert A\cap B\rvert &lt; l(k)]
= \Pr[\lvert A\cap B\rvert &lt; \frac{1}{2}\mathbb{E}[\lvert A\cap B\rvert]] \leq e^{-\frac{1}{4}\mathbb{E}[\lvert A\cap B\rvert]/2}
=e^{-\frac{1}{4}l(k)}  = 2^{-\Omega(l(k))}
$$</p>
"
"2399392","2399422","<blockquote>
  <p>The patient gets a second, <em>independent</em>, test done ... </p>
</blockquote>

<p>The word ""independent"" as used in the question does not refer to <a href=""https://en.wikipedia.org/wiki/Independence_(probability_theory)"" rel=""nofollow noreferrer"">statistical independence</a>. It refers to the second test being done without any direct causal influence from the first test. <strong>The lack of a causal relationship between the two tests does not imply the lack of a correlation between them</strong> (in fact, the results of the two tests are quite positively correlated with each other). </p>

<p>Incidentally, the $^{\text{(fallacious)}}$ idea that <a href=""https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation"" rel=""nofollow noreferrer"">correlation implies causation</a> is equivalent (by <a href=""https://en.wikipedia.org/wiki/Contraposition"" rel=""nofollow noreferrer"">contrapositive</a>) to the $^{\text{(also fallacious)}}$idea that a lack of causation implies a lack of correlation. </p>

<p>Perhaps the folks over at the <a href=""https://english.stackexchange.com/"">English Language &amp; Usage Stack Exchange</a> can clarify the non-mathematical use of the word ""independent"" better than I can. </p>

<hr>

<p>We know that for any two events, regardless of independence, the following is true:</p>

<p>$$P(T_1 \cap T_2) = P(T_1) P(T_2 \,|\, T_1)$$</p>

<p>Suppose, for contradiction, that $T_1$ and $T_2$ are statistically independent. By definition,  $$P(T_1 \cap T_2) = P(T_1)P(T_2) $$</p>

<p>In order for the first and second equations above to both be satisfied, it must be the case that $$P(T_2\,|\,T_1)=P(T_2)$$</p>

<p>This is a contradiction. If the first test is positive, the second test clearly must be more likely to also be positive. Therefore, $$P(T_2 \,|\, T_1) \neq P(T_2) \,\,\implies \,\,P(T_1 \cap T_2) \neq P(T_1)P(T_2) \,\, \implies \,\, T_1 \text{ and } T_2 \text{ are not independent }$$</p>

<hr>

<p>The original problem, which asks for the value of $P(D\,|\,T_1, T_2)$, can be solved using <a href=""https://en.wikipedia.org/wiki/Bayes%27_theorem"" rel=""nofollow noreferrer"">Bayes' theorem</a>, and no assumption about the statistical independence between $T_1$ and $T_2$ is necessary.</p>
"
"2399407","2399420","<p>We have that
\begin{align*}\int_0^\infty\frac{dx}{1+x^2\sin^2x}
&amp;=\sum_{n=0}^{\infty}
\int_0^{\pi}\frac{dx}{1+(x+n\pi)^2\sin^2(x+n\pi)}
\\&amp;\geq  \sum_{n=0}^{\infty}
\int_0^{\pi/2}\frac{2dx}{1+\pi^2(1+n)^2\sin^2(x)}\\
&amp;=\sum_{n=0}^{\infty}\frac{\pi}{\sqrt{1+\pi^2(n+1)^2}}\\
&amp;\geq \frac{\pi}{\sqrt{1+\pi^2}}\sum_{n=0}^{\infty}\frac{1}{n+1}=+\infty
\end{align*}
where we used the fact that
$$\int\frac{\ dx}{1+a^2\sin^2(x)}=\frac{\arctan(\sqrt{1+a^2}\tan(x))}{\sqrt{1+a^2}}+C.$$
P.S. We can also use the inequality $\sin^2(x)\leq x^2$ and evaluate 
$$\int_0^{\pi/2}\frac{2dx}{1+\pi^2(1+n)^2x^2}=\frac{2\arctan\left((n+1)\pi^2/2\right)}{\pi(n+1)}.$$</p>
"
"2399409","2399411","<p>By AM-GM inequality, $$\dfrac{\dfrac a2+\dfrac a2+\dfrac b3+\dfrac b3+\dfrac b3}5\ge\sqrt[5]{\dfrac a2\cdot\dfrac a2\cdot\dfrac b3\cdot\dfrac b3\cdot\dfrac b3}$$</p>

<p>Can you use the same idea for $$x+x+x+y+y?$$</p>
"
"2399418","2399444","<p>Something of which that proposition is reminiscent is true:
$$
\Pr(A\ \&amp;\ B \mid C) = \Pr(A\mid B\ \&amp;\ C)\Pr(B\mid C). \tag 1
$$</p>

<p>Now suppose $A\subseteq B\subseteq C.$ In <b>that</b> case we have
$$
\Pr(A \mid C) = \Pr(A\mid B)\Pr(B\mid C) \tag 2
$$
since in the case $A\subseteq B\subseteq C,$ the proposition $(2)$ follows from $(1).$</p>
"
"2399419","2399441","<p>Note that $e^{n\log\left(1-\frac{x^2}{n}\right)}=\left(1-\frac{x^2}{n}\right)^n$.  Then, letting $a_n=\left(1-\frac{x^2}{n}\right)^n$, we have for $n&gt;x^2$</p>

<p>$$\begin{align}
\frac{a_{n+1}}{a_n}&amp;=\frac{\left(1-\frac{x^2}{n+1}\right)^{n+1}}{\left(1-\frac{x^2}{n}\right)^n}\\\\
&amp;=\left(1-\frac{x^2}{n}\right)\left(1+\frac{x^2}{(n+1)(n-x^2)}\right)^{n+1}\tag 1\\\\
&amp;\ge \left(1-\frac{x^2}{n}\right)\left(1+\frac{x^2}{n-x^2}\right)\tag2\\\\
&amp;=1
\end{align}$$</p>

<p>where in going from $(1)$ to $(2)$ we used Bernoulli's Inequality.</p>

<p>Inasmuch as $\frac{a_{n+1}}{a_n}\ge1$, $a_n$ is increasing.  And we are done!</p>
"
"2399435","2399501","<p>Suppose pick the point $[1 : a] \in \mathbb P^1$. The fibre above this point is the variety:
$$ (y = ax, \ \  \ x^4 + y^4 + z^4 + w^4 = 0) \subset \mathbb P^3$$
But of course, this is the same variety as this:
$$ ((1 + a^4) x^4 + z^4 + w^4 = 0) \subset \mathbb P^2,$$
where we have dropped the $y$ coordinate, and the $\mathbb P^2$ is parametrised by the $x, z$ and $w$ coordinates.</p>

<p>For generic choices of $a$, this variety is smooth curve in $\mathbb P^2$ of degree $d = 4$. (The smoothness of this variety for generic choices of $a$ is a consequence of <a href=""https://en.wikipedia.org/wiki/Theorem_of_Bertini"" rel=""nofollow noreferrer"">Bertini's theorem</a>.) By the <a href=""https://en.wikipedia.org/wiki/Genus%E2%80%93degree_formula"" rel=""nofollow noreferrer"">genus-degree formula</a>, this plane quartic has genus
$$ g = \frac 1 2 (d - 1)(d - 2) = 3.$$</p>
"
"2399468","2399527","<p>The first three steps of your proof (as an outline) are fine but $4$ seems to fall apart.  You say ""so in order to prove the uniqueness of such a separating point"" but you haven't proven an existence of such a point.  You haven't proven that either $u$ or $l$ are such a point.  By proving $l = u$ that will prove that $l = u = r$ would be such a point but not that it is unique.  You can't assume wlog that $l &lt; u$.  You can prove that if $l &gt; u$ leads to a contradiction (as $u &lt; x &lt; l \implies x \not \in A \cup B$) and that $u &gt; 1$ leads to a contradiction ($l &lt; x &lt;u \implies \exists a\in A; b\in B: b &lt; x &lt; a$).  That would prove $l=u$ which in turn proves $(\infty, r=u)\subset A, (r=l, \infty)\subset B$ is such a dividing $r$.  But it doesn't prove it is unique.</p>

<p>Maybe what you want to say first is that i) if there exists such an $r$ so that $(-\infty, r) \subset A$ and $(r, \infty)\subset B$ that $r$ must be equal to both the least upper bound of $A$ and then greatest lower bound of $B$.  Thus if such a point exists it must be unique. ii) Likewise if $\sup A = \inf B$ then $r=\sup A = \inf B$ would be such a point. iii) And then prove $\sup A = \inf B$.</p>
"
"2399481","2399493","<p>Over any field, if the matrix is invertible/has non-zero determinant than there is a unique solution for any vector on the right-hand side.</p>

<p>However, if the matrix in non-invertible/has zero-determinant there are two possibilities for a given right-hand side. </p>

<p>Either there is no solution or there are multiple solutions (that form an affine subspace). </p>

<p>In the case of $2$ and $3$ you land in the former case, in the case of $7$ you land in the latter.  </p>

<p>As you ask in a comment how to solve it for $7$, put the extended matrix in echelon form, and substitute. It is the same as you would do for an under-determined system over the reals.</p>

<p>$\begin{bmatrix}8&amp;3\\2&amp;6\end{bmatrix}$ and $B=\begin{bmatrix}3\\-1\end{bmatrix}$</p>

<p>reducing modulo seven gives</p>

<p>$\begin{bmatrix}1&amp;3\\2&amp;6\end{bmatrix}$ and $\begin{bmatrix}3\\-1\end{bmatrix}$</p>

<p>Now subtract twice the first from the second line to get: </p>

<p>$\begin{bmatrix}1&amp;3\\0&amp;0\end{bmatrix}$ and $\begin{bmatrix}3\\0\end{bmatrix}$</p>

<p>Crucially note that the second coordinate of the vector is $0$ as $-1 -2 \times 3 =0$. Now, introduce a parameter for one of the two variables and express the second using the one remaining equation. </p>

<p>To answer the additional question, one cannot decide for which prime there will be a solution based on the determinant of $A$ or even based on $A$ alone. One really has to check for each prime what happens for the given right-hand side. In that sense it is as in the reals, it really depends on the combination of matrix <em>and</em> right-hand side.</p>

<p>Yet, if $B$ is the zero-vector, one will always have multiple solutions. </p>

<p>One could say informally, and also make rigorous, that for small primes it is more likely that there are multiple solution but this is rather besides the point and a distraction in this context.  </p>
"
"2399486","2399510","<p>$\Bbb Z_p$ is not free over $\Bbb Z$. Observe that $(p+1)\Bbb Z_p=\Bbb Z_p$. For a free Abelian group $G$, $mG=G$ for some integer $m&gt;1$
implies that $G=\{0\}$.</p>
"
"2399487","2399497","<p>Yes, it is correct. A few remarks:</p>

<ul>
<li>The expression âFundamental Theorem of Linear Mapsâ is not a standard one. The usual expression is ârankânullity theoremâ.</li>
<li>In order to prove that the map $T$ that you defined is indeed injective, it is easier to observe that$$\begin{align*}T\left(\sum_{j=1}^ma_jv_j\right)=0&amp;\iff \sum_{j=1}^ma_jw_j=0\\&amp;\iff\bigl(\forall j\in\{1,2,\ldots,m\}\bigr):a_j=0,\end{align*}$$since $\{w_1,w_2,\ldots,w_m\}$ is a subset of a basis.</li>
</ul>
"
"2399512","2399523","<p>You should prove first that you have discontinuity at a point $y$ if and only if the set $\{x\in [0,1]:\, f(x)=y\}$ has positive measure. So if $f$ is strictly increasing, then that set will be a singleton and so have measure zero.</p>
"
"2399513","2399514","<p>What do you think about $$f(x,y)=(e^x,e^y) \ \ ?$$</p>
"
"2399533","2399538","<p>We would need to reach a contradiction for each premise. However, in your proof, there was no need to split between cases because
$$
x\in B\setminus A \subseteq A\ \triangle\ B\subseteq A
$$
implies $x\in A$.</p>
"
"2399550","2399565","<p>As you said, they are element s.t. $$x^3-x\mathcal G=P(x)(x^3-x)\iff\mathcal G=P(x)(1-x^2)+x^2=Q(x)(1-x^2)+1,$$
thus the class of $\mathcal F=1$ are elements that has remainder $1$ in division by $1-x^2$.</p>
"
"2399551","2399557","<p>Your approach is somewhat unclear, as $\lim\limits_{(x,y)\to(0,1)}y/x$ does not exist.</p>

<p>A simple approach would be to consider different paths that go to $(0,1)$. Mainly, I recommend trying</p>

<p>$y=x+1$</p>

<p>As $x\to0^+$ and $x\to0^-$, the limits come out to be $\pi/2$ and $-\pi/2$ respectively.</p>
"
"2399562","2399567","<p>If $f(x)$ is a positive valued function, then $g(x)=\frac{1}{1+f(x)}$ will always take on values between $0$ and $1$. It kind of flips things around, because when $f$ gets big, $g$ gets small, and vice versa. The ""$1+$"" part of the denominator keeps the fraction from blowing up when $f$ gets very small.</p>

<p>In other words, if you want a function that is close to $0$ when $f$ is very large, and close to $1$ when $f$ is very small, then this one fits the bill!</p>

<p>You might want a function that takes values between $0$ and $1$ for various reasons, e.g., if you want to interpret an answer as a proportion, or as a probability. You can also place the range between $0$ and any other positive value simply by replacing the $1$ on top with whatever you want your maximum value to be.</p>
"
"2399572","2399593","<p>The genus is a topological invariant, in particular a surface of genus $g \geq 1$ can't be homeomorphic to the Riemann sphere. Moreover, any subset of the Riemann sphere has boundary unlike a surface of genus $g$ so such an homeomorphism is also not possible. </p>

<p>On the other hand, any surface $S$ of genus $g \geq 2$ can be obtained as a quotient of the unit disk $D \subset \Bbb C$. A surface of genus 1 is more or less by definition $\Bbb C/ \Lambda$ where $\Lambda$ is a lattice.</p>
"
"2399582","2399670","<p>As you've discovered, the chain rule can be difficult to use with matrix functions. Instead let's stick with differentials, and change the independent variable as necessary, until we obtain an expression in terms of $d\lambda.\,$ Then in the final step, we can recover the gradient.</p>

<p>For convenience, define some new variable
$$\eqalign{
S &amp;= S^T = \Sigma =J^TCJ \cr
E &amp;= E^T = ee^T \cr
M &amp;= M^T = S^{-1} -S^{-1}ES^{-1} \cr
G &amp;= \frac{\partial J}{\partial\lambda} \cr
}$$
Let's also use a colon to denote the inner/Frobenius product, which is a convenient notation for the matrix trace, $$A:B={\rm tr}(A^TB)$$</p>

<p><br>Now we can write the objective function in terms of these definitions
$$\eqalign{
2L &amp;= \log\det S + E:S^{-1} \cr 
\cr
2\,dL &amp;= d\log\det S + E:dS^{-1} \cr 
 &amp;= d{\rm tr}\log S - E:S^{-1}\,dS\,S^{-1} \cr
 &amp;= (S^{-1} - S^{-1}ES^{-1}):dS \cr 
 &amp;= M:dS \cr 
 &amp;= M:(dJ^T\,CJ + J^TC\,dJ) \cr 
 &amp;= M(CJ)^T:dJ^T + C^TJM:dJ \cr 
 &amp;= (CJM + C^TJM):dJ \cr 
 &amp;= (C+C^T)JM:G\,d\lambda \cr
\cr
\frac{\partial L}{\partial\lambda}
 &amp;= \frac{1}{2}(C+C^T)JM:G \cr 
 &amp;= \frac{1}{2}(C+C^T)\,J\,(\Sigma^{-1}-\Sigma^{-1}ee^T\Sigma^{-1}):\frac{\partial J}{\partial\lambda} \cr
\cr 
}$$
I think that last expression has cast everything back in terms of your original variables. </p>

<p>Note that the rules for rearranging the Frobenius product follow from the properties of the trace. Here is a quick list
$$\eqalign{
A:BC
 &amp;= B^TA:C \cr 
 &amp;= AC^T:B \cr 
 &amp;= BC:A \cr 
 &amp;= (BC)^T:A^T \cr 
}$$</p>
"
"2399584","2399586","<p>Yes, it is obviously a free module with basis consisting of the $n^2$ ""unit"" matrices.  (That is, the collection of matrices which are $1$ on precisely one position.)</p>
"
"2399600","2399659","<p>""Consider the Zariski cone topology on $X$"" means consider the topology generated by sets of the form $U\subset X$ where $U$ is Zariski-open and $\Bbb G_m$-equivariant. This is a coarser topology than the usual Zariski topology (ie fewer open sets).</p>

<p>A ""cone-scheme"" is a scheme $X$ that comes with a specified action of $\Bbb G_m$. Saying that $X$ is a cone-scheme over $S$ means that there's map of schemes $X\to S$ such that $S$ is the fixed point locus of the $\Bbb G_m$ action, and the fibers of $X\to S$ are $\Bbb G_m$ equivariant.</p>
"
"2399615","2399622","<p>I would just use the factorization
$$f(x)=x^r-1=\prod_{i=0}^{r-1}(x-\gamma^i).$$
It implies that
$$
x^r-y^r=y^rf(x/y)=y^r\prod_{i=0}^{r-1}(x/y-\gamma^i)=\prod_{i=0}^{r-1}(x-y\gamma^i).
$$
You can then cancel the factor $x-y$ corresponding to $i=0$.</p>

<p>The trick is known as <a href=""https://en.wikipedia.org/wiki/Homogeneous_polynomial#Homogenization"" rel=""nofollow noreferrer""><em>homogenization</em></a>. It adds one more variable to a polynomial, and gives, as an end product, a homogeneous polynomial, i.e. a polynomial such that all the terms share the same total degree.</p>
"
"2399643","2399679","<p>I'm going to use the name $x$ for $w_0$, $y$ for $w_1$, $P$ for $L_0$ and $Q$ for $L_1$. Then the $ij$ entry of $Q$ is
\begin{align}
q_{ij} 
&amp;= \sum_s x_{is} (y \cdot P)_{sj}\\
&amp;= \sum_s x_{is} \sum_t y_{st} p_{tj}\\
&amp;= \sum_{s,t} x_{is} y_{st} p_{tj}\\
\end{align}
The derivative of this with respect to $x_{ab}$ involves derivatives of $y_{is}$ with respect to that (all zeroes!), derivatives of $p_{tj}$ with respect to that (all zeroes!), and derivatives of $x_{is}$ with respect to that, which are all zeroes unless $i = a$ and $s = b$. So we get
\begin{align}
\frac{\partial q_{ij} }{\partial x_{ab}} 
&amp;= \frac{\partial}{\partial x_{ab}} \left( \sum_{s,t} x_{is} y_{st} p_{tj}\right)\\
&amp;= \begin{cases}
0 &amp; i \ne a \\
\sum_{t}y_{bt}p_{tb} &amp; i = a 
\end{cases}
\end{align}</p>

<p>If you apply this formula to every value $i,j$ that constitutes a legal index of $L_1$, and every $a,b$ that constitutes a legal index of $w_0$, you'll get your answer. </p>

<p>By the way, your statement that ""The right derivative with respect to some matrix must yield a matrix with the same shape as that varying matrix."" doesn't seem entirely correct to me, at least not without some very clear description of what you think a derivative is. If you have a function from $\Bbb R$ to $M(n, k)$ (the set of $n \times k$ matrices, say $ t \mapsto A(t)$, then the derivative with respect to the single variable $t$ is a matrix with the same shape as $A(t)$.  If you have a function $(x, y) \mapsto B(x, y)$ from two variables to $M(n, k)$, then the derivative with respect to each is an $n \times k$ matrix, so the derivative with respect to <em>both</em> $x$ and $y$ should have at least two times $nk$ components. </p>
"
"2399654","2399685","<p>To sum it up : $M(t)$ exists whenever $t\leq 0$ because $Y$ is almost surely positive. There is, however, no known closed form for $M(t)$.</p>

<p>Source : <a href=""https://en.wikipedia.org/wiki/Log-normal_distribution#Characteristic_function_and_moment_generating_function"" rel=""nofollow noreferrer"">the Wikipedia page</a>.</p>
"
"2399661","2399671","<p>First off, note that the integrals w.r.t. $y$ and $z$ are quite trivial to evaluate. Then, consider $x\mapsto\pi-x$, since trig functions are symmetric about $\pi/2$.</p>

<p>$$I=\int_0^\pi\frac{x\sin(x)}{1+\cos^2(x)}~\mathrm dx=\int_0^\pi\frac{(\pi-x)\sin(x)}{1+\cos^2(x)}~\mathrm dx$$</p>

<p>Add these together and apply $\cos(x)\mapsto x$.</p>

<p>$$\begin{align}\frac2\pi I&amp;=\int_0^\pi\frac{\sin(x)}{1+\cos^2(x)}~\mathrm dx\\&amp;=\int_{-1}^1\frac1{1+x^2}~\mathrm dx\\&amp;=\arctan(1)-\arctan(-1)\\&amp;=\frac\pi2\end{align}\\\implies I=\frac{\pi^2}4$$</p>
"
"2399667","2399682","<p>It is true that $f(x) = (x^2+1)(x^2+3)$ splits in $\mathbb{Q}(i, \sqrt{3})$ and that this field has automorphism group $V_4$.  It is necessary, however, to check that $\mathbb{Q}(i, \sqrt{3})$ is the <em>smallest</em> field in which this polynomial splits.  If it is, then indeed $\text{Gal}(f) \cong V_4$.</p>

<p>If it is not the smallest field in which $f$ splits, then $f$ will split in one of its subfields: $\mathbb{Q}(i \sqrt{3})$, $\mathbb{Q}(i)$, and $\mathbb{Q}(\sqrt{3})$.  If this is the case, then the Galois group will be one of the subgroups of $V_4$.  One can manually confirm that none of these contain all the roots of $f$.  For example, if it splits in $\mathbb{Q}(i)$, then we should be able to write $i\sqrt{3} = a + bi$ for some $a, b \in \mathbb{Q}$.  This means $-3 = (a + bi)^2 = a^2 + 2abi + b^2 \implies $ $a =0 \text{ or } b=0 \implies -3$ has a square root in $\mathbb{Q}$, a contradiction.</p>

<hr>

<p>As a general rule$^\dagger$, if $p$ and $q$ are irreducible polynomials over a field $F$ with splitting fields $K/F$ and $L/F$ and Galois groups $G_1$ and $G_2$ respectively, then the polynomial $pq$ has Galois group $G_1 \times G_2$ whenever $K \cap L = F$.  We can apply this rule to this problem:  $\mathbb{Q}(i\sqrt{3}) \cap \mathbb{Q}(i) = \mathbb{Q}$.</p>

<hr>

<p>$^\dagger$ Click <strong><a href=""https://math.stackexchange.com/questions/2277300/the-galois-group-of-two-irreducible-polynomials/2277313#2277313"">here</a></strong> for further discussion.</p>
"
"2399692","2399700","<p>These are called <a href=""https://www.mathsisfun.com/sets/functions-piecewise.html"" rel=""nofollow noreferrer"">Piecewise functions</a>. They have different definitions, depending on the input domain.</p>

<p><strong>Note</strong>. In that case though, it doesn't make much sense since it can simply be replaced by $$y=\sqrt{\vert x\vert}$$</p>
"
"2399708","2399715","<p>Note that $P(\tau = s) \leq P(B_s=10)=0$ since the event $\{\tau=s\}$ is contained in the event $\{B_s=10\}$, and since $B_s$ has a continuous distribution.</p>

<p>Note that this only shows that the law of $\tau$ is atomless, which doesn't necessarily imply that a density exists with respect to Lebesgue measure (think Cantor-type or local-time measures). If you want an explicit density for $\tau$ see theorem 2.32 on page 56 of <a href=""https://www.stat.berkeley.edu/~peres/bmbook.pdf"" rel=""nofollow noreferrer"">this book</a>.</p>
"
"2399727","2399735","<p>$\|A\|=\sup_{\|x\|=1}\|A(x)\|$. This implies that for $x\neq 0$, $\|A({x\over{\|x\|}})\leq \|A\|$. We deduce that $\|A(x)\|\leq\|A\|\|x\|$.</p>

<p>This implies that $\|AB\|=Sup_{\|x\|=1}\|A(B(x))\|\leq sup_{\|x\|=1}\|A\|\|B(x)\|\leq sup_{\|x\|=1}\|A\|\|B\|\|x\|=\|A\|\|B\|$.</p>
"
"2399737","2399740","<p>The thing is you can't guarantee it'll hold for the $n$ case. One example for instance are the <a href=""https://en.wikipedia.org/wiki/Borwein_integral"" rel=""nofollow noreferrer"">Borwein integrals</a>, who follow a pattern up to $n=14$ then fail at the next iteration. This is the worry with many conjectures out there like the <a href=""https://en.wikipedia.org/wiki/Collatz_conjecture#Experimental_evidence"" rel=""nofollow noreferrer"">Collatz conjecture</a> that's been checked for $ 5Ã2^{60}$ iterations, but could <em>technically</em> fail at one point, until we prove it always holds (induction doesn't necessarily work for Collatz, but you get my drift.)</p>

<p>Induction doesn't ""right off the bat"" <em>prove</em> it works for all integers, you just show it works for a base case and induction guarantees it'll hold for the next integer -- and as a result, the one after, and one after, etc.</p>
"
"2399759","2399766","<p>Hint: Let the monomorphism map $(z_i)_{i=1}^\infty$ to $(z_1,0,z_2,0,z_3,0,\ldots)$ and let the epimorphism map $(z_i)_{i=1}^\infty$ to $(z_2,z_4,z_6,\ldots)$.</p>
"
"2399790","2399792","<p>Consider $x_1=1$, $x_2=-1$, $a_1=a_2=1$, and $b_1=b_2=\pi$. Then $b_i$ is not a multiple of $a_i$ for $i=1,2$ and
$$
a_1x_1+a_2x_2=b_1x_1+b_2x_2=0.
$$</p>
"
"2399791","2399801","<p>Hint:$$\lim_{n \to \infty}\sqrt[n]{2^n+3^n+5^n}=\\\lim_{n \to \infty}\sqrt[n]{5^n((\frac 25)^n+(\frac 35)^n+1)} \to 5$$You can facotr $max\{|x_i|\}$ in radical ,to proove what you need 
$$||\vec x||_\infty=\lim_{n \to \infty}\sqrt[n]{\sum_{i=1}^m|x_i|^n} $$take $max\{|x_i|\}=M$ so $M \geq \underbrace{|x_i|}_{i=1,2,3,...m}$
now
$$\lim_{n \to \infty}\sqrt[n]{\sum_{i=1}^m|x_i|^n}=\\\lim_{n \to \infty}\sqrt[n]{|x_1|^n+|x_2|^n+...+M^n+...+|X_m|^n}=\\
\lim_{n \to \infty}M\sqrt[n]{(\frac{|x_1|}{M})^n+(\frac{|x_2|}{M})^n+...+(\frac{M}{M})^n+...+(\frac{|x_m|}{M})^n}=\\
M\sqrt[n]{0+0+...+1+0+0+..}=\\M=max\{|x_i|\}$$</p>
"